- en: 7 Steps to Mastering Data Preparation for Machine Learning with Python — 2019
    Edition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2019版《掌握机器学习数据准备的7个步骤》
- en: 原文：[https://www.kdnuggets.com/2019/06/7-steps-mastering-data-preparation-python.html](https://www.kdnuggets.com/2019/06/7-steps-mastering-data-preparation-python.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/06/7-steps-mastering-data-preparation-python.html](https://www.kdnuggets.com/2019/06/7-steps-mastering-data-preparation-python.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: Interested in mastering data preparation with Python?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣掌握Python的数据准备吗？
- en: Data preparation, cleaning, pre-processing, cleansing, wrangling. Whatever term
    you choose, they refer to a roughly related set of pre-modeling data activities
    in the machine learning, data mining, and data science communities.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备、清理、预处理、清洗、整理。无论你选择哪个术语，它们都指的是机器学习、数据挖掘和数据科学社区中的一组大致相关的预建模数据活动。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的3个推荐课程
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Wikipedia defines [data cleansing](https://en.wikipedia.org/wiki/Data_cleansing)
    as:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科定义[数据清洗](https://en.wikipedia.org/wiki/Data_cleansing)为：
- en: '...is the process of detecting and correcting (or removing) corrupt or inaccurate
    records from a record set, table, or database and refers to identifying incomplete,
    incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying,
    or deleting the dirty or coarse data. Data cleansing may be performed interactively
    with data wrangling tools, or as batch processing through scripting.'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …是从记录集、表格或数据库中检测和纠正（或删除）损坏或不准确记录的过程，指的是识别数据中不完整、不正确、不准确或不相关的部分，然后替换、修改或删除这些脏数据或粗糙数据。数据清洗可以通过数据处理工具进行交互式操作，也可以通过脚本进行批处理。
- en: '[Data wrangling](https://en.wikipedia.org/wiki/Data_wrangling), for comparison,
    is defined by Wikipedia as:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[数据整理](https://en.wikipedia.org/wiki/Data_wrangling)的定义与此相比，维基百科中定义为：'
- en: '...the process of manually converting or mapping data from one "raw" form into
    another format that allows for more convenient consumption of the data with the
    help of semi-automated tools. This may include further munging, data visualization,
    data aggregation, training a statistical model, as well as many other potential
    uses. Data munging as a process typically follows a set of general steps which
    begin with extracting the data in a raw form from the data source, "munging" the
    raw data using algorithms (e.g. sorting) or parsing the data into predefined data
    structures, and finally depositing the resulting content into a data sink for
    storage and future use.'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …手动将数据从一种“原始”形式转换或映射到另一种格式，以便更方便地使用这些数据，借助半自动化工具。这可能包括进一步的清理、数据可视化、数据聚合、训练统计模型以及许多其他潜在用途。数据清理作为一个过程通常遵循一组通用步骤，从从数据源提取原始数据开始，使用算法（例如排序）或将数据解析成预定义的数据结构来“清理”原始数据，最后将结果内容存入数据存储区以备将来使用。
- en: '![Figure](../Images/2ebc5d14359a29620a0fe3466edd6938.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/2ebc5d14359a29620a0fe3466edd6938.png)'
- en: Data preparation in both the [KDD Process](http://www2.cs.uregina.ca/~dbd/cs831/notes/kdd/1_kdd.html)
    (left) and the [CRISP-DM model](https://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining)
    (right).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备在[KDD过程](http://www2.cs.uregina.ca/~dbd/cs831/notes/kdd/1_kdd.html)（左）和[CRISP-DM模型](https://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining)（右）中的表现。
- en: I would say that it is "identifying incomplete, incorrect, inaccurate or irrelevant
    parts of the data and then replacing, modifying, or deleting the dirty or coarse
    data" in the context of "mapping data from one 'raw' form into another..." all
    the way up to "training a statistical model" which I like to think of data preparation
    as encompassing, or "everything from data sourcing right up to, but not including,
    model building." That is the vague-yet-oddly-precise definition we'll move forward
    with.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我会说，在“将数据从一种‘原始’形式映射到另一种形式……”的背景下，数据准备是“识别数据中不完整、不正确、不准确或不相关的部分，然后替换、修改或删除这些脏数据或粗糙数据”，以及“训练统计模型”这两部分。我喜欢把数据准备理解为涵盖了“从数据源获取到模型构建前的所有内容”。这就是我们将继续使用的模糊但奇怪准确的定义。
- en: This article will [update a previous version from 2017](/2017/06/7-steps-mastering-data-preparation-python.html),
    in order to freshen up some of the materials throughout. I have tried to select
    a quality tutorial or two, along with video when appropriate, as a good representation
    of the particular lesson in each step.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章将[更新 2017 年的旧版本](/2017/06/7-steps-mastering-data-preparation-python.html)，以便更新一些内容。我尝试选择一两个优质的教程，并在适当的情况下附上视频，作为每个步骤中特定课程的良好代表。
- en: Keep in mind that the article covers one particular set of data preparation
    techniques, and additional, or completely different, techniques may be used in
    a given circumstance, based on requirements. You should find that the prescription
    held herein is one which is both orthodox and general in approach.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，文章涵盖了一组特定的数据准备技术，在特定情况下可能会使用额外或完全不同的技术，具体取决于要求。你会发现这里的处方既是正统的，又是通用的。
- en: Grab a snack and sit back, as we learn to master data preparation with Python.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 拿上小吃，坐下来吧，我们将学习如何用 Python 掌握数据准备。
- en: 'Step 1: Preparing for the Preparation'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第一步：准备准备工作
- en: 'First, let''s stress what everyone else has already told you: it could be argued
    that this data preparation phase is not a preliminary step prior to a machine
    learning task, but actually an integral component (or even a majority) of what
    a typical machine learning task would encompass. For our purposes, however, we
    will separate the data preparation from the modeling as its own regimen.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们强调一下其他人已经告诉过你的：可以认为数据准备阶段并不是机器学习任务之前的初步步骤，而是一个典型机器学习任务的组成部分（甚至是主要部分）。然而，为了我们的目的，我们将数据准备与建模分开，作为一个独立的过程。
- en: As Python is the ecosystem in which we will be immersed, the following resources
    are a good jumping off point to ensure appropriate familiarity.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Python 是我们将要深入的生态系统，以下资源是确保适当熟悉的良好起点。
- en: '**[10 Minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)**'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[10 分钟入门 Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)**'
- en: '**[Matplotlib Beginner''s Guide](https://matplotlib.org/users/beginner.html)**'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[Matplotlib 初学者指南](https://matplotlib.org/users/beginner.html)**'
- en: '**[Official seaborn tutorial](https://seaborn.pydata.org/tutorial.html)**'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[官方 Seaborn 教程](https://seaborn.pydata.org/tutorial.html)**'
- en: Data preparation can be seen in the CRISP-DM model (though it can be reasonably
    argued that "data understanding" falls within our definition as well). We can
    also equate our data preparation with the framework of the KDD Process — specifically
    the first 3 major steps — which are **selection**, **preprocessing**, and **transformation**.
    We can break these down into finer granularity, but at a macro level, these steps
    of the KDD Process encompass what data wrangling is.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备可以在 CRISP-DM 模型中看到（尽管可以合理地认为“数据理解”也包含在我们的定义中）。我们还可以将数据准备等同于 KDD 过程的框架——特别是前三个主要步骤——即**选择**、**预处理**和**转换**。我们可以将这些步骤细化，但在宏观层面上，KDD
    过程的这些步骤涵盖了数据处理的内容。
- en: 'While readers should be able to follow this guide with few additional resources,
    for those interested in a more holistic take on Pandas (likely the most important
    data preparation library in the Python ecosystem), helpful information can be
    found in the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然读者应该可以在几乎不需要额外资源的情况下跟随本指南，但对于那些对 Pandas（可能是 Python 生态系统中最重要的数据准备库）有更全面兴趣的人，以下信息可能会有所帮助：
- en: '**[Intro to pandas data structures](http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/)**,
    by Greg Reda'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[Pandas 数据结构简介](http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/)**，作者
    Greg Reda'
- en: '**[Modern Pandas (in 7 parts)](http://tomaugspurger.github.io/modern-1-intro.html)**,
    by Tom Augspurger'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[现代 Pandas（7 部分）](http://tomaugspurger.github.io/modern-1-intro.html)**，作者
    Tom Augspurger'
- en: 'Finally, for some feedback on the data preparation process from 3 insiders
    — Sebastian Raschka, Clare Bernard, and Joe Boutros — read this interview on **[Data
    Preparation Tips, Tricks, and Tools: An Interview with the Insiders](/2016/10/data-preparation-tips-tricks-tools.html)**
    before moving on.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在继续之前，阅读关于数据准备过程的三位内部人士——Sebastian Raschka、Clare Bernard和Joe Boutros——的采访，**[数据准备技巧、窍门和工具：与内部人士的访谈](/2016/10/data-preparation-tips-tricks-tools.html)**。
- en: 'Step 2: Exploratory Data Analysis'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第二步：探索性数据分析
- en: '[Exploratory data analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis)
    (EDA) is an integral aspect of any greater data analysis, data science, or machine
    learning project. Understanding data before working with it isn''t just a pretty
    good idea, it is a priority if you plan on accomplishing anything of consequence.
    [Andrew Andrade](https://datascienceguide.github.io/exploratory-data-analysis)
    concisely describes EDA as follows.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[探索性数据分析](https://en.wikipedia.org/wiki/Exploratory_data_analysis)（EDA）是任何大型数据分析、数据科学或机器学习项目的重要组成部分。在处理数据之前理解数据不仅是一个很好的主意，如果你计划完成任何有意义的事情，它更是一个优先事项。[Andrew
    Andrade](https://datascienceguide.github.io/exploratory-data-analysis) 简明扼要地描述了EDA。'
- en: The purpose of EDA is to use summary statistics and visualizations to better
    understand data, and find clues about the tendencies of the data, its quality
    and to formulate assumptions and the hypothesis of our analysis.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: EDA的目的是使用摘要统计和可视化来更好地理解数据，找出数据的趋势、质量，并制定假设和分析的假设。
- en: The basic gist is that we need to know the makeup of our data before we can
    effectively select predictive algorithms or map out the remaining steps of our
    data preparation. Throwing our dataset at the hottest algorithm and hoping for
    the best is not a strategy.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基本要点是，我们需要了解数据的组成，才能有效地选择预测算法或规划数据准备的剩余步骤。随便将数据集丢给最新算法并希望获得最佳结果并不是一种策略。
- en: To gain some intuition, watch this video by Prof. Patrick Meyer of the University
    of Virginia which provides an overview of EDA.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得一些直观的了解，请观看弗吉尼亚大学Patrick Meyer教授提供的关于EDA的概述视频。
- en: Then read Andrade's article on **[Exploratory data analysis](https://datascienceguide.github.io/exploratory-data-analysis)**,
    which provides additional details on how to go about EDA, and what its practical
    benefits are.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后阅读**[探索性数据分析](https://datascienceguide.github.io/exploratory-data-analysis)**的文章，它提供了关于如何进行EDA的额外细节，以及其实际好处。
- en: For a Python based approach tutorial on EDA, check out the article **[Exploratory
    Data Analysis (EDA) and Data Visualization with Python](https://kite.com/blog/python/data-analysis-visualization-python)**
    by Vigneshwer Dhinakaran, which actually goes a bit beyond traditional EDA in
    my view, and will introduce you to some of the additional concepts covered later
    in this article.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 关于EDA的基于Python的方法教程，查看Vigneshwer Dhinakaran的文章**[使用Python进行探索性数据分析（EDA）和数据可视化](https://kite.com/blog/python/data-analysis-visualization-python)**，在我看来，这实际上超越了传统的EDA，并将向你介绍一些在本文后面介绍的额外概念。
- en: A library which dramatically shortens the code you need to write to perform
    EDA is **[Pandas Profiling](https://github.com/pandas-profiling/pandas-profiling)**,
    which creates HTML reports from Pandas DataFrames.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个显著缩短进行EDA所需编写代码的库是**[Pandas Profiling](https://github.com/pandas-profiling/pandas-profiling)**，它从Pandas
    DataFrames创建HTML报告。
- en: Generates profile reports from a pandas `DataFrame`. The pandas `df.describe()`
    function is great but a little basic for serious exploratory data analysis. `pandas_profiling`
    extends the pandas DataFrame with `df.profile_report()` for quick data analysis.
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从pandas `DataFrame`生成概要报告。pandas的`df.describe()`函数非常好，但对于严肃的探索性数据分析来说有些基础。`pandas_profiling`通过`df.profile_report()`扩展了pandas
    DataFrame，用于快速数据分析。
- en: 'You can run Pandas Profiling interactively in Jupyter notebooks with a single
    line of code:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过一行代码在Jupyter notebooks中交互式地运行Pandas Profiling：
- en: '`df.profile_report()`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`df.profile_report()`'
- en: Read the project's GitHub Readme for more information, and give it a try for
    yourself.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读项目的GitHub Readme获取更多信息，并自己尝试一下。
- en: 'Step 3: Missing Values'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第三步：缺失值
- en: There are many strategies for dealing with missing data, none of which are applicable
    universally. Some people will say "never use instances which include empty values."
    Others will argue "never use an attribute's mean value to replace missing values."
    Conversely, you may hear more complex methods endorsed wholesale, such as "only
    first clustering a dataset into the number of known classes and then using intra-cluster
    regression to calculate missing values is valid."
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失数据的方法有很多种，没有一种是普遍适用的。有人会说“绝不要使用包含空值的实例。”也有人会争论“绝不要用属性的均值来替代缺失值。”相反，你可能会听到更多复杂的方法被全盘接受，例如“仅在将数据集首先按已知类别进行聚类，然后使用聚类内部回归来计算缺失值是有效的。”
- en: Listen to none of this. "Never" and "only" and other inflexible assertions hold
    no value in the nuanced world of data finessing; different types of data and processes
    suggest different best practices for dealing with missing values. However, since
    this type of knowledge is both experience and domain based, we will focus on the
    more basic strategies which can be employed.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 不要听信这些说法。“绝不要”和“仅仅”以及其他不灵活的主张在数据精细化的复杂世界中没有价值；不同类型的数据和过程建议处理缺失值的最佳实践不同。然而，由于这种知识既基于经验也基于领域，我们将重点关注可以采用的更基本的策略。
- en: 'Some commonly used methods for dealing with missing values include:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常用的处理缺失值的方法包括：
- en: dropping instances with missing values
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丢弃缺失值的实例
- en: dropping attributes with missing values
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丢弃缺失值的属性
- en: imputing the attribute { mean | median | mode } for all missing values
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用属性的{均值 | 中位数 | 众数}填补所有缺失值
- en: imputing the attribute missing values via linear regression
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过线性回归填补属性的缺失值
- en: 'Combination strategies may also be employed: drop any instances with more than
    2 missing values and use the mean attribute value imputation those which remain.
    Clearly the type of modeling methods being employed will have an effect on your
    decision — for example, decision trees are not amenable to missing values. Additionally,
    you could technically entertain any statistical method you could think of for
    determining missing values from the dataset, but the listed approaches are tried,
    tested, and commonly used.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用组合策略：丢弃任何缺失值超过2个的实例，并用剩余实例的均值填补缺失值。显然，所采用的建模方法会对你的决策产生影响——例如，决策树不适合缺失值。此外，你可以从技术上考虑任何你能想到的统计方法来确定数据集中的缺失值，但列出的这些方法已经经过尝试、测试，并且被广泛使用。
- en: Since we are focusing on the Python ecosystem, from the Pandas user guide you
    can read more about **[Working with missing data](http://pandas.pydata.org/pandas-docs/stable/missing_data.html)**,
    as well as reference the API documentation on the **[Pandas `DataFrame` object's
    `fillna()` function](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html)**.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们关注的是Python生态系统，你可以通过Pandas用户指南了解更多关于**[处理缺失数据](http://pandas.pydata.org/pandas-docs/stable/missing_data.html)**的信息，并参考**[Pandas
    `DataFrame` 对象的 `fillna()` 函数](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html)**的API文档。
- en: 'There are a lot of ways to accomplish filling missing values in a Pandas DataFrame.
    Here are a few basic examples:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在Pandas DataFrame中填补缺失值有很多方法。这里有一些基本示例：
- en: You can also watch this video from codebasics on handling missing values with
    Pandas.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以观看codebasics关于使用Pandas处理缺失值的视频。
- en: 'Step 4: Outliers'
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤4：异常值
- en: This is not a tutorial on drafting a strategy to deal with outliers in your
    data when modeling; there are times when including outliers in modeling is appropriate,
    and there are times when they are not (regardless of what anyone tries to tell
    you). This is situation-dependent, and no one can make sweeping assertions as
    to whether your situation belongs in column A or column B.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个关于在建模时制定处理异常值策略的教程；在建模中包含异常值在某些情况下是合适的，在另一些情况下则不是（无论别人怎么告诉你）。这取决于具体情况，没有人可以对你的情况是否属于A列或B列做出全面的断言。
- en: 'Outliers can be the result of poor data collection, or they can be genuinely
    good, anomalous data. These are 2 different scenarios, and must be approached
    differently, and so no "one size fits all" advice is applicable here, similar
    to that of dealing with missing values. A particularly good point of insights
    from the Analysis Factor article from above is as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值可能是由于数据收集不当造成的，也可能是实际上具有真实意义的异常数据。这是两种不同的情况，必须采取不同的方法，因此没有“一刀切”的建议适用于这里，这类似于处理缺失值的方法。来自上述分析因素文章的一个特别有价值的见解如下：
- en: One option is to try a transformation. Square root and log transformations both
    pull in high numbers. This can make assumptions work better if the outlier is
    a dependent variable and can reduce the impact of a single point if the outlier
    is an independent variable.
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个选项是尝试转换。平方根和对数转换都可以收敛高值。如果离群值是因变量，这可以使假设更好地工作；如果离群值是自变量，这可以减少单个点的影响。
- en: 'Read this discussion, **[Outliers: To Drop or Not to Drop](http://www.theanalysisfactor.com/outliers-to-drop-or-not-to-drop/)**
    on The Analysis Factor, and the discussion **[Is it OK to remove outliers from
    data?](https://stats.stackexchange.com/questions/200534/is-it-ok-to-remove-outliers-from-data/200923)**
    on Stack Exchange, for further insight into this issue.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读这个讨论，**[离群值：删除还是不删除](http://www.theanalysisfactor.com/outliers-to-drop-or-not-to-drop/)**，以及在Stack
    Exchange上的讨论**[从数据中删除离群值可以吗？](https://stats.stackexchange.com/questions/200534/is-it-ok-to-remove-outliers-from-data/200923)**，以进一步了解这个问题。
- en: You can have a look at **[Removing Outliers Using Standard Deviation with Python](https://www.kdnuggets.com/2017/02/removing-outliers-standard-deviation-python.html)**
    as a simple example of removing outliers with Python. Then read this Stack Overflow
    discussion, **[Remove Outliers in Pandas DataFrame using Percentiles](https://stackoverflow.com/questions/35827863/remove-outliers-in-pandas-dataframe-using-percentiles)**.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看**[使用Python通过标准差去除离群值](https://www.kdnuggets.com/2017/02/removing-outliers-standard-deviation-python.html)**，作为使用Python去除离群值的简单示例。然后阅读这个Stack
    Overflow讨论，**[在Pandas DataFrame中使用百分位数去除离群值](https://stackoverflow.com/questions/35827863/remove-outliers-in-pandas-dataframe-using-percentiles)**。
- en: In the end, the decision as to whether or not to remove outliers will be task-dependent,
    and the reasoning and decision will be much more of a concern than the technical
    approach to doing so.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，是否去除离群值将取决于任务本身，推理和决策比技术方法更为重要。
- en: 'Step 5: Imbalanced Data'
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第5步：不平衡数据
- en: 'So, what if your otherwise robust dataset is made up of 2 classes: one which
    includes 95 percent of the instances, and the other which includes a mere 5 percent?
    Or worse, 99.8 vs 0.2 percent?'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果你原本健壮的数据集由两个类别组成：一个包含95%的实例，另一个仅包含5%？或者更糟，99.8%对0.2%？
- en: '![Recognizing and dealing with imbalance is important](../Images/c9cbb484ef13d59bff657b921d1b6624.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![识别和处理不平衡很重要](../Images/c9cbb484ef13d59bff657b921d1b6624.png)'
- en: If so, your dataset is imbalanced, at least as far as the classes are concerned.
    This can be problematic, in ways which I'm sure do not need to be pointed out.
    But no need to to toss the data to the side yet; there are, of course, strategies
    for dealing with this.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是这样，你的数据集是不平衡的，至少在类别方面如此。这可能会引发一些问题，这些问题我相信不需要指出。但还不需要立即丢弃数据；当然有策略可以应对这个问题。
- en: 'A good explanation of why we can run into imbalanced data, and why we can do
    so in some domains much more frequently than in others (from 7 Techniques to Handle
    Imbalanced Data, below):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的解释说明了为什么我们会遇到不平衡数据，以及为什么在某些领域这种情况比其他领域更为频繁（来自“处理不平衡数据的7种技术”，如下）：
- en: Data used in these areas often have less than 1% of rare, but “interesting”
    events (e.g. fraudsters using credit cards, user clicking advertisement or corrupted
    server scanning its network). However, most machine learning algorithms do not
    work very well with imbalanced datasets. The following seven techniques can help
    you, to train a classifier to detect the abnormal class.
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这些领域使用的数据通常有不到1%的稀有但“有趣”的事件（例如，信用卡欺诈、用户点击广告或被破坏的服务器扫描其网络）。然而，大多数机器学习算法在处理不平衡数据集时效果不佳。以下七种技术可以帮助你训练分类器以检测异常类别。
- en: Note that, while this may not genuinely be a data preparation task, such a dataset
    characteristic will make itself known early in the data preparation stage (the
    importance of EDA), and the validity of such data can certainly be assessed preliminarily
    during this preparation stage.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然这可能不是真正的数据准备任务，但这样的数据集特征会在数据准备阶段早期显现（EDA的重要性），并且在这一准备阶段可以初步评估数据的有效性。
- en: Tom Fawcett discusses this in his article **[Learning from Imbalanced Classes](/2016/08/learning-from-imbalanced-classes.html)**.
    Read it to get a better idea of the issue.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Tom Fawcett在他的文章**[从不平衡类别中学习](/2016/08/learning-from-imbalanced-classes.html)**中讨论了这个问题。阅读它可以更好地了解这个问题。
- en: Then read this article, **[7 Techniques to Handle Imbalanced Data](/2017/06/7-techniques-handle-imbalanced-data.html)**
    by Ye Wu & Rick Radewagen, which covers techniques for handling class imbalance.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后阅读这篇文章，**[处理不平衡数据的7种技术](/2017/06/7-techniques-handle-imbalanced-data.html)**，作者Ye
    Wu & Rick Radewagen，涵盖了处理类别不平衡的技术。
- en: 'Step 6: Data Transformations'
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第6步：数据转换
- en: 'Wikipedia defines [data transformation](https://en.wikipedia.org/wiki/Data_transformation_(statistics))
    as:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科定义的[数据转换](https://en.wikipedia.org/wiki/Data_transformation_(statistics))是：
- en: In statistics, data transformation is the application of a deterministic mathematical
    function to each point in a data set — that is, each data point zi is replaced
    with the transformed value *y[i]* = *f(z[i])*, where *f* is a function. Transforms
    are usually applied so that the data appear to more closely meet the assumptions
    of a statistical inference procedure that is to be applied, or to improve the
    interpretability or appearance of graphs.
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在统计学中，数据转换是将一个确定性数学函数应用到数据集中的每个点——也就是说，每个数据点zi都被转换值*y[i]* = *f(z[i])*所替代，其中*f*是一个函数。转换通常是为了使数据看起来更符合将要应用的统计推断程序的假设，或者改善图表的解释性或外观。
- en: Transforming data is one of the most important aspects of data preparation,
    requiring more finesse than some others. When missing values manifest themselves
    in data, they are generally easy to find, and can be dealt with by one of the
    common methods outlined above — or by more complex measures gained from insight
    over time in a domain. However, when and if data transformations are required
    is often not as easily identifiable, to say nothing of the type of transformation
    required.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换是数据准备中最重要的方面之一，需要比其他一些方法更多的技巧。当数据中出现缺失值时，它们通常很容易找到，并可以通过上述常见方法之一来处理——或者通过在领域内长期积累的更复杂的措施来处理。然而，何时以及是否需要数据转换通常不那么容易识别，更不用说所需的转换类型。
- en: Let's look at a few specific transformations in order to get a better handle
    on them.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解几个具体的转换，以便更好地掌握它们。
- en: First, this overview of **[Preprocessing data](http://scikit-learn.org/stable/modules/preprocessing.html)**
    from Scikit-learn's documentation gives some rationale for some of the most important
    preprocessing transformations, namely standardization, normalization, binarization,
    and a few others.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，这个关于**[数据预处理](http://scikit-learn.org/stable/modules/preprocessing.html)**的概述来自Scikit-learn文档，给出了一些最重要的预处理转换的理由，即标准化、归一化、二值化以及其他一些。
- en: 'Standardization and normalization are a pair of often employed data transformations
    in machine learning projects. Both are data scaling methods: standardization refers
    to scaling the data to have a mean of 0 and a standard deviation of 1; normalization
    refers to the scaling the data values to fit into a predetermined range, generally
    between 0 and 1\. Read this article by Shay Geller, **[Normalization vs Standardization — Quantitative
    analysis](https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf)**,
    to understand how the transformations work, how to perform them in the Python
    ecosystem, and gain some insight into best practice from the author.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化和归一化是在机器学习项目中经常使用的一对数据转换方法。两者都是数据缩放方法：标准化是指将数据缩放到均值为0，标准差为1；归一化是指将数据值缩放到一个预定的范围，通常在0到1之间。阅读这篇文章，**[归一化与标准化——定量分析](https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf)**，以了解这些转换如何工作，如何在Python生态系统中执行它们，并从作者那里获得一些最佳实践的见解。
- en: One-hot encoding is a method for transforming categorical features to a format
    which will better work for classification and regression. Watch this video on
    one-hot encoding to gain a better understanding of how it does so, and see how
    it can be accomplished with Python tools.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一次性编码是一种将类别特征转换为更适合分类和回归的格式的方法。观看这个关于一次性编码的视频，以更好地理解它是如何实现的，并查看如何使用Python工具完成。
- en: Logarithmic distribution transformation is useful for transforming non-linear
    models into linear models and working with skewed data. Read this Stack Exchange
    discussion, **[When (and why) should you take the log of a distribution (of numbers)?](https://stats.stackexchange.com/questions/18844/when-and-why-should-you-take-the-log-of-a-distribution-of-numbers)**,
    for the intuition. You can also have a look at this short tutorial from Data Science
    Made Simple, **[Log and natural Logarithmic value of a column in pandas python](http://www.datasciencemadesimple.com/log-natural-logarithmic-value-column-pandas-python-2/)**,
    for a quick overview of using Numpy to accomplish the transformation in Python.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对数分布转换对于将非线性模型转换为线性模型和处理偏斜数据非常有用。阅读这个Stack Exchange讨论，**[什么时候（以及为什么）你应该对分布（数字）取对数？](https://stats.stackexchange.com/questions/18844/when-and-why-should-you-take-the-log-of-a-distribution-of-numbers)**，以获得直观理解。你还可以查看来自Data
    Science Made Simple的这个简短教程，**[pandas python中列的对数和自然对数值](http://www.datasciencemadesimple.com/log-natural-logarithmic-value-column-pandas-python-2/)**，快速了解如何使用Numpy在Python中完成转换。
- en: This short tutorial from Ontario Tech University, **[Introduction to Exponential
    and Logarithmic Functions](https://nool.uoit.ca/mathematics/exponential-logarithmic-functions/basics/index.php)**,
    takes a mathematical approach to explaining logarithmic and exponential transformations,
    along with visualizations, and can add to your intuition of what is happening
    to underlying data distributions when these transformations are performed. There
    are 3 pages in the tutorial, with the third having 2 videos which help drive the
    point home.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个来自安大略科技大学的简短教程，**[指数和对数函数简介](https://nool.uoit.ca/mathematics/exponential-logarithmic-functions/basics/index.php)**，采用数学方法解释对数和指数转换，以及可视化，并能帮助你更好地理解在进行这些转换时底层数据分布的变化。教程共有3页，第三页有2个视频，有助于深入理解。
- en: There are numerous additional standard data transformations which are regularly
    employed, depending on the data and your requirements. Experience with data preprocessing
    and preparation should provide intuition on what types of transformations are
    required in which circumstance.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据和需求，还有许多其他标准数据转换方法。通过数据预处理和准备的经验应能提供在不同情况下所需转换类型的直观理解。
- en: 'Step 7: Finishing Touches & Moving Ahead'
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第7步：最后修饰与前进
- en: Alright. Your data is "clean." But what do you do with it?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧。你的数据已经“干净”了。但是你接下来该怎么做？
- en: If you want to go right to feeding your data into a machine learning algorithm
    in order to attempt building a model, you probably need your data in a more appropriate
    representation. In the Python ecosystem, that would generally be a Numpy ndarray
    (or matrix). This Stack Overflow discussion, **[Turning a Pandas Dataframe to
    an array and evaluate Multiple Linear Regression Model](https://stackoverflow.com/questions/28334091/turning-a-pandas-dataframe-to-an-array-and-evaluate-multiple-linear-regression-m)**,
    can give some preliminary ideas on getting there.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想直接将数据输入机器学习算法以尝试建立模型，你可能需要将数据转换为更合适的表示形式。在Python生态系统中，这通常是Numpy ndarray（或矩阵）。这个Stack
    Overflow讨论，**[将Pandas Dataframe转换为数组并评估多元线性回归模型](https://stackoverflow.com/questions/28334091/turning-a-pandas-dataframe-to-an-array-and-evaluate-multiple-linear-regression-m)**，可以为你提供一些初步的思路。
- en: '![Very simple data preparation process](../Images/2b1892e803954b3888cf48f32e0c71b7.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![非常简单的数据准备过程](../Images/2b1892e803954b3888cf48f32e0c71b7.png)'
- en: '*Note that most of our data preparation was performed in a combination of Pandas
    and Numpy in the preceding text; however, Pandas sits atop Numpy, and so learning
    how to manipulate the underlying Numpy matrix directly is a useful skill. [Learn
    a bit more about that here](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html).*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*请注意，我们在前面的文本中大部分数据准备工作是在Pandas和Numpy的组合中完成的；然而，Pandas建立在Numpy之上，因此直接操作底层Numpy矩阵是一个有用的技能。[在这里了解更多](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html)。*'
- en: What if you aren't quite ready to model the data yet, and instead want to store
    your clean Pandas DataFrame for later use? **[Quick HDF5 with Pandas](https://dzone.com/articles/quick-hdf5-pandas)**
    by Giuseppe Vettigli will show you one such way to do so.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有准备好对数据建模，而是想将你干净的Pandas DataFrame存储以备后用？**[使用Pandas快速HDF5](https://dzone.com/articles/quick-hdf5-pandas)**由Giuseppe
    Vettigli展示了一种这样的方式。
- en: Once you have clean data in a proper representation for machine learning in
    Python, why not get right to the machine learning? First you will want to read
    **[7 Steps to Mastering Basic Machine Learning with Python — 2019 Edition](/2019/01/7-steps-mastering-basic-machine-learning-python.html)**
    to gain an introductory understanding of machine learning in the Python ecosystem.
    Follow that up with **[7 Steps to Mastering Intermediate Machine Learning with
    Python — 2019 Edition](/2019/06/7-steps-mastering-intermediate-machine-learning-python.html)**
    to enhance your knowledge (and be on the look out for an "advanced" installment
    as well).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了用于Python中机器学习的干净数据和适当的表示，为什么不直接进行机器学习呢？首先，你需要阅读**[掌握基本机器学习的7个步骤 — 2019版](/2019/01/7-steps-mastering-basic-machine-learning-python.html)**，以获得Python生态系统中机器学习的初步理解。接着，阅读**[掌握中级机器学习的7个步骤
    — 2019版](/2019/06/7-steps-mastering-intermediate-machine-learning-python.html)**以提升你的知识（同时也请留意“高级”版）。
- en: 'For some differing viewpoints on data preparation, have a look at these:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据准备有不同观点的，请查看以下内容：
- en: '**[Tidying Data in Python](/2017/01/tidying-data-python.html)**, by Jean-Nicholas
    Hould'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[在Python中整理数据](/2017/01/tidying-data-python.html)**，作者：Jean-Nicholas Hould'
- en: '**[Doing Data Science: A Kaggle Walkthrough Part 3 – Cleaning Data](/2016/06/doing-data-science-kaggle-walkthrough-data-cleaning.html)**,
    by Brett Romero'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[数据科学实践：Kaggle实战 第3部分 — 数据清洗](/2016/06/doing-data-science-kaggle-walkthrough-data-cleaning.html)**，作者：Brett
    Romero'
- en: '**[Machine Learning Workflows in Python from Scratch Part 1: Data Preparation](/2017/05/machine-learning-workflows-python-scratch-part-1.html)**'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[从头开始学习Python中的机器学习工作流 第1部分：数据准备](/2017/05/machine-learning-workflows-python-scratch-part-1.html)**'
- en: 'Note that this entire discussion is also fully and intentionally skipping any
    mention of feature selection for a specific reason: it deserves far more than
    a simple few sentences in this much more broad discussion. Be on the lookout for
    a similar guide for feature selection.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这整个讨论也完全且有意跳过了特征选择的提及，原因很简单：特征选择在这更广泛的讨论中值得比几句话更多的关注。请留意关于特征选择的类似指南。
- en: '**Related**:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关**：'
- en: '[7 Steps to Mastering Basic Machine Learning with Python — 2019 Edition](/2019/01/7-steps-mastering-basic-machine-learning-python.html)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握基本机器学习的7个步骤 — 2019版](/2019/01/7-steps-mastering-basic-machine-learning-python.html)'
- en: '[7 Steps to Mastering Intermediate Machine Learning with Python — 2019 Edition](/2019/06/7-steps-mastering-intermediate-machine-learning-python.html)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握中级机器学习的7个步骤 — 2019版](/2019/06/7-steps-mastering-intermediate-machine-learning-python.html)'
- en: '[7 Steps to Mastering SQL for Data Science — 2019 Edition](/2019/05/7-steps-mastering-sql-data-science-2019-edition.html)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握SQL以进行数据科学的7个步骤 — 2019版](/2019/05/7-steps-mastering-sql-data-science-2019-edition.html)'
- en: More On This Topic
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为优秀数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个初学者数据科学家应该掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家应该了解的三个R库（即使你使用Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用管道编写干净的Python代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么让Python成为创业公司的理想编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
