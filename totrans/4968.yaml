- en: 'Dask and Pandas and XGBoost: Playing nicely between distributed systems'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/04/dask-pandas-xgboost-playing-nicely-distributed-systems.html](https://www.kdnuggets.com/2017/04/dask-pandas-xgboost-playing-nicely-distributed-systems.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**By Matthew Rocklin, Continuum Analytics.**'
  prefs: []
  type: TYPE_NORMAL
- en: '*This work is supported by [Continuum Analytics](http://continuum.io/) the
    [XDATA Program](https://www.darpa.mil/program/XDATA) and the Data Driven Discovery
    Initiative from the [Moore Foundation](https://www.moore.org/)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Editor''s note:** For an introduction to Dask, consider reading [Introducing
    Dask for Parallel Programming: An Interview with Project Lead Developer](/2016/09/introducing-dask-parallel-programming.html).
    To read more about the most recent release, see [Dask Release 0.14.1](http://matthewrocklin.com/blog/work/2017/03/23/dask-0.14.1).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This post talks about distributing Pandas Dataframes with Dask and then handing
    them over to distributed XGBoost for training.
  prefs: []
  type: TYPE_NORMAL
- en: More generally it discusses the value of launching multiple distributed systems
    in the same shared-memory processes and smoothly handing data back and forth between
    them.
  prefs: []
  type: TYPE_NORMAL
- en: '[Notebook](https://gist.github.com/mrocklin/3696fe2398dc7152c66bf593a674e4d9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Screencast](https://youtu.be/Cc4E-PdDSro)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Github issue](https://github.com/dmlc/xgboost/issues/2032)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Dask](../Images/0b8634648f8ce3dace49e6acf212ca23.png)'
  prefs: []
  type: TYPE_IMG
- en: Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: XGBoost is a well-loved library for a popular class of machine learning algorithms,
    gradient boosted trees. It is used widely in business and is one of the most popular
    solutions in Kaggle competitions. For larger datasets or faster training, XGBoost
    also comes with its own distributed computing system that lets it scale to multiple
    machines on a cluster. Fantastic. Distributed gradient boosted trees are in high
    demand.
  prefs: []
  type: TYPE_NORMAL
- en: 'However before we can use distributed XGBoost we need to do three things:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare and clean our possibly large data, probably with a lot of Pandas wrangling
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up XGBoost master and workers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hand data our cleaned data from a bunch of distributed Pandas dataframes to
    XGBoost workers across our cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This ends up being surprisingly easy. This blogpost gives a quick example using
    Dask.dataframe to do distributed Pandas data wrangling, then using a new [dask-xgboost](https://github.com/dask/dask-xgboost)
    package to setup an XGBoost cluster inside the Dask cluster and perform the handoff.
  prefs: []
  type: TYPE_NORMAL
- en: After this example we’ll talk about the general design and what this means for
    other distributed systems.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have a ten-node cluster with eight cores each (`m4.2xlarges` on EC2)
  prefs: []
  type: TYPE_NORMAL
- en: 'We load the Airlines dataset using dask.dataframe (just a bunch of Pandas dataframes
    spread across a cluster) and do a bit of preprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This loaded a few hundred pandas dataframes from CSV data on S3\. We then had
    to downsample because how we are going to use XGBoost in the future seems to require
    a lot of RAM. I am not an XGBoost expert. Please forgive my ignorance here. At
    the end we have two dataframes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`df`: Data from which we will learn if flights are delayed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`is_delayed`: Whether or not those flights were delayed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data scientists familiar with Pandas will probably be familiar with the code
    above. Dask.dataframe is *very* similar to Pandas, but operates on a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![DF head](../Images/bb4d687a932ec89c5649e823836e6425.png)'
  prefs: []
  type: TYPE_IMG
- en: Categorize and One Hot Encode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: XGBoost doesn’t want to work with text data like destination=”LAX”. Instead
    we create new indicator columns for each of the known airports and carriers. This
    expands our data into many boolean columns. Fortunately Dask.dataframe has convenience
    functions for all of this baked in (thank you Pandas!)
  prefs: []
  type: TYPE_NORMAL
- en: This expands our data out considerably, but makes it easier to train on.
  prefs: []
  type: TYPE_NORMAL
- en: Split and Train
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Great, now we’re ready to split our distributed dataframes
  prefs: []
  type: TYPE_NORMAL
- en: Start up a distributed XGBoost instance, and train on this data
  prefs: []
  type: TYPE_NORMAL
- en: Great, so we were able to train an XGBoost model on this data in about a minute
    using our ten machines. What we get back is just a plain XGBoost Booster object.
  prefs: []
  type: TYPE_NORMAL
- en: We could use this on normal Pandas data locally
  prefs: []
  type: TYPE_NORMAL
- en: Of we can use `dask-xgboost` again to train on our distributed holdout data,
    getting back another Dask series.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can bring these predictions to the local process and use normal Scikit-learn
    operations to evaluate the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e57f05381a23402623b5ec9a7531cee7.png)'
  prefs: []
  type: TYPE_IMG
- en: We might want to play with our parameters above or try different data to improve
    our solution. The point here isn’t that we predicted airline delays well, it was
    that if you are a data scientist who knows Pandas and XGBoost, everything we did
    above seemed *pretty familiar*. There wasn’t a whole lot of new material in the
    example above. We’re using the same tools as before, just at a larger scale.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OK, now that we’ve demonstrated that this works lets talk a bit about what just
    happened and what that means generally for cooperation between distributed services.
  prefs: []
  type: TYPE_NORMAL
- en: What dask-xgboost does
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The [dask-xgboost](https://github.com/dask/dask-xgboost) project is pretty small
    and pretty simple (200 TLOC). Given a Dask cluster of one central scheduler and
    several distributed workers it starts up an XGBoost scheduler in the same process
    running the Dask scheduler and starts up an XGBoost worker within each of the
    Dask workers. They share the same physical processes and memory spaces. Dask was
    built to support this kind of situation, so this is relatively easy.
  prefs: []
  type: TYPE_NORMAL
- en: Then we ask the Dask.dataframe to fully materialize in RAM and we ask where
    all of the constituent Pandas dataframes live. We tell each Dask worker to give
    all of the Pandas dataframes that it has to its local XGBoost worker and then
    just let XGBoost do its thing. Dask doesn’t power XGBoost, it’s just sets it up,
    gives it data, and lets it do it’s work in the background.
  prefs: []
  type: TYPE_NORMAL
- en: People often ask what machine learning capabilities Dask provides, how they
    compare with other distributed machine learning libraries like H2O or Spark’s
    MLLib. For gradient boosted trees the 200-line dask-xgboost package is the answer.
    Dask has no need to make such an algorithm because XGBoost already exists, works
    well and provides Dask users with a fully featured and efficient solution.
  prefs: []
  type: TYPE_NORMAL
- en: Because both Dask and XGBoost can live in the same Python process they can share
    bytes between each other without cost, can monitor each other, etc.. These two
    distributed systems co-exist together in multiple processes in the same way that
    NumPy and Pandas operate together within a single process. Sharing distributed
    processes with multiple systems can be really beneficial if you want to use multiple
    specialized services easily and avoid large monolithic frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to Other distributed systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A while ago I wrote [a similar blogpost](http://matthewrocklin.com/blog/work/2017/02/11/dask-tensorflow)
    about hosting TensorFlow from Dask in exactly the same way that we’ve done here.
    It was similarly easy to setup TensorFlow alongside Dask, feed it data, and let
    TensorFlow do its thing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally speaking this “serve other libraries” approach is how Dask operates
    when possible. We’re only able to cover the breadth of functionality that we do
    today because we lean heavily on the existing open source ecosystem. Dask.arrays
    use Numpy arrays, Dask.dataframes use Pandas, and now the answer to gradient boosted
    trees with Dask is just to make it really really easy to use distributed XGBoost.
    Ta da! We get a fully featured solution that is maintained by other devoted developers,
    and the entire connection process was done over a weekend (see [dmlc/xgboost #2032](https://github.com/dmlc/xgboost/issues/2032)
    for details).'
  prefs: []
  type: TYPE_NORMAL
- en: Since this has come out we’ve had requests to support other distributed systems
    like [Elemental](http://libelemental.org/) and to do general hand-offs to MPI
    computations. If we’re able to start both systems with the same set of processes
    then all of this is pretty doable. Many of the challenges of inter-system collaboration
    go away when you can hand numpy arrays between the workers of one system to the
    workers of the other system within the same processes.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Thanks to [Tianqi Chen](http://homes.cs.washington.edu/~tqchen/) and [Olivier
    Grisel](http://ogrisel.com/) for their help when [building and testing](https://github.com/dmlc/xgboost/issues/2032)
    `dask-xgboost`. Thanks to [Will Warner](https://github.com/electronwill) for his
    help in editing this post.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Matthew Rocklin](http://matthewrocklin.com/)** is an open source software
    developer focusing on efficient computation and parallel computing, primarily
    within the Python ecosystem. He has contributed to many of the PyData libraries
    and today works on Dask a framework for parallel computing. Matthew holds a PhD
    in computer science from the University of Chicago where he focused on numerical
    linear algebra, task scheduling, and computer algebra. Matthew lives in Brooklyn,
    NY and is employed by [**Continuum Analytics**](https://www.continuum.io/).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](http://matthewrocklin.com/blog/work/2017/03/28/dask-xgboost). Reposted
    with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Introducing Dask for Parallel Programming: An Interview with Project Lead
    Developer](/2016/09/introducing-dask-parallel-programming.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[XGBoost: Implementing the Winningest Kaggle Algorithm in Spark and Flink](/2016/03/xgboost-implementing-winningest-kaggle-algorithm-spark-flink.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Beginner’s Guide to Tweet Analytics with Pandas](/2017/03/beginners-guide-tweet-analytics-pandas.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[WTF is the Difference Between GBM and XGBoost?](https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Speed Up XGBoost Model Training](https://www.kdnuggets.com/2021/12/speed-xgboost-model-training.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are the Assumptions of XGBoost?](https://www.kdnuggets.com/2022/08/assumptions-xgboost.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Tuning XGBoost Hyperparameters](https://www.kdnuggets.com/2022/08/tuning-xgboost-hyperparameters.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Leveraging XGBoost for Time-Series Forecasting](https://www.kdnuggets.com/2023/08/leveraging-xgboost-timeseries-forecasting.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets™ News 22:n07, Feb 16: How to Learn Math for Machine…](https://www.kdnuggets.com/2022/n07.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
