- en: 'Transforming AI with LangChain: A Text Data Game Changer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/08/transforming-ai-langchain-text-data-game-changer.html](https://www.kdnuggets.com/2023/08/transforming-ai-langchain-text-data-game-changer.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![XXXXX](../Images/fae8bf3635873f99b3b132732050fed6.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Over the past few years, Large Language Models — or LLMs for friends — **have
    taken the world of artificial intelligence by storm. **
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: With the groundbreaking release of OpenAI’s GPT-3 in 2020, we have witnessed
    a steady surge in the popularity of LLMs, which has only intensified with recent
    advancements in the field.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '**These powerful AI models have opened up new possibilities for natural language
    processing applications**, enabling developers to create more sophisticated, human-like
    interactions.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '***Isn’t it?***'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: However, when dealing with this AI technology it is hard to scale and generate
    reliable algorithms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Amidst this rapidly evolving landscape, **LangChain has emerged as a versatile
    framework designed to help developers harness the full potential of LLMs for a
    wide range of applications. One of the most important use cases is to deal with
    large** **amounts** **of text data.**
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive in and start harnessing the power of LLMs today!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: LangChain can be used in chatbots, question-answering systems, summarization
    tools, and beyond. **However, one of the most useful - and used - applications
    of LangChain is dealing with text.**
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Today’s world is flooded with data. And one of the most notorious types is text
    data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: All websites and apps are being bombed with tons and tons of words every single
    day. No human can process this amount of information…
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '**But can computers?**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: LLM techniques together with LangChain are a great way to reduce the amount
    of text while maintaining the most important parts of the message. This is why
    today we will cover two basic — but really useful — use cases of LangChain to
    deal with text.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '**Summarization:** Express the most important facts about a body of text or
    chat interaction. It can reduce the amount of data while maintaining the most
    important parts.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extraction:** Pull structured data from a body of text or some user query.
    It can detect and extract keywords within the text.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether you’re new to the world of LLMs or looking to take your language generation
    projects to the next level, this guide will provide you with valuable insights
    and hands-on examples to unlock the full potential of LangChain to deal with text.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是对LLM世界感到陌生还是希望将你的语言生成项目提升到一个新水平，本指南将为你提供宝贵的见解和动手示例，帮助你充分发挥LangChain处理文本的潜力。
- en: ⚠️ If you want to have some basic grasp, you can go check ????????
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ⚠️ 如果你想了解一些基础知识，可以查看 ????????
- en: '[**LangChain 101: Build Your Own GPT-Powered Applications — KDnuggets**](/2023/04/langchain-101-build-gptpowered-applications.html)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[**LangChain 101：构建你自己的GPT驱动应用程序 — KDnuggets**](/2023/04/langchain-101-build-gptpowered-applications.html)'
- en: Always remember that for working with OpenAI and GPT models, we need to have
    the OpenAI library installed on our local computer and have an active OpenAI key.
    If you do not know how to do that, you can go check [here](https://medium.com/@rfeers/openai-a-step-by-step-guide-to-getting-your-api-key-gpt-usage-control-artificial-intelligence-2a0917c70f3f).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请始终记住，要使用OpenAI和GPT模型，我们需要在本地计算机上安装OpenAI库，并拥有一个有效的OpenAI密钥。如果你不知道如何操作，可以查看[这里](https://medium.com/@rfeers/openai-a-step-by-step-guide-to-getting-your-api-key-gpt-usage-control-artificial-intelligence-2a0917c70f3f)。
- en: 1\. Summarization
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 总结
- en: ChatGPT together with LangChain **can summarize information quickly and in a
    very reliable way. **
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT结合LangChain**可以快速且非常可靠地总结信息。**
- en: LLM summarization techniques are a great way to reduce the amount of text while
    maintaining the most important parts of the message. This is why LLMs can be the
    best ally to any digital company that needs to process and analyze large volumes
    of  text data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LLM总结技术是一种减少文本量同时保留消息最重要部分的好方法。这就是为什么LLM可以成为任何需要处理和分析大量文本数据的数字公司的最佳盟友。
- en: 'To perform the following examples, the following libraries are required:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下示例所需的库有：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 1.1\. Short text summarization
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1\. 简短文本总结
- en: For summaries of short texts, the method is straightforward, in fact, you don’t
    need to do anything fancy other than simple prompting with instructions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于短文本的总结，这种方法很简单，实际上，你只需要简单地进行提示并附上指令即可。
- en: Which basically means generating a template with an input variable.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上意味着生成一个带有输入变量的模板。
- en: I know you might be wondering… **what is exactly a prompt template?**
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道你可能在想……**什么是提示模板？**
- en: A prompt template refers to a reproducible way to generate a prompt. It contains
    a text string - a template - that can take in a set of parameters from the end
    user and generates a prompt.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 提示模板指的是一种可重复生成提示的方式。它包含一个文本字符串——模板——可以接受来自最终用户的一组参数并生成提示。
- en: 'A prompt template contains:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 提示模板包含：
- en: '**instructions to the language model** - that allow us to standardize some
    steps for our LLM.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对语言模型的指令** - 允许我们标准化一些步骤，以便于我们的LLM。'
- en: '**an input variable** -  that allows us to apply the previous instructions
    to any input text.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入变量** - 允许我们将先前的指令应用于任何输入文本。'
- en: Let’s see this in a simple example. I can standardize a prompt that generates
    a name of a brand that produces a specific product.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在一个简单的例子中看看。我可以标准化一个生成特定产品品牌名称的提示。
- en: '![XXXXX](../Images/1c86fcb34a81da268a6f7bb73f911bcc.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/1c86fcb34a81da268a6f7bb73f911bcc.png)'
- en: Screenshot of my Jupyter Notebook.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我Jupyter Notebook的截图。
- en: As you can observe in the previous example, the magic of LangChain is that we
    can define a standardized prompt with a changing input variable.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在之前的例子中看到的，LangChain的魔力在于我们可以定义一个带有变化的输入变量的标准化提示。
- en: The instructions to generate a name for a brand remain always the same.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成品牌名称的指令始终保持不变。
- en: The product variable works as an input that can be changed.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品变量作为一个可以改变的输入。
- en: This allows us to define versatile prompts that can be used in different scenarios.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够定义多功能的提示，可在不同的场景中使用。
- en: '*So now that we know what a prompt template is… *'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*所以现在我们知道什么是提示模板了……*'
- en: Let’s imagine we want to define a prompt that summarizes any text using super
    easy-to-understand vocabulary. We can define a prompt template with some specific
    instructions and a text variable that changes depending on the input variable
    we define.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设想一下我们想要定义一个总结任何文本的提示，该提示使用超级易懂的词汇。我们可以定义一个包含一些具体指令和一个根据输入变量定义而变化的文本变量的提示模板。
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: So let’s try this prompt template. Using the wikipedia API, I am going to get
    the summary of the USA country and further summarize it in a really easy-to-understand
    tone.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![XXXXX](../Images/796b446babb8aea865223f36c584ae3d.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Screenshot of my Jupyter Notebook.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: So now that we know how to summarize a short text… can I spice this up a bit?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '*Sure we can with… *'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 1.2\. Long text summarization
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When dealing with long texts, the main problem is that we cannot communicate
    them to our AI model directly via prompt, as they contain too many tokens.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '*And now you might be wondering… what is a token?*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Tokens are how the model sees the input — single characters, words, parts of
    words, or segments of text. As you can observe, the definition is not really precise
    and it depends on every model. For instance, OpenAI’s GPT 1000 tokens are approximately
    750 words.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: But the most important thing to learn is that our cost depends on the number
    of tokens and that we cannot send as many tokens as we want in a single prompt. 
    To have a longer text, we will repeat the same example as before but using the
    whole Wikipedia page text.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![XXXXX](../Images/fb4c7a2f319dd269e3149f4a4aafbf76.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: Screenshot of my Jupyter Notebook.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: If we check how long it is… it is around 17K tokens.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Which is quite a lot to be sent directly to our API.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '*So what now?*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll need to split it up. This process is called **chunking** or **splitting**
    your text into smaller pieces. I usually use [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/recursive_text_splitter.html)
    because it’s easy to control but there are a [bunch](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)
    you can try.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: After using it, instead of just having a single piece of text, we get 23 pieces
    which facilitate the work of our GPT model.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Next we need to load up a chain which will make successive calls to the LLM
    for us.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'LangChain provides the Chain interface for such **chained** applications. We
    define a Chain very generically as a sequence of calls to components, which can
    include other chains. The base interface is simple:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you want to learn more about chains, you can go check directly in the [LangChain
    documentation. ](https://python.langchain.com/docs/get_started/introduction.html)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: So if we repeat again the same procedure with the splitted text - called docs
    - the LLM can easily generate a summary of the whole page.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![XXXXX](../Images/dff9ad393005d1e90ee5bf3a9f423920.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: Screenshot of my Jupyter Notebook.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '*Useful right?*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: So now that we know how to summarize text, we can move to the second use case!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Extraction
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Extraction is the process of parsing data from a piece of text. This is commonly
    used with output parsing to *structure* our data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Extracting key data is really useful in order to identify and parse key words
    within a text.  Common use cases are extracting a structured row from a sentence
    to insert into a database or extracting multiple rows from a long document to
    insert into a database.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine we are running a digital e-commerce company and we need to process
    all reviews that are stated on our website.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '**I could go read all of them one by one… which would be crazy. **'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '*Or I can simply EXTRACT the information that I need from each of them and
    analyze all the data. *'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Sounds easy… right?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with a quite simple example. First, we need to import the following
    libraries:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 2.1\. Extracting specific words
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I can try to look for specific words within some text. In this case, I want
    to parse all fruits that are contained within a text.  Again, it is quite straightforward
    as before. We can easily define a prompt giving clear instructions to our LLM
    stating that identifies all fruits contained in a text and gives back a JSON-like
    structure containing such fruits and their corresponding colors.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![XXXXX](../Images/1bab02cad584e793442e4d6652b3961e.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: Screenshot of my Jupyter Notebook.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: And as we can see before, it works perfectly!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: So now… let’s play a little bit more with it. While this worked this time, it’s
    not a long term reliable method for more advanced use cases. And this is where
    a fantastic LangChain concept comes into play…
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Using LangChain’s Response Schema
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LangChain’s response schema will do two main things for us:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '**Generate a prompt with bonafide format instructions.** This is great because
    I don’t need to worry about the prompt engineering side, I’ll leave that up to
    LangChain!'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Read the output from the LLM and turn it into a proper python object for
    me.** Which means, always generate a given structure that is useful and that my
    system can parse.'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**And to do so, I just need to define what response I except from the model. **'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: So let’s imagine I want to determine the products and brands that users are
    stating in their comments. I could easily perform as before with a simple prompt
    - take advantage of LangChain to generate a more reliable method.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: So first I need to define a response_schema where I define every keyword I want
    to parse with a name and a description.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: After defining our parser, we generate the format of our instruction using the
    .get_format_instructions() command from LangChain and define the final prompt
    using the ChatPromptTemplate. And now it is as easy as using this output_parser
    object with any input query I can think of, and it will automatically generate
    an output with my desired keywords.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![XXXXX](../Images/cf56304ad3a4912ba5cb6bbcd3ebaa4c.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: Screenshot of my Jupyter Notebook.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can observe in the example below, with the input of “I run out of Yogurt
    Danone, No-brand Oat Milk and those vegan bugers made by Heura”, the LLM gives
    me the following output:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![XXXXX](../Images/17bdd2865f3f9ccc6c3eca312c694ac3.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: Screenshot of my Jupyter Notebook.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我的Jupyter Notebook截图。
- en: Main Takeaways
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要要点
- en: LangChain is a versatile Python library that helps developers harness the full
    potential of LLMs, especially for dealing with large amounts of text data. It
    excels at two main use cases for dealing with text. LLMs enable developers to
    create more sophisticated and human-like interactions in natural language processing
    applications.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain是一个多功能的Python库，帮助开发者充分利用LLMs，特别是在处理大量文本数据时。它在处理文本方面表现出色。LLMs使开发者能够在自然语言处理应用中创建更复杂和类人化的交互。
- en: 'Summarization: **LangChain can quickly and reliably summarize information**,
    reducing the amount of text while preserving the most important parts of the message.'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总结：**LangChain可以快速而可靠地总结信息**，减少文本量，同时保留信息中最重要的部分。
- en: 'Extraction: **The library can parse data from a piece of text, allowing for
    structured output** and enabling tasks like inserting data into a database or
    making API calls based on extracted parameters.'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取：**该库可以解析文本中的数据，允许结构化输出**，并支持将数据插入数据库或根据提取的参数进行API调用等任务。
- en: LangChain facilitates prompt engineering, which is a crucial technique for maximizing
    the performance of AI models like ChatGPT. With prompt engineering, developers
    can design standardized prompts that can be reused across different use cases,
    making the AI application more versatile and effective.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LangChain促进了提示工程，这是一项关键技术，用于最大化像ChatGPT这样的AI模型的性能。通过提示工程，开发者可以设计可以在不同用例中重复使用的标准化提示，从而使AI应用更加多功能和高效。
- en: Overall, LangChain serves as a powerful tool to enhance AI usage, especially
    when dealing with text data, and prompt engineering is a key skill for effectively
    leveraging AI models like ChatGPT in various applications.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，LangChain作为一个强大的工具，可以增强AI的使用，尤其是在处理文本数据时，而提示工程是有效利用像ChatGPT这样的AI模型在各种应用中的关键技能。
- en: '**[Josep Ferrer](https://www.linkedin.com/in/josep-ferrer-sanchez)** is an
    analytics engineer from Barcelona. He graduated in physics engineering and is
    currently working in the Data Science field applied to human mobility. He is a
    part-time content creator focused on data science and technology. You can contact
    him on [LinkedIn](https://www.linkedin.com/in/josep-ferrer-sanchez/), [Twitter](https://twitter.com/rfeers)
    or [Medium](https://medium.com/@rfeers).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Josep Ferrer](https://www.linkedin.com/in/josep-ferrer-sanchez)** 是一位来自巴塞罗那的分析工程师。他毕业于物理工程专业，目前在应用于人类流动性的Data
    Science领域工作。他是一名兼职内容创作者，专注于数据科学和技术。你可以通过[LinkedIn](https://www.linkedin.com/in/josep-ferrer-sanchez/)、[Twitter](https://twitter.com/rfeers)或[Medium](https://medium.com/@rfeers)与他联系。'
- en: More On This Topic
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Breaking the Data Barrier: How Zero-Shot, One-Shot, and Few-Shot…](https://www.kdnuggets.com/2023/08/breaking-data-barrier-zeroshot-oneshot-fewshot-learning-transforming-machine-learning.html)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[打破数据壁垒：零样本、单样本和少样本…](https://www.kdnuggets.com/2023/08/breaking-data-barrier-zeroshot-oneshot-fewshot-learning-transforming-machine-learning.html)'
- en: '[The AIoT Revolution: How AI and IoT Are Transforming Our World](https://www.kdnuggets.com/2022/07/aiot-revolution-ai-iot-transforming-world.html)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AIoT革命：AI和IoT如何改变我们的世界](https://www.kdnuggets.com/2022/07/aiot-revolution-ai-iot-transforming-world.html)'
- en: '[KDnuggets News, July 27: The AIoT Revolution: How AI and IoT Are…](https://www.kdnuggets.com/2022/n30.html)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets新闻，7月27日：AIoT革命：AI和IoT如何…](https://www.kdnuggets.com/2022/n30.html)'
- en: '[How AI is Transforming the Retail Industry](https://www.kdnuggets.com/how-ai-is-transforming-the-retail-industry)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AI如何改变零售行业](https://www.kdnuggets.com/how-ai-is-transforming-the-retail-industry)'
- en: '[Future-Proof Your Data Game: Top Skills Every Data Scientist Needs in 2023](https://www.kdnuggets.com/futureproof-your-data-game-top-skills-every-data-scientist-needs-in-2023)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[未来-proof你的数据技能：2023年每个数据科学家需要的顶级技能](https://www.kdnuggets.com/futureproof-your-data-game-top-skills-every-data-scientist-needs-in-2023)'
- en: '[Step up your Python game with Fast Python for Data Science!](https://www.kdnuggets.com/2022/06/manning-step-python-game-fast-python-data-science.html)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过《数据科学的快速Python》提升你的Python技能！](https://www.kdnuggets.com/2022/06/manning-step-python-game-fast-python-data-science.html)'
