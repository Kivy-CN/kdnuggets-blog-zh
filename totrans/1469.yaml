- en: Learn how to use PySpark in under 5 minutes (Installation + Tutorial)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习如何在5分钟内使用PySpark（安装 + 教程）
- en: 原文：[https://www.kdnuggets.com/2019/08/learn-pyspark-installation-tutorial.html](https://www.kdnuggets.com/2019/08/learn-pyspark-installation-tutorial.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/08/learn-pyspark-installation-tutorial.html](https://www.kdnuggets.com/2019/08/learn-pyspark-installation-tutorial.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Georgios Drakos](https://gdcoder.com/author/), Data Scientist at TUI**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Georgios Drakos](https://gdcoder.com/author/)，TUI的数据科学家**'
- en: I’ve found that is a little difficult to get started with Apache Spark (this
    will focus on PySpark) and install it on local machines for most people. With
    this simple tutorial you’ll get there really fast!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现对于大多数人来说，开始使用Apache Spark（本教程将重点介绍PySpark）并在本地机器上安装它有点困难。通过这个简单的教程，你将能非常快速地掌握！
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前3个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 在IT领域支持你的组织'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[**Apache Spark**](http://spark.apache.org/)** is a must for Big data’s lovers **as
    it is a fast, easy-to-use general engine for big data processing with built-in
    modules for streaming, SQL, machine learning and graph processing. This technology
    is an in-demand skill for data engineers, but also data scientists can benefit
    from learning Spark when doing Exploratory Data Analysis (EDA), feature extraction
    and, of course, ML. But please remember that Spark is only truly realized when
    it is run on a cluster with a large number of nodes.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Apache Spark**](http://spark.apache.org/)** 对于大数据爱好者来说是必备的**，因为它是一个快速、易用的大数据处理通用引擎，具有流处理、SQL、机器学习和图形处理等内置模块。这项技术是数据工程师的热门技能，但数据科学家在进行探索性数据分析（EDA）、特征提取和机器学习时也能从学习Spark中受益。但请记住，Spark只有在大规模节点集群上运行时才能真正发挥作用。'
- en: Table of Contents
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目录
- en: Introduction
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍
- en: Spark definition
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark定义
- en: Spark Application
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark应用程序
- en: Install PySpark on Mac
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Mac上安装PySpark
- en: Open Jupyter Notebook with PySpark
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PySpark打开Jupyter Notebook
- en: Launching a SparkSession
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动SparkSession
- en: Conclussion
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结论
- en: References
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考文献
- en: '![figure-name](../Images/35cf5da02d3e664136e0ff5dee351c8a.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![figure-name](../Images/35cf5da02d3e664136e0ff5dee351c8a.png)'
- en: Introduction
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Apache Spark is one of the hottest and largest open source project in data
    processing framework with rich high-level APIs for the programming languages like
    Scala, Python, Java and R. It realizes the potential of bringing together both
    Big Data and machine learning. This is because:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是数据处理框架中最炙手可热和规模最大的开源项目之一，具有丰富的高级API，支持Scala、Python、Java和R等编程语言。它实现了将大数据和机器学习结合起来的潜力。这是因为：
- en: Spark is fast (up to 100x faster than traditional [Hadoop MapReduce](https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm))
    due to in-memory operation.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于内存操作，Spark运行速度非常快（比传统的 [Hadoop MapReduce](https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm)
    快多达100倍）。
- en: It offers robust, distributed, fault-tolerant data objects (called [RDDs](https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm))
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了强大、分布式、容错的数据对象（称为 [RDDs](https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm)）
- en: It integrates beautifully with the world of machine learning and graph analytics
    through supplementary packages like [MLlib](https://spark.apache.org/mllib/) and [GraphX](https://spark.apache.org/graphx/).
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过附加包如 [MLlib](https://spark.apache.org/mllib/) 和 [GraphX](https://spark.apache.org/graphx/)
    与机器学习和图分析领域无缝集成。
- en: Spark is implemented on [Hadoop/HDFS](https://www.bernardmarr.com/default.asp?contentID=1080) and
    written mostly in [Scala](https://www.scala-lang.org/), a functional programming
    language.However, for most beginners, Scala is not a great first language to learn
    when venturing into the world of data science.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 基于 [Hadoop/HDFS](https://www.bernardmarr.com/default.asp?contentID=1080)
    实现，主要使用 [Scala](https://www.scala-lang.org/) 语言编写，这是一种函数式编程语言。然而，对于大多数初学者来说，Scala
    并不是进入数据科学领域时的最佳首选语言。
- en: Fortunately, Spark provides a wonderful Python API called [PySpark](https://spark.apache.org/docs/latest/api/python/index.html).
    This allows Python programmers to interface with the Spark framework — letting
    you manipulate data at scale and work with objects over a distributed file system.
    So, Spark is not a new programming language that you have to learn but a framework
    working on top of HDFS.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Spark 提供了一个很棒的 Python API，叫做 [PySpark](https://spark.apache.org/docs/latest/api/python/index.html)。这允许
    Python 程序员与 Spark 框架接口——让你在大规模数据上进行操作，并在分布式文件系统上处理对象。因此，Spark 并不是你必须学习的新编程语言，而是一个在
    HDFS 之上的框架。
- en: This presents new concepts like nodes, lazy evaluation, and the transformation-action
    (or ‘map and reduce’) paradigm of programming.In fact, Spark is versatile enough
    to work with other file systems than Hadoop — like Amazon S3 or Databricks (DBFS).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这引入了节点、惰性求值以及变换-行动（或“映射和归约”）编程范式等新概念。事实上，Spark 足够灵活，可以与 Hadoop 以外的其他文件系统配合使用——如
    Amazon S3 或 Databricks (DBFS)。
- en: Internet powerhouses such as Netflix, Yahoo, and eBay have deployed Spark at
    massive scale, collectively processing multiple petabytes of data on clusters
    of over 8,000 nodes.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网巨头如 Netflix、Yahoo 和 eBay 已大规模部署 Spark，集体处理多个 PB 的数据，集群节点超过 8,000 个。
- en: Spark Definition
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark 定义
- en: Typically when you think of a computer you think about one machine sitting on
    your desk at home or at work. This machine works perfectly well for applying machine
    learning on small dataset . However, when you have huge dataset(in tera bytes
    or giga bytes), there are some things that your computer is not powerful enough
    to perform. One particularly challenging area is data processing. Single machines
    do not have enough power and resources to perform computations on huge amounts
    of information (or you may have to wait for the computation to finish).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当你想到计算机时，会想到家中或工作中的一台机器。这台机器在处理小数据集的机器学习应用时表现良好。然而，当你拥有巨大的数据集（以 TB 或 GB 计）时，有些事情是你的计算机无法完成的。一个特别具有挑战性的领域是数据处理。单台机器没有足够的能力和资源来处理大量信息（否则你可能要等到计算完成）。
- en: '![figure-name](../Images/c36231c95b1b58e1c053a9d632f4efad.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![figure-name](../Images/c36231c95b1b58e1c053a9d632f4efad.png)'
- en: A cluster, or group of machines, pools the resources of many machines together
    allowing us to use all the cumulative resources as if they were one. Now a group
    of machines alone is not powerful, you need a framework to coordinate work across
    them. **Spark is a tool for just that, managing and coordinating the execution
    of tasks on data across a cluster of computers.**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 集群，即机器组，将多台机器的资源汇集在一起，使我们可以像使用单台机器一样使用所有累积的资源。单独的机器并不强大，你需要一个框架来协调它们之间的工作。**Spark
    就是这样一个工具，管理和协调在计算机集群上执行任务的执行。**
- en: Spark Application
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark 应用程序
- en: 'A Spark Application consists of:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Spark 应用程序包括：
- en: Driver
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驱动程序
- en: Executors (set of distributed worker processes)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行器（分布式工作进程的集合）
- en: Driver
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 驱动程序
- en: 'The Driver runs the main() method of our application having the following duties:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序运行应用程序的 main() 方法，负责以下职责：
- en: Runs on a node in our cluster, or on a client, and schedules the job execution
    with a cluster manager
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的集群节点上运行，或在客户端上运行，并通过集群管理器调度作业执行
- en: Responds to user’s program or input
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应用户的程序或输入
- en: Analyzes, schedules, and distributes work across the executors
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析、调度并分发工作到执行器
- en: Executors
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行器
- en: An executor is a distributed process responsible for the execution of tasks.
    Each Spark Application has its own set of executors, which stay alive for the
    life cycle of a single Spark application.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 执行器是一个负责执行任务的分布式进程。每个 Spark 应用程序都有自己的执行器集合，这些执行器在单个 Spark 应用程序的生命周期内保持活跃。
- en: Executors perform all data processing of a Spark job
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行器负责处理 Spark 作业的所有数据处理。
- en: Stores results in memory, only persisting to disk when specifically instructed
    by the driver program
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将结果存储在内存中，仅在驱动程序程序明确指示时才持久化到磁盘
- en: Returns results to the driver once they have been completed
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦完成任务，将结果返回给驱动程序
- en: Each node can have anywhere from 1 executor per node to 1 executor per core
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个节点可以从每个节点一个执行器到每个核心一个执行器
- en: '** Node is single entity machine or server .'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '** 节点是单个实体机器或服务器。'
- en: '![figure-name](../Images/ce110256af4c8d0e3e7b93dde3d91146.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![figure-name](../Images/ce110256af4c8d0e3e7b93dde3d91146.png)'
- en: Spark’s Application Workflow
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark 的应用程序工作流
- en: When you submit a job to Spark for processing, there is a lot that goes on behind
    the scenes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当你提交作业给 Spark 进行处理时，后台有很多事情在发生。
- en: Our Standalone Application is kicked off, and initializes its SparkContext.
    Only after having a SparkContext can an app be referred to as a Driver
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的独立应用程序启动并初始化其 SparkContext。只有在拥有 SparkContext 后，应用程序才能称为驱动程序。
- en: Our Driver program asks the Cluster Manager for resources to launch its executors
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的驱动程序向集群管理器请求资源以启动其执行器
- en: The Cluster Manager launches the executors
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 集群管理器启动执行器
- en: Our Driver runs our actual Spark code
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的驱动程序运行实际的 Spark 代码
- en: Executors run tasks and send their results back to the driver
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行器运行任务并将其结果发送回驱动程序
- en: SparkContext is stopped and all executors are shut down, returning resources
    back to the cluster
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SparkContext 停止，所有执行器关闭，将资源返回到集群
- en: Install Spark on Mac (locally)
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Mac 上安装 Spark（本地）
- en: '***First Step: Install Brew***'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '***第一步：安装 Brew***'
- en: 'You will need to install brew if you have it already skip this step:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有安装 brew，请安装它；如果已经安装，可以跳过此步骤：
- en: 1\. open terminal on your mac. You can go to spotlight and type terminal to
    find it easily (alternative you can find it on /Applications/Utilities/).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 打开 Mac 上的终端。你可以通过 Spotlight 搜索终端（或者可以在 /Applications/Utilities/ 中找到它）。
- en: 2\. Enter the command bellow.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 输入以下命令。
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 3\. Hit Return and the script will run. It will output to your terminal a log
    of what is going to install. Hit Return to continue or any other key to abort.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 按下 Return 键，脚本将运行。它会在终端输出将要安装的日志。按 Return 键继续，或按其他任意键中止。
- en: 4\. It might ask for **sudo **privileges. If this happens you will have to type
    your admin password and hit Return again.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 可能会要求**sudo**权限。如果发生这种情况，你需要输入管理员密码并再次按下 Return 键。
- en: '**Notes:** Command line tools (Apple''s XCode) will be installed after this
    guide.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 命令行工具（Apple 的 XCode）将在本指南之后安装。'
- en: The installation will look like as the image below.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 安装将如下图所示。
- en: '![figure-name](../Images/0561cfe8388405c80924de741a51e719.png)Installation
    of Homebrew through command line'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![figure-name](../Images/0561cfe8388405c80924de741a51e719.png)通过命令行安装 Homebrew'
- en: When the installation finishes successfully it will look  as the image below.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 安装成功完成后，将如下图所示。
- en: '![figure-name](../Images/e72a72372711566e135617971f063ce3.png)caption'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![figure-name](../Images/e72a72372711566e135617971f063ce3.png)说明'
- en: By default Homebrew is sending anonymous data and analytics. You can find additional
    information [here](https://docs.brew.sh/Analytics). You can choose to opt-out
    by running the command.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Homebrew 会发送匿名数据和分析。你可以在[这里](https://docs.brew.sh/Analytics)找到更多信息。你可以通过运行命令选择退出。
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***Second Step: Install Anaconda***'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '***第二步：安装 Anaconda***'
- en: In the same terminal just simple type: `$ brew cask install anaconda`. Please
    see resources section in case you face any issue in that step.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一个终端中简单地输入：`$ brew cask install anaconda`。如果在此步骤中遇到任何问题，请查看资源部分。
- en: '***Third final Step: Install PySpark***'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '***第三步：安装 PySpark***'
- en: 1\. ona terminal type `$ brew install apache-spark`
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 在终端中输入`$ brew install apache-spark`
- en: 2\. if you see this error message, enter `$ brew cask install caskroom/versions/java8`to
    install Java8, you will not see this error if you have it already installed.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 如果你看到此错误信息，输入`$ brew cask install caskroom/versions/java8`来安装 Java8，如果你已经安装了它，则不会看到此错误。
- en: '![figure-name](../Images/c41b2ed8a71ba60c5e55ad5b6b5f89fd.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![figure-name](../Images/c41b2ed8a71ba60c5e55ad5b6b5f89fd.png)'
- en: '3\. check if pyspark is properly install by typing on the terminal `$ pyspark`.
    If you see the below it means that it has been installed properly:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 通过在终端中输入`$ pyspark`检查 PySpark 是否正确安装。如果你看到以下内容，说明它已正确安装：
- en: '![figure-name](../Images/48936f60f3da3b12e50bc0a205db863c.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![figure-name](../Images/48936f60f3da3b12e50bc0a205db863c.png)'
- en: Open Jupyter Notebook with PySpark Ready
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 打开 Jupyter Notebook，PySpark 已准备就绪
- en: This section assumes that PySpark has been installed properly and no error appear
    when typing on a terminal `$ pyspark`. At this step, I present the steps you have
    to follow in order create Jupyter Notebooks automatically initialised with SparkContext.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 本节假设 PySpark 已正确安装，并且在终端输入`$ pyspark`时没有出现错误。在此步骤中，我将展示创建自动初始化 SparkContext
    的 Jupyter Notebooks 的步骤。
- en: In order to create a global profile for your terminal session, you will need
    to create or modify your .bash_profile or .bashrc file. Here, I will use .bash_profile
    as my example
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建终端会话的全局配置文件，你需要创建或修改.bash_profile或.bashrc文件。在这里，我将以.bash_profile作为示例。
- en: 1\. Check if you have .bash_profile in your system `$ ls -a`, if you don't have
    one, create one using `$ touch ~/.bash_profile`
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 检查系统中是否有.bash_profile，通过`$ ls -a`，如果没有，使用`$ touch ~/.bash_profile`创建一个。
- en: 2\. Find Spark path by running `$ brew info apache-spark`
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 查找Spark路径，通过`$ brew info apache-spark`
- en: '![figure-name](../Images/0ed7b859ff7ba04f678b5c67416b5240.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![figure-name](../Images/0ed7b859ff7ba04f678b5c67416b5240.png)'
- en: '3\. If you already have a .bash_profile, open it by `$ vim ~/.bash_profile`,
    press `I` in order to insert, and paste the following codes in any location (DO
    NOT delete anything in your file):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经有一个.bash_profile，打开它通过`$ vim ~/.bash_profile`，按`I`进入插入模式，将以下代码粘贴到任何位置（不要删除文件中的任何内容）：
- en: '[PRE2]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 4\. Press `ESC` to exit insert mode, enter `:wq` to exit VIM. *You could fine
    more VIM commands here.*
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 按`ESC`退出插入模式，输入`:wq`退出VIM。*你可以在这里找到更多VIM命令。*
- en: 5\. Refresh terminal profile by `$ source ~/.bash_profile`
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 刷新终端配置文件，通过`$ source ~/.bash_profile`
- en: My favourite way to use PySpark in a Jupyter Notebook is by installing [findSpark](https://github.com/minrk/findspark)package
    which allow me to make a Spark Context available in my code.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我在Jupyter Notebook中使用PySpark的最喜欢的方法是安装[findSpark](https://github.com/minrk/findspark)包，它允许我在代码中使用Spark
    Context。
- en: '*findSpark package is not specific to Jupyter Notebook, you can use this trick
    in your favorite IDE too.*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*findSpark包并不特定于Jupyter Notebook，你也可以在你喜欢的IDE中使用这个技巧。*'
- en: Install findspark by running the following command on a terminal
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在终端上运行以下命令安装findspark
- en: '[PRE3]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Launch a regular Jupyter Notebook and run the following command:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 启动一个普通的Jupyter Notebook，并运行以下命令：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output should be:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应为：
- en: '![figure-name](../Images/272c85c537c5ad986fc8099a97520f23.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![figure-name](../Images/272c85c537c5ad986fc8099a97520f23.png)'
- en: 'Please note that with Spark 2.2 a lot of people recommend just to simply do `pip
    install pyspark` .I try using `pip` to install `pyspark` but I couldn’t get the `pyspark`cluster
    to get started properly. Reading several answers on Stack Overflow and the [official
    documentation](https://pypi.org/project/pyspark/2.2.0/), I came across this:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用Spark 2.2时，许多人建议只需执行`pip install pyspark`。我尝试使用`pip`安装`pyspark`但无法正确启动`pyspark`集群。阅读了多个Stack
    Overflow的答案和[官方文档](https://pypi.org/project/pyspark/2.2.0/)，我发现了这个：
- en: '*The Python packaging for Spark is not intended to replace all of the other
    use cases. This Python packaged version of Spark is suitable for interacting with
    an existing cluster (be it Spark standalone, YARN, or Mesos) - but does not contain
    the tools required to setup your own standalone Spark cluster. You can download
    the full version of Spark from the [Apache Spark downloads page](http://spark.apache.org/downloads.html).*'
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Spark的Python打包并不是为了替代所有其他使用情况。这个Python打包版本的Spark适合与现有集群（无论是Spark独立集群、YARN还是Mesos）进行交互，但不包含设置自己独立Spark集群所需的工具。你可以从[Apache
    Spark下载页面](http://spark.apache.org/downloads.html)下载Spark的完整版。*'
- en: Therefore, I would suggest to follow the steps that I described above.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我建议遵循我上述描述的步骤。
- en: Launching a SparkSession
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启动SparkSession
- en: 'Well, it’s the main entry point for Spark functionality: it represents the
    connection to a Spark cluster and you can use it to create RDDs and to broadcast
    variables on that cluster. When you’re working with Spark, everything starts and
    ends with this SparkSession. **Note** that SparkSession is a new feature of Spark
    2.0 which minimize the number of concepts to remember or construct. (before Spark
    2.0.0, the three main connection objects were SparkContext, SqlContext and HiveContext).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Spark功能的主要入口点：它代表与Spark集群的连接，你可以使用它来创建RDDs和广播变量。当你使用Spark时，一切都从这个SparkSession开始并结束。**注意**，SparkSession是Spark
    2.0的新特性，它最小化了需要记住或构建的概念数量。（在Spark 2.0.0之前，主要的连接对象是SparkContext、SqlContext和HiveContext）。
- en: In interactive environments, a SparkSession will already be created for you
    in a variable named spark. For consistency, you should use this name when you
    create one in your own application.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在交互环境中，会为你创建一个名为spark的SparkSession。为了保持一致性，你在自己应用程序中创建SparkSession时应该使用这个名称。
- en: 'You can create a new SparkSession through a Builder pattern which uses a "fluent
    interface" style of coding to build a new object by chaining methods together.
    Spark properties can be passed in, as shown in these examples:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 Builder 模式创建一个新的 SparkSession，该模式使用“流畅接口”风格的编码，通过方法链来构建一个新的对象。可以传入 Spark
    属性，如下例所示：
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'At the end of your application, please remember to call `spark.stop()`  in
    order to end the SparkSession. Let''s understand the various settings that we
    define above:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序结束时，请记得调用 `spark.stop()` 以结束 SparkSession。让我们了解一下我们上面定义的各种设置：
- en: '`**master**`**: **Sets the Spark master URL to connect to, such as “local”
    to run locally, “local[4]” to run locally with 4 cores, or “spark://master:7077”
    to run on a Spark standalone cluster.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**master**`**: **设置要连接的 Spark master URL，例如“local”用于本地运行，“local[4]”用于本地运行并使用
    4 个核心，或“spark://master:7077”用于在 Spark 独立集群上运行。'
- en: '`**config**`**:**Sets a config option by specifying a (key, value) pair.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**config**`**:**通过指定（键，值）对来设置配置选项。'
- en: '`**appName**`**: **Sets a name for the application, if no name is set, a randomly
    generated name will be used.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**appName**`**: **为应用程序设置一个名称，如果未设置名称，将使用随机生成的名称。'
- en: '`**getOrCreate**`**:**Gets an existing [`**SparkSession**`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?#pyspark.sql.SparkSession) or,
    if there is no existing one, creates a new one based on the options set in this
    builder. In case an existing SparkSession is returned, the config options specified
    in this builder affecting the `SQLContext` configuration will applied. As `SparkContext` configuration
    cannot be modified on runtime (you have to stop existing context first) while`SQLContext` configuration
    can be modified on runtime.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**getOrCreate**`**:**获取现有的 [`**SparkSession**`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?#pyspark.sql.SparkSession) ，如果没有现有的，则根据此构建器中设置的选项创建一个新的。如果返回现有的
    SparkSession，则此构建器中影响 `SQLContext` 配置的配置选项将会应用。由于 `SparkContext` 配置无法在运行时修改（必须先停止现有上下文），而
    `SQLContext` 配置可以在运行时修改。'
- en: Conclusion
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: Spark has seen immense growth over the past several years. Hundreds of contributors
    working collectively have made Spark an amazing piece of technology powering the
    de facto standard for big data processing and data sciences across all industries.
    But please remember to use it for manipulations of **huge dataset **when facing
    performance issues otherwise it may have opposite effects. For small datasets
    (few gigabytes) it is advisable instead to use [Pandas](https://pandas.pydata.org/).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 在过去几年里经历了巨大的增长。数百名贡献者共同努力，使 Spark 成为一项出色的技术，为各个行业的大数据处理和数据科学提供了事实上的标准。但是请记住，当面临性能问题时，**大数据集**的操作更适合使用它，否则可能会产生相反的效果。对于小数据集（几GB），建议使用 [Pandas](https://pandas.pydata.org/)。
- en: Thanks for reading and I am looking forward to hearing your questions :)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读，期待你的问题 :)
- en: '*Stay tuned and Happy Machine Learning.*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*敬请关注，祝机器学习愉快。*'
- en: References
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[https://medium.com/ayuth/install-anaconda-on-macos-with-homebrew-c94437d63a37](https://medium.com/ayuth/install-anaconda-on-macos-with-homebrew-c94437d63a37)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://medium.com/ayuth/install-anaconda-on-macos-with-homebrew-c94437d63a37](https://medium.com/ayuth/install-anaconda-on-macos-with-homebrew-c94437d63a37)'
- en: '[https://picocoder.io/how-to-install-homebrew-on-mac/](https://picocoder.io/how-to-install-homebrew-on-mac/)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://picocoder.io/how-to-install-homebrew-on-mac/](https://picocoder.io/how-to-install-homebrew-on-mac/)'
- en: '[https://stackoverflow.com/](https://stackoverflow.com/questions/46387574/installing-pyspark-on-macbook)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://stackoverflow.com/](https://stackoverflow.com/questions/46387574/installing-pyspark-on-macbook)'
- en: '[https://www.oreilly.com/library/view/learning-spark/9781449359034/](https://www.oreilly.com/library/view/learning-spark/9781449359034/)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.oreilly.com/library/view/learning-spark/9781449359034/](https://www.oreilly.com/library/view/learning-spark/9781449359034/)'
- en: '**Bio: [Georgios Drakos](https://gdcoder.com/author/)** is a Data Scientist
    with a BSc and MSc in Electrical and Computer Engineering (National Technical
    University of Athens) as well as a MSc from Imperial College London, currently
    working as a Data Scientist for TUI in the travel industry. He specialised in
    Computational Intelligence for Decision Support, Data Engineering, Sophisticated
    Analytics and Technological Innovation Management. Highly interested in Cloud-based
    technologies, Distributed Computing, Big Data and the business applications of
    Data Science & Machine Learning.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Georgios Drakos](https://gdcoder.com/author/)** 是一名数据科学家，拥有雅典国家技术大学电气与计算机工程的学士和硕士学位，以及伦敦帝国学院的硕士学位，目前在
    TUI 从事数据科学工作。他专注于决策支持的计算智能、数据工程、复杂分析和技术创新管理。对基于云的技术、分布式计算、大数据以及数据科学和机器学习的商业应用非常感兴趣。'
- en: '[Original](https://gdcoder.com/learn-how-to-use-pyspark-in-under-5-minutes/).
    Reposted with permission.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://gdcoder.com/learn-how-to-use-pyspark-in-under-5-minutes/)。经授权转载。'
- en: '**Related:**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Spark NLP: Getting Started With The World’s Most Widely Used NLP Library In
    The Enterprise](/2019/06/spark-nlp-getting-started-with-worlds-most-widely-used-nlp-library-enterprise.html)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Spark NLP：企业中最广泛使用的 NLP 库入门](/2019/06/spark-nlp-getting-started-with-worlds-most-widely-used-nlp-library-enterprise.html)'
- en: '[Scalable Python Code with Pandas UDFs: A Data Science Application](/2019/06/scalable-python-code-pandas-udfs.html)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Pandas UDFs 的可扩展 Python 代码：数据科学应用](/2019/06/scalable-python-code-pandas-udfs.html)'
- en: '[Analyzing Tweets with NLP in Minutes with Spark, Optimus and Twint](/2019/05/analyzing-tweets-nlp-spark-optimus-twint.html)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 Spark、Optimus 和 Twint 在几分钟内分析推文](https://www.kdnuggets.com/2019/05/analyzing-tweets-nlp-spark-optimus-twint.html)'
- en: More On This Topic
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多相关话题
- en: '[FastAPI Tutorial: Build APIs with Python in Minutes](https://www.kdnuggets.com/fastapi-tutorial-build-apis-with-python-in-minutes)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FastAPI 教程：用 Python 在几分钟内构建 API](https://www.kdnuggets.com/fastapi-tutorial-build-apis-with-python-in-minutes)'
- en: '[PySpark for Data Science](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PySpark 数据科学](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
- en: '[Data Validation for PySpark Applications using Pandera](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Pandera 对 PySpark 应用进行数据验证](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
- en: '[Understanding Bias-Variance Trade-Off in 3 Minutes](https://www.kdnuggets.com/2020/09/understanding-bias-variance-trade-off-3-minutes.html)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3 分钟了解偏差-方差权衡](https://www.kdnuggets.com/2020/09/understanding-bias-variance-trade-off-3-minutes.html)'
- en: '[Build a Machine Learning Web App in 5 Minutes](https://www.kdnuggets.com/2022/03/build-machine-learning-web-app-5-minutes.html)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 5 分钟内构建机器学习 Web 应用](https://www.kdnuggets.com/2022/03/build-machine-learning-web-app-5-minutes.html)'
- en: '[Build a Web Scraper with Python in 5 Minutes](https://www.kdnuggets.com/2022/02/build-web-scraper-python-5-minutes.html)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 Python 在 5 分钟内构建网络爬虫](https://www.kdnuggets.com/2022/02/build-web-scraper-python-5-minutes.html)'
