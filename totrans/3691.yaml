- en: Multimodal Models Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态模型解释
- en: 原文：[https://www.kdnuggets.com/2023/03/multimodal-models-explained.html](https://www.kdnuggets.com/2023/03/multimodal-models-explained.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/03/multimodal-models-explained.html](https://www.kdnuggets.com/2023/03/multimodal-models-explained.html)
- en: '![Multimodal Models Explained](../Images/f61a619c4285c475d4798e805c871a5f.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/f61a619c4285c475d4798e805c871a5f.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Deep learning is a type of intelligence developed to mimic the systems and neurons
    in the human brain, which play an essential role in the human thinking process.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一种智能类型，旨在模仿人脑中的系统和神经元，这些系统和神经元在人的思维过程中扮演了至关重要的角色。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速入门网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织在 IT 方面'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This technology utilizes deep neural networks, which are composed of layers
    of artificial neurons that can analyze and process vast amounts of data, enabling
    them to learn and improve over time.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这项技术利用深度神经网络，这些网络由多层人工神经元组成，可以分析和处理大量数据，使其能够随着时间的推移学习和改进。
- en: We, humans, rely on our five senses to interpret the world around us. We use
    our senses of sight, hearing, touch, taste, and smell to gather information about
    our environment and make sense of it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们人类依赖五感来解读周围的世界。我们利用视、听、触、味、嗅等感官来收集环境信息并理解这些信息。
- en: In a similar vein, multimodal learning is an exciting new field of AI that seeks
    to replicate this ability by combining information from multiple models. By integrating
    information from diverse sources such as text, image, audio, and video, multimodal
    models can build a richer and more complete understanding of the underlying data,
    unlock new insights, and enable a wide range of applications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，多模态学习是一个令人兴奋的人工智能新领域，试图通过结合来自多个模型的信息来复制这种能力。通过整合来自文本、图像、音频和视频等多种来源的信息，多模态模型可以建立对基础数据更丰富、更完整的理解，揭示新的洞察，并支持广泛的应用。
- en: The techniques used in multimodal learning include fusion-based approaches,
    alignments-based approaches, and late fusion.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态学习中使用的技术包括基于融合的方法、基于对齐的方法以及后期融合。
- en: In this article, we will explore the fundamentals of multimodal learning, including
    the different techniques used to fuse information from diverse sources, as well
    as its many exciting applications, from speech recognition to autonomous cars
    and beyond.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨多模态学习的基本原理，包括融合不同来源信息的各种技术，以及其从语音识别到自动驾驶等众多令人兴奋的应用。
- en: What is Multimodal Learning?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是多模态学习？
- en: '![Multimodal Models Explained](../Images/dac0d519e923f158fca3021294a95ad9.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/dac0d519e923f158fca3021294a95ad9.png)'
- en: Image by Author
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Multimodal learning is a subfield of artificial intelligence that seeks to effectively
    process and analyze data from multiple modalities.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态学习是人工智能的一个子领域，旨在有效处理和分析来自多种模态的数据。
- en: In simple terms, this means combining information from different sources such
    as text, image, audio, and video to build a more complete and accurate understanding
    of the underlying data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，这意味着将来自不同来源的信息如文本、图像、音频和视频结合起来，以建立对基础数据更完整、更准确的理解。
- en: The concept of multimodal learning has found applications in a wide range of
    subjects, including speech recognition, autonomous car, and emotion recognition.
    We’ll talk about them in the following sections.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态学习的概念已经在包括语音识别、自动驾驶和情感识别等广泛领域找到了应用。我们将在接下来的部分中讨论这些内容。
- en: The multimodal learning techniques enable models to process and analyze data
    from multiple modalities effectively, providing a more complete and accurate understanding
    of the underlying data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态学习技术使模型能够有效地处理和分析来自多种模态的数据，从而提供对底层数据更全面和准确的理解。
- en: In the next section, we will mention these techniques, but before doing that,
    let’s talk about combining models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将提到这些技术，但在此之前，让我们先讨论组合模型。
- en: These two concepts might look alike, but you’ll soon discover they aren’t.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个概念可能看起来相似，但你很快会发现它们并不相同。
- en: Combining Models
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组合模型
- en: Combining models is a technique in machine learning that involves using multiple
    models to improve the performance of a single model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 组合模型是一种机器学习技术，涉及使用多个模型来提高单个模型的性能。
- en: The idea behind combining models is that one model's strengths can compensate
    for another's weakness, resulting in a more accurate and robust prediction. Ensemble
    models, stacking, and bagging are techniques used in combining models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 组合模型的理念在于，一个模型的优势可以弥补另一个模型的不足，从而产生更准确和更稳健的预测。集成模型、堆叠和装袋是用于组合模型的技术。
- en: '![Multimodal Models Explained](../Images/005efd5895d339d294e7f3594d2c552b.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/005efd5895d339d294e7f3594d2c552b.png)'
- en: Image by Author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Ensemble models
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成模型
- en: '![Multimodal Models Explained](../Images/1d74bdadb6f025c9cce32e2ad0ef09e9.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/1d74bdadb6f025c9cce32e2ad0ef09e9.png)'
- en: Image by Author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Ensemble models involve combining the outputs of multiple base models to produce
    a better overall model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型涉及将多个基础模型的输出进行组合，以产生一个更好的整体模型。
- en: One example of an ensemble model is random forests. Random forests are a decision
    tree algorithm that combines multiple decision trees to improve the model's accuracy.
    Each decision tree is trained on a different subset of the data, and the final
    prediction is made by averaging the predictions of all the trees.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个集成模型的例子是随机森林。随机森林是一种决策树算法，通过结合多棵决策树来提高模型的准确性。每棵决策树都在数据的不同子集上进行训练，最终的预测是通过对所有树的预测结果进行平均来得出的。
- en: You can see how to use the random forests algorithm in the scikit-learn library
    [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [这里](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
    查看如何在 scikit-learn 库中使用随机森林算法。
- en: Stacking
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠
- en: '![Multimodal Models Explained](../Images/fea9196c2057a9d8e2e7d4b1f458cdcf.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/fea9196c2057a9d8e2e7d4b1f458cdcf.png)'
- en: Image by Author
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Stacking involves using the outputs of multiple models as inputs to another
    model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠涉及将多个模型的输出作为另一个模型的输入。
- en: One real-life example of stacking is in natural language processing, where it
    can be applied to sentiment analysis.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠的一个现实生活中的例子是自然语言处理中的情感分析。
- en: For instance, the [Stanford Sentiment Treebank dataset](https://www.kaggle.com/datasets/atulanandjha/stanford-sentiment-treebank-v2-sst2)
    contains movie reviews with sentiment labels ranging from very negative to very
    positive. In this case, multiple models such as [random forest](https://www.stratascratch.com/blog/decision-tree-and-random-forest-algorithm-explained/?utm_source=blog&utm_medium=click&utm_campaign=kdn+multimodal+models),
    [Support Vector Machines (SVM)](https://www.stratascratch.com/blog/machine-learning-algorithms-explained-support-vector-machine/?utm_source=blog&utm_medium=click&utm_campaign=kdn+multimodal+models),
    and Naive Bayes can be trained to predict the sentiment of the reviews.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[斯坦福情感树库数据集](https://www.kaggle.com/datasets/atulanandjha/stanford-sentiment-treebank-v2-sst2)包含了情感标签从非常负面到非常正面的电影评论。在这种情况下，可以训练多个模型，如
    [随机森林](https://www.stratascratch.com/blog/decision-tree-and-random-forest-algorithm-explained/?utm_source=blog&utm_medium=click&utm_campaign=kdn+multimodal+models)、[支持向量机（SVM）](https://www.stratascratch.com/blog/machine-learning-algorithms-explained-support-vector-machine/?utm_source=blog&utm_medium=click&utm_campaign=kdn+multimodal+models)
    和朴素贝叶斯来预测评论的情感。
- en: The predictions of these models can then be combined using a meta-model such
    as logistic regression or neural network, which is trained on the outputs of the
    base models to make the final prediction.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型的预测结果可以通过使用元模型（如逻辑回归或神经网络）进行组合，该元模型在基础模型的输出上进行训练，以得出最终预测结果。
- en: Stacking can improve the accuracy of sentiment prediction and make sentiment
    analysis more robust.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠可以提高情感预测的准确性，并使情感分析更为稳健。
- en: Bagging
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 装袋
- en: '![Multimodal Models Explained](../Images/4e5bf44d5da242bace04cbb436b418ca.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/4e5bf44d5da242bace04cbb436b418ca.png)'
- en: Image by Author
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Bagging is another way of creating ensembles, where several base models are
    trained on different subsets of data, and their predictions are averaged to make
    a final decision.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习是另一种创建模型集成的方法，其中多个基础模型在不同的数据子集上进行训练，并将它们的预测结果平均以作出最终决策。
- en: One example of bagging is the bootstrap aggregating method, where multiple models
    are trained on different subsets of the training data. The final prediction is
    made by averaging the predictions of all the models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习的一个例子是自助聚合方法，其中多个模型在训练数据的不同子集上进行训练。最终预测通过对所有模型的预测结果进行平均得出。
- en: An example of bagging in real life is in finance. The [S&P 500 dataset](https://www.kaggle.com/datasets/camnugent/sandp500)
    contains historical data on the stock prices of the 500 largest publicly traded
    companies in the United States between 2012 and 2018.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现实生活中的一个集成学习示例是金融领域。[S&P 500 数据集](https://www.kaggle.com/datasets/camnugent/sandp500)包含2012年至2018年间美国500家最大上市公司的股票价格历史数据。
- en: Bagging can be used in this dataset by training multiple models in scikit-learn,
    such as [random forests](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
    and [gradient boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html),
    to predict the companies' stock prices.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在 scikit-learn 中训练多个模型（如 [随机森林](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
    和 [梯度提升](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)）来使用集成学习预测公司的股票价格。
- en: Each model is trained on a different subset of the training data, and their
    predictions are averaged to make the final prediction. The use of bagging can
    improve the accuracy of the stock price prediction and make the financial analysis
    more robust.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型在训练数据的不同子集上进行训练，然后将它们的预测结果平均，以得到最终预测。使用集成学习可以提高股票价格预测的准确性，使财务分析更具鲁棒性。
- en: Difference Between Combining Models & Multimodal Learning
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合模型与多模态学习的区别
- en: In combining models, the models are trained independently, and the final prediction
    is made by combining the outputs of these models using techniques such as ensemble
    models, stacking, or bagging.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在结合模型中，模型是独立训练的，最终预测通过使用集成模型、堆叠或集成学习等技术将这些模型的输出结合起来。
- en: Combining models is particularly useful when the individual models have complementary
    strengths and weaknesses, as the combination can lead to a more accurate and robust
    prediction.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当单个模型具有互补的优缺点时，结合模型特别有用，因为这种结合可以导致更准确和更稳健的预测。
- en: In multimodal learning, the goal is to combine information from different modalities
    to perform a prediction task. This can involve using techniques such as the fusion-based
    approach, alignment-based approach, or late fusion to create a high-dimensional
    representation that captures the semantic information of the data from each modality.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在多模态学习中，目标是结合来自不同模态的信息以执行预测任务。这可能涉及使用融合方法、对齐方法或晚期融合等技术，创建捕捉每种模态数据语义信息的高维表示。
- en: Multimodal learning is particularly useful when different modalities provide
    complementary information that can improve the accuracy of the prediction.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态学习在不同模态提供互补信息、能提高预测准确性时特别有用。
- en: The main difference between combining models and multimodal learning is that
    combining models involve using multiple models to improve the performance of a
    single model. In contrast, multimodal learning involves learning from and combining
    information from multiple modalities such as image, text, and audio to perform
    a prediction test.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 结合模型和多模态学习的主要区别在于，结合模型涉及使用多个模型来提高单个模型的性能。相比之下，多模态学习涉及从多个模态（如图像、文本和音频）中学习并结合信息，以进行预测测试。
- en: Now, let’s look at multimodal learning techniques.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解多模态学习技术。
- en: Multimodal Learning Techniques
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态学习技术
- en: '![Multimodal Models Explained](../Images/9053201e86bae0e9a62bdc0cc842e625.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/9053201e86bae0e9a62bdc0cc842e625.png)'
- en: Image by Author
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Fusion-Based Approach
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于融合的方法
- en: The fusion-based approach involves encoding the different modalities into a
    common representation space, where the representations are fused to create a single
    modality-invariant representation that captures the semantic information from
    all modalities.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 基于融合的方法涉及将不同的模态编码到一个共同的表示空间中，在这个空间中，这些表示被融合以创建一个单一的模态不变表示，捕捉来自所有模态的语义信息。
- en: This approach can be further divided into early fusion and mid-fusion techniques,
    depending on when the fusion occurs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以进一步分为早期融合和中期融合技术，具体取决于融合发生的时间。
- en: Text Captioning
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本标题生成
- en: A typical example of a fusion-based approach is an image and text captioning.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的基于融合的方法是图像和文本标题生成。
- en: It’s the fusion-based approach because the image's visual features and the text's
    semantic information are encoded into a common representation space and then fused
    to generate a single modality-invariant representation that captures the semantic
    information from both modalities.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种基于融合的方法，因为图像的视觉特征和文本的语义信息被编码到一个公共表示空间中，然后融合生成一个单一的模态不变表示，从而捕捉两个模态的语义信息。
- en: Specifically, the visual features of the image are extracted using a convolutional
    neural network (CNN), and the semantic information of the text is captured using
    a recurrent neural network (RNN).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，图像的视觉特征是通过卷积神经网络（CNN）提取的，而文本的语义信息则是通过递归神经网络（RNN）捕捉的。
- en: These two modalities are then encoded into a common representation space. The
    visual and textual features are fused using concatenation or element-wise multiplication
    techniques to create a single modality-invariant representation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种模态然后被编码到一个公共表示空间中。视觉和文本特征通过连接或逐元素乘法技术融合，以创建一个单一的模态不变表示。
- en: This final representation can then be used to generate a caption for the image.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最终表示可以用来为图像生成标题。
- en: One open-source dataset that can be used to perform image and text captioning
    is the [Flickr30k dataset](http://shannon.cs.illinois.edu/DenotationGraph/), which
    contains 31,000 images along with five captions per image. This dataset contains
    images of various everyday scenes, with each image annotated by multiple people
    to provide diverse captions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可以用于图像和文本标题生成的开源数据集是[Flickr30k数据集](http://shannon.cs.illinois.edu/DenotationGraph/)，该数据集包含31,000张图像，每张图像配有五个标题。该数据集包含各种日常场景的图像，每张图像由多个标注者注释，以提供多样化的标题。
- en: '![Multimodal Models Explained](../Images/a4a2a1e298391d5c4ab98511d20012bc.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/a4a2a1e298391d5c4ab98511d20012bc.png)'
- en: 'Source: [https://paperswithcode.com/dataset/flickr30k-cna](https://paperswithcode.com/dataset/flickr30k-cna)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [https://paperswithcode.com/dataset/flickr30k-cna](https://paperswithcode.com/dataset/flickr30k-cna)'
- en: The Flickr30k dataset can be used to apply the fusion-based approach for image
    and text captioning by extracting visual features from pre-trained CNNs and using
    techniques such as word embeddings or bag-of-words representations for textual
    features. The resulting fused representation can be used to generate more accurate
    and informative captions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Flickr30k数据集可以通过从预训练CNN中提取视觉特征，并使用诸如词嵌入或词袋表示等技术来应用基于融合的方法进行图像和文本标题生成。生成的融合表示可以用来生成更准确和信息量更大的标题。
- en: Alignment-Based Approach
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于对齐的方法
- en: This approach involves aligning the different modalities so that they can be
    compared directly.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法涉及对齐不同的模态，以便它们可以直接进行比较。
- en: The goal is to create modality-invariant representations that can be compared
    across modalities. This approach is advantageous when the modalities share a direct
    relationship, such as in audio-visual speech recognition.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是创建可以跨模态比较的模态不变表示。当模态之间有直接关系时，如音频-视觉语音识别，这种方法是有利的。
- en: Sign Language Recognition
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手语识别
- en: '![Multimodal Models Explained](../Images/d91defa682a24eb9c19af0a0bc720d99.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/d91defa682a24eb9c19af0a0bc720d99.png)'
- en: Image by Author
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: One example of an alignment-based approach is in the task of sign language recognition.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐方法的一个例子是在手语识别任务中。
- en: This use involves the alignment-based approach because it requires the model
    to align the temporal information of both visual (video frames) and audio (audio
    waveform) modalities.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这种使用涉及基于对齐的方法，因为它要求模型对视觉（视频帧）和音频（音频波形）两种模态的时间信息进行对齐。
- en: The task is for the model to recognize sign language gestures and translate
    them into text. The gestures are captured using a video camera, and the corresponding
    audio and the two modalities must be aligned to recognize the gestures accurately.
    This involves identifying the temporal alignment between the video frames and
    the audio waveform to recognize the gestures and the corresponding spoken words.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是让模型识别手语手势并将其翻译成文本。手势通过视频摄像机捕捉，音频和两种模态必须对齐，以准确识别手势。这涉及识别视频帧和音频波形之间的时间对齐，以识别手势和相应的语音。
- en: One open-source dataset for sign language recognition is the [RWTH-PHOENIX-Weather
    2014T dataset](https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX/)
    which contains video recordings of German Sign Language (DGS) from various signers.
    The dataset includes both visual and audio modalities, making it suitable for
    multimodal learning tasks that require alignment-based approaches.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于手语识别的开源数据集是 [RWTH-PHOENIX-Weather 2014T 数据集](https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX/)，它包含了各种手语使用者的德国语手语（DGS）视频录制。该数据集包括视觉和音频模态，适合需要对齐方法的多模态学习任务。
- en: Late Fusion
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 晚期融合
- en: This approach involves combining the predictions from models trained on each
    modality separately. The individual predictions are then combined to create a
    final prediction. This approach is particularly useful when the modalities are
    not directly related, or the individual modalities provide complementary information.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法涉及结合分别在每种模态上训练的模型的预测结果。然后将各个预测结果结合以生成最终预测。这种方法在模态之间没有直接关系，或单个模态提供互补信息时特别有用。
- en: Emotion Recognition
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 情感识别
- en: '![Multimodal Models Explained](../Images/d49bee101c25b6e6ef1b83f047d0a57f.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/d49bee101c25b6e6ef1b83f047d0a57f.png)'
- en: Image by Author
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: A real-life example of late fusion is emotion recognition in music. In this
    task, the model must recognize the emotional content of a piece of music using
    both the audio features and the lyrics.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 晚期融合的一个现实生活中的例子是音乐中的情感识别。在这个任务中，模型必须使用音频特征和歌词来识别一首音乐的情感内容。
- en: The late fusion approach is applied in this example because it combines the
    predictions from models trained on separate modalities (audio features and lyrics)
    to create a final prediction.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中应用了晚期融合方法，因为它结合了分别在不同模态（音频特征和歌词）上训练的模型的预测结果，以创建最终预测。
- en: The individual models are trained separately on each modality, and the predictions
    are combined at a later stage.Therefore, late fusion.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 各个模型分别在每种模态上训练，预测结果随后被结合。因此，使用晚期融合。
- en: The audio features can be extracted using techniques such as Mel-frequency cepstral
    coefficients (MFCCs), while the lyrics can be encoded using techniques such as
    bag-of-words or word embeddings. Models can be trained separately on each modality,
    and the predictions can be combined using late fusion to generate a final prediction.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 音频特征可以使用梅尔频率倒谱系数（MFCCs）等技术提取，而歌词可以使用词袋模型或词嵌入技术进行编码。可以在每种模态上分别训练模型，然后通过晚期融合将预测结果结合起来生成最终预测。
- en: The [DEAM dataset](https://cvml.unige.ch/databases/DEAM/) was designed to support
    research on music emotion recognition and analysis, and it includes both audio
    features and lyrics for a collection of over 2,000 songs. The audio features include
    various descriptors such as MFCCs, spectral contrast, and rhythm features, while
    the lyrics are represented using bag-of-words and word embedding techniques.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[DEAM 数据集](https://cvml.unige.ch/databases/DEAM/)旨在支持音乐情感识别和分析的研究，它包括了超过2000首歌曲的音频特征和歌词。音频特征包括MFCCs、谱对比度和节奏特征等各种描述符，而歌词则使用词袋模型和词嵌入技术表示。'
- en: The late fusion approach can be applied to the DEAM dataset by combining the
    predictions from separate models trained on each modality (audio and lyrics).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将分别在每种模态（音频和歌词）上训练的模型的预测结果结合起来，可以将晚期融合方法应用于DEAM数据集。
- en: '![Multimodal Models Explained](../Images/6b26f00321e980307d5ea0b278667f29.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/6b26f00321e980307d5ea0b278667f29.png)'
- en: 'Source: [DEAM Dataset - Emotional Analysis in Music](https://www.kaggle.com/datasets/imsparsh/deam-mediaeval-dataset-emotional-analysis-in-music)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[DEAM 数据集 - 音乐中的情感分析](https://www.kaggle.com/datasets/imsparsh/deam-mediaeval-dataset-emotional-analysis-in-music)
- en: For example, you can train a separate machine learning model to predict the
    emotional content of each song using audio features, such as MFCCs and spectral
    contrast.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以训练一个单独的机器学习模型，使用音频特征，如MFCCs和谱对比度，来预测每首歌的情感内容。
- en: Another model can be trained to predict the emotional content of each song using
    the lyrics, represented using techniques such as bag-of-words or word embeddings.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个模型可以使用歌词来预测每首歌的情感内容，歌词通过词袋模型或词嵌入等技术表示。
- en: After training the individual models, the predictions from each model can be
    combined using the late fusion approach to generate a final prediction.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 训练各个单独模型后，可以使用后期融合方法将每个模型的预测结果结合起来，以生成最终预测。
- en: Multimodal Learning Challenges
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态学习挑战
- en: '![Multimodal Models Explained](../Images/68dc06f828f13da31418301422805d88.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/68dc06f828f13da31418301422805d88.png)'
- en: Image by Author
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Representation
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示
- en: Multimodal data can come in different modalities, such as text and audio. Combining
    them in a way that preserves their individual characteristics while still capturing
    their relationships can be challenging.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态数据可以来自不同的模态，如文本和音频。将它们以保留其个性特征同时捕捉它们之间关系的方式结合起来是具有挑战性的。
- en: This can lead to issues such as the model failing to generalize well, being
    biased towards one modality, or not effectively capturing the joint information.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能导致模型无法很好地泛化、对某一模态有偏见，或者不能有效地捕捉联合信息等问题。
- en: 'To solve representation problems in multimodal learning, several strategies
    can be employed:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决多模态学习中的表示问题，可以采用几种策略：
- en: '**Joint representation**: As mentioned earlier, this approach involves encoding
    both modalities into a shared high-dimensional space. Techniques like deep learning-based
    fusion methods can be used to learn optimal joint representations.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**联合表示**：如前所述，这种方法涉及将两个模态编码到一个共享的高维空间中。可以使用基于深度学习的融合方法来学习最佳的联合表示。'
- en: '**Coordinated representation**: Instead of fusing the modalities directly,
    this approach maintains separate encodings for each modality but ensures that
    their representations are related and convey the same meaning. Alignment or attention
    mechanisms can be used to achieve this coordination.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**协调表示**：这种方法不是直接融合模态，而是保持每个模态的独立编码，但确保它们的表示是相关的并传达相同的意义。可以使用对齐或注意力机制来实现这种协调。'
- en: Image-Caption Pairs
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像-标题对
- en: The [MS COCO dataset](https://cocodataset.org/) is widely used in computer vision
    and natural language processing research, containing many images with objects
    in various contexts, along with multiple textual captions describing the content
    of the images.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[MS COCO数据集](https://cocodataset.org/)在计算机视觉和自然语言处理研究中被广泛使用，包含了许多具有各种背景的对象图像，以及描述图像内容的多个文本标题。'
- en: When working with the MS COCO dataset, two main strategies for handling representation
    challenges are joint representation and coordinated representation.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理MS COCO数据集时，处理表示挑战的两种主要策略是联合表示和协调表示。
- en: '**Joint representation**: By combining the information from both modalities,
    the model can understand their combined meaning. For instance, you can use a deep
    learning model with layers designed to process and merge features from image and
    text data. This results in a joint representation that captures the relationship
    between the image and caption.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**联合表示**：通过结合两个模态的信息，模型可以理解它们的综合意义。例如，你可以使用一个深度学习模型，设计处理和合并图像和文本数据特征的层。这会产生一个捕捉图像和标题之间关系的联合表示。'
- en: '**Coordinated representation**: In this approach, the image and caption are
    encoded separately, but their representations are related and convey the same
    meaning. Instead of directly fusing the modalities, the model maintains separate
    encodings for each modality while ensuring they are meaningfully associated.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**协调表示**：在这种方法中，图像和标题是分开编码的，但它们的表示是相关的，并传达相同的意义。模型不直接融合模态，而是保持每个模态的独立编码，同时确保它们在意义上的关联。'
- en: Both joint and coordinated representation strategies can be employed when working
    with the MS COCO dataset to effectively handle the challenges of multimodal learning
    and create models that can process and understand the relationships between visual
    and textual information.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理MS COCO数据集时，可以使用联合表示和协调表示策略，以有效应对多模态学习的挑战，并创建能够处理和理解视觉与文本信息之间关系的模型。
- en: Fusion
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 融合
- en: Fusion is a technique used in multimodal learning to combine information from
    different data modalities, such as text, images, and audio, to create a more comprehensive
    understanding of a particular situation or context. The fusion process helps models
    make better predictions and decisions based on the combined information instead
    of relying on a single modality.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 融合是一种在多模态学习中使用的技术，用于将来自不同数据模态的信息（如文本、图像和音频）结合起来，以创建对特定情况或背景的更全面理解。融合过程帮助模型基于综合信息做出更好的预测和决策，而不是依赖单一模态。
- en: One challenge in multimodal learning is determining the best way to fuse the
    different modalities. Different fusion techniques may be more effective than others
    for specific tasks or situations, and finding the right one can be difficult.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态学习中的一个挑战是确定最佳的融合方式。不同的融合技术可能对特定任务或情况更有效，找到合适的方法可能会很困难。
- en: Movie Rating
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 电影评分
- en: '![Multimodal Models Explained](../Images/2d7dd04a65b17c61467699ec45528ee1.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/2d7dd04a65b17c61467699ec45528ee1.png)'
- en: Image by Author
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: A real-life example of fusion in multimodal learning is a movie recommendation
    system. In this case, the system might use text data (movie descriptions, reviews,
    or user profiles), audio data (soundtracks, dialogue), and visual data (movie
    posters, video clips) to generate personalized recommendations for users.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在多模态学习中的一个实际例子是电影推荐系统。在这种情况下，系统可能会使用文本数据（电影描述、评论或用户档案）、音频数据（原声带、对话）和视觉数据（电影海报、视频片段）来生成个性化的推荐。
- en: The fusion process combines these different sources of information to create
    a more accurate and meaningful understanding of the user's preferences and interests,
    leading to better movie suggestions.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 融合过程将这些不同来源的信息结合起来，以创建对用户偏好和兴趣的更准确和有意义的理解，从而提供更好的电影推荐。
- en: One real-life dataset suitable for developing a movie recommendation system
    with fusion is the [MovieLens dataset](https://grouplens.org/datasets/movielens/).
    MovieLens is a collection of movie ratings and metadata, including user-generated
    tags, collected by the GroupLens Research project at the University of Minnesota.
    The dataset contains information about movies, such as titles, genres, user ratings,
    and user profiles.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 适合开发融合电影推荐系统的一个实际数据集是 [MovieLens 数据集](https://grouplens.org/datasets/movielens/)。MovieLens
    是由明尼苏达大学 GroupLens 研究项目收集的一组电影评分和元数据，包括用户生成的标签。数据集中包含关于电影的信息，如标题、类型、用户评分和用户档案。
- en: To create a multimodal movie recommendation system using the MovieLens dataset,
    you could combine the textual information (movie titles, genres, and tags) with
    additional visual data (movie posters) and audio data (soundtracks, dialogue).
    You can obtain movie posters and audio data from other sources, such as IMDB or
    TMDB.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用 MovieLens 数据集创建一个多模态电影推荐系统，你可以将文本信息（电影标题、类型和标签）与额外的视觉数据（电影海报）和音频数据（原声带、对话）结合起来。你可以从其他来源（如
    IMDB 或 TMDB）获取电影海报和音频数据。
- en: How Fusion Might Be a Challenge?
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 融合可能会面临什么挑战？
- en: Fusion might be challenging when applying multimodal learning to this dataset
    because you need to determine the most effective way to combine the different
    modalities.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在将多模态学习应用于此数据集时，融合可能会面临挑战，因为你需要确定最有效的方式来结合不同的模态。
- en: For example, you need to find the right balance between the importance of textual
    data (genres, tags), visual data (posters), and audio data (soundtracks, dialogue)
    for the recommendation task.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你需要找到文本数据（类型、标签）、视觉数据（海报）和音频数据（原声带、对话）在推荐任务中的重要性之间的正确平衡。
- en: Additionally, some movies may have missing or incomplete data, such as lacking
    posters or audio samples.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些电影可能会缺少或不完整的数据，比如缺少海报或音频样本。
- en: In this case, the recommendation system should be robust enough to handle missing
    data and still provide accurate recommendations based on the available information.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，推荐系统应该足够强大，能够处理缺失的数据，并且根据可用的信息提供准确的推荐。
- en: In summary, using the MovieLens dataset, along with additional visual and audio
    data, you can develop a multimodal movie recommendation system that leverages
    fusion techniques.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，使用 MovieLens 数据集，以及额外的视觉和音频数据，你可以开发一个多模态电影推荐系统，利用融合技术。
- en: However, challenges may arise when determining the most effective fusion method
    and handling missing or incomplete data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在确定最有效的融合方法和处理缺失或不完整的数据时可能会遇到挑战。
- en: Alignment
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对齐
- en: Alignment is a crucial task in applications such as audio-visual speech recognition.
    In this task, audio and visual modalities must be aligned to recognize speech
    accurately.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐在如视听语音识别等应用中是一个关键任务。在这个任务中，音频和视觉模态必须对齐，以准确识别语音。
- en: Researchers have used alignment methods such as the hidden Markov model and
    dynamic time warping to achieve this synchronization.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员使用隐马尔可夫模型和动态时间规整等对齐方法来实现这一同步。
- en: For example, the hidden Markov model can be used to model the relationship between
    the audio and visual modalities and to estimate the alignment between the audio
    waveform and the video frames. Dynamic time warping can be used to align the data
    sequences by stretching or compressing them in time so that they match more closely.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，隐马尔可夫模型可以用来建模音频和视觉模态之间的关系，并估计音频波形与视频帧之间的对齐。动态时间规整可以通过在时间上拉伸或压缩数据序列来对齐它们，从而使它们更紧密地匹配。
- en: Audio-Visual Speech Recognition
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 视听语音识别
- en: '![Multimodal Models Explained](../Images/53c5cb2279932797b612cbc62616ca88.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/53c5cb2279932797b612cbc62616ca88.png)'
- en: Image by Author
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: By aligning the audio and visual data in the GRID Corpus dataset, researchers
    can create coordinated representations that capture the relationships between
    the modalities and then use these representations to recognize speech accurately
    using both modalities.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对齐GRID Corpus数据集中的音频和视觉数据，研究人员可以创建协调的表示，这些表示捕捉了模态之间的关系，然后使用这些表示来准确地使用这两种模态识别语音。
- en: The [GRID Corpus dataset](https://spandh.dcs.shef.ac.uk//gridcorpus/) contains
    audio-visual recordings of speakers producing sentences in English. Each recording
    includes the audio waveform and the video of the speaker's face, which captures
    the movement of the lips and other facial features. The dataset is widely used
    for research in audio-visual speech recognition, where the goal is to recognize
    speech accurately using audio and visual modalities.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[GRID Corpus数据集](https://spandh.dcs.shef.ac.uk//gridcorpus/)包含了讲英语的说话者发音句子的视听录音。每个录音包括音频波形和说话者面部的视频，这些视频捕捉了嘴唇和其他面部特征的运动。该数据集在视听语音识别研究中被广泛使用，其目标是利用音频和视觉模态准确识别语音。'
- en: Translation
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 翻译
- en: The translation is a common multimodal challenge where different modalities
    of data, such as text and images, must be aligned to create a coherent representation.
    One example of such a challenge is the task of image captioning, where an image
    needs to be described in natural language.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译是一个常见的多模态挑战，其中不同的数据模态，如文本和图像，必须对齐以创建一致的表示。例如，图像描述任务就是这样一个挑战，其中需要用自然语言描述图像。
- en: In this task, a model needs to be able to recognize not only the objects and
    context in an image but also generate a natural language description that accurately
    conveys the meaning of the image.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个任务中，模型需要能够识别图像中的对象和背景，并生成准确传达图像意义的自然语言描述。
- en: This requires aligning the visual and textual modalities to create coordinated
    representations that capture the relationships between them.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要对齐视觉和文本模态，以创建协调的表示，捕捉它们之间的关系。
- en: Dall-E
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dall-E
- en: An oil painting of pandas meditating in Tibet
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一幅油画，画中熊猫在西藏冥想
- en: '![An oil painting of pandas meditating in Tibet](../Images/bc8776aab47f0167b68d753d2e30f564.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![一幅油画，画中熊猫在西藏冥想](../Images/bc8776aab47f0167b68d753d2e30f564.png)'
- en: 'Reference: Dall-E'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 参考：Dall-E
- en: One recent example of a model that can perform multimodal translation is DALL-E2\.
    DALL-E2 is a neural network model developed by OpenAI that can generate high-quality
    images from textual descriptions. It can also generate textual descriptions from
    images, effectively translating between the visual and textual modalities.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的一个多模态翻译模型的例子是DALL-E2。DALL-E2是由OpenAI开发的神经网络模型，它可以根据文本描述生成高质量的图像。它也可以根据图像生成文本描述，从而有效地在视觉和文本模态之间进行翻译。
- en: DALL-E2 achieves this by learning a joint representation space that captures
    the relationships between the visual and textual modalities.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: DALL-E2通过学习一个联合表示空间来实现这一点，该空间捕捉了视觉模态和文本模态之间的关系。
- en: The model is trained on a large dataset of the image-caption pairs and learns
    to associate images with their corresponding captions. It can then generate images
    from textual descriptions by sampling from the learned representation space and
    generate textual descriptions from images by decoding the learned representation.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在一个大规模的图像-标题对数据集上进行训练，学习将图像与其对应的标题关联。然后，它可以通过从学习的表示空间中采样来生成图像，也可以通过解码学习的表示从图像生成文本描述。
- en: Overall, multimodal translation is a significant challenge that requires aligning
    different modalities of data to create coordinated representations. Models like
    DALL-E2 can perform this task by learning joint representation spaces that capture
    the relationships between the visual and textual modalities and can be applied
    to tasks such as image captioning and visual question answering.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，多模态翻译是一个重大挑战，需要对不同的数据模态进行对齐以创建协调的表示。像 DALL-E2 这样的模型可以通过学习联合表示空间来捕捉视觉和文本模态之间的关系，并应用于图像描述和视觉问答等任务。
- en: Co-learning
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共学习
- en: Multimodal co-learning aims to transfer knowledge learned through one or more
    modalities to tasks involving another.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态共学习旨在将通过一个或多个模态学到的知识转移到涉及其他任务中。
- en: Co-learning is especially important in low-resource target tasks, entirely/partly
    missing or noisy modalities.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 共学习在低资源目标任务中，完全/部分缺失或噪声模态尤其重要。
- en: However, finding effective methods to transfer knowledge from one modality to
    another while retaining the semantic meaning is a significant challenge in multimodal
    learning.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，找到有效的知识转移方法以保持语义含义是在多模态学习中的一个重大挑战。
- en: In medical diagnosis, different medical imaging modalities, such as CT scans
    and MRI scans, provide complementary information for a diagnosis. Multimodal co-learning
    can be used to combine these modalities to improve the accuracy of the diagnosis.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在医学诊断中，不同的医学成像模态，如 CT 扫描和 MRI 扫描，提供了对诊断的互补信息。多模态共学习可以用来结合这些模态，以提高诊断的准确性。
- en: Multimodal Tumor Segmentation
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多模态肿瘤分割
- en: '![Multimodal Models Explained](../Images/fad94eee408277eec7ae72db7f10211f.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解析](../Images/fad94eee408277eec7ae72db7f10211f.png)'
- en: 'Source: [https://www.med.upenn.edu/sbia/brats2018.html](https://www.med.upenn.edu/sbia/brats2018.html)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://www.med.upenn.edu/sbia/brats2018.html](https://www.med.upenn.edu/sbia/brats2018.html)
- en: For instance, in the case of brain tumors, MRI scans provide high-resolution
    images of soft tissues, while CT scans provide detailed images of the bone structure.
    Combining these modalities can provide a complete picture of the patient's condition
    and inform treatment decisions.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在脑肿瘤的情况下，MRI 扫描提供了高分辨率的软组织图像，而 CT 扫描提供了骨结构的详细图像。结合这些模态可以提供患者病情的完整图像，并指导治疗决策。
- en: A dataset that includes MRI and CT scans of brain tumors for use in multimodal
    co-learning is the multimodal [Brain Tumor Segmentation (BraTS) dataset](https://www.kaggle.com/datasets/sanglequang/brats2018).
    This dataset includes MRI and CT scans of brain tumors and annotations for segmentation
    of the tumor regions.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 包括脑肿瘤的 MRI 和 CT 扫描的多模态[脑肿瘤分割（BraTS）数据集](https://www.kaggle.com/datasets/sanglequang/brats2018)用于多模态共学习。该数据集包含脑肿瘤的
    MRI 和 CT 扫描以及肿瘤区域分割的注释。
- en: To implement co-learning with the MRI and CT scans of brain tumors, we need
    to develop an approach that combines the information from both modalities in a
    way that improves the accuracy of the diagnosis. One possible approach is to use
    a multimodal deep learning model trained on MRI and CT scans.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实施包含脑肿瘤的 MRI 和 CT 扫描的共学习，我们需要开发一种将两种模态的信息结合起来以提高诊断准确性的方式。一种可能的方法是使用在 MRI 和
    CT 扫描上训练的多模态深度学习模型。
- en: Popular Applications of Multimodal Learning
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态学习的流行应用
- en: We will mention several other applications of multimodal learning, like speech
    recognition and autonomous cars.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将提到多模态学习的其他几个应用，如语音识别和自动驾驶汽车。
- en: Speech Recognition
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语音识别
- en: '![Multimodal Models Explained](../Images/b3656f1c5b5fdff71bc8d79d9c6c6f5a.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解析](../Images/b3656f1c5b5fdff71bc8d79d9c6c6f5a.png)'
- en: Image by Author
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Multimodal learning can improve speech recognition accuracy by combining audio
    and visual data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态学习可以通过结合音频和视觉数据来提高语音识别的准确性。
- en: For instance, a multimodal model can analyze both the audio signal of speech
    and corresponding lip movements to improve speech recognition accuracy. By combining
    audio and visual modalities, multimodal models can reduce the effects of noise
    and variability in speech signals, leading to improved speech recognition performance.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，多模态模型可以同时分析语音的音频信号和对应的唇部运动，以提高语音识别的准确性。通过结合音频和视觉模态，多模态模型可以减少噪音和语音信号变异的影响，从而提高语音识别性能。
- en: '![Multimodal Models Explained](../Images/2dbc1ebed16337fffc221e842f71b485.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/2dbc1ebed16337fffc221e842f71b485.png)'
- en: 'Source: [CMU-MOSEI Dataset](http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [CMU-MOSEI数据集](http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/)'
- en: One example of a multimodal dataset that can be used for speech recognition
    is the [CMU-MOSEI dataset](http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/).
    This dataset contains 23,500 sentences pronounced by 1,000 Youtube speakers and
    includes both audio and visual data of the speakers.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可以用于语音识别的多模态数据集例子是[CMU-MOSEI数据集](http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/)。这个数据集包含了1,000名YouTube讲者发音的23,500个句子，并包括讲者的音频和视觉数据。
- en: The dataset can be used to develop multimodal models for emotion recognition,
    sentiment analysis, and speaker identification.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集可以用于开发用于情感识别、情绪分析和说话人识别的多模态模型。
- en: By combining the audio signal of speech with the visual characteristics of the
    speaker, multimodal models can improve the accuracy of speech recognition and
    other related tasks.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将语音的音频信号与说话者的视觉特征结合起来，多模态模型可以提高语音识别和其他相关任务的准确性。
- en: Autonomous Car
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动驾驶汽车
- en: '![Multimodal Models Explained](../Images/4ebc80d86e52ceb02b7f7169f982cb75.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/4ebc80d86e52ceb02b7f7169f982cb75.png)'
- en: 'Source: [Waymo; Business Insider](https://www.businessinsider.in/transportation/heres-how-waymos-brand-new-self-driving-cars-see-the-world/articleshow/56648715.cms)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [Waymo; Business Insider](https://www.businessinsider.in/transportation/heres-how-waymos-brand-new-self-driving-cars-see-the-world/articleshow/56648715.cms)'
- en: Multimodal learning can be used to enhance the capabilities of robots by integrating
    information from multiple sensors.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态学习可以通过整合来自多个传感器的信息来增强机器人的能力。
- en: For instance, it is essential in developing self-driving cars, which rely on
    information from multiple sensors such as cameras, lidar, and radar to navigate
    and make decisions.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在开发自动驾驶汽车时，这一点至关重要，因为这些汽车依赖于来自多个传感器的信息，如摄像头、激光雷达和雷达，以进行导航和决策。
- en: Multimodal learning can help to integrate information from these sensors, allowing
    the car to perceive and react to its environment more accurately and efficiently.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态学习可以帮助整合这些传感器的信息，使汽车能够更准确、高效地感知和反应环境。
- en: One example of a dataset for self-driving cars is the [Waymo Open Dataset](https://waymo.com/open/),
    which includes high-resolution sensor data from Waymo's self-driving cars, along
    with labels for objects such as vehicles, pedestrians, and cyclists. Waymo is
    Google’s self-driving car company.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自动驾驶汽车数据集的例子是[Waymo开放数据集](https://waymo.com/open/)，它包括Waymo自动驾驶汽车的高分辨率传感器数据，以及车辆、行人和骑自行车者等物体的标签。Waymo是谷歌的自动驾驶汽车公司。
- en: The dataset can be used to develop and evaluate multimodal models for various
    tasks related to self-driving cars, such as object detection, tracking, and predictions.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集可以用于开发和评估用于自动驾驶汽车相关任务的多模态模型，如物体检测、跟踪和预测。
- en: Voice Recording Analysis
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语音录音分析
- en: '![Multimodal Models Explained](../Images/0004f1ba42c9905680540a9424c7d7ed.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/0004f1ba42c9905680540a9424c7d7ed.png)'
- en: Image by Author
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: The [Voice Recordings Analysis project](https://platform.stratascratch.com/data-projects/voice-recordings-analysis?utm_source=blog&utm_medium=click&utm_campaign=kdn+multimodal+models)
    came up during the interviews for the data science positions at Sandvik. It is
    an excellent example of a multimodal learning application, as it seeks to predict
    a person's gender based on vocal features derived from audio data.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[语音录音分析项目](https://platform.stratascratch.com/data-projects/voice-recordings-analysis?utm_source=blog&utm_medium=click&utm_campaign=kdn+multimodal+models)是在Sandvik的数据科学职位面试过程中提出的。它是多模态学习应用的一个优秀例子，因为它试图基于音频数据中的语音特征预测一个人的性别。'
- en: 'In this scenario, the problem involves analyzing and processing information
    from two distinct modalities: audio signals and textual features. These modalities
    contribute valuable information that can enhance the accuracy and effectiveness
    of the predictive model.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，问题涉及分析和处理来自两个不同模态的信息：音频信号和文本特征。这些模态提供了宝贵的信息，可以提高预测模型的准确性和有效性。
- en: 'Expanding on the multimodal nature of this project:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展该项目的多模态特性：
- en: Audio Signals
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 音频信号
- en: '![Multimodal Models Explained](../Images/eed8b5091a98e39f7ebd8d79e398a0fa.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/eed8b5091a98e39f7ebd8d79e398a0fa.png)'
- en: Image by Author
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: The primary data source in this project is the audio recordings of the English-speaking
    male and female voices. These audio signals contain rich and complex information
    about the speaker's vocal characteristics. By extracting relevant features from
    these audio signals, such as pitch, frequency, and spectral entropy, the model
    can identify patterns and trends that relate to gender-specific vocal properties.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的主要数据来源是英语男性和女性声音的音频录音。这些音频信号包含了关于说话者声音特征的丰富而复杂的信息。通过提取这些音频信号中的相关特征，如音高、频率和谱熵，模型可以识别与性别特定声音属性相关的模式和趋势。
- en: Textual Features
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本特征
- en: '![Multimodal Models Explained](../Images/e01ddcdf37c2f571a7368e4f8a47bc98.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模型解释](../Images/e01ddcdf37c2f571a7368e4f8a47bc98.png)'
- en: Image by Author
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: Accompanying each audio recording is a text file that provides crucial information
    about the sample, such as the speaker's gender, the language spoken, and the phrase
    uttered by the person. This text file not only offers the ground truth (gender
    labels) for training and evaluating the machine learning models but can also be
    used to create additional features in combination with the audio data. By leveraging
    the information in the text file, the model can better understand the context
    and content of each audio sample, potentially improving its overall predictive
    performance.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 每个音频录音都附带一个文本文件，该文件提供有关样本的重要信息，如说话者的性别、所说语言以及说话者所说的短语。这个文本文件不仅为训练和评估机器学习模型提供了真实标签（性别标签），还可以与音频数据结合创建额外的特征。通过利用文本文件中的信息，模型可以更好地理解每个音频样本的上下文和内容，从而可能提高其整体预测性能。
- en: So, the Voice Recordings Analysis project exemplifies a multimodal learning
    application by leveraging data from multiple modalities, audio signals, and textual
    features to predict a person's gender using extracted vocal characteristics.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，语音录音分析项目通过利用来自多种模态的数据、音频信号和文本特征来预测一个人的性别，从而体现了多模态学习应用的实例。
- en: This approach highlights the importance of considering different data types
    when developing machine learning models, as it can help uncover hidden patterns
    and relationships that may not be apparent when analyzing each modality in isolation.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法强调在开发机器学习模型时考虑不同数据类型的重要性，因为这有助于发现隐藏的模式和关系，这些模式和关系在单独分析每种模态时可能不会显现出来。
- en: Conclusion
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In summary, multimodal learning has become a potent tool for integrating diverse
    data to enhance the power of the precision of [machine learning algorithms](https://www.stratascratch.com/blog/machine-learning-algorithms-you-should-know-for-data-science/?utm_source=blog&utm_medium=click&utm_campaign=kdn+multimodal+models).
    Combining different data types, including text, audio, and visual information,
    can yield more robust and accurate predictions. This is particularly true in speech
    recognition, text and image fusion, and the autonomous car industry.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，多模态学习已成为整合多样数据以提高[机器学习算法](https://www.stratascratch.com/blog/machine-learning-algorithms-you-should-know-for-data-science/?utm_source=blog&utm_medium=click&utm_campaign=kdn+multimodal+models)精度的强大工具。结合不同的数据类型，包括文本、音频和视觉信息，可以产生更稳健和准确的预测。这在语音识别、文本与图像融合以及自动驾驶汽车行业尤为重要。
- en: Yet, multimodal learning does come with several challenges, such as those related
    to representation, fusion, alignment, translation, and co-learning. This necessitates
    careful consideration and attention.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，多模态学习确实存在若干挑战，如表示、融合、对齐、翻译和共同学习等问题。这需要谨慎考虑和关注。
- en: Nevertheless, as machine learning techniques and computing power continue to
    evolve, we can anticipate the emergence of even more advanced multimodal in the
    years to come.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，随着机器学习技术和计算能力的不断发展，我们可以预见未来几年会出现更先进的多模态技术。
- en: '**[Nate Rosidi](https://www.stratascratch.com)** is a data scientist and in
    product strategy. He''s also an adjunct professor teaching analytics, and is the
    founder of [StrataScratch](https://www.stratascratch.com/), a platform helping
    data scientists prepare for their interviews with real interview questions from
    top companies. Connect with him on [Twitter: StrataScratch](https://twitter.com/StrataScratch)
    or [LinkedIn](https://www.linkedin.com/in/nathanrosidi/).'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**[内特·罗西迪](https://www.stratascratch.com)** 是一名数据科学家，专注于产品战略。他还是一名兼职教授，教授分析学，并且是
    [StrataScratch](https://www.stratascratch.com/) 的创始人，这个平台帮助数据科学家准备来自顶级公司的真实面试问题。可以在
    [Twitter: StrataScratch](https://twitter.com/StrataScratch) 或 [LinkedIn](https://www.linkedin.com/in/nathanrosidi/)
    上与他联系。'
- en: More On This Topic
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Multimodal Grounded Learning with Vision and Language](https://www.kdnuggets.com/2022/11/multimodal-grounded-learning-vision-language.html)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基于视觉和语言的多模态基础学习](https://www.kdnuggets.com/2022/11/multimodal-grounded-learning-vision-language.html)'
- en: '[Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](https://www.kdnuggets.com/introduction-to-nextgpt-anytoany-multimodal-large-language-model)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NExT-GPT介绍：任意到任意的多模态大型语言模型](https://www.kdnuggets.com/introduction-to-nextgpt-anytoany-multimodal-large-language-model)'
- en: '[Large Language Models Explained in 3 Levels of Difficulty](https://www.kdnuggets.com/large-language-models-explained-in-3-levels-of-difficulty)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[三种难度等级解释的大型语言模型](https://www.kdnuggets.com/large-language-models-explained-in-3-levels-of-difficulty)'
- en: '[5 Machine Learning Models Explained in 5 Minutes](https://www.kdnuggets.com/5-machine-learning-models-explained-in-5-minutes)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5分钟解释5种机器学习模型](https://www.kdnuggets.com/5-machine-learning-models-explained-in-5-minutes)'
- en: '[Decision Tree Algorithm, Explained](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[决策树算法解释](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)'
- en: '[Key-Value Databases, Explained](https://www.kdnuggets.com/2021/04/nosql-explained-understanding-key-value-databases.html)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[键值数据库解释](https://www.kdnuggets.com/2021/04/nosql-explained-understanding-key-value-databases.html)'
