- en: 'LGBMClassifier: A Getting Started Guide'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'LGBMClassifier: 入门指南'
- en: 原文：[https://www.kdnuggets.com/2023/07/lgbmclassifier-gettingstarted-guide.html](https://www.kdnuggets.com/2023/07/lgbmclassifier-gettingstarted-guide.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/07/lgbmclassifier-gettingstarted-guide.html](https://www.kdnuggets.com/2023/07/lgbmclassifier-gettingstarted-guide.html)
- en: '![LGBMClassifier: A Getting-Started Guide](../Images/9691d4e5776180b0d0451b093e2119c0.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![LGBMClassifier: 入门指南](../Images/9691d4e5776180b0d0451b093e2119c0.png)'
- en: Image by Editor
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑者提供的图片
- en: There are a vast number of machine learning algorithms that are apt to model
    specific phenomena. While some models utilize a set of attributes to outperform
    others, others include weak learners to utilize the remainder of attributes for
    providing additional information to the model, known as ensemble models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法种类繁多，适合于建模特定现象。一些模型利用一组属性来优于其他模型，而另一些模型则包括弱学习者，以利用剩余属性为模型提供额外信息，这就是集成模型。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'The premise of the ensemble models is to improve the model performance by combining
    the predictions from different models by reducing their errors. There are two
    popular ensembling techniques: bagging and boosting.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型的前提是通过结合不同模型的预测以减少其误差来提高模型性能。有两种流行的集成技术：bagging和boosting。
- en: Bagging, aka Bootstrapped Aggregation, trains multiple individual models on
    different random subsets of the training data and then averages their predictions
    to produce the final prediction. Boosting, on the other hand, involves training
    individual models sequentially, where each model attempts to correct the errors
    made by the previous models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging，又称为自助聚合，在不同的随机子集上训练多个单独的模型，然后平均它们的预测结果以生成最终预测。而提升（Boosting）则涉及顺序训练单独的模型，每个模型试图纠正前一个模型的错误。
- en: Now that we have context about the ensemble models, let us double-click on the
    boosting ensemble model, specifically the Light GBM (LGBM) algorithm developed
    by Microsoft.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们了解了集成模型的背景，让我们深入探讨一下提升集成模型，特别是微软开发的Light GBM（LGBM）算法。
- en: What is LGBMClassifier?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是LGBMClassifier？
- en: LGBMClassifier stands for Light Gradient Boosting Machine Classifier. It uses
    decision tree algorithms for ranking, classification, and other machine-learning
    tasks. LGBMClassifier uses a novel technique of Gradient-based One-Side Sampling
    (GOSS) and Exclusive Feature Bundling (EFB) to handle large-scale data with accuracy,
    effectively making it faster and reducing memory usage.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LGBMClassifier代表Light Gradient Boosting Machine Classifier。它使用决策树算法进行排序、分类和其他机器学习任务。LGBMClassifier使用一种新颖的基于梯度的单边采样（GOSS）和独占特征捆绑（EFB）技术，能够准确地处理大规模数据，从而提高速度并减少内存使用。
- en: What is Gradient-based One-Side Sampling (GOSS)?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是基于梯度的单边采样（GOSS）？
- en: Traditional gradient boosting algorithms use all the data for training, which
    can be time-consuming when dealing with large datasets. LightGBM's GOSS, on the
    other hand, keeps all the instances with large gradients and performs random sampling
    on the instances with small gradients. The intuition behind this is that instances
    with large gradients are harder to fit and thus carry more information. GOSS introduces
    a constant multiplier for the data instances with small gradients to compensate
    for the information loss during sampling.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的梯度提升算法使用所有数据进行训练，这在处理大型数据集时可能非常耗时。而LightGBM的GOSS则保留了所有大梯度的实例，并对小梯度的实例进行随机采样。其背后的直觉是，大梯度的实例更难拟合，因此包含更多信息。GOSS为小梯度的数据实例引入了一个常数乘数，以弥补采样过程中信息的丧失。
- en: What is Exclusive Feature Bundling (EFB)?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是独占特征捆绑（EFB）？
- en: In a sparse dataset, most of the features are zeros. EFB is a near-lossless
    algorithm that bundles/combines mutually exclusive features (features that are
    not non-zero simultaneously) to reduce the number of dimensions, thereby accelerating
    the training process. Since these features are "exclusive", the original feature
    space is retained without significant information loss.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在稀疏数据集中，大多数特征为零。EFB是一种近乎无损的算法，它将相互排斥的特征（即不同时为非零的特征）进行捆绑/组合，以减少维度，从而加速训练过程。由于这些特征是“排斥”的，因此保留了原始特征空间，没有显著的信息损失。
- en: Installation
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装
- en: 'The LightGBM package can be installed directly using pip – python''s package
    manager. Type the command shared below either on the terminal or command prompt
    to download and install the LightGBM library onto your machine:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 可以直接使用pip——Python的包管理器来安装LightGBM。在终端或命令提示符中输入以下命令，以下载和安装LightGBM库到你的机器上：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Anaconda users can install it using the “conda install” command as listed below.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Anaconda用户可以使用以下列出的“conda install”命令来安装它。
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Based on your OS, you can choose the installation method using [this guide](https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的操作系统，你可以使用[本指南](https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html)选择安装方法。
- en: Hands-On!
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践操作！
- en: 'Now, let''s import LightGBM and other necessary libraries:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们导入LightGBM和其他必要的库：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Preparing the Dataset
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据集
- en: 'We are using the popular Titanic dataset, which contains information about
    the passengers on the Titanic, with the target variable signifying whether they
    survived or not. You can download the dataset from [Kaggle](https://www.kaggle.com/competitions/titanic/data)
    or use the following code to load it directly from Seaborn, as shown below:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用流行的泰坦尼克号数据集，其中包含关于泰坦尼克号乘客的信息，目标变量表示他们是否幸存。你可以从[Kaggle](https://www.kaggle.com/competitions/titanic/data)下载数据集，或者使用以下代码直接从Seaborn加载，如下所示：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Drop unnecessary columns such as “deck”, “embark_town”, and “alive” because
    they are redundant or do not contribute to the survival of any person on the ship.
    Next, we observed that the features “age”, “fare”, and “embarked” have missing
    values – note that different attributes are imputed with appropriate statistical
    measures.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 删除不必要的列，如“deck”、“embark_town”和“alive”，因为它们冗余或不影响船上人员的生存。接下来，我们观察到“age”、“fare”和“embarked”特征有缺失值——注意不同属性用适当的统计措施进行填补。
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Lastly, we convert the categorical variables to numerical variables using pandas'
    categorical codes. Now, the data is prepared to start the model training process.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用pandas的分类代码将分类变量转换为数字变量。现在，数据已准备好开始模型训练过程。
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Training the LGBMClassifier Model
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练LGBMClassifier模型
- en: To begin training the LGBMClassifier model, we need to split the dataset into
    input features and target variables, as well as training and testing sets using
    the train_test_split function from scikit-learn.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始训练LGBMClassifier模型，我们需要使用scikit-learn中的train_test_split函数将数据集拆分为输入特征和目标变量，以及训练集和测试集。
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s label encode categorical (“who”) and ordinal data (“class”) to ensure
    that the model is supplied with numerical data, as LGBM doesn’t consume non-numerical
    data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对分类数据（“who”）和序数数据（“class”）进行标签编码，以确保模型接收数字数据，因为LGBM不处理非数字数据。
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, we specify the model hyperparameters as arguments to the constructor,
    or we can pass them as a dictionary to the set_params method.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将模型超参数指定为构造函数的参数，或者可以将它们作为字典传递给set_params方法。
- en: The last step to initiate the model training is to load the dataset by creating
    an instance of the LGBMClassifier class and fitting it to the training data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 启动模型训练的最后一步是通过创建LGBMClassifier类的实例并将其拟合到训练数据上来加载数据集。
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Next, let us evaluate the trained classifier’s performance on the unseen or
    test dataset.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们评估训练好的分类器在未见或测试数据集上的表现。
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Hyperparameter Tuning
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调整
- en: 'The LGBMClassifier allows for much flexibility via hyperparameters which you
    can tune for optimal performance. Here, we will briefly discuss some of the key
    hyperparameters:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: LGBMClassifier通过超参数提供了很大的灵活性，你可以调整这些超参数以获得最佳性能。这里，我们将简要讨论一些关键的超参数：
- en: '**num_leaves**: This is the main parameter to control the complexity of the
    tree model. Ideally, the value of num_leaves should be less than or equal to 2^(max_depth).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**num_leaves**：这是控制树模型复杂度的主要参数。理想情况下，num_leaves的值应该小于或等于2^(max_depth)。'
- en: '**min_data_in_leaf**: This is an important parameter to prevent overfitting
    in a leaf-wise tree. Its optimal value depends on the number of training samples
    and num_leaves.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**min_data_in_leaf**：这是防止叶子节点树过拟合的重要参数。其最佳值取决于训练样本的数量和 num_leaves。'
- en: '**max_depth**: You can use this to limit the tree depth explicitly. It''s best
    to tune this parameter in case of overfitting.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_depth**：你可以使用这个参数来明确限制树的深度。如果出现过拟合，最好调整这个参数。'
- en: 'Let''s tune these hyperparameters and train a new model:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调整这些超参数并训练一个新模型：
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note that the actual tuning of hyperparameters is a process that involves trial
    and error and may also be guided by experience and a deeper understanding of the
    boosting algorithm and subject matter expertise (domain knowledge) of the business
    problem you're working on.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，超参数的实际调整是一个涉及试验和错误的过程，也可能受到经验以及对提升算法和业务问题（领域知识）的深入理解的指导。
- en: In this post, you learned about the LightGBM algorithm and its Python implementation.
    It is a flexible technique that is useful for various types of classification
    problems and should be a part of your machine-learning toolkit.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你了解了 LightGBM 算法及其 Python 实现。这是一种灵活的技术，适用于各种类型的分类问题，应该成为你机器学习工具包的一部分。
- en: '**[Vidhi Chugh](https://vidhi-chugh.medium.com/)** is an AI strategist and
    a digital transformation leader working at the intersection of product, sciences,
    and engineering to build scalable machine learning systems. She is an award-winning
    innovation leader, an author, and an international speaker. She is on a mission
    to democratize machine learning and break the jargon for everyone to be a part
    of this transformation.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Vidhi Chugh](https://vidhi-chugh.medium.com/)** 是一位人工智能战略家和数字化转型领导者，她在产品、科学和工程交汇处工作，致力于构建可扩展的机器学习系统。她是一位屡获殊荣的创新领导者、作者和国际演讲者。她的使命是使机器学习民主化，并打破术语，使每个人都能参与这场转型。'
- en: More On This Topic
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题
- en: '[Getting Started with Automated Text Summarization](https://www.kdnuggets.com/2019/11/getting-started-automated-text-summarization.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动化文本摘要入门](https://www.kdnuggets.com/2019/11/getting-started-automated-text-summarization.html)'
- en: '[Getting Started Cleaning Data](https://www.kdnuggets.com/2022/01/getting-started-cleaning-data.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据清理入门](https://www.kdnuggets.com/2022/01/getting-started-cleaning-data.html)'
- en: '[Getting Started with SQL Cheatsheet](https://www.kdnuggets.com/2022/08/getting-started-sql-cheatsheet.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQL 入门备忘单](https://www.kdnuggets.com/2022/08/getting-started-sql-cheatsheet.html)'
- en: '[Getting Started with spaCy for NLP](https://www.kdnuggets.com/2022/11/getting-started-spacy-nlp.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 spaCy 进行 NLP 入门](https://www.kdnuggets.com/2022/11/getting-started-spacy-nlp.html)'
- en: '[Getting Started with PyCaret](https://www.kdnuggets.com/2022/11/getting-started-pycaret.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyCaret 入门指南](https://www.kdnuggets.com/2022/11/getting-started-pycaret.html)'
- en: '[Getting Started with PyTorch Lightning](https://www.kdnuggets.com/2022/12/getting-started-pytorch-lightning.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch Lightning 入门指南](https://www.kdnuggets.com/2022/12/getting-started-pytorch-lightning.html)'
