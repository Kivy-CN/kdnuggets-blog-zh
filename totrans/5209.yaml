- en: The Benefits & Examples of Using Apache Spark with PySpark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/04/benefits-apache-spark-pyspark.html](https://www.kdnuggets.com/2020/04/benefits-apache-spark-pyspark.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '**What is Apache Spark? **'
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Apache Spark](https://spark.apache.org/) is one of the hottest new trends
    in the technology domain. It is the framework with probably the highest potential
    to realize the fruit of the marriage between Big Data and Machine Learning.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: It runs fast (up to 100x faster than traditional [Hadoop MapReduce](https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm) due
    to in-memory operation, offers robust, distributed, fault-tolerant data objects
    (called [RDD](https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm)),
    and integrates beautifully with the world of machine learning and graph analytics
    through supplementary packages like [Mlib](https://spark.apache.org/mllib/) and [GraphX](https://spark.apache.org/graphx/).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '![The Benefits & Examples of Using Apache Spark with PySpark in Python | Apache
    Spark](../Images/30cb3911f9df0322fd922f31ff23852a.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: Spark is implemented on [Hadoop/HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html) and
    written mostly in [Scala](https://www.scala-lang.org/), a functional programming
    language, similar to Java. In fact, Scala needs the latest Java installation on
    your system and runs on JVM. However, for most beginners, Scala is not a language
    that they learn first to venture into the world of data science. Fortunately,
    Spark provides a wonderful Python integration, called **PySpark**, which lets
    Python programmers to interface with the Spark framework and learn how to manipulate
    data at scale and work with objects and algorithms over a distributed file system.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will learn the basics of PySpark. There are a lot of concepts
    (constantly evolving and introduced), and therefore, we just focus on fundamentals
    with a few simple examples. Readers are encouraged to build on these and explore
    more on their own.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '**The Short History of Apache Spark**'
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apache Spark started as a research project at the UC Berkeley AMPLab in 2009,
    and was open sourced in early 2010\. It was a class project at UC Berkeley. Idea
    was to build a cluster management framework, which can support different kinds
    of cluster computing systems. Many of the ideas behind the system were presented
    in various research papers over the years. After being released, Spark grew into
    a broad developer community, and moved to the Apache Software Foundation in 2013\.
    Today, the project is developed collaboratively by a community of hundreds of
    developers from hundreds of organizations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 起初是 2009 年在 UC Berkeley AMPLab 的一个研究项目，并于 2010 年初开源。它最初是 UC Berkeley
    的一个课程项目，目的是建立一个集群管理框架，支持不同类型的集群计算系统。多年来，系统背后的许多理念在各种研究论文中提出。发布后，Spark 成长为一个广泛的开发者社区，并于
    2013 年迁移到 Apache 软件基金会。如今，该项目由来自数百个组织的数百名开发者共同开发。
- en: '**Spark is Not a Programming Language**'
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**Spark 不是一种编程语言**'
- en: One thing to remember is that Spark is not a programming language like Python
    or Java. It is a general-purpose distributed data processing engine, suitable
    for use in a wide range of circumstances. It is particularly useful for big data
    processing both at scale and with high speed.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一点是，Spark 不是像 Python 或 Java 那样的编程语言。它是一个通用的分布式数据处理引擎，适用于广泛的场景。它特别适合大规模和高速度的大数据处理。
- en: Application developers and data scientists generally incorporate Spark into
    their applications to rapidly query, analyze, and transform data at scale. Some
    of the tasks that are most frequently associated with Spark, include, – ETL and
    SQL batch jobs across large data sets (often of terabytes of size), – processing
    of streaming data from IoT devices and nodes, data from various sensors, financial
    and transactional systems of all kinds, and – machine learning tasks for e-commerce
    or IT applications.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 应用开发者和数据科学家通常将 Spark 集成到他们的应用程序中，以快速查询、分析和转换大规模数据。一些最常与 Spark 相关联的任务包括：– 跨大型数据集（通常为
    TB 级别）的 ETL 和 SQL 批处理作业，– 处理来自 IoT 设备和节点的流数据、各种传感器的数据、各种金融和事务系统的数据，以及 – 电子商务或
    IT 应用的机器学习任务。
- en: 'At its core, Spark builds on top of the Hadoop/HDFS framework for handling
    distributed files. It is mostly implemented with Scala, a functional language
    variant of Java. There is a core Spark data processing engine, but on top of that,
    there are many libraries developed for SQL-type query analysis, distributed machine
    learning, large-scale graph computation, and streaming data processing. Multiple
    programming languages are supported by Spark in the form of easy interface libraries:
    Java, Python, Scala, and R.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，Spark 建立在处理分布式文件的 Hadoop/HDFS 框架之上。它主要用 Scala 实现，Scala 是 Java 的一种函数式语言变体。虽然有一个核心的
    Spark 数据处理引擎，但在其之上，还有许多用于 SQL 类型查询分析、分布式机器学习、大规模图计算和流数据处理的库。Spark 支持多种编程语言，以易于使用的接口库形式提供：Java、Python、Scala
    和 R。
- en: '**Spark Uses the MapReduce Paradigm for Distributed Processing**'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**Spark 使用 MapReduce 范式进行分布式处理**'
- en: The basic idea of distributed processing is to divide the data chunks into small
    manageable pieces (including some filtering and sorting), bring the computation
    close to the data i.e. use small nodes of a large cluster for specific jobs and
    then re-combine them back. The dividing portion is called the ‘Map’ action and
    the recombination is called the ‘Reduce’ action. Together, they make the famous
    ‘MapReduce’ paradigm, which was introduced by Google around 2004 (see the [original
    paper here](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式处理的基本思想是将数据块分割成小的可管理的部分（包括一些过滤和排序），将计算任务接近数据，即在大型集群的各个小节点上执行特定任务，然后再将它们重新组合。分割部分称为“Map”操作，重新组合部分称为“Reduce”操作。它们共同构成了著名的“MapReduce”范式，该范式由
    Google 于 2004 年左右引入（见[原始论文](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)）。
- en: For example, if a file has 100 records to be processed, 100 mappers can run
    together to process one record each. Or maybe 50 mappers can run together to process
    two records each. After all the mappers complete processing, the framework shuffles
    and sorts the results before passing them on to the reducers. A reducer cannot
    start while a mapper is still in progress. All the map output values that have
    the same key are assigned to a single reducer, which then aggregates the values
    for that key.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个文件有 100 条记录需要处理，100 个映射器可以一起运行，每个处理一条记录。或者，也许 50 个映射器可以一起运行，每个处理两条记录。所有映射器完成处理后，框架会在将结果传递给减少器之前进行洗牌和排序。一个减少器在映射器仍在进行时不能开始。所有具有相同键的映射输出值会被分配给一个减少器，减少器然后对该键的值进行聚合。
- en: '**How to Setup PySpark**'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**如何设置 PySpark**'
- en: If you’re already familiar with Python and libraries such as Pandas and Numpy,
    then PySpark is a great extension/framework to learn in order to create more scalable,
    data-intensive analyses and pipelines by utilizing the power of Spark in the background.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经熟悉 Python 及 Pandas 和 Numpy 等库，那么 PySpark 是一个很好的扩展/框架，利用 Spark 的强大功能，你可以创建更具可扩展性、数据密集型的分析和管道。
- en: The exact process of installing and setting up PySpark environment (on a standalone
    machine) is somewhat involved and can vary slightly depending on your system and
    environment. The goal is to get your regular Jupyter data science environment
    working with Spark in the background using the PySpark package.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 安装和设置 PySpark 环境（在独立机器上）的确切过程有些复杂，并且可能会根据你的系统和环境略有不同。目标是使你的常规 Jupyter 数据科学环境在后台使用
    PySpark 包与 Spark 一起工作。
- en: '[**This article**](https://medium.com/free-code-camp/how-to-set-up-pyspark-for-your-jupyter-notebook-7399dd3cb389) on
    Medium provides more details on the step-by-step setup process.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[**这篇文章**](https://medium.com/free-code-camp/how-to-set-up-pyspark-for-your-jupyter-notebook-7399dd3cb389)
    在 Medium 上提供了关于逐步设置过程的更多细节。'
- en: '![](../Images/881536150793aa5d361af9092960c036.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/881536150793aa5d361af9092960c036.png)'
- en: Alternatively, you can use Databricks setup for practicing Spark. This company
    was created by the original creators of Spark and have an excellent ready-to-launch
    environment to do distributed analysis with Spark.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你可以使用 Databricks 设置来练习 Spark。这家公司由 Spark 的原始创建者创建，并提供了一个出色的即用环境来进行分布式分析。
- en: But the idea is always the same. You are distributing (and replicating) your
    large dataset in small fixed chunks over many nodes. You then bring the compute
    engine close to them so that the whole operation is parallelized, fault-tolerant
    and scalable.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 但想法总是一样的。你将大型数据集分布（并复制）到许多节点上的小固定块中。然后你将计算引擎靠近它们，使整个操作实现并行化、容错和可扩展。
- en: By working with PySpark and Jupyter notebook, you can learn all these concepts
    without spending anything on AWS or Databricks platform. You can also easily interface
    with SparkSQL and MLlib for database manipulation and machine learning. It will
    be much easier to start working with real-life large clusters if you have internalized
    these concepts beforehand!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 PySpark 和 Jupyter notebook，你可以在不花费 AWS 或 Databricks 平台费用的情况下学习所有这些概念。你还可以轻松与
    SparkSQL 和 MLlib 接口，用于数据库操作和机器学习。如果你事先掌握了这些概念，那么开始使用真实的大型集群将会容易得多！
- en: '**Resilient Distributed Dataset (RDD) and SparkContext**'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**弹性分布式数据集（RDD）和 SparkContext**'
- en: Many Spark programs revolve around the concept of a resilient distributed dataset
    (RDD), which is a fault-tolerant collection of elements that can be operated on
    in parallel. SparkContext resides in the Driver program and manages the distributed
    data over the worker nodes through the cluster manager. The good thing about using
    PySpark is that all this complexity of data partitioning and task management is
    handled automatically at the back and the programmer can focus on the specific
    analytics or machine learning job itself.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 Spark 程序围绕弹性分布式数据集（RDD）的概念展开，RDD 是一个可以并行操作的容错元素集合。SparkContext 存在于 Driver
    程序中，通过集群管理器在工作节点上管理分布式数据。使用 PySpark 的好处是所有这些数据分区和任务管理的复杂性在后台自动处理，程序员可以专注于具体的分析或机器学习任务。
- en: '![The Benefits & Examples of Using Apache Spark with PySpark in Python | Rdd-1](../Images/5bb0f6532941bb2fa5a40915e804a5df.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Apache Spark 和 PySpark 在 Python 中的好处与示例 | Rdd-1](../Images/5bb0f6532941bb2fa5a40915e804a5df.png)'
- en: '*rdd-1*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*rdd-1*'
- en: There are two ways to create RDDs–parallelizing an existing collection in your
    driver program, or referencing a dataset in an external storage system, such as
    a shared file- system, HDFS, HBase, or any data source offering a Hadoop InputFormat.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: For illustration with a Python-based approach, we will give examples of the
    first type here. We can create a simple Python array of 20 random integers (between
    0 and 10), using Numpy random.randint(), and then create an RDD object as following,
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***Note the ‘4’ in the argument*****. It denotes 4 computing cores (in your
    local machine) to be used for this ****SparkContext**** object**. If we check
    the type of the RDD object, we get the following,'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Opposite to parallelization is the collection (with collect()) which brings
    all the distributed elements and returns them to the head node.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: But A is no longer a simple Numpy array. We can use the glom() method to check
    how the partitions are created.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now stop the SC and reinitialize it with 2 cores and see what happens when you
    repeat the process.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***The RDD is now distributed over two chunks, not four! ***'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '**You have learned about the first step in distributed data analytics i.e. controlling
    how your data is partitioned over smaller chunks for further processing**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '**Some Examples of Basic Operations with RDD & PySpark**'
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Count the elements**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**The first element (****first****) and the first few elements (****take****)**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Removing duplicates with using ****distinct**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '*NOTE*: This operation requires a **shuffle** in order to detect duplication
    across partitions. So, it is a slow operation. Don’t overdo it.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**To sum all the elements use ****reduce**** method**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Note the use of a lambda function in this,
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Or the direct ****sum()**** method**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Finding maximum element by ****reduce**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Finding longest word in a blob of text**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Use ****filter**** for logic-based filtering**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Write regular Python functions to use with ****reduce()**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note here the x < y does a lexicographic comparison and determines that Macintosh is
    larger than computers!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '**Mapping operation with a lambda function with PySpark**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Mapping with a regular Python function in PySpark**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**groupby**** returns a RDD of grouped elements (iterable) as per a given group
    operation**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we use a list-comprehension along with the groupby to
    create a list of two elements, each having a header (the result of the lambda
    function, simple modulo 2 here), and a sorted list of the elements which gave
    rise to that result. You can imagine easily that this kind of seperation can come
    particularly handy for processing data which needs to be binned/canned out based
    on particular operation performed over them.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Using ****histogram**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: The histogram() method takes a list of bins/buckets and returns a tuple with
    result of the histogram (binning),
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Set operations**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: You can also do regular set operations on RDDs like – union(), intersection(), subtract(),
    or cartesian().
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Check out [**this Jupyter notebook**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/SparkContext%20and%20RDD%20Basics.ipynb) for
    more examples.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[**这个Jupyter笔记本**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/SparkContext%20and%20RDD%20Basics.ipynb)获取更多示例。
- en: '**Lazy evaluation with PySpark (and *****Caching*****)**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用PySpark的惰性计算（和**缓存**）**'
- en: Lazy evaluation is an evaluation/computation strategy which prepares a detailed
    step-by-step internal map of the execution pipeline for a computing task, but
    delays the final execution until when it is absolutely needed. This strategy is
    at the heart of Spark for speeding up many parallelized Big Data operations.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 惰性计算是一种评估/计算策略，它为计算任务准备了详细的逐步内部执行管道图，但将最终执行延迟到绝对需要的时候。这种策略是Spark加速许多并行化大数据操作的核心。
- en: Let’s use two CPU cores for this example,
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用两个CPU核心作为这个例子的说明，
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Make a RDD with 1 million elements**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**创建一个包含100万个元素的RDD**'
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**Some computing function – ****taketime**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**某些计算函数 – **taketime**'
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Check how much time is taken by taketime function**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**检查taketime函数所花费的时间**'
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Remember this result, the taketime() function took a wall time of 31.5 us. Of
    course, the exact number will depend on the machine you are working on.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 记住这个结果，`taketime()`函数的实际时间为31.5微秒。当然，确切的数字将取决于你使用的机器。
- en: '**Now do the map operation on the function**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**现在对函数进行map操作**'
- en: '[PRE22]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '*How come each **taketime** function takes 45.8 us but the map operation with
    a 1 million elements RDD also took similar time?*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*为什么每个**taketime**函数花费45.8微秒，但处理100万个元素的RDD的map操作也花费了类似的时间？*'
- en: '**Because of lazy evaluation i.e. nothing was computed in the previous step,
    just a plan of execution was made**. The variable interim does not point to a
    data structure, instead it points to a plan of execution, expressed as a dependency
    graph. The dependency graph defines how RDDs are computed from each other.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**由于惰性计算，即在之前的步骤中没有计算任何内容，只是制定了执行计划**。变量`interim`并不指向数据结构，而是指向一个执行计划，表示为依赖关系图。依赖关系图定义了RDD如何从彼此之间计算。'
- en: '**The actual execution by ****reduce**** method**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过**reduce**方法的实际执行**'
- en: '[PRE23]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: So, the wall time here is 15.6 seconds. Remember, the taketime() function had
    a wall time of 31.5 us? Therefore, we expect the total time to be on the order
    of ~ 31 seconds for a 1-million array. Because of parallel operation on two cores,
    it took ~ 15 seconds.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这里的实际时间是15.6秒。记住，`taketime()`函数的时间是31.5微秒吗？因此，我们期望对于一个100万的数组，总时间约为31秒。由于在两个核心上并行操作，它花费了大约15秒。
- en: Now, we have not saved (materialized) any intermediate results in interim, so
    another simple operation (e.g. counting elements > 0) will take almost same time.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们没有在`interim`中保存（具体化）任何中间结果，所以另一个简单操作（例如计数元素 > 0）将花费几乎相同的时间。
- en: '[PRE24]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '**Caching to reduce computation time on similar operation (spending memory)**'
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**缓存以减少类似操作的计算时间（消耗内存）**'
- en: Remember the dependency graph that we built in the previous step? We can run
    the same computation as before with cache method to tell the dependency graph
    to plan for caching.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 记住我们在前一步中构建的依赖关系图吗？我们可以使用缓存方法运行相同的计算，告诉依赖关系图规划缓存。
- en: '[PRE25]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The first computation will not improve, but it caches the interim result,
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次计算不会改善，但它缓存了中间结果，
- en: '[PRE26]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now run the same filter method with the help of cached result,
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用缓存结果运行相同的过滤方法，
- en: '[PRE27]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Wow! The compute time came down to less than a second from 12 seconds earlier!
    This way, caching and parallelization with lazy excution, is the core feature
    of programming with Spark.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！计算时间从之前的12秒降到了不到1秒！这就是使用Spark编程的核心特性：缓存和并行化以及惰性执行。
- en: '**Dataframe and SparkSQL**'
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**DataFrame和SparkSQL**'
- en: Apart from the RDD, the second key data structure in the Spark framework, is
    the *DataFrame*. If you have done work with Python Pandas or R DataFrame, the
    concept may seem familiar.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 除了RDD，Spark框架中的第二个关键数据结构是*DataFrame*。如果你做过Python Pandas或R DataFrame的工作，这个概念可能会很熟悉。
- en: 'A DataFrame is a distributed collection of rows under named columns. It is
    conceptually equivalent to a table in a relational database, an Excel sheet with
    Column headers, or a data frame in R/Python, but with richer optimizations under
    the hood. DataFrames can be constructed from a wide array of sources such as:
    structured data files, tables in Hive, external databases, or existing RDDs. It
    also shares some common characteristics with RDD:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 是在命名列下的分布式行集合。它在概念上等同于关系数据库中的表、带列标题的 Excel 表格或 R/Python 中的数据框，但具有更丰富的底层优化。DataFrames
    可以从各种来源构建，如结构化数据文件、Hive 中的表、外部数据库或现有的 RDD。它还与 RDD 共享一些共同特征：
- en: 'Immutable in nature : We can create DataFrame / RDD once but can’t change it.
    And we can transform a DataFrame / RDD after applying transformations.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本质上是不可变的：我们可以创建 DataFrame / RDD，但不能更改它。我们可以在应用转换后对 DataFrame / RDD 进行变换。
- en: 'Lazy Evaluations: Which means that a task is not executed until an action is
    performed. Distributed: RDD and DataFrame both are distributed in nature.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 惰性计算：意味着任务不会执行，直到进行某个操作。分布式：RDD 和 DataFrame 本质上都是分布式的。
- en: '![The Benefits & Examples of Using Apache Spark with PySpark in Python | DataFrame
    in PySpark](../Images/197e1ce09b894ea1628c7da1c8d3c87d.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Apache Spark 和 PySpark 在 Python 中的好处与示例 | PySpark 中的 DataFrame](../Images/197e1ce09b894ea1628c7da1c8d3c87d.png)'
- en: '**Advantages of the DataFrame**'
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**DataFrame 的优点**'
- en: DataFrames are designed for processing large collection of structured or semi-structured
    data.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrames 设计用于处理大量结构化或半结构化数据。
- en: Observations in Spark DataFrame are organised under named columns, which helps
    Apache Spark to understand the schema of a DataFrame. This helps Spark optimize
    execution plan on these queries.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark DataFrame 中的观察数据在命名列下组织，这帮助 Apache Spark 理解 DataFrame 的模式。这有助于 Spark 优化这些查询的执行计划。
- en: DataFrame in Apache Spark has the ability to handle petabytes of data.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark 中的 DataFrame 具有处理 PB 级数据的能力。
- en: DataFrame has a support for wide range of data format and sources.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame 支持多种数据格式和来源。
- en: It has API support for different languages like Python, R, Scala, Java.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持 Python、R、Scala、Java 等多种语言的 API。
- en: '**DataFrame basics example**'
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**DataFrame 基础示例**'
- en: For fundamentals and typical usage examples of DataFrames, please see the following
    Jupyter Notebooks,
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 DataFrames 的基本原理和典型使用示例，请参阅以下 Jupyter Notebooks，
- en: '[**Spark DataFrame basics**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/Dataframe_basics.ipynb)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Spark DataFrame 基础**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/Dataframe_basics.ipynb)'
- en: '[**Spark DataFrame operations**](https://github.com/tirthajyoti/Spark-with-Python/blob/masterDataFrame_operations_basics.ipynb)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Spark DataFrame 操作**](https://github.com/tirthajyoti/Spark-with-Python/blob/masterDataFrame_operations_basics.ipynb)'
- en: '**SparkSQL Helps to Bridge the Gap for PySpark**'
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**SparkSQL 帮助弥合 PySpark 的差距**'
- en: Relational data stores are easy to build and query. Users and developers often
    prefer writing easy-to-interpret, declarative queries in a human-like readable
    language such as SQL. However, as data starts increasing in volume and variety,
    the relational approach does not scale well enough for building Big Data applications
    and analytical systems.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 关系型数据存储容易构建和查询。用户和开发者通常更喜欢使用易于理解的声明性查询语言，如 SQL。然而，随着数据量和种类的增加，关系型方法在构建大数据应用程序和分析系统时扩展性不够好。
- en: We have had success in the domain of Big Data analytics with Hadoop and the
    MapReduce paradigm. This was powerful, but often slow, and gave users a low-level, **procedural
    programming interface** that required people to write a lot of code for even very
    simple data transformations. However, once Spark was released, it really revolutionized
    the way Big Data analytics was done with a focus on in-memory computing, fault
    tolerance, high-level abstractions, and ease of use.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在大数据分析领域通过 Hadoop 和 MapReduce 范式取得了成功。这虽然强大，但往往速度较慢，并且为用户提供了一个低级的**过程编程接口**，要求人们即使对非常简单的数据转换也需要编写大量代码。然而，一旦
    Spark 发布，它真正革新了大数据分析的方式，专注于内存计算、容错、高级抽象以及易用性。
- en: Spark SQL essentially tries to bridge the gap between the two models we mentioned
    previously—the relational and procedural models. Spark SQL works through the DataFrame
    API that can perform relational operations on both external data sources and Spark’s
    built-in distributed collections—at scale!
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 本质上试图弥合我们之前提到的两种模型——关系模型和过程模型之间的差距。Spark SQL 通过 DataFrame API 执行关系操作，可以在外部数据源和
    Spark 内置的分布式集合上进行大规模操作！
- en: '![The Benefits & Examples of Using Apache Spark with PySpark in Python | Spark
    SQL](../Images/fbbca3e99c3728cb51c0284181460269.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![使用 PySpark 和 Python 的 Apache Spark 的好处与示例 | Spark SQL](../Images/fbbca3e99c3728cb51c0284181460269.png)'
- en: Why is Spark SQL so fast and optimized? The reason is because of a new extensible
    optimizer, **Catalyst**, based on functional programming constructs in Scala.
    Catalyst supports both rule-based and cost-based optimization. While extensible
    optimizers have been proposed in the past, they have typically required a complex
    domain-specific language to specify rules. Usually, this leads to having a significant
    learning curve and maintenance burden. In contrast, Catalyst uses standard features
    of the Scala programming language, such as pattern-matching, to let developers
    use the full programming language while still making rules easy to specify.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么 Spark SQL 这么快且经过优化？原因在于一种新的可扩展优化器**Catalyst**，基于 Scala 中的函数式编程构造。Catalyst
    支持基于规则和基于成本的优化。虽然过去也提出过可扩展的优化器，但通常需要复杂的领域特定语言来指定规则。这通常会导致显著的学习曲线和维护负担。相比之下，Catalyst
    使用 Scala 编程语言的标准特性，如模式匹配，使开发人员可以使用完整的编程语言，同时仍然使规则易于指定。
- en: 'You can refer to the following Jupyter notebook for an introduction to Database
    operations with SparkSQL:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下 Jupyter 笔记本来了解 SparkSQL 的数据库操作：
- en: '[**SparkSQL database operations basics**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/Dataframe_SQL_query.ipynb)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[**SparkSQL 数据库操作基础**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/Dataframe_SQL_query.ipynb)'
- en: '**How Are You Going to Use PySpark in Your Project? **'
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**你打算在你的项目中如何使用 PySpark？**'
- en: We covered the fundamentals of the Apache Spark ecosystem and how it works along
    with some basic usage examples of core data structure RDD with the Python interface
    PySpark. Also, DataFrame and SparkSQL were discussed along with reference links
    for example code notebooks.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了 Apache Spark 生态系统的基础知识及其工作原理，并提供了一些关于核心数据结构 RDD 使用的基本示例，使用的是 Python 接口
    PySpark。此外，还讨论了 DataFrame 和 SparkSQL，并提供了示例代码笔记本的参考链接。
- en: There is so much more to learn and experiment with Apache Spark being used with
    Python. The [PySpark website is](https://spark.apache.org/docs/latest/api/python/index.html) a
    good reference to have on your radar, and they make regular updates and enhancements–so
    keep an eye on that.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python 的 Apache Spark 还有很多内容可以学习和实验。[PySpark 网站](https://spark.apache.org/docs/latest/api/python/index.html)是一个很好的参考，它们会定期更新和改进——所以请保持关注。
- en: And, if you are interested in doing large-scale, distributed machine learning
    with Apache Spark, then check out the [MLLib portion of the PySpark ecosystem](https://spark.apache.org/mllib/).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对使用 Apache Spark 进行大规模分布式机器学习感兴趣，可以查看 [PySpark 生态系统的 MLLib 部分](https://spark.apache.org/mllib/)。
- en: '[Original](https://blog.exxactcorp.com/the-benefits-examples-of-using-apache-spark-with-pyspark-using-python/).
    Reposted with permission.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://blog.exxactcorp.com/the-benefits-examples-of-using-apache-spark-with-pyspark-using-python/)。经许可转载。'
- en: '**Related:**'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Learn how to use PySpark in under 5 minutes (Installation + Tutorial)](/2019/08/learn-pyspark-installation-tutorial.html)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习如何在 5 分钟内使用 PySpark（安装 + 教程）](/2019/08/learn-pyspark-installation-tutorial.html)'
- en: '[Time Series Analysis: A Simple Example with KNIME and Spark](/2019/10/time-series-analysis-simple-example-knime-spark.html)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[时间序列分析：使用 KNIME 和 Spark 的简单示例](/2019/10/time-series-analysis-simple-example-knime-spark.html)'
- en: '[Spark NLP 101: LightPipeline](/2019/11/spark-nlp-101-lightpipeline.html)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Spark NLP 101：LightPipeline](/2019/11/spark-nlp-101-lightpipeline.html)'
- en: More On This Topic
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Data Validation for PySpark Applications using Pandera](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Pandera 对 PySpark 应用程序进行数据验证](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
- en: '[Benefits Of Becoming A Data-First Enterprise](https://www.kdnuggets.com/2022/07/benefits-becoming-datafirst-enterprise.html)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为数据优先企业的好处](https://www.kdnuggets.com/2022/07/benefits-becoming-datafirst-enterprise.html)'
- en: '[3 Benefits to A/B Testing (+ Where to Get Started)](https://www.kdnuggets.com/2022/08/sphere-3-benefits-ab-testing-get-started.html)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[A/B 测试的 3 个好处（+ 从哪里开始）](https://www.kdnuggets.com/2022/08/sphere-3-benefits-ab-testing-get-started.html)'
- en: '[The Benefits of Natural Language AI for Content Creators](https://www.kdnuggets.com/2022/08/benefits-natural-language-ai-content-creators.html)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言 AI 对内容创作者的好处](https://www.kdnuggets.com/2022/08/benefits-natural-language-ai-content-creators.html)'
- en: '[PySpark for Data Science](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于数据科学的 PySpark](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
- en: '[Picking Examples to Understand Machine Learning Model](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[挑选实例以理解机器学习模型](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)'
