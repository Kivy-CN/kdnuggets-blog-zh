- en: 'Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/comparing-natural-language-processing-techniques-rnns-transformers-bert](https://www.kdnuggets.com/comparing-natural-language-processing-techniques-rnns-transformers-bert)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT](../Images/0bd14f1879f44952bb0f2a35143da063.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Freepik](https://www.freepik.com/free-vector/hand-drawn-flat-design-npl-illustration_22379566.htm#query=natural%20language%20processing&position=9&from_view=search&track=ais)
  prefs: []
  type: TYPE_NORMAL
- en: Natural Language Processing, or NLP, is a field within artificial intelligence
    for machines to have the ability to understand textual data. NLP research has
    existed for a long time, but only recently has it become more prominent with the
    introduction of big data and higher computational processing power.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: With the NLP field becoming bigger, many researchers would try to improve the
    machine's capability to understand the textual data better. Through much progress,
    many techniques are proposed and applied in the NLP field.
  prefs: []
  type: TYPE_NORMAL
- en: This article will compare various techniques for processing text data in the
    NLP field. This article will focus on discussing RNN, Transformers, and BERT because
    it’s the one that is often used in research. Let’s get into it.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Recurrent Neural Network](https://en.wikipedia.org/wiki/Recurrent_neural_network)
    or RNN was developed in 1980 but only recently gained attraction in the NLP field.
    RNN is a particular type within the neural network family used for sequential
    data or data that can’t be independent of each other. Sequential data examples
    are time series, audio, or text sentence data, basically any kind of data with
    meaningful order.'
  prefs: []
  type: TYPE_NORMAL
- en: RNNs are different from regular feed-forward neural networks as they process
    information differently. In the normal feed-forward, the information is processed
    following the layers. However,  RNN is using a loop cycle on the information input
    as consideration. To understand the differences, let’s see the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT](../Images/1c48bad07f4ce12a89b0e314bbe3fd82.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the RNNs model implements a loop cycle during the information
    processing. RNNs would consider the current and previous data input when processing
    this information. That’s why the model is suitable for any type of sequential
    data.
  prefs: []
  type: TYPE_NORMAL
- en: If we take an example in the text data, imagine we have the sentence “I wake
    up at 7 AM”, and we have the word as input. In the feed-forward neural network,
    when we reach the word “up,” the model would already forget the words “I,” “wake,”
    and “up.” However, RNNs would use every output for each word and loop them back
    so the model would not forget.
  prefs: []
  type: TYPE_NORMAL
- en: In the NLP field, RNNs are often used in many textual applications, such as
    text classification and generation. It’s often used in word-level applications
    such as Part of Speech tagging, next-word generation, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the RNNs more in-depth on the textual data, there are many types
    of RNNs. For example, the below image is the many-to-many types.
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT](../Images/c8aa7900dbf0cdac49becd02f5d6eb4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the image above, we can see that the output for each step (time-step
    in RNN) is processed one step at a time, and every iteration always considers
    the previous information.
  prefs: []
  type: TYPE_NORMAL
- en: Another RNN type used in many NLP applications is the encoder-decoder type (Sequence-to-Sequence).
    The structure is shown in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT](../Images/a458219e621d57011c85d559fe3fe587.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: This structure introduces two parts that are used in the model. The first part
    is called Encoder, which is a part that receives data sequence and creates a new
    representation based on it. The representation would be used in the second part
    of the model, which is the decoder. With this structure, the input and output
    lengths don’t necessarily need to be equal. The example use case is a language
    translation, which often does not have the same length between the input and output.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various benefits of using RNNs to process natural language data,
    including:'
  prefs: []
  type: TYPE_NORMAL
- en: RNN can be used to process text input without length limitations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model shares the same weights across all the time steps, which allows the
    neural network to use the same parameter in each step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Having the memory of past input makes RNN suitable for any sequential data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'But, there are several disadvantages as well:'
  prefs: []
  type: TYPE_NORMAL
- en: RNN is susceptible to both vanishing and exploding gradients. This is where
    the gradient result is the near-zero value (vanishing), causing network weight
    to only be updated for a tiny amount, or the gradient result is so significant
    (exploding) that it assigns an unrealistic enormous importance to the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Long time of training because of the sequential nature of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Short-term memory means that the model starts to forget the longer the model
    is trained. There is an extension of RNN called [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)
    to alleviate this problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformers is an NLP model architecture that tries to solve the sequence-to-sequence
    tasks previously encountered in the RNNs. As mentioned above, RNNs have problems
    with short-term memory. The longer the input, the more prominent the model was
    in forgetting the information. This is where the attention mechanism could help
    solve the problem.
  prefs: []
  type: TYPE_NORMAL
- en: The attention mechanism is introduced in the paper by [Bahdanau *et al*. (2014)](https://arxiv.org/abs/1409.0473)
    to solve the long input problem, especially with encoder-decoder type of RNNs.
    I would not explain the attention mechanism in detail. Basically, it is a layer
    that allows the model to focus on the critical part of the model input while having
    the output prediction. For example, the word input “Clock” would correlate highly
    with “Jam” in Indonesian if the task is for translation.
  prefs: []
  type: TYPE_NORMAL
- en: The transformers model is introduced by [Vaswani *et al.* (2017)](https://arxiv.org/pdf/1706.03762v5.pdf).
    The architecture is inspired by the encoder-decoder RNN and built with the attention
    mechanism in mind and does not process data in sequential order. The overall transformers
    model is structured like the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT](../Images/90d580b1b12ee73411994c316ec1d188.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformers Architecture ([Vaswani *et al*. 2017](https://arxiv.org/pdf/1706.03762v5.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: In the structure above, the transformers encode the data vector sequence into
    the word embedding with positional encoding in place while using the decoding
    to transform data into the original form. With the attention mechanism in place,
    the encoding can given importance according to the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers provide few advantages compared to the other model, including:'
  prefs: []
  type: TYPE_NORMAL
- en: The parallelization process increases the training and inference speed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Capable of processing longer input, which offers a better understanding of the
    context
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are still some disadvantages to the transformers model:'
  prefs: []
  type: TYPE_NORMAL
- en: High computational processing and demand.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The attention mechanism might require the text to be split because of the length
    limit it can handle.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Context might be lost if the split were done wrong.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**BERT**'
  prefs: []
  type: TYPE_NORMAL
- en: BERT, or Bidirectional Encoder Representations from Transformers, is a model
    developed by [Devlin *et al.* (2019)](https://arxiv.org/pdf/1810.04805.pdf) that
    involves two steps (pre-training and fine-tuning) to create the model. If we compare,
    BERT is a stack of transformers encoder (BERT Base has 12 Layers while BERT Large
    has 24 layers).
  prefs: []
  type: TYPE_NORMAL
- en: BERT's overall model development can be shown in the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT](../Images/8ea1ee96e72c9847b3af563700d17031.png)'
  prefs: []
  type: TYPE_IMG
- en: BERT overall procedures ([Devlin *et al.* (2019)](https://arxiv.org/pdf/1810.04805.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training tasks initiate the model's training at the same time, and once
    it is done, the model can be fine-tuned for various downstream tasks (question-answering,
    classification, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: What makes BERT special is that it is the first unsupervised bidirectional language
    model that is pre-trained on text data. BERT was previously pre-trained on the
    entire Wikipedia and book corpus, consisting of over 3000 million words.
  prefs: []
  type: TYPE_NORMAL
- en: BERT is considered bidirectional because it didn’t read data input sequentially
    (from left to right or vice versa), but the transformer encoder read the whole
    sequence simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike directional models, which read the text input sequentially (left-to-right
    or right-to-left), the Transformer encoder reads the entire sequence of words
    simultaneously. That’s why the model is considered bidirectional and allows the
    model to understand the whole context of the input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve bidirectional, BERT uses two techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mask Language Model (MLM)** — Word masking technique. The technique would
    mask 15% of the input words and try to predict this masked word based on the non-masked
    word.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Next Sentence Prediction (NSP)** —  BERT tries to learn the relationship
    between sentences. The model has pairs of sentences as the data input and tries
    to predict if the subsequent sentence exists in the original document.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are a few advantages to using BERT in the NLP field, including:'
  prefs: []
  type: TYPE_NORMAL
- en: BERT is easy to use for pre-trained various NLP downstream tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bidirectional makes BERT understand the text context better.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s a popular model that has much support from the community
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Although, there are still a few disadvantages, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Requires high computational power and long training time for some downstream
    task fine-tuning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The BERT model might result in a big model requiring much bigger storage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s better to use for complex tasks as the performance for simple tasks is
    not much different than using simpler models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NLP has become more prominent recently, and much research has focused on improving
    the applications. In this article, we discuss three NLP techniques that are often
    used:'
  prefs: []
  type: TYPE_NORMAL
- en: RNN
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BERT
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each of the techniques has its advantages and disadvantages, but overall, we
    can see the model evolving in a better way.
  prefs: []
  type: TYPE_NORMAL
- en: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** is a data science
    assistant manager and data writer. While working full-time at Allianz Indonesia,
    he loves to share Python and data tips via social media and writing media. Cornellius
    writes on a variety of AI and machine learning topics.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[N-gram Language Modeling in Natural Language Processing](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Fine-Tune BERT for Sentiment Analysis with Hugging Face Transformers](https://www.kdnuggets.com/how-to-fine-tune-bert-sentiment-analysis-hugging-face-transformers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Natural Language Processing Key Terms, Explained](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Representation for Natural Language Processing Tasks](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Transfer Learning for Image Recognition and Natural Language Processing](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Start Using Natural Language Processing With PyTorch](https://www.kdnuggets.com/2022/04/start-natural-language-processing-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
