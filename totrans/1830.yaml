- en: How to Rank 10% in Your First Kaggle Competition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在第一次 Kaggle 竞赛中排名前10%
- en: 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3)
- en: '**Model Training**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型训练**'
- en: We can improve a model’s performance by tuning its parameters. A model usually
    have many parameters, but only a few of them are significant to its performance.
    For example, the most important parameters for a random forset is the number of
    trees in the forest and the maximum number of features used in developing each
    tree. **We need to understand how models work and what impact does each parameter
    have to the model’s performance, be it accuracy, robustness or speed.**
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调整模型的参数来提升模型的性能。一个模型通常有很多参数，但其中只有少数几个对其性能有显著影响。例如，随机森林最重要的参数是森林中树的数量以及在每棵树的构建中使用的最大特征数量。**我们需要了解模型的工作原理以及每个参数对模型性能的影响，无论是准确性、鲁棒性还是速度。**
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT 需求'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Normally we would find the best set of parameters by a process called **[grid
    search](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html))**.
    Actually what it does is simply iterating through all the possible combinations
    and find the best one.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们会通过一种叫做**[网格搜索](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html))**的过程找到最佳参数集。实际上，它的工作原理是通过遍历所有可能的组合来找到最佳的一个。
- en: By the way, random forest usually reach optimum when `max_features` is set to
    the square root of the total number of features.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，当`max_features`设置为特征总数的平方根时，随机森林通常会达到最佳效果。
- en: 'Here I’d like to stress some points about tuning XGB. These parameters are
    generally considered to have real impacts on its performance:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我想强调一些关于调整 XGB 的要点。这些参数通常被认为对其性能有实际影响：
- en: '`eta`: Step size used in updating weights. Lower `eta` means slower training
    but better convergence.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta`：用于更新权重的步长。较低的`eta`意味着训练较慢但收敛效果更好。'
- en: '`num_round`: Total number of iterations.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_round`：总的迭代次数。'
- en: '`subsample`: The ratio of training data used in each iteration. This is to
    combat overfitting.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subsample`：每次迭代中使用的训练数据比例。这是为了防止过拟合。'
- en: '`colsample_bytree`: The ratio of features used in each iteration. This is like `max_features` in `RandomForestClassifier`.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`colsample_bytree`：每次迭代中使用的特征比例。这类似于`RandomForestClassifier`中的`max_features`。'
- en: '`max_depth`: The maximum depth of each tree. Unlike random forest, **gradient
    boosting would eventually overfit if we do not limit its depth**.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：每棵树的最大深度。与随机森林不同，**梯度提升如果不限制其深度最终会出现过拟合**。'
- en: '`early_stopping_rounds`: If we don’t see an increase of validation score for
    a given number of iterations, the algorithm will stop early. This is to combat
    overfitting, too.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`early_stopping_rounds`：如果在给定次数的迭代中没有看到验证分数的提高，算法将提前停止。这也是为了防止过拟合。'
- en: 'Usual tuning steps:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通常的调整步骤：
- en: Reserve a portion of training set as the validation set.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一部分训练集保留为验证集。
- en: Set `eta` to a relatively high value (e.g. 0.05 ~ 0.1), `num_round` to 300 ~
    500.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`eta`设置为相对较高的值（例如0.05 ~ 0.1），将`num_round`设置为300 ~ 500。
- en: Use grid search to find the best combination of other parameters.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用网格搜索找到其他参数的最佳组合。
- en: Gradually lower `eta` until we reach the optimum.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逐渐降低`eta`直到达到最佳值。
- en: '**Use the validation set as `watch_list` to re-train the model with the best
    parameters. Observe how score changes on validation set in each iteration. Find
    the optimal value for `early_stopping_rounds`.**'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用验证集作为`watch_list`重新训练模型，并使用最佳参数。观察每次迭代中验证集上的得分变化。找到`early_stopping_rounds`的最佳值。**'
- en: Finally, note that models with randomness all have a parameter like `seed` or `random_state` to
    control the random seed. **You must record this** with all other parameters when
    you get a good model. Otherwise you wouldn’t be able to reproduce it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，所有具有随机性的模型都有一个像`seed`或`random_state`这样的参数来控制随机种子。**在获得好的模型时，你必须记录这个参数**及所有其他参数。否则你将无法重现它。
- en: '**Cross Validation**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉验证**'
- en: '**[Cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))** is
    an essential step in model training. It tells us whether our model is at high
    risk of overfitting. In many competitions, public LB scores are not very reliable.
    Often when we improve the model and get a better local CV score, the LB score
    becomes worse. **It is widely believed that we should trust our CV scores under
    such situation.**Ideally we would want **CV scores obtained by different approaches
    to improve in sync with each other and with the LB score**, but this is not always
    possible.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**[交叉验证](https://en.wikipedia.org/wiki/Cross-validation_(statistics))**是在模型训练中的一个重要步骤。它告诉我们模型是否有较高的过拟合风险。在许多竞赛中，公共LB得分并不可靠。经常在我们改进模型并获得更好的本地CV得分时，LB得分反而变差。**在这种情况下，人们普遍认为我们应该相信我们的CV得分。**理想情况下，我们希望**不同方法获得的CV得分与LB得分同步提高**，但这并不总是可能的。'
- en: Usually **5-fold CV** is good enough. If we use more folds, the CV score would
    become more reliable, but the training takes longer to finish as well. However,
    we shouldn’t use too many folds if our training data is limited. Otherwise we
    would have too few samples in each fold to guarantee statistical significance.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通常**5折交叉验证**已经足够好。如果使用更多折数，CV得分会变得更可靠，但训练也需要更长时间。然而，如果我们的训练数据有限，就不应该使用过多的折数，否则每折中的样本数量过少，无法保证统计显著性。
- en: How to do CV properly is not a trivial problem. It requires constant experiment
    and case-by-case discussion. Many Kagglers share their CV approaches (like [this
    one](https://www.kaggle.com/c/telstra-recruiting-network/forums/t/19277/what-is-your-cross-validation-method))
    after competitions when they feel that reliable CV is not easy.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如何正确进行交叉验证并非小事。这需要不断的实验和逐案讨论。许多Kaggler在竞赛后分享他们的CV方法（如[这个](https://www.kaggle.com/c/telstra-recruiting-network/forums/t/19277/what-is-your-cross-validation-method)），因为他们觉得可靠的CV并不容易。
- en: '**Ensemble Generation**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成生成**'
- en: '[Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning) refers
    to the technique of combining different models. It **reduces both bias and variance
    of the final model** (you can find a proof [here](http://link.springer.com/chapter/10.1007%2F3-540-33019-4_19)),
    thus **increasing the score and reducing the risk of overfitting**. Recently it
    became virtually impossible to win prize without using ensemble in Kaggle competitions.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[集成学习](https://en.wikipedia.org/wiki/Ensemble_learning)指的是将不同模型组合在一起的技术。它**降低了最终模型的偏差和方差**（你可以在[这里](http://link.springer.com/chapter/10.1007%2F3-540-33019-4_19)找到证明），从而**提高了得分并减少了过拟合的风险**。最近，没有使用集成方法几乎不可能在Kaggle竞赛中获胜。'
- en: 'Common approaches of ensemble learning are:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的集成学习方法包括：
- en: '**Bagging**: Use different random subsets of training data to train each base
    model. Then all the base models vote to generate the final predictions. This is
    how random forest works.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**装袋**：使用训练数据的不同随机子集训练每个基础模型。然后，所有基础模型投票生成最终预测。这就是随机森林的工作原理。'
- en: '**Boosting**: Train base models iteratively, modify the weights of training
    samples according to the last iteration. This is how gradient boosted trees work.
    (Actually it’s not the whole story. Apart from boosting, GBTs try to learn the
    residuals of earlier iterations.) It performs better than bagging but is more
    prone to overfitting.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升**：迭代地训练基础模型，根据上一轮的结果调整训练样本的权重。这就是梯度提升树的工作原理。（实际上，这并不是全部。除了提升，GBT还尝试学习早期迭代的残差。）它的表现优于装袋，但更容易过拟合。'
- en: '**Blending**: Use non-overlapping data to train different base models and take
    a weighted average of them to obtain the final predictions. This is easy to implement
    but uses less data.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合**：使用非重叠的数据训练不同的基础模型，并对它们取加权平均以获得最终预测。这种方法实现简单，但使用的数据较少。'
- en: '**Stacking**: To be discussed next.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**堆叠**：将在接下来的内容中讨论。'
- en: 'In theory, for the ensemble to perform well, two factors matter:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上讲，为了使集成表现良好，两个因素很重要：
- en: '**Base models should be as unrelated as possibly**. This is why we tend to
    include non-tree-based models in the ensemble even though they don’t perform as
    well. The math says that the greater the diversity, and less bias in the final
    ensemble.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础模型应该尽可能不相关**。这就是为什么我们倾向于在集成中包含非树模型，即使它们的表现不如树模型。数学上说，最终集成的多样性越大，偏差越小。'
- en: '**Performance of base models shouldn’t differ to much.**'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础模型的性能不应差异过大。**'
- en: Actually we have a **trade-off** here. In practice we may end up with highly
    related models of comparable performances. Yet we ensemble them anyway because
    it usually increase the overall performance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们在这里有一个**权衡**。在实践中，我们可能会得到性能相当的高度相关模型。然而，我们仍然将它们进行集成，因为这通常会提高整体性能。
- en: '**Stacking**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**堆叠**'
- en: 'Compared with blending, stacking makes better use of training data. Here’s
    a diagram of how it works:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 与混合方法相比，堆叠方法更好地利用了训练数据。以下是它如何工作的示意图：
- en: '[![Stacking](../Images/fb2fd03a6a8cf5851969a1254b354966.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-diagram-stacking.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[![堆叠](../Images/fb2fd03a6a8cf5851969a1254b354966.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-diagram-stacking.jpg)'
- en: '*(Taken from [Faron](https://www.kaggle.com/mmueller). Many thanks!)*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*(摘自 [Faron](https://www.kaggle.com/mmueller)。非常感谢！)*'
- en: 'It’s much like cross validation. Take 5-fold stacking as an example. First
    we split the training data into 5 folds. Next we will do 5 iterations. In each
    iteration, train every base model on 4 folds and predict on the hold-out fold. **You
    have to keep the predictions on the testing data as well.** This way, in each
    iteration every base model will make predictions on 1 fold of the training data
    and all of the testing data. After 5 iterations we will obtain a matrix of shape `#(samples
    in training data) X #(base models)`. This matrix is then fed to the stacker (it’s
    just another model) in the second level. After the stacker is fitted, use the
    predictions on testing data by base models (**each base model is trained 5 times,
    therefore we have to take an average to obtain a matrix of the same shape**) as
    the input for the stacker and obtain our final predictions.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '这很像交叉验证。以5折堆叠为例。首先，我们将训练数据分成5个折。接下来我们将进行5次迭代。在每次迭代中，训练每个基础模型在4个折上，并在保留折上进行预测。**你还需要保持对测试数据的预测。**这样，在每次迭代中，每个基础模型都会对训练数据的1个折和所有的测试数据进行预测。经过5次迭代后，我们将得到一个形状为`#(训练数据中的样本数)
    X #(基础模型)`的矩阵。然后将这个矩阵传递给第二层的堆叠器（这只是另一个模型）。堆叠器拟合后，使用基础模型对测试数据的预测（**每个基础模型被训练了5次，因此我们必须取平均值以获得相同形状的矩阵**）作为堆叠器的输入，从而获得最终预测结果。'
- en: 'Maybe it’s better to just show the codes:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 也许直接展示代码更好：
- en: Prize winners usually have larger and much more complicated ensembles. For beginner,
    implementing a correct 5-fold stacking is good enough.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 奖品获得者通常有更大、更复杂的集成。对于初学者来说，实现一个正确的5折堆叠已经足够了。
- en: '***Pipeline**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '***管道***'
- en: 'We can see that the workflow for a Kaggle competition is quite complex, especially
    for model selection and ensemble. Ideally, we need a highly automated pipeline
    capable of:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，Kaggle竞赛的工作流程非常复杂，特别是在模型选择和集成方面。理想情况下，我们需要一个高度自动化的流程，能够：
- en: '**Modularized feature transformations**. We only need to write a few lines
    of codes (or better, rules / DSLs) and the new feature is added to the training
    set.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模块化特征转换**。我们只需编写几行代码（或者更好，使用规则/DSLs），新特征就会被添加到训练集中。'
- en: '**Automated grid search**. We only need to set up models and parameter grid,
    the search will be run and the best parameters will be recorded.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化网格搜索**。我们只需要设置模型和参数网格，搜索将被执行，最佳参数将被记录。'
- en: '**Automated ensemble selection**. Use K best models for training the ensemble
    as soon as we put another base model into the pool.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化的集成选择**。当我们将另一个基础模型添加到池中时，使用K个最佳模型进行集成训练。'
- en: For beginners, the first one is not very important because the number of features
    is quite manageable; the third one is not important either because typically we
    only do several ensembles at the end of the competition. But the second one is
    good to have because**manually recording the performance and parameters of each
    model is time-consuming and error-prone**.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于初学者来说，第一个并不是很重要，因为特征数量是相对可管理的；第三个也不重要，因为通常我们只在竞赛结束时进行几次集成。但第二个很有用，因为**手动记录每个模型的性能和参数既耗时又容易出错**。
- en: '[Chenglong Chen](https://www.kaggle.com/chenglongchen), the winner of [Crowdflower
    Search Results Relevance](https://www.kaggle.com/c/crowdflower-search-relevance),
    once released his pipeline on [GitHub](https://github.com/ChenglongChen/Kaggle_CrowdFlower).
    It’s very complete and efficient. Yet it’s very hard to understand and extract
    all his logic to build a general framework. This is something you might want to
    do when you have plenty of time.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[陈成龙](https://www.kaggle.com/chenglongchen)，[Crowdflower 搜索结果相关性](https://www.kaggle.com/c/crowdflower-search-relevance)的获胜者，曾在[GitHub](https://github.com/ChenglongChen/Kaggle_CrowdFlower)上发布了他的管道。它非常完整且高效。但理解并提取他的所有逻辑来构建通用框架非常困难。这是你在有充足时间时可能想要做的事情。'
- en: More On This Topic
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[How LinkedIn Uses Machine Learning To Rank Your Feed](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LinkedIn 如何使用机器学习来排序你的动态](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)'
- en: '[How to Get Your First Job in Data Science without Any Work Experience](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在没有任何工作经验的情况下获得你的第一个数据科学工作](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)'
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 TensorFlow 和 Keras 构建和训练你的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
- en: '[It''s alive! Build your first robots with Python and some cheap,…](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[它活过来了！用 Python 和一些便宜的……构建你的第一个机器人](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
- en: '[From Zero to Hero: Create Your First ML Model with PyTorch](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从零到英雄：使用 PyTorch 创建你的第一个 ML 模型](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
- en: '[Deploying Your First Machine Learning Model](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[部署你的第一个机器学习模型](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
