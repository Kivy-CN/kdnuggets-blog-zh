# 集成学习以改善机器学习结果

> 原文：[https://www.kdnuggets.com/2017/09/ensemble-learning-improve-machine-learning-results.html](https://www.kdnuggets.com/2017/09/ensemble-learning-improve-machine-learning-results.html)

**由 Vadim Smolyakov 提供，[Statsbot](https://statsbot.co/)。**

*集成学习通过结合多个模型来帮助提高机器学习结果。这种方法能够比单一模型产生更好的预测性能。这就是为什么集成方法在许多著名的机器学习竞赛中获胜，例如 Netflix 竞赛、KDD 2009 和 Kaggle。*

*[*Statsbot*](http://statsbot.co/?utm_source=kdnuggets)* 团队希望为你提供这种方法的优势，并请数据科学家 Vadim Smolyakov 深入探讨三种基本的集成学习技术。*

集成方法是将几种机器学习技术组合成一个预测模型的元算法，以**减少****方差**（Bagging）、**偏差**（Boosting）或**改善预测**（Stacking）。

集成方法可以分为两类：

+   *序列*集成方法，其中基础学习器是按序生成的（例如 AdaBoost）。

    序列方法的基本动机是**利用基础学习器之间的依赖性**。通过加大对之前标记错误样本的权重，可以提升整体性能。

+   *并行*集成方法，其中基础学习器是并行生成的（例如随机森林）。

    并行方法的基本动机是**利用基础学习器之间的独立性**，因为通过平均可以显著减少误差。

大多数集成方法使用单一基础学习算法来生成同质基础学习器，即相同类型的学习器，从而形成*同质集成*。

还有一些方法使用异质学习器，即不同类型的学习器，从而形成*异质集成*。为了使集成方法比任何单个成员都更准确，基础学习器必须尽可能准确且尽可能多样化。

### Bagging

Bagging 代表自助聚合。减少估计方差的一种方法是对多个估计值进行平均。例如，我们可以在数据的不同子集（随机选择且允许重复）上训练 M 棵不同的树，并计算集成：

![](../Images/2cceadad307359316c9dd527421af74e.png)

Bagging 使用自助抽样来获取训练基础学习器的数据子集。为了汇总基础学习器的输出，Bagging 使用*分类投票*和*回归平均*。

我们可以在 Iris 数据集的分类背景下研究 Bagging。我们可以选择两个基础估计器：决策树和 k-NN 分类器。图 1 显示了基础估计器学习到的决策边界，以及它们的 Bagging 集成应用于 Iris 数据集的效果。

准确率：0.63 (+/- 0.02) [决策树]

准确率：0.70 (+/- 0.02) [K-NN]

准确率：0.64 (+/- 0.01) [袋装树]

准确率：0.59 (+/- 0.07) [袋装K-NN]

![](../Images/33f9ea715faa9a1a1c6673eb09c5d0c9.png)

决策树显示了轴的平行边界，而k=1的最近邻则紧密地拟合数据点。袋装集成是使用10个基本估计器训练的，训练数据的子样本为0.8，特征的子样本也为0.8。

决策树袋装集成在准确率上优于k-NN袋装集成。K-NN对训练样本的扰动不太敏感，因此被称为稳定的学习器。

> *结合稳定的学习器并不太有优势，因为集成不会有助于提高泛化性能。*

图中还显示了测试准确率如何随着集成大小的增加而提高。根据交叉验证结果，我们可以看到准确率在大约10个基本估计器时上升，然后保持平稳。因此，超过10个基本估计器只会增加计算复杂性，而不会提高Iris数据集的准确率。

我们还可以看到袋装树集成的学习曲线。注意训练数据的平均误差为0.3，以及测试数据的U型误差曲线。训练和测试误差之间的最小差距发生在训练集大小的约80%时。

> *一种常用的集成算法类别是随机化树的森林。*

在**随机森林**中，集合中的每棵树都是从训练集中通过替换抽样（即自助样本）构建的。此外，除了使用所有特征之外，还会随机选择特征的子集，从而进一步随机化树的结构。

因此，森林的偏差略微增加，但由于对相关性较低的树进行平均，方差降低，从而整体上模型表现更好。

![](../Images/47a13d5d5d421a4b111aa4691973b497.png)

在**极端随机树**算法中，随机性进一步提升：分裂阈值被随机化。不是寻找最具判别力的阈值，而是为每个候选特征随机抽取阈值，并选择这些随机生成的阈值中最好的作为分裂规则。这通常可以稍微降低模型的方差，但偏差会略微增加。

### **提升（Boosting）**

提升（Boosting）指的是一类能够将弱学习器转换为强学习器的算法。提升的主要原理是将一系列弱学习器（即稍微优于随机猜测的模型，如小型决策树）拟合到加权版本的数据中。对早期轮次错误分类的示例赋予更多权重。

然后，通过加权多数投票（分类）或加权求和（回归）来组合预测，以产生最终预测。提升方法与委员会方法（如 bagging）的主要区别在于基学习器是在数据的加权版本上按顺序训练的。

下面的算法描述了最广泛使用的提升算法形式，即 **AdaBoost**，它代表自适应提升。

![](../Images/8a53b5e180f2642f01752d190c29b478.png)

我们看到第一个基分类器 y1(x) 是使用所有相等的权重系数进行训练的。在随后的提升回合中，权重系数会增加对误分类的数据点，减少对正确分类的数据点的权重。

数量 epsilon 代表每个基分类器的加权错误率。因此，权重系数 alpha 给予更准确的分类器更大的权重。

![](../Images/66139941634db7f9ed0861cb2c93e1a9.png)

AdaBoost 算法如上图所示。每个基学习器由深度为 1 的决策树组成，从而基于特征阈值对数据进行分类，该阈值将空间分成由一个与轴平行的线性决策面分隔的两个区域。图中还显示了随着集合大小的增加，测试准确度如何提高，以及训练和测试数据的学习曲线。

**梯度树提升** 是对提升方法的一种推广，适用于任意可微损失函数。它可用于回归和分类问题。梯度提升以顺序方式构建模型。

![](../Images/e584f410462ef7eb0adbaaab2e999d99.png)

在每个阶段，选择决策树 hm(x) 来最小化给定当前模型 Fm-1(x) 的损失函数 L：

![](../Images/6bb6db30a3c9eda743f67859a8b19ec8.png)

回归和分类的算法在使用的损失函数类型上有所不同。

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织 IT

* * *

### 更多相关话题

+   [带示例的集成学习](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)

+   [集成学习技术：Python 中随机森林的详细介绍](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)

+   [何时选择集成技术？](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)

+   [掌握季节性并提升业务结果的终极指南](https://www.kdnuggets.com/2023/08/media-mix-modeling-ultimate-guide-mastering-seasonality-boosting-business-results.html)

+   [提升机器学习模型的7种方法](https://www.kdnuggets.com/7-ways-to-improve-your-machine-learning-models)

+   [为什么谦逊自己能提高你的数据科学技能](https://www.kdnuggets.com/2022/01/humbling-improve-data-science-skills.html)
