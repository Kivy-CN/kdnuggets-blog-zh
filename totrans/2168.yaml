- en: Demystifying Decision Trees for the Real World
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/demystifying-decision-trees-for-the-real-world](https://www.kdnuggets.com/demystifying-decision-trees-for-the-real-world)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Decision Trees for Real World](../Images/ea4f96c2e9a77ccb1faac326d2e08a98.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees break down difficult decisions into straightforward, easily followed
    phases, thereby functioning like human brains.
  prefs: []
  type: TYPE_NORMAL
- en: In data science, these strong instruments are extensively applied to assist
    in data analysis and the direction of decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will go over how decision trees operate, give real-world
    examples, and give some tips for enhancing them.
  prefs: []
  type: TYPE_NORMAL
- en: '**Structure of Decision Trees**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fundamentally, decision trees are simple and clear tools. They break down difficult
    options into simpler, sequential choices, therefore reflecting human decision-making.
    Let us now explore the main elements forming a decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: '**Nodes, Branches, and Leaves**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Three basic components define a decision tree: leaves, branches, and nodes.
    Every one of these is absolutely essential for the process of making decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nodes: They are decision points whereby the tree decides depending on the input
    data. When representing all the data, the root node is the starting point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Branches: They relate the result of a decision and link nodes. Every branch
    matches a potential result or value of a decision node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Leaves: The decision tree''s ends are leaves, sometimes known as leaf nodes.
    Each leaf node offers a certain consequence or label; they reflect the last choice
    or classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conceptual Example**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose you are choosing whether to venture outside depending on the temperature.
    "Is it raining?" the root node would ask. If so, you might find a branch headed
    toward "Take an umbrella." This should not be the case; another branch could say,
    "Wear sunglasses."
  prefs: []
  type: TYPE_NORMAL
- en: These structures make decision trees easy to interpret and visualize, so they
    are popular in various fields.
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-World Example: The Loan Approval Adventure**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Picture this: You''re a wizard at Gringotts Bank, deciding who gets a loan
    for their new broomstick.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Root Node: "Is their credit score magical?"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If yes → Branch to "Approve faster than you can say Quidditch!"
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If no → Branch to "Check their goblin gold reserves."
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If high →, "Approve, but keep an eye on them."
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If low → "Deny faster than a Nimbus 2000."
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here is the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Structure of Decision Trees in Machine Learning](../Images/3e9b93689e0ded4344ef485f406e1e7c.png) When
    you run this spell, you''ll see a tree appear! It''s like the Marauder''s Map
    of loan approvals:'
  prefs: []
  type: TYPE_NORMAL
- en: The root node splits on Credit_Score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it's ≤ 675, we venture left
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it's > 675, we journey right
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The leaves show our final decisions: "Yes" for approved, "No" for denied'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Voila! You've just created a decision-making crystal ball!
  prefs: []
  type: TYPE_NORMAL
- en: 'Mind Bender: If your life were a decision tree, what would be the root node
    question? "Did I have coffee this morning?" might lead to some interesting branches!'
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision Trees: Behind the Branches**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decision trees function similarly to a flowchart or tree structure, with a succession
    of decision points. They begin by dividing a dataset into smaller pieces, and
    then they build a decision tree to go along with it. The way these trees deal
    with data splitting and different variables is something we should look at.
  prefs: []
  type: TYPE_NORMAL
- en: '**Splitting Criteria: Gini Impurity and Information Gain**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Choosing the best quality to divide the data is the primary goal of building
    a decision tree. It is possible to determine this procedure using criteria provided
    by Information Gain and Gini Impurity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gini Impurity: Picture yourself in the midst of a game of guessing. How often
    would you be mistaken if you randomly selected a label? That''s what Gini Impurity
    measures. We can make better guesses and have a happier tree with a lower Gini
    coefficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Information gain: The "aha!" moment in a mystery story is what you may compare
    this to. How much a hint (attribute) aids in solving the case is measured by it.
    A bigger "aha!" means more gain, which means an ecstatic tree!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To predict whether a customer would buy a product from your dataset, you can
    start with basic demographic information like age, income, and purchasing history.
    The approach takes all of these into account and finds the one that separates
    the buyers from the others.
  prefs: []
  type: TYPE_NORMAL
- en: '**Handling Continuous and Categorical Data**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are no types of info that our tree detectives can't look into.
  prefs: []
  type: TYPE_NORMAL
- en: For features that are easy to change, like age or income, the tree sets up a
    speed trap. "Anyone over 30, this way!"
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to categorical data, like gender or product type, it's more of
    a lineup. "Smartphones stand on the left; laptops on the right!"
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-World Cold Case: The Customer Purchase Predictor**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To better understand how decision trees work, let''s look at a real-life example:
    using a customer''s age and income to guess whether they will buy a product.'
  prefs: []
  type: TYPE_NORMAL
- en: To guess what people will buy, we'll make a simple collection and a decision
    tree.
  prefs: []
  type: TYPE_NORMAL
- en: A description of the code
  prefs: []
  type: TYPE_NORMAL
- en: We import libraries like pandas to work with the data, DecisionTreeClassifier
    from scikit-learn to build the tree, and matplotlib to show the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create Dataset: Age, income, and buying status are used to make a sample dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Get Features and Goals Ready: The goal variable (Purchased) and features (Age,
    Income) are set up.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Train the Model: The information is used to set up and train the decision tree
    classifier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See the Tree: Finally, we draw the decision tree so that we can see how choices
    are made.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here is the code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here is the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Behind the Branches of Decision Trees in Machine Learning](../Images/7f5d804f75257f96f06d7adfdd01c418.png)'
  prefs: []
  type: TYPE_IMG
- en: The final decision tree will show how the tree splits up based on age and income
    to figure out if a customer is likely to buy a product. Each node is a decision
    point, and the branches show different outcomes. The final decision is shown by
    the leaf nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at how interviews can be used in the real world!
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-World Applications**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Real World Applications for Decision Trees](../Images/b89c0285925b1a8b2b25a340d3d31864.png)'
  prefs: []
  type: TYPE_IMG
- en: This project is designed as a take-home assignment for Meta (Facebook) data
    science positions. The objective is to build a classification algorithm that predicts
    whether a movie on Rotten Tomatoes is labeled 'Rotten', 'Fresh', or 'Certified
    Fresh.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the link to this project: [https://platform.stratascratch.com/data-projects/rotten-tomatoes-movies-rating-prediction](https://platform.stratascratch.com/data-projects/rotten-tomatoes-movies-rating-prediction?utm_source=blog&utm_medium=click&utm_campaign=kdn+decision+trees+for+real+world)'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s break down the solution into codeable steps.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step-by-Step Solution**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Data Preparation: We will merge the two datasets on the rotten_tomatoes_link
    column. This will give us a comprehensive dataset with movie information and critic
    reviews.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Feature Selection and Engineering: We will select relevant features and perform
    necessary transformations. This includes converting categorical variables to numerical
    ones, handling missing values, and normalizing the feature values.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Model Training: We will train a decision tree classifier on the processed dataset
    and use cross-validation to evaluate the model''s robust performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Evaluation: Finally, we will evaluate the model''s performance using metrics
    like accuracy, precision, recall, and F1-score.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here is the code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here is the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Real World Applications for Decision Trees](../Images/deacf1b18f3680ce5099414ce3641d35.png)'
  prefs: []
  type: TYPE_IMG
- en: The model shows high accuracy and F1 scores across the classes, indicating good
    performance. Let’s see the key takeaways.
  prefs: []
  type: TYPE_NORMAL
- en: Key Takeaways
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection is crucial for model performance. Content rating genres directors'
    runtime and ratings proved valuable predictors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A decision tree classifier effectively captures complex relationships in movie
    data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cross-validation ensures model reliability across different data subsets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: High performance in the "Certified-Fresh" class warrants further investigation
    into potential class imbalance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model shows promise for real-world application in predicting movie ratings
    and enhancing user experience on platforms like Rotten Tomatoes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Enhancing Decision Trees: Turning Your Sapling into a Mighty Oak**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, you've grown your first decision tree. Impressive! But why stop there? Let's
    turn that sapling into a forest giant that would make even Groot jealous. Ready
    to beef up your tree? Let's dive in!
  prefs: []
  type: TYPE_NORMAL
- en: '**Pruning Techniques**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pruning is a method used to cut a decision tree's size by eliminating parts
    that have minimal ability in target variable prediction. This helps to reduce
    overfitting in particular.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-pruning: Often referred to as early stopping, this entails stopping the
    tree''s growth right away. Before training, the model is specified parameters,
    including maximum depth (max_depth), minimum samples required to split a node
    (min_samples_split), and minimum samples required at a leaf node (min_samples_leaf).
    This keeps the tree from growing overly complicated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Post-pruning: This method grows the tree to its maximum depth and removes nodes
    that don''t offer much power. Though more computationally taxing than pre-pruning,
    post-pruning can be more successful.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble Methods**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ensemble techniques combine several models to generate performance above that
    of any one model. Two primary forms of ensemble techniques applied with decision
    trees are bagging and boosting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bagging (Bootstrap Aggregating): This method trains several decision trees
    on several subsets of the data (generated by sampling with replacement) and then
    averages their predictions. One often used bagging technique is Random Forest.
    It lessens variance and aids in overfit prevention. Check out "[Decision Tree
    and Random Forest Algorithm](https://www.stratascratch.com/blog/decision-tree-and-random-forest-algorithm-explained/?utm_source=blog&utm_medium=click&utm_campaign=kdn+decision+trees+for+real+world)"
    to deeply address everything related to the Decision Tree algorithm and its extension
    “Random Forest algorithm”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Boosting: Boosting creates trees one after the other as each one seeks to fix
    the mistakes of the next one. Boosting techniques abound in algorithms including
    AdaBoost and Gradient Boosting. By emphasizing challenging-to-predict examples,
    these algorithms sometimes provide more exact models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperparameter Tuning**'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Hyperparameter tuning is the process of determining the optimal hyperparameter
    set for a decision tree model to raise its performance. Using methods like Grid
    Search or Random Search, whereby several combinations of hyperparameters are assessed
    to identify the best configuration, this can be accomplished.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, we’ve discussed the structure, working mechanism, real-world
    applications, and methods for enhancing decision tree performance.
  prefs: []
  type: TYPE_NORMAL
- en: Practicing decision trees is crucial to mastering their use and understanding
    their nuances. Working on real-world data projects can also provide valuable experience
    and improve problem-solving skills.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://twitter.com/StrataScratch)****[Nate Rosidi](https://twitter.com/StrataScratch)****
    is a data scientist and in product strategy. He''s also an adjunct professor teaching
    analytics, and is the founder of StrataScratch, a platform helping data scientists
    prepare for their interviews with real interview questions from top companies.
    Nate writes on the latest trends in the career market, gives interview advice,
    shares data science projects, and covers everything SQL.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Machine Learning from Scratch: Decision Trees](https://www.kdnuggets.com/2022/11/machine-learning-scratch-decision-trees.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Decision Trees vs Random Forests, Explained](https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Generalized and Scalable Optimal Sparse Decision Trees(GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Explainable AI: 10 Python Libraries for Demystifying Your Model''s Decisions](https://www.kdnuggets.com/2023/01/explainable-ai-10-python-libraries-demystifying-decisions.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Demystifying Bad Science](https://www.kdnuggets.com/2022/01/demystifying-bad-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Demystifying Machine Learning](https://www.kdnuggets.com/demystifying-machine-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
