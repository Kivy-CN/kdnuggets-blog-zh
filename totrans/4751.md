# Python 中的文本预处理：步骤、工具和示例

> 原文：[https://www.kdnuggets.com/2018/11/text-preprocessing-python.html](https://www.kdnuggets.com/2018/11/text-preprocessing-python.html)

![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](/2018/11/text-preprocessing-python.html?page=2#comments)

**作者：Olga Davydova，[数据怪兽](https://datamonsters.com)**。

在获取文本后，我们开始文本规范化。文本规范化包括：

+   将所有字母转换为小写或大写

+   将数字转换为单词或移除数字

+   移除标点符号、重音符号和其他变音符号

+   移除空格

+   扩展缩写

+   移除停用词、稀疏词汇和特定词汇

+   文本规范化

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持组织的 IT

* * *

我们将详细描述文本规范化步骤。

![](../Images/1b57451b11f6a00cd631ccd21101e282.png)

### **将文本转换为小写字母**

**示例 1\. 将文本转换为小写字母**

**Python 代码：**

```py

input_str = ”The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.”
input_str = input_str.lower()
print(input_str)

```

**输出：**

```py

the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.

```

### 移除数字

如果数字与分析无关，请移除它们。通常使用正则表达式来移除数字。

**示例 2\. 数字移除**

**Python 代码：**

```py

import re
input_str = ’Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.’
result = re.sub(r’\d+’, ‘’, input_str)
print(result)

```

**输出：**

```py

Box A contains red and white balls, while Box B contains red and blue balls.

```

### 移除标点符号

以下代码移除这一组符号 [!”#$%&’()*+,-./:;<=>?@[\]^_`{|}~]:

**示例 3\. 标点符号移除**

**Python 代码：**

```py

import string
input_str = “This &is [an] example? {of} string. with.? punctuation!!!!” # Sample string
result = input_str.translate(string.maketrans(“”,””), string.punctuation)
print(result)

```

**输出：**

```py

This is an example of string with punctuation

```

### 移除空白字符

要移除前导和结尾空格，可以使用 *strip()* 函数：

**示例 4\. 空格移除**

**Python 代码：**

```py

input_str = “ \t a string example\t “
input_str = input_str.strip()
input_str

```

**输出：**

```py

‘a string example’

```

### 分词

分词是将给定文本拆分成称为标记的小块的过程。单词、数字、标点符号等都可以视为标记。在 [这个表格](https://docs.google.com/spreadsheets/d/1-9rMhfcmxFv2V2Q5ZWn1FfLDZZYsuwb1eoSp9CiEEOg/edit?usp=sharing) (“分词”工作表)中描述了几种实现分词的工具。

表 1: **分词工具**

| **名称, 开发者, 初始发布** | **功能** | **编程语言** | **许可证** |
| --- | --- | --- | --- |
| [**自然语言工具包 (NLTK)**, 宾夕法尼亚大学, 2001](http://www.nltk.org/index.html) | 支持 Mac/Unix/Windows | Python | [Apache 许可证第 2.0 版](http://www.apache.org/licenses/LICENSE-2.0) |
| [包含许多语料库、玩具语法、训练模型等 [1].](http://www.nltk.org/index.html) |
| [**TextBlob**，Steven Loria，2013](http://textblob.readthedocs.io/en/dev/) | 将文本拆分为单词和句子 | Python | [http://textblob.readthedocs.io/en/dev/license.html](http://textblob.readthedocs.io/en/dev/license.html) |
| [WordNet 集成 [2]](http://textblob.readthedocs.io/en/dev/) |
| [**Spacy**，Explosion AI，2016](https://spacy.io/) | 运行于 Unix/Linux、MacOS/OS X 和 Windows。 | Python | [MIT 许可证](https://github.com/explosion/spaCy/blob/master/LICENSE) |
| 神经网络模型 |
| [多语言支持 [3]](https://spacy.io/usage/facts-figures) |
| [**Gensim**，RaRe Technologies，2009](https://radimrehurek.com/gensim/) | 可以处理大规模的网络语料库 | Python | [GNU LGPLv2.1 许可证](https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html) |
| 运行在 Linux、Windows 和 OS X 上 |
| [向量空间建模和主题建模 [4]](https://radimrehurek.com/gensim/) |
| [**Apache OpenNLP**，Apache 软件基金会，2004](https://opennlp.apache.org/) | 包含大量为各种语言预构建的模型 | Java | [Apache 许可证 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| [包括注释文本资源 [5]](https://opennlp.apache.org/) |
| [**OpenNMT**，Yoon Kim，harvardnlp，2016](http://opennmt.net/) | 是一个通用的深度学习框架，主要专注于序列到序列模型 | Python | [MIT 许可证](https://github.com/OpenNMT/OpenNMT/blob/master/LICENSE.md) |
| [可以通过命令行应用程序、客户端-服务器或库使用 [6]](http://opennmt.net/) | Lua |
| 目前有 3 个主要实现（OpenNMT-lua、OpenNMT-py、OpenNMT-tf） |   |
| [**General Architecture for Text Engineering (GATE)**，GATE 研究团队，谢菲联大学，1995](https://gate.ac.uk/) | 包含一个信息提取系统 | Java | [GNU 许可证及其他](http://www.gnu.org/licenses/) |
| 支持多种语言 |
| [接受多种格式的输入 [7]](https://gate.ac.uk/) |
| [**Apache UIMA**，IBM，Apache 软件基金会，2006](https://uima.apache.org/) | [包含附加组件和沙盒](https://uima.apache.org/sandbox.html) | Java、C++ | [Apache 许可证 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| 跨平台 |
| [REST 请求支持 [8]](https://uima.apache.org/) |
| [**Memory-Based Shallow Parser (MBSP)**，Vincent Van Asch，Tom De Smedt，2010](https://www.clips.uantwerpen.be/pages/MBSP#tokenizer) | 客户端-服务器架构 | Python | [GPL](http://www.gnu.org/licenses/gpl.html) |
| 包含二进制文件（TiMBL、MBT 和 MBLEM）为 Mac OS X 预编译 |
| [Cygwin 在 Windows 上的使用 [9]](https://www.clips.uantwerpen.be/pages/MBSP#tokenizer) |
| [**RapidMiner**，RapidMiner，2006](https://rapidminer.com/) | 统一平台 | RapidMiner 提供一个 GUI 来设计和执行分析工作流 | [AGPL](https://en.wikipedia.org/wiki/Affero_General_Public_License) |
| 可视化工作流设计 |
| 功能广泛 |
| [广泛的连接性 [10]](https://rapidminer.com/) |
| [**语言学习工具包 (MALLET)**，Andrew Kachites McCallum，马萨诸塞大学阿默斯特分校，2002](http://mallet.cs.umass.edu/) | 包含用于文档分类和序列标记的复杂工具 | Java | [通用公共许可证](https://opensource.org/licenses/cpl1.0.php) |
| [一般图形模型的推断支持 [11]](http://mallet.cs.umass.edu/) |
| [**Pattern**，T. De Smedt & W. Daeleman, 2012](https://www.clips.uantwerpen.be/pages/pattern-en#parser) | 网络挖掘模块 | Python | [BSD](http://www.linfo.org/bsdlicense.html) |
| 运行于 Windows、Mac 和 Linux |
| [支持多语言 [12]](https://www.clips.uantwerpen.be/pages/pattern) |
| [**斯坦福分词器**，斯坦福自然语言处理小组，2010](https://nlp.stanford.edu/software/tokenizer.html) | [分词器未单独分发，但包含在多个软件包中；](https://nlp.stanford.edu/software/) | Java | [GNU 通用公共许可证](http://www.gnu.org/licenses/gpl-2.0.html) |
| 速率约为每秒 1,000,000 个标记， |
| [有多种选项影响分词的执行方式 [13]](https://nlp.stanford.edu/software/tokenizer.html#About) |
| [**FreeLing**，TALP 研究中心，加泰罗尼亚理工大学](http://nlp.lsi.upc.edu/freeling/) | 提供语言分析功能 | C++ | [Affero GNU 通用公共许可证](http://www.gnu.org/licenses/agpl.html) |
| 支持多种语言 |
| 提供命令行前端 |
| [输出格式：XML, JSON, CoNLL [45]](http://nlp.lsi.upc.edu/freeling/) |

### 移除停用词

“停用词”是语言中最常见的词汇，如“the”、“a”、“on”、“is”、“all”。这些词不承载重要含义，通常会从文本中移除。可以使用[Natural Language Toolkit (NLTK)](http://www.nltk.org/)来移除停用词，这是一个用于符号和统计自然语言处理的库和程序套件。

**示例 7. 停用词移除**

**代码：**

```py

input_str = “NLTK is a leading platform for building Python programs to work with human language data.”
stop_words = set(stopwords.words(‘english’))
from nltk.tokenize import word_tokenize
tokens = word_tokenize(input_str)
result = [i for i in tokens if not i in stop_words]
print (result)

```

**输出：**

```py

[‘NLTK’, ‘leading’, ‘platform’, ‘building’, ‘Python’, ‘programs’, ‘work’, ‘human’, ‘language’, ‘data’, ‘.’]

```

[scikit-learn](http://scikit-learn.org/stable/)工具也提供了一个停用词列表：

```py

from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS

```

也可以使用[spaCy](https://spacy.io/)，这是一个免费的开源库：

```py

from spacy.lang.en.stop_words import STOP_WORDS

```

### 移除稀疏词汇和特定单词

在某些情况下，有必要从文本中移除稀疏词汇或特定单词。可以使用停用词去除技术来完成这项任务，因为任何词组都可以被选择为停用词。

### 词干提取

词干提取是将词语简化为其词干、基本形式或根形式的过程（例如，books — book, looked — look）。主要有两种算法，[Porter 词干提取算法](https://tartarus.org/martin/PorterStemmer/)（从词语中去除常见的形态学和屈折结尾[[14])](https://tartarus.org/martin/PorterStemmer/)和[Lancaster 词干提取算法](http://web.archive.org/web/20140827005744/http:/www.comp.lancs.ac.uk/computing/research/stemming/index.htm)（一种更激进的词干提取算法）。在[“词干提取”表格的工作表](https://docs.google.com/spreadsheets/d/1-9rMhfcmxFv2V2Q5ZWn1FfLDZZYsuwb1eoSp9CiEEOg/edit?usp=sharing)中描述了一些词干提取器。

| **名称, 开发者, 初始发布** | **特性** | **编程语言** | **许可证** |
| --- | --- | --- | --- |
| [**自然语言工具包（NLTK）**, 宾夕法尼亚大学, 2001](http://www.nltk.org/index.html) | 支持 Mac/Unix/Windows | Python | [Apache 许可证第 2.0 版](http://www.apache.org/licenses/LICENSE-2.0) |
| [包含许多语料库、玩具语法、训练模型等 [1].](http://www.nltk.org/index.html) |
| [**TextBlob**, Steven Loria, 2013](http://textblob.readthedocs.io/en/dev/) | 将文本拆分为词和句子 | Python | [http://textblob.readthedocs.io/en/dev/license.html](http://textblob.readthedocs.io/en/dev/license.html) |
| [WordNet 集成 [2]](http://textblob.readthedocs.io/en/dev/) |
| [**Spacy**, Explosion AI, 2016](https://spacy.io/) | 运行于 Unix/Linux、MacOS/OS X 和 Windows。 | Python | [MIT 许可证](https://github.com/explosion/spaCy/blob/master/LICENSE) |
| 神经网络模型 |
| [多语言支持 [3]](https://spacy.io/usage/facts-figures) |
| [**Gensim**, RaRe Technologies, 2009](https://radimrehurek.com/gensim/) | 可以处理大型的网络规模语料库 | Python | [GNU LGPLv2.1 许可证](https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html) |
| 运行于 Linux、Windows 和 OS X |
| [向量空间建模和主题建模 [4]](https://radimrehurek.com/gensim/) |
| [**Apache OpenNLP**, Apache 软件基金会, 2004](https://opennlp.apache.org/) | 包含大量预构建的各种语言模型 | Java | [Apache 许可证，第 2.0 版](https://www.apache.org/licenses/LICENSE-2.0) |
| [包含注释文本资源 [5]](https://opennlp.apache.org/) |
| [**OpenNMT**, Yoon Kim, harvardnlp, 2016](http://opennmt.net/) | 是一个通用的深度学习框架，主要专注于序列到序列模型 | Python | [MIT 许可证](https://github.com/OpenNMT/OpenNMT/blob/master/LICENSE.md) |
| [可以通过命令行应用程序、客户端-服务器或库使用 [6]](http://opennmt.net/) | Lua |
| 目前有 3 个主要实现（OpenNMT-lua, OpenNMT-py, OpenNMT-tf） |  |
| [**文本工程通用架构 (GATE)**, GATE 研究团队, 什鲁斯伯大学, 1995](https://gate.ac.uk/) | 包含信息提取系统 | Java | [GNU 许可证及其他](http://www.gnu.org/licenses/) |
| 多语言支持 |
| [接受多种格式的输入 [7]](https://gate.ac.uk/) |
| [**Apache UIMA**, IBM, Apache 软件基金会, 2006](https://uima.apache.org/) | [包含附加组件和沙盒](https://uima.apache.org/sandbox.html) | Java, C++ | [Apache 许可证 2.0](https://www.apache.org/licenses/LICENSE-2.0) |
| 跨平台 |
| [REST 请求支持 [8]](https://uima.apache.org/) |
| [**基于记忆的浅层解析器 (MBSP)**, Vincent Van Asch, Tom De Smedt, 2010](https://www.clips.uantwerpen.be/pages/MBSP#tokenizer) | 客户端-服务器架构 | Python | [GPL](http://www.gnu.org/licenses/gpl.html) |
| 包括为 Mac OS X 预编译的二进制文件（TiMBL、MBT 和 MBLEM） |
| [Windows 上的 Cygwin 使用 [9]](https://www.clips.uantwerpen.be/pages/MBSP#tokenizer) |
| [**RapidMiner**, RapidMiner, 2006](https://rapidminer.com/) | 统一平台 | RapidMiner 提供图形界面设计和执行分析工作流 | [AGPL](https://en.wikipedia.org/wiki/Affero_General_Public_License) |
| 可视化工作流设计 |
| 功能广泛 |
| [广泛的连接性 [10]](https://rapidminer.com/) |
| [**语言工具包机器学习 (MALLET)**, Andrew Kachites McCallum, 马萨诸塞大学阿默斯特分校, 2002](http://mallet.cs.umass.edu/) | 包含文档分类和序列标记的复杂工具 | Java | [通用公共许可证](https://opensource.org/licenses/cpl1.0.php) |
| [对一般图形模型的推理支持 [11]](http://mallet.cs.umass.edu/) |
| [**Pattern**, T. De Smedt & W. Daeleman, 2012](https://www.clips.uantwerpen.be/pages/pattern-en#parser) | 网络挖掘模块 | Python | [BSD](http://www.linfo.org/bsdlicense.html) |
| 支持 Windows、Mac 和 Linux |
| [多语言支持 [12]](https://www.clips.uantwerpen.be/pages/pattern) |
| [**斯坦福分词器**, 斯坦福自然语言处理小组, 2010](https://nlp.stanford.edu/software/tokenizer.html) | [分词器没有单独分发，但包含在几个软件下载中；](https://nlp.stanford.edu/software/) | Java | [GNU 通用公共许可证](http://www.gnu.org/licenses/gpl-2.0.html) |
| 每秒约 1,000,000 个标记 |
| [有多个选项影响分词的执行方式 [13]](https://nlp.stanford.edu/software/tokenizer.html#About) |
| [**FreeLing**, TALP 研究中心, 加泰罗尼亚理工大学](http://nlp.lsi.upc.edu/freeling/) | 提供语言分析功能 | C++ | [Affero GNU 通用公共许可证](http://www.gnu.org/licenses/agpl.html) |
| 支持多种语言 |
| 提供命令行前端 |
| [输出格式：XML, JSON, CoNLL [45]](http://nlp.lsi.upc.edu/freeling/) |

**词干提取工具**

**示例 8\. 使用 NLTK 的词干提取：**

**代码：**

```py

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
stemmer= PorterStemmer()
input_str=”There are several types of stemming algorithms.”
input_str=word_tokenize(input_str)
for word in input_str:
    print(stemmer.stem(word))

```

**输出：**

```py

There are sever type of stem algorithm.

```

### 词形还原

词形还原的目标与词干提取类似，即将词形变化形式简化为一个通用的基础形式。与词干提取不同，词形还原不仅仅是剪切词形变化。相反，它使用词汇知识库来获取单词的正确基础形式。

词形还原工具在上述库中提供： [NLTK (WordNet 词形还原器)](http://www.nltk.org/_modules/nltk/stem/wordnet.html)， [spaCy](https://spacy.io/api/lemmatizer)， [TextBlob](http://textblob.readthedocs.io/en/dev/quickstart.html#words-inflection-and-lemmatization)， [Pattern](https://www.clips.uantwerpen.be/pages/pattern-en#conjugation)， [gensim](https://radimrehurek.com/gensim/utils.html)， [斯坦福 CoreNLP](https://stanfordnlp.github.io/CoreNLP/simple.html)， [基于记忆的浅层解析器 (MBSP)](https://www.clips.uantwerpen.be/pages/MBSP#lemmatizer)， [Apache OpenNLP](https://opennlp.apache.org/docs/1.8.4/manual/opennlp.html#tools.lemmatizer.tagging.cmdline)， [Apache Lucene](http://lucene.apache.org/core/)， [通用文本工程架构 (GATE)](https://gate.ac.uk/)， [伊利诺伊词形还原器](https://cogcomp.org/page/software_view/illinois-lemmatizer)，以及 [DKPro Core](https://dkpro.github.io/dkpro-core/releases/1.8.0/docs/component-reference.html#_lemmatizer)。

**示例 9\. 使用 NLTK 的词形还原：**

**代码：**

```py

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
lemmatizer=WordNetLemmatizer()
input_str=”been had done languages cities mice”
input_str=word_tokenize(input_str)
for word in input_str:
    print(lemmatizer.lemmatize(word))

```

**输出：**

```py

be have do language city mouse

```

### 词性标注 (POS)

词性标注的目的是根据每个单词的定义和上下文，将其分配为词性（如名词、动词、形容词等）。有许多包含词性标注器的工具，包括 [NLTK](http://www.nltk.org/book/ch05.html)， [spaCy](https://spacy.io/usage/linguistic-features)， [TextBlob](http://textblob.readthedocs.io/en/dev/quickstart.html#part-of-speech-tagging)， [Pattern](https://www.clips.uantwerpen.be/pages/pattern-en#parser)， [斯坦福 CoreNLP](https://nlp.stanford.edu/software/tagger.shtml)， [基于记忆的浅层解析器 (MBSP)](https://www.clips.uantwerpen.be/pages/MBSP#parser)， [Apache OpenNLP](https://opennlp.apache.org/docs/1.8.4/manual/opennlp.html#tools.postagger.tagging)， [Apache Lucene](https://lucene.apache.org/core/)， [通用文本工程架构 (GATE)](https://gate.ac.uk/)， [FreeLing](http://nlp.lsi.upc.edu/freeling/)， [伊利诺伊词性标注器](https://cogcomp.org/page/software_view/POS)，以及 [DKPro Core](https://dkpro.github.io/dkpro-core/releases/1.9.0/docs/component-reference.html#_part_of_speech_tagger)。

**示例 10\. 使用 TextBlob 的词性标注：**

**代码：**

```py

input_str=”Parts of speech examples: an article, to write, interesting, easily, and, of”
from textblob import TextBlob
result = TextBlob(input_str)
print(result.tags)

```

**输出：**

```py

[(‘Parts’, u’NNS’), (‘of’, u’IN’), (‘speech’, u’NN’), (‘examples’, u’NNS’), (‘an’, u’DT’), (‘article’, u’NN’), (‘to’, u’TO’), (‘write’, u’VB’), (‘interesting’, u’VBG’), (‘easily’, u’RB’), (‘and’, u’CC’), (‘of’, u’IN’)]

```

### 词块提取（浅层解析）

Chunking 是一种自然语言处理过程，它识别句子的组成部分（名词、动词、形容词等），并将其链接到具有离散语法意义的更高级单元（名词组或短语、动词组等）[[23]](https://en.wikipedia.org/wiki/Shallow_parsing)。Chunking 工具： [NLTK](http://www.nltk.org/book/ch07.html)， [TreeTagger chunker](http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/)， [Apache OpenNLP](https://opennlp.apache.org/docs/1.8.4/manual/opennlp.html#tools.parser.chunking)， [通用文本工程架构 (GATE)](https://gate.ac.uk/)， [FreeLing](http://nlp.lsi.upc.edu/freeling/)。

**示例 11\. 使用 NLTK 进行 chunking：**

第一步是确定每个词的词性：

**代码：**

```py

input_str=”A black television and a white stove were bought for the new apartment of John.”
from textblob import TextBlob
result = TextBlob(input_str)
print(result.tags)

```

**输出：**

```py

[(‘A’, u’DT’), (‘black’, u’JJ’), (‘television’, u’NN’), (‘and’, u’CC’), (‘a’, u’DT’), (‘white’, u’JJ’), (‘stove’, u’NN’), (‘were’, u’VBD’), (‘bought’, u’VBN’), (‘for’, u’IN’), (‘the’, u’DT’), (‘new’, u’JJ’), (‘apartment’, u’NN’), (‘of’, u’IN’), (‘John’, u’NNP’)]

```

第二步是 chunking：

**代码：**

```py

reg_exp = “NP: {?*}”
rp = nltk.RegexpParser(reg_exp)
result = rp.parse(result.tags)
print(result)

```

**输出：**

```py

(S (NP A/DT black/JJ television/NN) and/CC (NP a/DT white/JJ stove/NN) were/VBD bought/VBN for/IN (NP the/DT new/JJ apartment/NN)
of/IN John/NNP)

```

也可以使用代码 result.draw() 绘制句子树结构

![](../Images/decd12c76c9568338ba6cc4043f64bfa.png)

### 命名实体识别

命名实体识别（NER）的目标是找出文本中的命名实体，并将它们分类到预定义的类别中（人名、地点、组织、时间等）。

命名实体识别工具： [NLTK](http://www.nltk.org/book/ch07.html)， [spaCy](https://spacy.io/usage/linguistic-features#section-named-entities)， [通用文本工程架构 (GATE) — ANNIE](https://gate.ac.uk/sale/tao/splitch6.html#chap:annie)， [Apache OpenNLP](https://opennlp.apache.org/docs/1.8.4/manual/opennlp.html#tools.namefind.recognition)， [斯坦福 CoreNLP](https://nlp.stanford.edu/software/CRF-NER.shtml)， [DKPro Core](https://dkpro.github.io/dkpro-core/releases/1.9.0/docs/component-reference.html#_named_entity_recognizer)， [MITIE](https://github.com/mit-nlp/MITIE)， [Watson 自然语言理解](https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/#entities)， [TextRazor](https://www.textrazor.com/)， [FreeLing](http://nlp.lsi.upc.edu/freeling/)在[“NER”表格页](https://docs.google.com/spreadsheets/d/1-9rMhfcmxFv2V2Q5ZWn1FfLDZZYsuwb1eoSp9CiEEOg/edit?usp=sharing)中描述。

| **名称，开发者，初始发布** | **特性** | **编程语言** | **许可证** |
| --- | --- | --- | --- |
| [**Baleen**, 国防科学与技术实验室 (Dstl), 2014](https://github.com/dstl/baleen) | 处理非结构化和半结构化数据源 | Java | [Apache 许可证 2.0](https://github.com/dstl/baleen/blob/master/LICENSE.txt) |
| 包含一个内置服务器 |
| [[25]](https://github.com/dstl/baleen/) |
| [**CogComp NER 标注器 (Illinois Named Entity Tagger)**, L. Ratinov, D. Roth, 认知计算组, 2009](https://github.com/CogComp/cogcomp-nlp/tree/master/ner) | 用命名实体标记纯文本 | Java | [许可协议](http://cogcomp.org/page/download_view/NETagger) |
| 4-标签类型集（人员/组织/地点/其他） |
| [18-label type set (基于OntoNotes语料库) [26]](https://github.com/CogComp/cogcomp-nlp/tree/master/ner) |
| [**Minimal Named-Entity Recognizer (MER)**, LaSIGE, Faculdade de Ciências, Universidade de Lisboa, Portugal, 2017](https://github.com/lasigeBioTM/MER) | 返回文本中识别出的术语列表，包括其确切位置（注释） | GNU awk | - |
| [仅需包含兴趣实体术语列表的词典（文本文件）RESTful Web 服务](http://labs.rd.ciencias.ulisboa.pt/mer/) |
| [[27]](https://github.com/lasigeBioTM/MER) |
| [**ParallelDots**, ParallelDots](https://www.paralleldots.com/named-entity-recognition) | 使用深度学习技术来确定字符分组的表示 | [excel 插件](https://www.paralleldots.com/excel-docs) | [定价](https://www.paralleldots.com/pricing) |
| 发现文本内容中最相关的实体 | [AI APIs](https://www.paralleldots.com/) |
| 精确、实时、可定制 |  |
| [[28]](https://blog.paralleldots.com/product/dig-relevant-text-elements-entity-extraction-api/) |  |
| [演示](https://www.paralleldots.com/text-analysis-apis#named-entity-recognition) |  |
| [**Open Calais**, Thomson Reuters Corporation](http://www.opencalais.com/about-open-calais/) | [提取实体（公司、人物、地点、产品等）、关系、事实、事件、主题 [29]](http://www.opencalais.com/about-open-calais/) | [API](http://www.opencalais.com/opencalais-api/) | [服务条款](http://www.opencalais.com/open-calais-terms-of-service/) |
| [**LingPipe**, Breck Baldwin, 1999](http://alias-i.com/lingpipe/index.html) | 查找人物、组织或地点的名称 | Java | [许可矩阵](http://alias-i.com/lingpipe/web/download.html) |
| 源代码和单元测试 |
| [多语言、多领域、多类型模型 [30]](http://alias-i.com/lingpipe/index.html) |
| [**命名实体识别工具**, Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer, 2016](https://github.com/glample/tagger) | 一种神经架构 | Python | [Apache 许可证 2.0](https://github.com/glample/tagger/blob/master/LICENSE.md) |
| [在4个CoNLL数据集（英语、西班牙语、德语和荷兰语）上实现了最先进的NER性能，而无需任何特定语言的知识或资源，例如地名词典 [31]](https://github.com/glample/tagger) |
| [**MinorThird**, William W. Cohen, Carnegie Mellon University, 2004](http://minorthird.sourceforge.net/old/doc/) | 结合了注释和可视化文本的工具与最先进的学习方法 | Java | [BSD 许可证](https://opensource.org/licenses/bsd-license.php) |
| 支持主动学习和在线学习 |
| [[32]](http://minorthird.sourceforge.net/old/doc/) |
| [**Watson 命名实体识别注释器**, IBM](https://www.ibm.com/support/knowledgecenter/en/SS8NLW_10.0.0/com.ibm.watson.wex.aac.doc/aac-tasystemt.html) | 人物、地点和组织注释器 | Python SDK | [定价](https://www.ibm.com/watson-analytics/pricing) |
| 英语、中文、法语、德语、日语、西班牙语 | Node SDK |
| [添加条目的可能性 [33]](https://www.ibm.com/support/knowledgecenter/en/SS8NLW_10.0.0/com.ibm.watson.wex.aac.doc/aac-tasystemt.html) | Swift SDK |
|  | Java SDK |
|  | Unity SDK |
|  | .NET 标准库 |
| [**PoolParty Semantic Suite**, 语义网公司, 2009](https://www.poolparty.biz/) | 模块化且灵活 | 数据被转换为 RDF 图形，并可以使用 SPARQL 查询 | [价格概览](https://www.poolparty.biz/priceoverview/) |
| 使用 W3C 定义的标准技术 |
| 通过有价值的元数据丰富信息 |
| [[34]](https://www.poolparty.biz/) |
| [**Rosette 实体提取器**, Basis Technology, 1995](https://www.basistech.com/text-analytics/rosette/entity-extractor/) | 支持 20 种语言 | 绑定：cURL、Python、PHP、Java、R、Ruby、C#、Node.js | - |
| 检测到 18 种实体类型 |
| 筛选关键实体 |
| [每个结果的置信度分数 [35]](https://www.basistech.com/text-analytics/rosette/entity-extractor/) |

**NER 工具**

### 更多相关话题

+   [在 Pandas 中清理和预处理 NLP 任务的文本数据](https://www.kdnuggets.com/cleaning-and-preprocessing-text-data-in-pandas-for-nlp-tasks)

+   [掌握数据清理和预处理技巧的 7 个步骤](https://www.kdnuggets.com/2023/08/7-steps-mastering-data-cleaning-preprocessing-techniques.html)

+   [SQL LIKE 操作符示例](https://www.kdnuggets.com/2022/09/sql-like-operator-examples.html)

+   [示例学习集](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)

+   [选择示例以理解机器学习模型](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)

+   [Python 数据预处理简单指南](https://www.kdnuggets.com/2020/07/easy-guide-data-preprocessing-python.html)
