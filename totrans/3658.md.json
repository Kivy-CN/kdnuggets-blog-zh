["```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport gradio as gr\nimport torch\n\ntitle = \"????AI ChatBot\"\ndescription = \"A State-of-the-Art Large-scale Pretrained Response generation model (DialoGPT)\"\nexamples = [[\"How are you?\"]]\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n\ndef predict(input, history=[]):\n    # tokenize the new input sentence\n    new_user_input_ids = tokenizer.encode(\n        input + tokenizer.eos_token, return_tensors=\"pt\"\n    )\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([torch.LongTensor(history), new_user_input_ids], dim=-1)\n\n    # generate a response\n    history = model.generate(\n        bot_input_ids, max_length=4000, pad_token_id=tokenizer.eos_token_id\n    ).tolist()\n\n    # convert the tokens to text, and then split the responses into lines\n    response = tokenizer.decode(history[0]).split(\"<|endoftext|>\")\n    # print('decoded_response-->>'+str(response))\n    response = [\n        (response[i], response[i + 1]) for i in range(0, len(response) - 1, 2)\n    ]  # convert to tuples of list\n    # print('response-->>'+str(response))\n    return response, history\n\ngr.Interface(\n    fn=predict,\n    title=title,\n    description=description,\n    examples=examples,\n    inputs=[\"text\", \"state\"],\n    outputs=[\"chatbot\", \"state\"],\n    theme=\"finlaymacklon/boxy_violet\",\n).launch()\n```", "```py\ntransformers\ntorch\n```"]