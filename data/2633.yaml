- en: Machine learning adversarial attacks are a ticking time bomb
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的对抗性攻击是一颗定时炸弹
- en: 原文：[https://www.kdnuggets.com/2021/01/machine-learning-adversarial-attacks.html](https://www.kdnuggets.com/2021/01/machine-learning-adversarial-attacks.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/01/machine-learning-adversarial-attacks.html](https://www.kdnuggets.com/2021/01/machine-learning-adversarial-attacks.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '![](../Images/cf462880f32504c8511171ef0dd53996.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf462880f32504c8511171ef0dd53996.png)'
- en: If you’ve been following news about artificial intelligence, you’ve probably
    heard of or seen modified images of pandas and turtles and stop signs that look
    ordinary to the human eye but cause AI systems to behave erratically. Known as [adversarial
    examples or adversarial attacks](https://bdtechtalks.com/2018/12/27/deep-learning-adversarial-attacks-ai-malware/),
    these images—and their [audio](https://bdtechtalks.com/2019/04/29/ai-audio-adversarial-examples/) and
    textual [counterparts](https://bdtechtalks.com/2019/04/02/ai-nlp-paraphrasing-adversarial-attacks/)—have
    become a source of growing interest and concern for the machine learning community.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你关注人工智能的新闻，你可能听说过或见过修改过的图像，例如看起来对人眼普通但让AI系统行为异常的熊猫、乌龟和停车标志。这些被称为[对抗性示例或对抗性攻击](https://bdtechtalks.com/2018/12/27/deep-learning-adversarial-attacks-ai-malware/)，这些图像及其[音频](https://bdtechtalks.com/2019/04/29/ai-audio-adversarial-examples/)和文本[对应物](https://bdtechtalks.com/2019/04/02/ai-nlp-paraphrasing-adversarial-attacks/)已成为机器学习社区日益关注的焦点。
- en: But despite the growing body of research on [adversarial machine learning](https://bdtechtalks.com/2020/07/15/machine-learning-adversarial-examples/),
    the numbers show that there has been little progress in tackling adversarial attacks
    in real-world applications.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管关于[对抗性机器学习](https://bdtechtalks.com/2020/07/15/machine-learning-adversarial-examples/)的研究不断增多，但数据显示在实际应用中应对对抗性攻击的进展甚微。
- en: The fast-expanding adoption of machine learning makes it paramount that the
    tech community traces a roadmap to secure the AI systems against adversarial attacks.
    Otherwise, adversarial machine learning can be a disaster in the making.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 快速扩展的机器学习应用使得技术社区必须制定一个路线图，以确保AI系统免受对抗性攻击的威胁。否则，对抗性机器学习可能会酿成灾难。
- en: '![](../Images/28f6c254c67fed2d1f983de3e635d807.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28f6c254c67fed2d1f983de3e635d807.png)'
- en: '*AI researchers discovered that by adding small black and white stickers to
    stop signs, they could make them invisible to computer vision algorithms (Source:
    arxiv.org).*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*AI研究人员发现，通过在停车标志上添加小的黑白贴纸，他们可以使这些标志对计算机视觉算法不可见（来源：arxiv.org）。*'
- en: What makes adversarial attacks different?
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么使得对抗性攻击不同？
- en: Every type of software has its own unique security vulnerabilities, and with
    new trends in software, new threats emerge. For instance, as web applications
    with database backends started replacing static websites, SQL injection attacks
    became prevalent. The widespread adoption of browser-side scripting languages
    gave rise to cross-site scripting attacks. Buffer overflow attacks overwrite critical
    variables and execute malicious code on target computers by taking advantage of
    the way programming languages such as C handle memory allocation. Deserialization
    attacks exploit flaws in the way programming languages such as Java and Python
    transfer information between applications and processes. And more recently, we’ve
    seen a surge in [prototype pollution attacks](https://portswigger.net/daily-swig/prototype-pollution-the-dangerous-and-underrated-vulnerability-impacting-javascript-applications),
    which use peculiarities in the JavaScript language to cause erratic behavior on
    NodeJS servers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 每种软件都有其独特的安全漏洞，随着软件的新趋势，新威胁也会出现。例如，随着数据库后台的网络应用程序开始取代静态网站，SQL注入攻击变得普遍。浏览器端脚本语言的广泛应用催生了跨站脚本攻击。缓冲区溢出攻击通过利用编程语言如C处理内存分配的方式，覆盖关键变量并在目标计算机上执行恶意代码。反序列化攻击利用编程语言如Java和Python在应用程序和进程之间传输信息的缺陷。最近，我们还看到[原型污染攻击](https://portswigger.net/daily-swig/prototype-pollution-the-dangerous-and-underrated-vulnerability-impacting-javascript-applications)的激增，这种攻击利用JavaScript语言中的特性导致NodeJS服务器上的异常行为。
- en: In this regard, adversarial attacks are no different than other cyber threats.
    As machine learning becomes [an important component of many applications](https://bdtechtalks.com/2019/12/30/computer-vision-applications-deep-learning/),
    bad actors will look for ways to plant and trigger malicious behavior in AI models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，对抗性攻击与其他网络威胁没有什么不同。随着机器学习成为许多应用的[重要组成部分](https://bdtechtalks.com/2019/12/30/computer-vision-applications-deep-learning/)，恶意行为者将寻找植入和触发
    AI 模型中恶意行为的方法。
- en: What makes adversarial attacks different, however, is their nature and the possible
    countermeasures. For most security vulnerabilities, the boundaries are very clear.
    Once a bug is found, security analysts can precisely document the conditions under
    which it occurs and find the part of the source code that is causing it. The response
    is also straightforward. For instance, SQL injection vulnerabilities are the result
    of not sanitizing user input. Buffer overflow bugs happen when you copy string
    arrays without setting limits on the number of bytes copied from the source to
    the destination.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使对抗性攻击与众不同的是其性质和可能的对策。对于大多数安全漏洞，界限非常清楚。一旦发现漏洞，安全分析师可以准确记录其发生的条件，并找到导致漏洞的源代码部分。响应也相对直接。例如，SQL
    注入漏洞是因为没有对用户输入进行清理。缓冲区溢出错误发生在复制字符串数组时没有设置从源到目的地的字节数限制。
- en: In most cases, adversarial attacks exploit peculiarities in the learned parameters
    of machine learning models. An attacker probes a target model by meticulously
    making changes to its input until it produces the desired behavior. For instance,
    by making gradual changes to the pixel values of an image, an attacker can cause
    the [convolutional neural network](https://bdtechtalks.com/2020/01/06/convolutional-neural-networks-cnn-convnets/) to
    change its prediction from, say, “turtle” to “rifle.” The adversarial perturbation
    is usually a layer of noise that is imperceptible to the human eye.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，对抗性攻击利用了机器学习模型中学习到的参数的特殊性。攻击者通过精细地对输入进行修改来探测目标模型，直到它产生期望的行为。例如，通过逐渐改变图像的像素值，攻击者可以使[卷积神经网络](https://bdtechtalks.com/2020/01/06/convolutional-neural-networks-cnn-convnets/)将预测结果从“乌龟”改变为“步枪”。对抗性扰动通常是一层对人眼不可察觉的噪音。
- en: '(Note: in some cases, such as [data poisoning](https://bdtechtalks.com/2020/10/07/machine-learning-data-poisoning/),
    adversarial attacks are made possible through vulnerabilities in other components
    of the machine learning pipeline, such as a tampered training data set.)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: （注：在某些情况下，例如[数据中毒](https://bdtechtalks.com/2020/10/07/machine-learning-data-poisoning/)，对抗性攻击通过机器学习管道中其他组件的漏洞成为可能，例如被篡改的训练数据集。）
- en: '![](../Images/de7ad72408df5d0445d289e0be91c91b.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de7ad72408df5d0445d289e0be91c91b.png)'
- en: '*A neural network thinks this is a picture of a rifle. The human vision system
    would never make this mistake (source: [LabSix](http://www.labsix.org/physical-objects-that-fool-neural-nets/)).*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*一个神经网络认为这是一张步枪的图片。人类视觉系统绝不会犯这样的错误（来源：[LabSix](http://www.labsix.org/physical-objects-that-fool-neural-nets/)）。*'
- en: The statistical nature of machine learning makes it difficult to find and patch
    adversarial attacks. An adversarial attack that works under some conditions might
    fail in others, such as a change of angle or lighting conditions. Also, you can’t
    point to a line of code that is causing the vulnerability because it spread across
    the thousands and millions of parameters that constitute the model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的统计特性使得发现和修补对抗性攻击变得困难。一个在某些条件下有效的对抗性攻击可能在其他条件下失效，例如角度或光照条件的变化。此外，你无法指出导致漏洞的代码行，因为它分布在构成模型的数千或数百万个参数中。
- en: Defenses against adversarial attacks are also a bit fuzzy. Just as you can’t
    pinpoint a location in an AI model that is causing an adversarial vulnerability,
    you also can’t find a precise patch for the bug. Adversarial defenses usually
    involve statistical adjustments or general changes to the architecture of the
    machine learning model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性攻击的防御也有些模糊。就像你无法精确定位 AI 模型中导致对抗性漏洞的位置一样，你也无法找到修复漏洞的精确补丁。对抗性防御通常涉及统计调整或对机器学习模型架构的总体修改。
- en: For instance, one popular method is adversarial training, where researchers
    probe a model to produce adversarial examples and then retrain the model on those
    examples and their correct labels. Adversarial training readjusts all the parameters
    of the model to make it robust against the types of examples it has been trained
    on. But with enough rigor, an attacker can find other noise patterns to create
    adversarial examples.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，一种流行的方法是对抗性训练，研究人员通过对模型进行测试，生成对抗性样本，然后在这些样本及其正确标签上重新训练模型。对抗性训练会调整模型的所有参数，使其对训练过的样本类型具有鲁棒性。但通过足够的严谨性，攻击者可以找到其他噪声模式来创建对抗性样本。
- en: The plain truth is, we are still learning how to cope with adversarial machine
    learning. Security researchers are used to perusing code for vulnerabilities.
    Now they must learn to find security holes in machine learning that are composed
    of millions of numerical parameters.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 事实是，我们仍在学习如何应对对抗性机器学习。安全研究人员习惯于审查代码中的漏洞。现在，他们必须学会在包含数百万个数值参数的机器学习中找到安全漏洞。
- en: Growing interest in adversarial machine learning
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对对抗性机器学习的兴趣不断增长
- en: Recent years have seen a surge in the number of papers on adversarial attacks.
    To track the trend, I searched the arXiv preprint server for papers that mention
    “adversarial attacks” or “adversarial examples” in the abstract section. In [2014](https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=%22adversarial+attack%22&terms-0-field=abstract&terms-1-operator=OR&terms-1-term=%22adversarial+example%22&terms-1-field=abstract&classification-physics_archives=all&classification-include_cross_list=include&date-filter_by=specific_year&date-year=2014&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size=50&order=-announced_date_first),
    there were zero papers on adversarial machine learning. [In 2020](https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=%22adversarial+attack%22&terms-0-field=abstract&terms-1-operator=OR&terms-1-term=%22adversarial+example%22&terms-1-field=abstract&classification-physics_archives=all&classification-include_cross_list=include&date-filter_by=specific_year&date-year=2020&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size=50&order=-announced_date_first),
    around 1,100 papers on adversarial examples and attacks were submitted to arxiv.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，对抗性攻击的论文数量激增。为了追踪这一趋势，我在arXiv预印本服务器上搜索了在摘要部分提到“对抗性攻击”或“对抗性样本”的论文。在[2014年](https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=%22adversarial+attack%22&terms-0-field=abstract&terms-1-operator=OR&terms-1-term=%22adversarial+example%22&terms-1-field=abstract&classification-physics_archives=all&classification-include_cross_list=include&date-filter_by=specific_year&date-year=2014&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size=50&order=-announced_date_first)，对抗性机器学习领域的论文数量为零。[在2020年](https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=%22adversarial+attack%22&terms-0-field=abstract&terms-1-operator=OR&terms-1-term=%22adversarial+example%22&terms-1-field=abstract&classification-physics_archives=all&classification-include_cross_list=include&date-filter_by=specific_year&date-year=2020&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size=50&order=-announced_date_first)，提交到arxiv的对抗性样本和攻击的论文数量约为1,100篇。
- en: '![](../Images/2ca23e3db2f231abbe93c59246d42cda.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ca23e3db2f231abbe93c59246d42cda.png)'
- en: '*From 2014 to 2020, arXiv.org has gone from zero papers on adversarial machine
    learning to 1,100 papers in one year.*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*从2014年到2020年，arXiv.org上的对抗性机器学习论文从零篇增加到一年内的1,100篇。*'
- en: Adversarial attacks and defense methods have also become a key highlight of
    prominent AI conferences such as NeurIPS and ICLR. Even cybersecurity conferences
    such as DEF CON, Black Hat, and Usenix have started featuring workshops and presentations
    on adversarial attacks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性攻击和防御方法也已成为著名AI会议如NeurIPS和ICLR的重点。即使是像DEF CON、Black Hat和Usenix这样的网络安全会议，也开始举办关于对抗性攻击的研讨会和报告。
- en: The research presented at these conferences shows tremendous progress in detecting
    adversarial vulnerabilities and developing defense methods that can make machine
    learning models more robust. For instance, researchers have found new ways to
    protect machine learning models against adversarial attacks using [random switching
    mechanisms](https://bdtechtalks.com/2019/08/20/ai-adversarial-examples-hierarchical-random-switching/) and [insights
    from neuroscience](https://bdtechtalks.com/2020/12/07/vonenet-neurscience-inspired-deep-learning/).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些会议上展示的研究在检测对抗性漏洞和开发可以使机器学习模型更加稳健的防御方法方面取得了巨大进展。例如，研究人员发现了保护机器学习模型免受对抗性攻击的新方法，如使用[random
    switching mechanisms](https://bdtechtalks.com/2019/08/20/ai-adversarial-examples-hierarchical-random-switching/)和[神经科学的见解](https://bdtechtalks.com/2020/12/07/vonenet-neurscience-inspired-deep-learning/)。
- en: It is worth noting, however, that AI and security conferences focus on cutting
    edge research. And there’s a sizeable gap between the work presented at AI conferences
    and the practical work done at organizations every day.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值得注意的是，AI 和安全会议聚焦于前沿研究。在 AI 会议上展示的工作与组织日常进行的实际工作之间存在较大的差距。
- en: The lackluster response to adversarial attacks
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对对抗性攻击的反应乏善可陈
- en: Alarmingly, despite growing interest in and louder warnings on the threat of
    adversarial attacks, there’s very little activity around tracking adversarial
    vulnerabilities in real-world applications.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 令人担忧的是，尽管对对抗性攻击的关注日益增加，警告声也越来越大，但对实际应用中对抗性漏洞的追踪活动却非常有限。
- en: I referred to several sources that track bugs, vulnerabilities, and bug bounties.
    For instance, out of more than 145,000 records in the NIST National Vulnerability
    Database, there are no entries on adversarial attacks or adversarial examples.
    A search for “machine learning” returns five results. Most of them are cross-site
    scripting (XSS) and XML external entity (XXE) vulnerabilities in systems that
    contain machine learning components. One of them regards a vulnerability that
    allows an attacker to create a copy-cat version of a machine learning model and
    gain insights, which could be a window to adversarial attacks. But there are no
    direct reports on adversarial vulnerabilities. A search for “deep learning” shows
    a single [critical flaw](https://nvd.nist.gov/vuln/detail/CVE-2017-5719) filed
    in November 2017\. But again, it’s not an adversarial vulnerability but rather
    a flaw in another component of a deep learning system.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我查阅了几个跟踪漏洞、漏洞和漏洞悬赏的来源。例如，在 NIST 国家漏洞数据库中超过 145,000 条记录中，没有关于对抗性攻击或对抗性样本的条目。搜索“机器学习”返回了五个结果。大多数是系统中包含机器学习组件的跨站脚本（XSS）和
    XML 外部实体（XXE）漏洞。其中一个涉及一个漏洞，允许攻击者创建一个机器学习模型的仿制版本并获取见解，这可能是对抗性攻击的窗口。但没有关于对抗性漏洞的直接报告。搜索“深度学习”显示了一个[严重漏洞](https://nvd.nist.gov/vuln/detail/CVE-2017-5719)，记录于
    2017 年 11 月。但这不是对抗性漏洞，而是深度学习系统中另一个组件的缺陷。
- en: '![](../Images/362cef7d3d7e04b914489427224ac384.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/362cef7d3d7e04b914489427224ac384.png)'
- en: '*The National Vulnerability Database contains very little information on adversarial
    attacks.*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*国家漏洞数据库中关于对抗性攻击的信息非常少。*'
- en: I also checked GitHub’s Advisory database, which tracks security and bug fixes
    on projects hosted on GitHub. Search for “adversarial attacks,” “adversarial examples,”
    “machine learning,” and “deep learning” yielded no results. A search for “TensorFlow”
    yields 41 records, but they’re mostly bug reports on the codebase of TensorFlow.
    There’s nothing about adversarial attacks or hidden vulnerabilities in the parameters
    of TensorFlow models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我还检查了 GitHub 的 Advisory 数据库，它跟踪托管在 GitHub 上的项目的安全性和漏洞修复。搜索“对抗性攻击”、“对抗性样本”、“机器学习”和“深度学习”没有结果。搜索“Tensoflow”显示了
    41 条记录，但这些大多数是有关 TensorFlow 代码库的漏洞报告。没有关于对抗性攻击或 TensorFlow 模型参数中隐藏漏洞的内容。
- en: This is noteworthy because GitHub already hosts many [deep learning](https://bdtechtalks.com/2019/02/15/what-is-deep-learning-neural-networks/) models
    and pre-trained [neural networks](https://bdtechtalks.com/2019/08/05/what-is-artificial-neural-network-ann/).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这值得注意，因为 GitHub 上已经托管了许多[深度学习](https://bdtechtalks.com/2019/02/15/what-is-deep-learning-neural-networks/)模型和预训练的[神经网络](https://bdtechtalks.com/2019/08/05/what-is-artificial-neural-network-ann/)。
- en: '![](../Images/1de45823b690ce5f50dfda94c7905cbe.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1de45823b690ce5f50dfda94c7905cbe.png)'
- en: '*GitHub Advisory contains no records on adversarial attacks.*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*GitHub Advisory 中没有关于对抗性攻击的记录。*'
- en: Finally, I checked HackerOne, the platform many companies use to run bug bounty
    programs. Here too, none of the reports contained any mention of adversarial attacks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我查看了HackerOne，这是许多公司用来运行漏洞奖励计划的平台。在这里，也没有报告提到对抗性攻击。
- en: While this might not be a very precise assessment, the fact that none of these
    sources have anything on adversarial attacks is very telling.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能不是一个非常准确的评估，但这些来源中没有任何关于对抗性攻击的信息这一事实非常具有指示性。
- en: The growing threat of adversarial attacks
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对抗性攻击的威胁不断增长
- en: Adversarial vulnerabilities are deeply embedded in the many parameters of machine
    learning models, which makes it hard to detect them with traditional security
    tools.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性漏洞深嵌在机器学习模型的许多参数中，这使得用传统的安全工具检测它们变得困难。
- en: '![](../Images/f3e9cd8cc27c1f7233a8340b6dce235b.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3e9cd8cc27c1f7233a8340b6dce235b.png)'
- en: '*Automated defense is another area that is worth discussing. When it comes
    to code-based vulnerabilities, developers have a large set of defensive tools
    at their disposal.*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*自动化防御是另一个值得讨论的领域。涉及基于代码的漏洞时，开发者可以使用一系列防御工具。*'
- en: Static analysis tools can help developers find vulnerabilities in their code.
    Dynamic testing tools examine an application at runtime for vulnerable patterns
    of behavior. Compilers already use many of these techniques to track and patch
    vulnerabilities. Today, even your browser is equipped with tools to find and block
    possibly malicious code in client-side script.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 静态分析工具可以帮助开发者发现他们代码中的漏洞。动态测试工具在运行时检查应用程序的脆弱行为模式。编译器已经使用了许多这些技术来跟踪和修补漏洞。今天，即使是你的浏览器也配备了工具来发现和阻止可能的恶意代码。
- en: At the same time, organizations have learned to combine these tools with the
    right policies to enforce secure coding practices. Many companies have adopted
    procedures and practices to rigorously test applications for known and potential
    vulnerabilities before making them available to the public. For instance, GitHub,
    Google, and Apple make use of these and other tools to vet the millions of applications
    and projects uploaded on their platforms.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，组织已经学会将这些工具与适当的政策结合起来，以实施安全编码实践。许多公司已采用程序和做法，严格测试应用程序以发现已知和潜在的漏洞，然后再将其公开。例如，GitHub、Google和Apple利用这些工具和其他工具来审核在其平台上上传的数百万个应用程序和项目。
- en: But the tools and procedures for defending machine learning systems against
    adversarial attacks are still in the preliminary stages. This is partly why we
    see very few reports and advisories on adversarial attacks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，用于防御机器学习系统对抗性攻击的工具和程序仍处于初步阶段。这也是我们看到关于对抗性攻击的报告和警告非常少的部分原因。
- en: Meanwhile, another worrying trend is the growing use of deep learning models
    by developers of all levels. Ten years ago, only people who had a full understanding
    of machine learning and deep learning algorithms could use them in their applications.
    You had to know how to set up a neural network, tune the hyperparameters through
    intuition and experimentation, and you also needed access to the compute resources
    that could train the model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，另一个令人担忧的趋势是各级开发者对深度学习模型的日益使用。十年前，只有对机器学习和深度学习算法有全面了解的人才能在他们的应用中使用这些模型。你需要知道如何建立神经网络，通过直觉和实验调整超参数，并且还需要能够训练模型的计算资源。
- en: But today, integrating a pre-trained neural network into an application is very
    easy.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 但如今，将预训练的神经网络集成到应用程序中非常简单。
- en: For instance, PyTorch, which is one of the leading Python deep learning platforms, [has
    a tool](https://pytorch.org/docs/stable/hub.html) that enables machine learning
    engineers to publish pre-trained neural networks on GitHub and make them accessible
    to developers. If you want to integrate an image classifier deep learning model
    into your application, you only need a rudimentary knowledge of deep learning
    and PyTorch.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，PyTorch，这是领先的Python深度学习平台之一，[提供了一个工具](https://pytorch.org/docs/stable/hub.html)，使机器学习工程师能够在GitHub上发布预训练的神经网络，并使其对开发者可用。如果你想将图像分类深度学习模型集成到应用程序中，你只需具备基础的深度学习和PyTorch知识即可。
- en: Since GitHub has no procedure to detect and block adversarial vulnerabilities,
    a malicious actor could easily use these kinds of tools to publish deep learning
    models that have hidden backdoors and exploit them after thousands of developers
    integrate them in their applications.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GitHub没有检测和阻止对抗性漏洞的程序，恶意行为者可能会利用这些工具发布具有隐藏后门的深度学习模型，并在数千名开发者将其集成到应用程序后加以利用。
- en: How to address the threat of adversarial attacks
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何应对对抗性攻击的威胁
- en: '![](../Images/4283a29402dfb17dd75300b4deb3981a.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4283a29402dfb17dd75300b4deb3981a.png)'
- en: Understandably, given the statistical nature of adversarial attacks, it’s difficult
    to address them with the same methods used against code-based vulnerabilities.
    But fortunately, there have been some positive developments that can guide future
    steps.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 可以理解，由于对抗性攻击的统计性质，使用与应对基于代码的漏洞相同的方法来解决这些问题是困难的。但幸运的是，已有一些积极的发展可以指导未来的步骤。
- en: The [Adversarial ML Threat Matrix](https://bdtechtalks.com/2020/10/26/adversarial-machine-learning-threat-matrix/),
    published last month by researchers at Microsoft, IBM, Nvidia, MITRE, and other
    security and AI companies, provides security researchers with a framework to find
    weak spots and potential adversarial vulnerabilities in software ecosystems that
    include machine learning components. The Adversarial ML Threat Matrix follows
    the ATT&CK framework, a known and trusted format among security researchers.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[对抗性机器学习威胁矩阵](https://bdtechtalks.com/2020/10/26/adversarial-machine-learning-threat-matrix/)，由微软、IBM、Nvidia、MITRE及其他安全和人工智能公司研究人员于上个月发布，为安全研究人员提供了一个框架，以便在包括机器学习组件的软件生态系统中找到薄弱环节和潜在的对抗性漏洞。对抗性机器学习威胁矩阵遵循ATT&CK框架，这是安全研究人员熟知和信赖的格式。'
- en: Another useful project is IBM’s Adversarial Robustness Toolbox, an open-source
    Python library that provides tools to evaluate machine learning models for adversarial
    vulnerabilities and help developers harden their AI systems.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的项目是IBM的对抗性鲁棒性工具箱，这是一个开源Python库，提供了评估机器学习模型对抗性漏洞的工具，并帮助开发者加固他们的人工智能系统。
- en: These and other adversarial defense tools that will be developed in the future
    need to be backed by the right policies to make sure machine learning models are
    safe. Software platforms such as GitHub and Google Play must establish procedures
    and integrate some of these tools into the vetting process of applications that
    include machine learning models. Bug bounties for adversarial vulnerabilities
    can also be a good measure to make sure the machine learning systems used by millions
    of users are robust.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些和其他未来开发的对抗性防御工具需要得到适当政策的支持，以确保机器学习模型的安全。像GitHub和Google Play这样的软件平台必须建立程序，并将一些这些工具整合到包含机器学习模型的应用程序的审查过程中。对抗性漏洞的漏洞赏金也可以是确保数百万用户使用的机器学习系统强健的一个好措施。
- en: New regulations for the security of machine learning systems might also be necessary.
    Just as the software that handles sensitive operations and information is expected
    to conform to a set of standards, machine learning algorithms used in critical
    applications such as biometric authentication and medical imaging must be audited
    for robustness against adversarial attacks.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 可能还需要为机器学习系统的安全制定新规。正如处理敏感操作和信息的软件被期望符合一系列标准一样，用于生物识别认证和医学成像等关键应用的机器学习算法也必须经过对抗性攻击的鲁棒性审计。
- en: As the adoption of machine learning continues to expand, the threat of adversarial
    attacks is becoming more imminent. Adversarial vulnerabilities are a ticking timebomb.
    Only a systematic response can defuse it.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习的广泛应用，对抗性攻击的威胁变得越来越迫近。对抗性漏洞是一颗定时炸弹。只有系统性的应对才能解除它。
- en: '[Original](https://bdtechtalks.com/2020/12/16/machine-learning-adversarial-attacks-against-machine-learning-time-bomb/).
    Reposted with permission.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://bdtechtalks.com/2020/12/16/machine-learning-adversarial-attacks-against-machine-learning-time-bomb/)。经许可转载。'
- en: '**Related:**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Adversarial Examples in Deep Learning – A Primer](https://www.kdnuggets.com/2020/11/adversarial-examples-deep-learning-primer.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习中的对抗性样本——入门指南](https://www.kdnuggets.com/2020/11/adversarial-examples-deep-learning-primer.html)'
- en: '[Intro to Adversarial Machine Learning and Generative Adversarial Networks](https://www.kdnuggets.com/2019/10/adversarial-machine-learning-generative-adversarial-networks.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[对抗性机器学习和生成对抗网络简介](https://www.kdnuggets.com/2019/10/adversarial-machine-learning-generative-adversarial-networks.html)'
- en: '[Why Machine Learning is vulnerable to adversarial attacks and how to fix it](https://www.kdnuggets.com/2019/06/machine-learning-adversarial-attacks.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么机器学习容易受到对抗性攻击以及如何修复它](https://www.kdnuggets.com/2019/06/machine-learning-adversarial-attacks.html)'
- en: '* * *'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的 IT 工作'
- en: '* * *'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[GPT-4 is Vulnerable to Prompt Injection Attacks on Causing Misinformation](https://www.kdnuggets.com/2023/05/gpt4-vulnerable-prompt-injection-attacks-causing-misinformation.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT-4 易受提示注入攻击影响，导致错误信息](https://www.kdnuggets.com/2023/05/gpt4-vulnerable-prompt-injection-attacks-causing-misinformation.html)'
- en: '[What is Adversarial Machine Learning?](https://www.kdnuggets.com/2022/03/adversarial-machine-learning.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是对抗性机器学习？](https://www.kdnuggets.com/2022/03/adversarial-machine-learning.html)'
- en: '[Feature Stores for Real-time AI & Machine Learning](https://www.kdnuggets.com/2022/03/feature-stores-realtime-ai-machine-learning.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实时 AI 和机器学习的特征存储](https://www.kdnuggets.com/2022/03/feature-stores-realtime-ai-machine-learning.html)'
- en: '[Cutting Down Implementation Time by Integrating Jupyter and KNIME](https://www.kdnuggets.com/2021/12/cutting-implementation-time-integrating-jupyter-knime.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过集成 Jupyter 和 KNIME 减少实施时间](https://www.kdnuggets.com/2021/12/cutting-implementation-time-integrating-jupyter-knime.html)'
- en: '[Market Data and News: A Time Series Analysis](https://www.kdnuggets.com/2022/06/market-data-news-time-series-analysis.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[市场数据和新闻：时间序列分析](https://www.kdnuggets.com/2022/06/market-data-news-time-series-analysis.html)'
- en: '[A Faster Way to Prepare Time-Series Data with the AI & Analytics Engine](https://www.kdnuggets.com/2021/12/piexchange-faster-way-prepare-timeseries-data-ai-analytics-engine.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 AI 和分析引擎更快准备时间序列数据](https://www.kdnuggets.com/2021/12/piexchange-faster-way-prepare-timeseries-data-ai-analytics-engine.html)'
