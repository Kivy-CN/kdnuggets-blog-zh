- en: The Data Science Machine, or ‘How To Engineer Feature Engineering’
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学机器，或‘如何进行特征工程’
- en: 原文：[https://www.kdnuggets.com/2015/10/data-science-machine.html](https://www.kdnuggets.com/2015/10/data-science-machine.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2015/10/data-science-machine.html](https://www.kdnuggets.com/2015/10/data-science-machine.html)
- en: '![The Analytical Engine](../Images/048f638a522df26fbc8711547695f942.png) Recent
    research by MIT Master''s student [Max Kanter](http://www.jmaxkanter.com/) has
    led to the implementation of what he refers to as the ''Data Science Machine.''
    A paper on the Data Science Machine (DSM) and its underlying innovation, the Deep
    Feature Synthesis algorithm, by Kanter and [Kalyan Veeramachaneni](http://www.kalyanv.org/),
    his thesis supervisor at [CSAIL](https://www.csail.mit.edu/), is set to be presented
    at the IEEE International Conference on Data Science and Advanced Analytics next
    week. Their paper ''[Deep Feature Synthesis: Towards Automating Data Science Endeavors](http://groups.csail.mit.edu/EVO-DesignOpt/groupWebSite/uploads/Site/DSAA_DSM_2015.pdf)''
    is available online now. The DSM is concisely described by Kanter & Veeramachaneni
    as "an automated system for generating predictive models from raw data," which
    combines the authors'' innovative [feature engineering](https://www.kdnuggets.com/2018/12/feature-engineering-explained.html)
    approach with an end-to-end data science pipeline. The DSM has, thus far, managed
    to beat 68.9% of teams in data science competitions that it has been entered into.
    Perhaps most noteworthy, submissions which attain this success rate are generally
    completed in under 12 hours, as opposed to the months which teams of humans can
    labor for. The DSM is premised on the observations that data science competition
    problems generally have the following properties in common: they are structured
    and relational, they model human interaction with a complex system, and there
    is an attempt made to predict some aspect of humanity. **Deep Feature Synthesis**
    As with any data science problem, features must first be identified from existing
    variables, or be created from leveraging existing variables. While conceding that
    feature engineering has made significant recent advancements in the areas of non-relational
    data such as text and images, Kanter & Veeramachaneni note that it is still this
    task that most relies on human intervention in the data science pipeline, and
    can be difficult and time consuming even for seasoned data scientists. It is also
    this task that must most closely replicate the efficiency of a human being if
    it is to be truly automated. Deep Feature Selection (DFS), the DSM''s feature
    engineering algorithm, is strictly for relational datasets, and is used to automate
    the identification and generation of insight-eliciting features. DFS takes relational
    tables as input, and is able to process the various types of data held within
    such a data structure. To be successful, DFS aims to think like a data scientist,
    looking to turn insightful questions into input features. The DFS algorithm walks
    the relationships and applies feature-selection functions as it does so, creating
    a final feature step by step. As it performs this walk, DFS stacks the calculations
    of the mathematical functions to a particular depth, and this is where the name
    DFS is derived. Depending on the input data types, a number of mathematical functions
    are applied at 2 distinct levels in the DSM: *entity* and *relational*. Entity
    level features focus on conversion and translation functions, such as changing
    data representations, rounding numbers, and extracting existing generalized attributes
    into more numerous and concise attributes. Relational level features are concerned
    with the relationships between entities in tables (think about your primary and
    foreign keys). These feature functions are then able to extract related data from
    other tables to associate with a given feature (for example, finding the max item
    price or item count associated with an order), data which could potentially be
    exploited as a useful feature to feed into a model. **Machine Learning Pathway**
    To start off the DSM''s machine learning pathway, one of the input features is
    chosen to model, which is referred to as the target value, and which is used to
    form the prediction problem. Appropriate features, known as *predictors*, are
    selected via metadata to help in the prediction process. The DSM then creates
    a pathway for data preprocessing, feature selection, dimensionality reduction,
    modeling, and evaluation, all of which is parameterized and available for re-use
    if necessary. Parameter optimization is accomplished using a [Copula Process](https://en.wikipedia.org/wiki/Copula_(probability_theory)),
    and an attempt is made to reduce the number of features by observing correlation.
    The reduced set of features is then tested on sample data, recombining them in
    different ways to optimize the accuracy of the predictions they yield. By its
    use of autotuning, which the authors argue is asolutely critical to its performance,
    the DSM was able to increase its score at all three of its competitions. **Discussion**
    What this all seems to suggest, essentially, is this: The DSM uses intelligent
    relational database relation-walking to help build and establish candidate features,
    narrows this feature set down by looking for correlated values, and uses combinatorics
    in what amounts to brute force feature engineering, to apply iterative feature
    subsets to sample data while recombining them for optimization until the best
    possible solution is found. To measure the DSM''s performance, it was entered
    in competitions at KDD Cup 2014, IJCAI, and KDD Cup 2015, where, as mentioned,
    it outperformed more than 2/3 of the human competitors. Kanter & Veeramachaneni
    claim that even during its worst performance (IJCAI), the DSM still managed to
    frame the prediction problem in similar terms to human competitors, evidenced
    by the fact that it proceeded in the task by pursuing similar avenues of data
    modeling. In this same competition, it finished with an AUC difference of approximately
    0.04 of the contest winner, suggesting that the DSM captured what could be considered
    the major aspects of the competition dataset. Kanter & Veeramachaneni argue that,
    while it cannot currently compete with the highest performing human scientists,
    the DSM nevertheless has a role alongside them. Even though a number of humans
    beat out the DSM in each of its competitions, it was able to outperform the majority
    of them with considerably less effort (less than 12 hours versus months, in some
    cases). They suggest that, in light of this, it can be used for setting benchmarks
    as well as for fostering creativity. Front-loading feature engineering and generating
    sets of potential top performing sets could allow humans to move on to rethinking
    the problem within hours, effectively starting with the DSM solution and moving
    forward from that point. It should be noted that, while the DSM is impressive,
    it''s hardly the first system aiming to automate machine learning. Other examples
    include many systems that automatically build models to bid on advertising, or
    [KXEN Model Factory](http://www.realwire.com/releases/KXEN-Unveils-Analytical-Data-Management-and-Modeling-Factory-Supercharging-Agility-and-Productivity-in-Predictive-Analytics)
    (now part of SAP), which offered Automated Model Building already in 2010\. Also,
    it is clear that the DSM is *not* useful for all types of data, and is a system
    implemented solely focusing on the exploition of relational datasets. It is also
    yet to be shown that it can be effective in relational datasets that do not conform
    to the previously-identified data science competition problem pattern. The DSM
    has already been spun-off into a startup called [FeatureLab](http://www.featurelab.co/),
    touted as "Insights with an Interface," with Kanter as its CEO. The website states
    "Do more with your data, without more data scientists," and claims that it is
    the "best solution for companies looking to increase their data science resources."
    These are both bold claims, especially in light of the fact that none of the individual
    pieces of DSM can really be considered breakthroughs. It is entirely possible
    that FeatureLab gets lost in a cloud of "business intelligence" service platforms.
    But Big Data is going nowhere, and feature engineering has been one of the hottest
    topics in machine learning over the past 12 months. It just may be that the DSM''s
    particular combination of technologies at what may end up being the right time
    leads to a new way of thinking about data science. Margo Seltzer, a Harvard computer
    science professor, has stated in reference to the DSM, "I think what they''ve
    done is going to become the standard quickly - very quickly." If this is the case,
    FeatureLabs stands to be well-positioned. You can read more about Kanter & Veeramachaneni''s
    Data Science Machine [here](http://news.mit.edu/2015/automating-big-data-analysis-1016).
    **Bio: [Matthew Mayo](https://twitter.com/mattmayo13)** is a computer science
    graduate student currently working on his thesis parallelizing machine learning
    algorithms. He is also a student of data mining, a data enthusiast, and an aspiring
    machine learning scientist. **Related:**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '![分析引擎](../Images/048f638a522df26fbc8711547695f942.png) 麻省理工学院硕士生[马克斯·坎特](http://www.jmaxkanter.com/)的最新研究实现了他所称之为“数据科学机器”的技术。坎特和他在[CSAIL](https://www.csail.mit.edu/)的论文导师[卡利扬·维拉马查内尼](http://www.kalyanv.org/)合作撰写了一篇关于数据科学机器（DSM）及其基础创新——深度特征合成算法的论文，该论文将于下周在IEEE国际数据科学与高级分析会议上发布。他们的论文《[深度特征合成：自动化数据科学工作](http://groups.csail.mit.edu/EVO-DesignOpt/groupWebSite/uploads/Site/DSAA_DSM_2015.pdf)》现已在线发布。坎特和维拉马查内尼简明地描述DSM为“一个从原始数据中生成预测模型的自动化系统”，它将作者们创新的[特征工程](https://www.kdnuggets.com/2018/12/feature-engineering-explained.html)方法与端到端数据科学管道相结合。迄今为止，DSM在其参加的数据科学竞赛中击败了68.9%的团队。也许最值得注意的是，取得这一成功率的提交通常在12小时内完成，而不是需要几个月的人工工作。DSM的基础是观察到数据科学竞赛问题通常具有以下共同特性：它们是结构化和关系型的，它们建模人类与复杂系统的互动，并且尝试预测某些人类方面的内容。**深度特征合成**
    与任何数据科学问题一样，特征必须首先从现有变量中识别出来，或从现有变量中创建出来。虽然特征工程在非关系数据（如文本和图像）领域取得了显著进展，但坎特和维拉马查内尼指出，这一任务仍然是数据科学管道中最依赖人工干预的任务，即使对于经验丰富的数据科学家来说也可能是困难且耗时的。如果要真正实现自动化，这一任务也必须最接近人类的效率。DSM的特征工程算法深度特征选择（DFS）专门用于关系型数据集，用于自动化识别和生成洞察性特征。DFS以关系表为输入，能够处理这种数据结构中包含的各种数据类型。成功的DFS算法旨在像数据科学家一样思考，将洞察性问题转化为输入特征。DFS算法遍历关系，并在此过程中应用特征选择函数，逐步创建最终特征。DFS在执行这一遍历时，将数学函数的计算叠加到特定深度，这也是DFS名称的由来。根据输入数据类型，DSM在两个不同层次上应用多个数学函数：*实体*和*关系*。实体层特征关注于转换和翻译函数，例如更改数据表示、四舍五入数字，以及将现有的广义属性提取为更多且更简洁的属性。关系层特征关注于表中实体之间的关系（比如你的主键和外键）。这些特征函数能够从其他表中提取相关数据以与给定特征关联（例如，找到与订单相关的最大项目价格或项目数量），这些数据可能作为有用特征输入到模型中。**机器学习路径**
    为启动DSM的机器学习路径，首先选择一个输入特征作为目标值，用于形成预测问题。通过元数据选择适当的特征，称为*预测变量*，以帮助预测过程。DSM然后创建一个数据预处理、特征选择、降维、建模和评估的路径，所有这些都被参数化并在必要时可供重用。参数优化通过[Copula过程](https://en.wikipedia.org/wiki/Copula_(probability_theory))完成，并通过观察相关性来减少特征数量。减少后的特征集在样本数据上进行测试，通过不同的组合来优化预测准确性。通过使用自动调节功能（作者认为这是其性能绝对关键的因素），DSM在所有三个竞赛中提高了其得分。**讨论**
    所有这些似乎基本上表明：DSM利用智能关系型数据库的关系遍历来帮助构建和建立候选特征，通过寻找相关值来缩小特征集，并在等同于蛮力特征工程的组合中，应用迭代特征子集到样本数据中，同时重新组合以优化，直到找到最佳解决方案。为了测量DSM的性能，它被投入到2014年KDD
    Cup、IJCAI和2015年KDD Cup等竞赛中，如前所述，它击败了超过2/3的人类竞争者。坎特和维拉马查内尼声称，即使在表现最差的时候（IJCAI），DSM仍能以类似于人类竞争者的方式构建预测问题，证据是它在任务中沿用类似的数据建模途径。在同一竞赛中，它与获胜者的AUC差距约为0.04，表明DSM捕捉到了竞赛数据集中的主要方面。坎特和维拉马查内尼认为，虽然它目前不能与最高表现的人类科学家竞争，但DSM仍然有其与他们并肩的作用。尽管在每场比赛中有许多人战胜了DSM，但它能够以相当少的努力（在某些情况下少于12小时，而不是几个月）击败大多数人。他们建议，考虑到这一点，它可以用来设定基准以及激发创造力。通过前置特征工程和生成潜在顶级特征集，能够让人类在数小时内重新思考问题，有效地从DSM解决方案出发，继续前进。需要注意的是，虽然DSM令人印象深刻，但它并不是第一个旨在自动化机器学习的系统。其他例子包括许多自动构建广告竞标模型的系统，或[KXEN
    Model Factory](http://www.realwire.com/releases/KXEN-Unveils-Analytical-Data-Management-and-Modeling-Factory-Supercharging-Agility-and-Productivity-in-Predictive-Analytics)（现在是SAP的一部分），它在2010年就已经提供了自动化模型构建。此外，DSM显然*并不*适用于所有类型的数据，它是一个专注于关系型数据集的系统。尚未证明它在不符合先前识别的数据科学竞赛问题模式的关系型数据集上有效。DSM已经衍生出一个名为[FeatureLab](http://www.featurelab.co/)的初创公司，称其为“界面上的洞察”，坎特担任首席执行官。该网站声明“利用你的数据，无需更多数据科学家”，并声称它是“寻求增加数据科学资源的公司的最佳解决方案”。这些都是大胆的声明，尤其是考虑到DSM的个别部分并不能真正算作突破。FeatureLab很可能会在“商业智能”服务平台的云雾中迷失。但大数据依然存在，特征工程在过去12个月里一直是机器学习的热门话题。DSM可能正是其特定技术组合在恰当的时间点上，导致了对数据科学的新思考方式。哈佛计算机科学教授玛戈·塞尔茨（Margo
    Seltzer）在谈到DSM时表示：“我认为他们做的事情将迅速成为标准——非常迅速。”如果情况确实如此，FeatureLabs可能会处于良好的位置。你可以在[这里](http://news.mit.edu/2015/automating-big-data-analysis-1016)阅读更多关于坎特和维拉马查内尼的数据科学机器的信息。**简介：[马修·梅奥](https://twitter.com/mattmayo13)**
    是一名计算机科学研究生，目前正在从事机器学习算法的并行化研究。他还是数据挖掘学生、数据爱好者和有志于成为机器学习科学家的人员。**相关链接：**'
- en: '[3 Things About Data Science You Won''t Find In Books](/2015/05/3-things-about-data-science.html)'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中书本上没有的3件事](/2015/05/3-things-about-data-science.html)'
- en: '[Seven Techniques for Data Dimensionality Reduction](/2015/05/7-methods-data-dimensionality-reduction.html)'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据维度减少的七种技术](/2015/05/7-methods-data-dimensionality-reduction.html)'
- en: '[Aug 2015 Analytics, Big Data, Data Mining, Data Science Acquisitions, Startup
    Roundup](/2015/09/august-analytics-big-data-science-company-activity.html)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2015年8月的分析、大数据、数据挖掘、数据科学收购和初创公司汇总](/2015/09/august-analytics-big-data-science-company-activity.html)'
- en: More On This Topic
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Feature Store Summit 2022: A free conference on Feature Engineering](https://www.kdnuggets.com/2022/10/hopsworks-feature-store-summit-2022-free-conference-feature-engineering.html)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2022年特征存储峰会：关于特征工程的免费会议](https://www.kdnuggets.com/2022/10/hopsworks-feature-store-summit-2022-free-conference-feature-engineering.html)'
- en: '[A Practical Approach To Feature Engineering In Machine Learning](https://www.kdnuggets.com/2023/07/practical-approach-feature-engineering-machine-learning.html)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的特征工程实用方法](https://www.kdnuggets.com/2023/07/practical-approach-feature-engineering-machine-learning.html)'
- en: '[Building a Tractable, Feature Engineering Pipeline for Multivariate…](https://www.kdnuggets.com/2022/03/building-tractable-feature-engineering-pipeline-multivariate-time-series.html)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为多变量时间序列构建可处理的特征工程管道](https://www.kdnuggets.com/2022/03/building-tractable-feature-engineering-pipeline-multivariate-time-series.html)'
- en: '[Using RAPIDS cuDF to Leverage GPU in Feature Engineering](https://www.kdnuggets.com/2023/06/rapids-cudf-leverage-gpu-feature-engineering.html)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用RAPIDS cuDF在特征工程中利用GPU](https://www.kdnuggets.com/2023/06/rapids-cudf-leverage-gpu-feature-engineering.html)'
- en: '[Feature Engineering for Beginners](https://www.kdnuggets.com/feature-engineering-for-beginners)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[特征工程初学者指南](https://www.kdnuggets.com/feature-engineering-for-beginners)'
- en: '[Go from Engineer to ML Engineer with Declarative ML](https://www.kdnuggets.com/2023/05/predibase-go-engineer-ml-engineer-declarative-ml.html)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过声明式机器学习从工程师转型为ML工程师](https://www.kdnuggets.com/2023/05/predibase-go-engineer-ml-engineer-declarative-ml.html)'
