# 以迁移学习和弱监督低成本构建自然语言处理分类器

> 原文：[https://www.kdnuggets.com/2019/03/building-nlp-classifiers-cheaply-transfer-learning-weak-supervision.html](https://www.kdnuggets.com/2019/03/building-nlp-classifiers-cheaply-transfer-learning-weak-supervision.html)

![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](/2019/03/building-nlp-classifiers-cheaply-transfer-learning-weak-supervision.html?page=2#comments)

**作者：[Abraham Starosta](https://www.linkedin.com/in/abraham-starosta-ba662764/)，斯坦福大学**

![图](../Images/d212ffd35c6a8cfd800cf7e15607aafb.png)

文本 + 智慧 = 黄金……但我们如何以低成本开采呢？

### 介绍

训练最先进的自然语言处理模型有一个问题：它们依赖于大量人工标记的训练集。这就是为什么[数据标记通常是瓶颈](https://arxiv.org/pdf/1812.00417.pdf)在开发自然语言处理应用和保持其更新。例如，想象一下支付医疗专家标记成千上万的电子健康记录需要多少钱。总的来说，让领域专家标记成千上万的示例是非常昂贵的。

除了最初的标记成本外，还有一个巨大的成本是保持模型与现实世界中不断变化的背景同步。Louise Matsakis 在[Wired](https://www.wired.com/story/break-hate-speech-algorithm-try-love/)中解释说，社交媒体平台难以检测仇恨言论的主要原因是“背景变化、环境改变和人们之间的分歧。”这主要是因为当背景变化时，我们通常需要标记成千上万的新示例或重新标记我们数据集中的大部分。这再次是非常昂贵的。

这是一个重要的问题，如果我们想要自动化从文本数据中获取知识，但这也是一个非常难以解决的问题。幸运的是，迁移学习、多任务学习和弱监督等新技术正在推动自然语言处理的发展，并可能最终收敛以提供解决方案。**在这篇博客中，我将带你了解一个个人项目，其中我通过结合弱监督和迁移学习，以低成本构建了一个检测反犹太主义推文的分类器，且没有公开数据集可用。我希望到最后你能掌握这种方法，以相对低廉的成本构建自己的分类器。**

**我们有3个步骤：**

1.  收集少量标记示例（约600个）

1.  使用弱监督从许多未标记的示例中构建训练集。

1.  使用大型预训练语言模型进行迁移学习

### 背景

****弱监督****

[弱监督（WS）](https://hazyresearch.github.io/snorkel/blog/ws_blog_post.html)帮助我们通过使我们能够便宜地利用主题专家来编程标记数百万数据点，从而缓解了**数据瓶颈**问题。更具体地说，它是一个框架，帮助主题专家（SMEs）将他们的知识以手写启发式规则或远程监督的形式注入到AI系统中。作为WS在现实世界中增加价值的一个例子，Google在2018年12月刚刚发布了一篇[论文](https://arxiv.org/abs/1812.00417)，描述了他们构建的Snorkel DryBell，一个内部工具，用于利用WS在短时间内构建三个强大的文本分类器。

![图像](../Images/a91891363f2df9bff6e3b9ef53a6bff6.png)

[数据编程](https://hazyresearch.github.io/snorkel/blog/ws_blog_post.html)范式的概述与Snorkel

在这个项目中，我使用了与Google相同的一般方法：[Snorkel](https://arxiv.org/abs/1711.10160)（Ratner等人，2018年）。斯坦福Infolab在一个名为[Snorkel Metal](https://github.com/HazyResearch/metal)的方便的Python包中实现了Snorkel框架。我建议你阅读[这个](https://github.com/HazyResearch/metal/blob/master/tutorials/Basics.ipynb)教程以了解基本工作流程，并阅读[这个](https://github.com/HazyResearch/babble/blob/master/tutorial/Tutorial3_Tradeoffs.ipynb)教程以了解如何充分利用它。

在Snorkel中，启发式方法被称为***标记函数（LFs）。*** 这里是一些常见类型的LFs：

+   **硬编码的启发式方法：**通常是正则表达式（regexes）

+   **句法：**例如，[Spacy的依赖树](https://explosion.ai/demos/displacy)

+   **远程监督：**外部知识库

+   **嘈杂的人工标签：**众包

+   **外部模型：**其他具有有用信号的模型

在你编写LFs后，Snorkel将训练一个**标记模型**，利用所有LF之间的冲突来估计它们的准确性。然后，在标记一个新数据点时，每个LF将投票：正面、负面或弃权。根据这些投票和LF的准确性估计，标记模型可以以编程方式将**概率标签**分配给数百万数据点。最后，目标是训练一个能够**超越我们的LFs**的分类器。

例如，下面是我写的一个LF的代码，用于检测反犹太主义的推文。这个LF专注于捕捉描绘富有的犹太人控制媒体和政治的常见阴谋论。

```py
*# Set voting values.*
ABSTAIN = 0 
POSITIVE = 1 
NEGATIVE = 2

*# Detects common conspiracy theories about jews owning the world.*
GLOBALISM = r"\b(Soros|Adelson|Rothschild|Khazar)"

def jews_symbols_of_globalism(tweet_text):
    return POSITIVE if re.search(GLOBALISM, tweet_text) else ABSTAIN
```

**弱监督的关键优势：**

+   **灵活性：**当我们需要更新模型时，我们只需更新LFs，重建训练集并重新训练分类器。

+   **召回率的提高：**一个判别模型将能够超越我们WS模型中的规则，从而通常提高召回率。

****迁移学习和ULMFiT****

迁移学习对计算机视觉产生了巨大的影响。使用在 ImageNet 上预训练的 ConvNet 作为初始化或对其进行微调以适应当前任务已经变得[非常普遍](http://cs231n.github.io/transfer-learning/)。但这在 NLP 中直到 [ULMFiT](http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html) 出现之前还未被采用。

![图](../Images/4237eb9977df9f002176bf7898485a41.png)

[ULMFiT](http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html)

类似于计算机视觉工程师使用在 ImageNet 上预训练的 ConvNets，Fast.ai 提供了一个通用语言模型，在数百万页维基百科上进行预训练，我们可以对其进行微调以适应我们的特定领域。然后，我们可以训练一个文本分类模型，该模型利用语言模型学习到的文本表示，这样即使在很少的样本（最多减少 100 倍数据）下也能进行学习。

![图](../Images/314a231817b92b29b71f716dcc97d9c3.png)

[ULMFiT 介绍](http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html)

**ULMFiT 包含 3 个阶段：**

1.  在通用语料库（维基百科）上预训练一个语言模型

1.  用大量未标记的数据点对语言模型进行微调以适应当前任务

1.  通过逐步解冻微调训练一个区分性分类器

**ULMFiT 的主要优势：**

+   仅凭 100 个标签，它的性能可以与用 100 倍数据从零开始训练的结果相匹配。

+   Fastai 的 API 使用起来非常简单。[这个教程](https://github.com/fastai/fastai/blob/master/examples/text.ipynb) 非常好。

+   生成一个我们可以在生产环境中部署的 PyTorch 模型

### **反犹太主义推文分类器的逐步指南**

在本节中，我们将更深入地探讨我建立反犹太主义推文分类器所采取的步骤，并分享我在这个过程中学到的一些更一般的知识。

### ****第一步：数据收集和设定目标****

**收集未标记数据：** 首先，准备一大批未标记的样本（至少 20,000 个）。对于反犹太主义推文分类器，我下载了近 25,000 条提到“jew”这个词的推文。

**标记 600 个样本：** 600 个样本虽然不多，但我认为对于大多数任务来说这是一个很好的起点，因为我们在每个数据分割中将有大约 200 个样本。如果你已经有标记好的样本，那就用这些！否则，随机挑选 600 个样本并标记它们。

作为标记工具，你可以使用 Google Sheets。或者，如果你像我一样更喜欢在手机上标记，你可以使用 [**Airtable.**](https://airtable.com/) Airtable 是免费的，它有一个简洁的 iPhone 应用。如果你在团队中工作，它还可以让你轻松分配任务。我可能会写另一篇博客专注于如何使用 Airtable 进行标记。你可以在滚动浏览示例时进行标记。如果你对我如何设置 Airtable 进行标记感兴趣，随时联系我。

![图](../Images/10dff01b2d2aaa79587d784bf8cdaf21.png)

Airtable 用于文本标记的视图

**数据划分：** 本项目的目的是将数据分为训练集、测试集和 LF 集。LF 集的目的是帮助验证我们的 LFs 并获得新 LFs 的想法。测试集用于检查模型的性能（我们不能查看它）。如果你想进行超参数调优，你需要为验证集标记约 200 个样本。

我有 24,738 条未标记的推文（训练集），733 条用于构建 LFs 的标记推文，以及 438 条用于测试的标记推文。所以我总共标记了 1,171 条，但我意识到这可能太多了。

```py
DATA_PATH = "../data"
train = pd.read_csv(os.path.join(DATA_PATH, "train_tweets.csv"))
test = pd.read_csv(os.path.join(DATA_PATH, "test_tweets.csv"))
LF_set = pd.read_csv(os.path.join(DATA_PATH, "LF_tweets.csv"))
train.shape, LF_set.shape, test.shape

>> ((24738, 6), (733, 7), (438, 7))
```

**设定我们的目标：** 在标记了几百个示例后，你将对任务的难度有更好的了解，并能为自己设定目标。我认为对于反犹太主义分类，拥有高精度是非常重要的，所以我给自己设定了至少 90% 精度和 30% 召回率的目标。

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业轨道。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的 IT 需求

* * *

### 更多相关主题

+   [弱监督建模，解释](https://www.kdnuggets.com/2022/05/weak-supervision-modeling-explained.html)

+   [图像识别和自然语言处理中的转移学习](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)

+   [TensorFlow 计算机视觉 - 转移学习变得简单](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)

+   [什么是转移学习？](https://www.kdnuggets.com/2022/01/transfer-learning.html)

+   [使用 PyTorch 的实用转移学习指南](https://www.kdnuggets.com/2023/06/practical-guide-transfer-learning-pytorch.html)

+   [探索转移学习在小数据场景中的潜力](https://www.kdnuggets.com/exploring-the-potential-of-transfer-learning-in-small-data-scenarios)
