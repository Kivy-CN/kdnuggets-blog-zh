- en: 'Loss Functions: An Explainer'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/03/loss-functions-explainer.html](https://www.kdnuggets.com/2022/03/loss-functions-explainer.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Loss function** is a method that evaluates how well the algorithm learns
    the data and produces correct outputs. It computes the distance between our predicted
    value and the actual value using a mathematical formula.'
  prefs: []
  type: TYPE_NORMAL
- en: In layman's terms, a loss function measures how wrong the model is in terms
    of its ability to estimate the relationship between x and y.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Loss functions can be categorized into two groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification** - which is about predicting a label, by identifying which
    category an object belongs to based on different parameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Regression** - which is about predicting a continuous output, by finding
    the correlations between dependent and independent variables.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Below is a list of types of loss functions for both Classification and Regression
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Cross Entropy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Cross Entropy** and Log Loss measure the same thing, however they are not
    the same and is used for Classification tasks. Cross Entropy measures the difference
    between two probability distributions for a given random variable or set of events.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Types of Cross Entropy:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Binary Cross Entropy - used for binary tasks
  prefs: []
  type: TYPE_NORMAL
- en: 'Categorical Cross Entropy - used for both binary and multiclass tasks. This
    types of Cross Entropy requires the label to be encoded as categorical. For example;
    one-hot encoding for 3 classes will use this representation: [0, 1, 0], [1,0,0]…)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sparse Cross Entropy: - used for both binary and multiclass tasks. This types
    of Cross Entropy requires the label to be an integer; 0 or 1 or n'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Code for Cross Entropy:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Log Loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Log Loss**, the binary of Cross Entropy is used in Classification tasks.
    It measures the performance of a classification model, where the output is a probability
    with values between 0 and 1.'
  prefs: []
  type: TYPE_NORMAL
- en: As the predicted probability gets further away from the actual label, Log loss
    increases. In the ideal world, a perfect model would have a Log Loss of 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![Loss Functions: An Explainer](../Images/f739f48a00ac473c7aa2f07e97123a57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [ml-cheatsheet](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)'
  prefs: []
  type: TYPE_NORMAL
- en: In the image above, you can see that as the predicted probability reaches 1,
    the Log Loss decreases. On the other hand, you can see that as the predicted probability
    decreases, the Log Loss rapidly increases.
  prefs: []
  type: TYPE_NORMAL
- en: Log Loss Using Scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Hinge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hinge** is a loss function used for Classification tasks. This type of loss
    function incorporate a margin from the classification boundary against a loss.
    Hinge penalises misclassfied samples aswell are classifying one that are within
    the defined margin from the decision boundary correctly.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loss Functions: An Explainer](../Images/6f2758eea8eb6e7e710645cff3cb3599.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [StackOverflow](https://math.stackexchange.com/questions/782586/how-do-you-minimize-hinge-loss)'
  prefs: []
  type: TYPE_NORMAL
- en: The x-axis represents the distance from the boundary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The y-axis represents the loss size, or the penalty the function will incur.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dotted line on the axis represents 1, which means when an instance’s distance
    from boundary is more than 1, the loss size is 0\. If the distance from boundary
    is at 0, the loss size is 1.
  prefs: []
  type: TYPE_NORMAL
- en: Correctly classified data points will have a low or 0 loss size; a low hinge
    loss. Whereas incorrectly classified data points will have a high or loss size
    of 1; a high hinge loss.
  prefs: []
  type: TYPE_NORMAL
- en: Hinge Loss Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Mean Square Error Loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mean Square Error Loss** is also known a L2 regularization and is used for
    Regression tasks. It tells you how close a regression line is to a set of data
    points.'
  prefs: []
  type: TYPE_NORMAL
- en: It calculates the square difference between the current output and the expected
    output divided by the number of output. However, Mean Square Error loss is more
    sensitive to outliers due to using the square difference.
  prefs: []
  type: TYPE_NORMAL
- en: '![Loss Functions: An Explainer](../Images/e087f1e6b8e997ea4958685404ba8c6f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [freecodecamp](https://www.freecodecamp.org/news/machine-learning-mean-squared-error-regression-line-c7dde9a26b93/)'
  prefs: []
  type: TYPE_NORMAL
- en: Mean Square Error Loss Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Mean Absolute Error Loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mean Absolute Error** is also known as L1 regularization and is used for
    Regression tasks. It computes the mean of squares of errors between labeled data
    and predicted data.'
  prefs: []
  type: TYPE_NORMAL
- en: It calculates the absolute difference between the current output and the expected
    output divided by the number of output. It’s aim is to minimise this absolute
    differences. Mean Absolute Error is not sensitive towards outliers as it is based
    on absolute value, unlike Mean Square Error.
  prefs: []
  type: TYPE_NORMAL
- en: In order to calculate the Mean Absolute Error, you need to take the difference
    between your model’s predictions and the actual labelled outputs. You then apply
    the absolute value to the difference, and then average it out across the whole
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Loss Functions: An Explainer](../Images/dcd53ca57a2d1fc99c851abd9fb8fee8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [TowardDataScience](https://medium.com/analytics-vidhya/a-comprehensive-guide-to-loss-functions-part-1-regression-ff8b847675d6)'
  prefs: []
  type: TYPE_NORMAL
- en: Mean Absolute Error Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Huber Loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Huber Loss** is a combination of Mean Absolute Error and Mean Square Error.
    However, the different is that it is influenced by an additional parameter called
    delta (δ).'
  prefs: []
  type: TYPE_NORMAL
- en: It is a combination of both as it tells us for loss values which are less than
    delta; use the Mean Square Error. For loss values which are greater than delta,
    use the Mean Absolute Error.
  prefs: []
  type: TYPE_NORMAL
- en: In the below image, Mean Absolute Error is in red, Mean Square Error is in blue,
    and Huber Loss is in green.
  prefs: []
  type: TYPE_NORMAL
- en: '![Loss Functions: An Explainer](../Images/dde530e71bd8f26927c7761b0e416f7a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [TowardsDataScience](https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3)'
  prefs: []
  type: TYPE_NORMAL
- en: Huber Loss Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have briefly gone over 3 loss functions for Classification and Regression
    each. However, there are many more that you can explore.
  prefs: []
  type: TYPE_NORMAL
- en: Below is an image showing a list of different types of loss functions for Classification
    and Regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Loss Functions: An Explainer](../Images/0c0647892e2e2b528e5aa732c98ddbc7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Heartbeat](https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0)'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Multi-label NLP: An Analysis of Class Imbalance and Loss Function…](https://www.kdnuggets.com/2023/03/multilabel-nlp-analysis-class-imbalance-loss-function-approaches.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are Moment-Generating Functions?](https://www.kdnuggets.com/2022/12/momentgenerating-functions.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python Lambda Functions, Explained](https://www.kdnuggets.com/2023/01/python-lambda-functions-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Pandas Plotting Functions You Might Not Know](https://www.kdnuggets.com/2023/02/5-pandas-plotting-functions-might-know.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[In-Database Analytics: Leveraging SQL''s Analytic Functions](https://www.kdnuggets.com/2023/07/indatabase-analytics-leveraging-sql-analytic-functions.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4 Python Itertools Filter Functions You Probably Didn''t Know](https://www.kdnuggets.com/2023/08/4-python-itertools-filter-functions-probably-didnt-know.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
