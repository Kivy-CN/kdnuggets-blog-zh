- en: 'A Layman’s Guide to Data Science. Part 3: Data Science Workflow'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/07/laymans-guide-data-science-workflow.html](https://www.kdnuggets.com/2020/07/laymans-guide-data-science-workflow.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Sciforce](https://sciforce.solutions)**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: here is part 1: [**How to Become a (Good) Data Scientist – Beginner Guide**](https://www.kdnuggets.com/2019/10/good-data-scientist-beginner-guide.html)
    and'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'part 2: [**A Layman’s Guide to Data Science. How to Build a Data Project**](https://www.kdnuggets.com/2020/04/guide-data-science-build-data-project.html)
    of this series'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4b0e7f575720d537f17451ed86bb3af.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Data science workflow.*'
  prefs: []
  type: TYPE_NORMAL
- en: By now, you have already gained enough [knowledge and skills about Data Science](https://medium.com/sciforce/a-laymans-guide-to-data-science-how-to-become-a-good-data-scientist-97927ad51ed8) and [have
    built your first (or even your second and third) project](https://www.kdnuggets.com/2020/04/guide-data-science-build-data-project.html).
    At this point, it is time to improve your workflow to facilitate further development
    process.
  prefs: []
  type: TYPE_NORMAL
- en: There is no specific template for solving any data science problem (otherwise,
    you’d see it in the first textbook you come across). Each new dataset and each
    new problem will lead to a different roadmap. However, there are similar high-level
    steps in many different projects.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we offer a clean workflow that can be used as a basis for data
    science projects. Every stage and step in it, of course, can be addressed on its
    own and can even be implemented by different specialists in larger-scale projects.
  prefs: []
  type: TYPE_NORMAL
- en: Framing the problem and the goals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you already know, at the starting point, you’re asking questions and trying
    to get a handle on what data you need. Therefore, think of the problem you are
    trying to solve. What do you want to learn more about? For now, forget about modeling,
    evaluation metrics, and data science-related things. Clearly stating your problem
    and defining goals are the first step to providing a good solution. Without it,
    you could lose the track in the data-science forest.
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation Phase
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In any Data Science project, getting the right kind of data is critical. Before
    any analysis can be done, you must acquire the relevant data, reformat it into
    a form that is amenable to computation and clean it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32d1dc2f3d53e0ed7d21a1e13d1d9ba3.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Acquire data**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in any data science workflow is to acquire the data to analyze.
    Data can come from a variety of sources:'
  prefs: []
  type: TYPE_NORMAL
- en: imported from CSV files from your local machine;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: queried from SQL servers;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: stripped from online repositories such as public websites;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: streamed on-demand from online sources via an API;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: automatically generated by physical apparatus, such as scientific lab equipment
    attached to computers;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: generated by computer software, such as logs from a webserver.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In many cases, collecting data can become messy, especially if the data isn’t
    something people have been collecting in an organized fashion. You’ll have to
    work with different sources and apply a variety of tools and methods to collect
    a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several key points to remember while collecting data:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Data provenance***: It is important to accurately track provenance, i.e.,
    where each piece of data comes from and whether it is still up-to-date since data
    often needs to be re-acquired later to run new experiments. Re-acquisition can
    be helpful if the original data sources get updated or if researchers want to
    test alternate hypotheses. Besides, we can use provenance to trace back downstream
    analysis errors to the original data sources.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Data management***: To avoid data duplication and confusion between different
    versions, it is critical to assign proper names to data files that they create
    or download and then organize those files into directories. When new versions
    of those files are created, corresponding names should be assigned to all versions
    to be able to keep track of their differences. For instance, scientific lab equipment
    can generate hundreds or thousands of data files that scientists must name and
    organize before running computational analyses on them.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Data storage***: With modern almost limitless access to data, it often happens
    that there is so much data that it cannot fit on a hard drive, so it must be stored
    on remote servers. While cloud services are gaining popularity, a significant
    amount of data analysis is still done on desktop machines with data sets that
    fit on modern hard drives (i.e., less than a terabyte).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27087f3ec8354e90f30977526e647bd0.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Reformat and clean data**'
  prefs: []
  type: TYPE_NORMAL
- en: Raw data is usually not in a convenient format to run an analysis since it was
    formatted by somebody else without that analysis in mind. Moreover, raw data often
    contains semantic errors, missing entries, or inconsistent formatting, so it needs
    to be “cleaned” prior to analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '***Data wrangling (munging)*** is the process of cleaning data, putting everything
    together into one workspace, and making sure your data has no faults in it. It
    is possible to reformat and clean the data either manually or by writing scripts.
    Getting all of the values in the correct format can involve stripping characters
    from strings, converting integers to floats, or many other things. Afterward,
    it is necessary to deal with missing values and null values that are common in
    sparse matrices. The process of handling them is called ***missing data imputation*** ,
    where the missing data are replaced with substituted data.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Data integration*** is a related challenge since data from all sources needs
    to be integrated into a central MySQL relational database, which serves as the
    master data source for his analyses.'
  prefs: []
  type: TYPE_NORMAL
- en: Usually, it consumes a lot of time and cannot be fully automated, but at the
    same time, it can provide insights into the data structure and quality as well
    as the models and analyses that might be optimal to apply.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88a694d4c783f6454db005e4e097bdf9.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Explore the data**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s where you’ll start getting summary-level insights of what you’re looking
    at and extracting the large trends. At this step, there are three dimensions to
    explore: whether the data imply supervised learning or unsupervised learning?
    Is this a classification problem, or is it a regression problem? Is this a prediction
    problem or an inference problem? These three sets of questions can offer a lot
    of guidance when solving your data science problem.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many tools that help you understand your data quickly. You can start
    by checking out the first few rows of the data frame to get the initial impression
    of the data organization. Automatic tools incorporated in multiple libraries,
    such as Pandas’ .describe(), can quickly give you the mean, count, standard deviation,
    and you might already see things worth diving deeper into. With this information,
    you’ll be able to determine which variable is our target and which features we
    think are important.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis Phase
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Analysis is the core phase of data science that includes writing, executing,
    and refining computer programs to analyze and obtain insights from the data prepared
    at the previous phase. Though there are many programming languages for data science
    projects ranging from interpreted “scripting” languages such as Python, Perl,
    R, and MATLAB to compiled ones such as Java, C, C++, or even Fortran, the workflow
    for writing analysis software is similar across the languages.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, analysis is a repeated *iteration cycle* of editing scripts
    or programs, executing to produce output files, inspecting the output files to
    gain insights and discover mistakes, debugging, and re-editing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e99fad50e35a25b75368c730230eb9d5.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Baseline Modeling**'
  prefs: []
  type: TYPE_NORMAL
- en: As a data scientist, you will build a lot of models with a variety of algorithms
    to perform different tasks. At the first approach to the task, it is worthwhile
    to avoid advanced complicated models but to stick to simpler and more traditional ***linear
    regression*** for regression problems and ***logistic regression*** for classification
    problems as a baseline upon which you can improve.
  prefs: []
  type: TYPE_NORMAL
- en: At the model preprocessing stage, you can separate out features from dependent
    variables, scale the data, and use a train-test-split or cross-validation to prevent
    overfitting of the model — the problem when a model too closely tracks the training
    data and doesn’t perform well with new data.
  prefs: []
  type: TYPE_NORMAL
- en: With the model ready, it can be fitted on the training data and tested by having
    it predict *y *values for the *X_test* data. Finally, the model is evaluated with
    the help of [metrics](https://www.saedsayad.com/model_evaluation_r.htm) that are
    appropriate for the task, such as [R-squared](https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit) for
    regression problems and [accuracy](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/) or [ROC-AUC ](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)scores
    for classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b623b8a5fcbbb667dd0fb21206201825.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/a5d1981372f82eb76cb142c8ed592995.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Secondary Modeling**'
  prefs: []
  type: TYPE_NORMAL
- en: Now it is time to go into deeper analysis and, if necessary, use more advanced
    models, such as **neural networks**, **XGBoost**, or Random Forests. It is important
    to remember that such models can initially render worse results than simple and
    easy-to-understand models due to a small dataset that cannot provide enough data
    or to the collinearity problem with features providing similar information.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the key task of the secondary modeling step is parameter tuning.
    Each algorithm has a set of parameters you can optimize. Parameters are the variables
    that a machine learning technique uses to adjust to the data. Hyperparameters
    that arethe variables that govern the training process itself, such as the number
    of nodes or hidden layers in a neural network, are tuned by running the whole
    training job, looking at the aggregate accuracy, and adjusting.
  prefs: []
  type: TYPE_NORMAL
- en: Reflection Phase
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data scientists frequently alternate between the *analysis* and *reflection* phases:
    whereas the analysis phase focuses on programming, the reflection phase involves
    thinking and communicating about the outputs of analyses. After inspecting a set
    of output files, a data scientist, or a group of data scientists can make comparisons
    between output variants and explore alternative paths by adjusting script code
    and/or execution parameters. Much of the data analysis process is trial-and-error:
    a scientist runs tests, graphs the output, reruns them, graphs the output, and
    so on. Therefore, graphs are the central comparison tool that can be displayed
    side-by-side on monitors to visually compare and contrast their characteristics.
    A supplementary tool is taking notes, both physical and digital, to keep track
    of the line of thought and experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: Communication Phase
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final phase of data science is disseminating results either in the form
    of a data science product or as written reports such as internal memos, slideshow
    presentations, business/policy white papers, or academic research publications.
  prefs: []
  type: TYPE_NORMAL
- en: A ***data science product*** implies getting your model into production. In
    most companies, data scientists will be working with the software engineering
    team to write the production code. The software can be used both to reproduce
    the experiments or play with the prototype systems and as an independent solution
    to tackle a known issue on the market, like, for example, assessing the risk of
    financial fraud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, to the data product, you can create a **data science report**.
    You can showcase your results with a presentation and offer a technical overview
    of the process. Remember to keep your audience in mind: go into more detail if
    presenting to fellow data scientists or focus on the findings if you address the
    sales team or executives. If your company allows publishing the results, it is
    also a good opportunity to have feedback from other specialists. Additionally,
    you can write a blog post and push your code to GitHub so the data science community
    can learn from your success. Communicating your results is an important part of
    the scientific process, so this phase should not be overlooked.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/sciforce/a-laymans-guide-to-data-science-part-3-data-science-workflow-eec301da3ffa?source=friends_link&sk=72bad5a9cdc66deb895c7023260a64c3).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio:** [SciForce](https://sciforce.solutions) is a Ukraine-based IT company
    specialized in development of software solutions based on science-driven information
    technologies. We have wide-ranging expertise in many key AI technologies, including
    Data Mining, Digital Signal Processing, Natural Language Processing, Machine Learning,
    Image Processing and Computer Vision.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Managing Machine Learning Cycles: Five Learnings from comparing Data Science
    Experimentation/ Collaboration Tools](https://www.kdnuggets.com/2020/01/managing-machine-learning-cycles.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Beginner’s Guide to the Data Science Pipeline](https://www.kdnuggets.com/2018/05/beginners-guide-data-science-pipeline.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science Project Flow for Startups](https://www.kdnuggets.com/2019/01/data-science-project-flow-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
