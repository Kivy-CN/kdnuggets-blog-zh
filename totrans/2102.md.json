["```py\npip install transformers torch datasets\n```", "```py\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"imdb\")\nprint(dataset)\n```", "```py\nfrom transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n```", "```py\nfrom datasets import train_test_split\n\ntrain_testvalid = tokenized_datasets['train'].train_test_split(test_size=0.2)\ntrain_dataset = train_testvalid['train']\nvalid_dataset = train_testvalid['test']\n```", "```py\nfrom torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=8)\n```", "```py\nfrom transformers import BertForSequenceClassification, AdamW\n\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2) \n```", "```py\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n)\n\ntrainer.train()\n```", "```py\nmetrics = trainer.evaluate()\nprint(metrics)\n```", "```py\npredictions = trainer.predict(valid_dataset)\nprint(predictions)\n```"]