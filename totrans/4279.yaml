- en: A Comprehensive Guide to Ensemble Learning – Exactly What You Need to Know
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习全面指南 – 正是你需要知道的内容
- en: 原文：[https://www.kdnuggets.com/2021/05/comprehensive-guide-ensemble-learning.html](https://www.kdnuggets.com/2021/05/comprehensive-guide-ensemble-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/05/comprehensive-guide-ensemble-learning.html](https://www.kdnuggets.com/2021/05/comprehensive-guide-ensemble-learning.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '[Ensemble learning techniques](https://www.toptal.com/machine-learning/ensemble-methods-machine-learning) have
    been proven to yield better performance on machine learning problems. We can use
    these techniques for regression as well as classification problems.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[集成学习技术](https://www.toptal.com/machine-learning/ensemble-methods-machine-learning)已经证明在机器学习问题上能取得更好的表现。我们可以将这些技术用于回归和分类问题。'
- en: The final prediction from these ensembling techniques is obtained by combining
    results from several base models. Averaging, voting and stacking are some of the
    ways the results are combined to obtain a final prediction.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些集成技术中获得的最终预测结果是通过结合多个基础模型的结果得到的。平均值、投票和堆叠是一些将结果组合以获得最终预测的方法。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 为你的组织提供IT支持'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In this article, we will explore how ensemble learning can be used to come up
    with optimal machine learning models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨如何使用集成学习来得出最佳的机器学习模型。
- en: What is ensemble learning?
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是集成学习？
- en: Ensemble learning is a combination of several machine learning models in one
    problem. These models are known as weak learners. The intuition is that when you
    combine several weak learners, they can become strong learners.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习是将多个机器学习模型组合在一个问题中的方法。这些模型被称为弱学习器。其直观的理解是，当你结合多个弱学习器时，它们可以变成强学习器。
- en: Each weak learner is fitted on the training set and provides predictions obtained.
    The final prediction result is computed by combining the results from all the
    weak learners.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 每个弱学习器都在训练集上进行训练并提供预测。最终的预测结果是通过结合所有弱学习器的结果来计算的。
- en: Basic ensemble learning techniques
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基本的集成学习技术
- en: Let’s take a moment and look at simple ensemble learning techniques.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间来看看简单的集成学习技术。
- en: '**Max voting**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大投票**'
- en: In classification, the prediction from each model is a vote. In max voting,
    the final prediction comes from the prediction with the most votes.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类中，每个模型的预测都是一票。在最大投票中，最终预测结果来自于票数最多的预测。
- en: 'Let’s take an example where you have three classifiers with the following predictions:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，假设你有三个分类器，其预测结果如下：
- en: classifier 1 – class A
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器 1 – 类 A
- en: classifier 2 – class B
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器 2 – 类 B
- en: classifier 3 – class B
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器 3 – 类 B
- en: The final prediction here would be class B since it has the most votes.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的预测结果是类 B，因为它获得了最多的票数。
- en: '**Averaging**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均值**'
- en: In averaging, the final output is an average of all predictions. This goes for
    regression problems. For example, in [random forest regression](https://neptune.ai/blog/random-forest-regression-when-does-it-fail-and-why),
    the final result is the average of the predictions from individual decision trees.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在平均值方法中，最终输出是所有预测值的平均值。这适用于回归问题。例如，在 [随机森林回归](https://neptune.ai/blog/random-forest-regression-when-does-it-fail-and-why)中，最终结果是来自各个决策树预测值的平均值。
- en: 'Let’s take an example of three regression models that predict the price of
    a commodity as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，其中三个回归模型预测商品价格如下：
- en: regressor 1 – 200
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归器 1 – 200
- en: regressor 2 – 300
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归器 2 – 300
- en: regressor 3 – 400
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归器 3 – 400
- en: The final prediction would be the average of 200, 300, and 400.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最终预测结果是200、300和400的平均值。
- en: '**Weighted average**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**加权平均**'
- en: In weighted averaging, the base model with higher predictive power is more important.
    In the price prediction example, each of the regressors would be assigned a weight.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在加权平均中，具有更高预测能力的基础模型更重要。在价格预测示例中，每个回归模型会被分配一个权重。
- en: 'The sum of the weights would equal one. Let’s say that the regressors are given
    weights of 0.35, 0.45, and 0.2 respectively. The final model prediction can be
    computed as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 权重的总和应等于1。假设回归模型的权重分别为0.35、0.45和0.2。最终模型的预测可以如下计算：
- en: 0.35 * 200 + 0.45*300 + 0.2*400 = 285
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 0.35 * 200 + 0.45 * 300 + 0.2 * 400 = 285
- en: Advanced ensemble learning techniques
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级集成学习技术
- en: Above are simple techniques, now let’s take a look at advanced techniques for
    ensemble learning.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 上述是一些简单的技术，现在让我们来看看集成学习的高级技术。
- en: Stacking
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 堆叠
- en: Stacking is the process of combining various estimators in order to reduce their
    biases. Predictions from each estimator are stacked together and used as input
    to a final estimator (usually called a *meta-model*) that computes the final prediction.
    Training of the final estimator happens via cross-validation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠是将各种估计器组合在一起以减少其偏差的过程。每个估计器的预测结果被堆叠在一起，并作为输入传递给最终估计器（通常称为*元模型*），以计算最终预测。最终估计器的训练通过交叉验证进行。
- en: Stacking can be done for both regression and classification problems.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠可以用于回归和分类问题。
- en: '![Ensemble learning techniques](../Images/171b9e69dd3a956d32a20582ebe4f151.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![集成学习技术](../Images/171b9e69dd3a956d32a20582ebe4f151.png)'
- en: '[*Source*](https://towardsdatascience.com/stacking-classifiers-for-higher-predictive-performance-566f963e4840)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[*来源*](https://towardsdatascience.com/stacking-classifiers-for-higher-predictive-performance-566f963e4840)'
- en: 'Stacking can be considered to happen in the following steps:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠可以被认为是以下步骤：
- en: Split the data into a training and validation set,
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分割为训练集和验证集，
- en: Divide the training set into K folds, for example 10,
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练集划分为K折，例如10折，
- en: Train a base model (say SVM) on 9 folds and make predictions on the 10th fold,
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在9折上训练一个基础模型（例如SVM）并对第10折进行预测，
- en: Repeat until you have a prediction for each fold,
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直到你对每一折有一个预测，
- en: Fit the base model on the whole training set,
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在整个训练集上拟合基础模型，
- en: Use the model to make predictions on the test set,
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型对测试集进行预测，
- en: Repeat step 3 – 6 for other base models (for example decision trees),
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对其他基础模型（例如决策树）重复步骤3至6，
- en: Use predictions from the test set as features to a new model – *the meta-model,*
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用测试集的预测作为新模型的特征——*元模型*，
- en: Make final predictions on the test set using the meta model.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用元模型对测试集做最终预测。
- en: With regression problems, the values passed to the meta-model are numeric. With
    classification problems, they’re probabilities or class labels.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，传递给元模型的值是数值型的。对于分类问题，它们是概率或类别标签。
- en: Blending
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合
- en: Blending is similar to stacking, but uses a holdout set from the training set
    to make predictions. So, predictions are done on the holdout set only. The predictions
    and holdout set are used to build a final model that makes predictions on the
    test set.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 混合类似于堆叠，但使用训练集的留出集进行预测。因此，预测仅在留出集上进行。预测和留出集用于构建一个最终模型，该模型在测试集上进行预测。
- en: You can think of blending as a type of stacking, where the meta-model is trained
    on predictions made by the base model on the hold-out validation set.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将混合视为一种堆叠，其中元模型在基础模型对留出的验证集进行的预测上进行训练。
- en: 'You can consider the *blending* process to be:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将*混合*过程视为：
- en: Split the data into a test and validation set,
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据分割为测试集和验证集，
- en: Fit base models on the validation set,
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在验证集上拟合基础模型，
- en: Make predictions on the validation and test set,
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对验证集和测试集进行预测，
- en: Use the validation set and its predictions to build a final model,
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用验证集及其预测结果来构建最终模型，
- en: Make final predictions using this model.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用此模型进行最终预测。
- en: The concept of blending [was made popular](https://netflixprize.com/assets/ProgressPrize2008_BellKor.pdf) by
    the [Netflix Prize competition](https://en.wikipedia.org/wiki/Netflix_Prize).
    The winning team used a blended solution to achieve a 10-fold performance improvement
    on Netflix’s movie recommendation algorithm.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 混合的概念[*由Netflix奖竞赛推广*](https://netflixprize.com/assets/ProgressPrize2008_BellKor.pdf)。获胜团队使用了混合解决方案，在Netflix的电影推荐算法上实现了10倍的性能提升。
- en: 'According to this [Kaggle ensembling guide](https://mlwave.com/kaggle-ensembling-guide/):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个[Kaggle 集成指南](https://mlwave.com/kaggle-ensembling-guide/)：
- en: “Blending is a word introduced by the Netflix winners. It’s very close to stacked
    generalization, but a bit simpler and less risk of an information leak. Some researchers
    use “stacked ensembling” and “blending” interchangeably.
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**“混合”**是Netflix获胜者引入的术语。它非常接近堆叠泛化，但更简单，且信息泄露的风险较小。一些研究人员将“堆叠集成”和“混合”互换使用。'
- en: ''
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With blending, instead of creating out-of-fold predictions for the train set,
    you create a small holdout set of say 10% of the train set. The stacker model
    then trains on this holdout set only.”
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用混合时，不是为训练集创建折外预测，而是创建一个小的保留集，比如训练集的10%。然后，堆叠模型仅在这个保留集上进行训练。”
- en: Blending vs stacking
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合与堆叠
- en: Blending is simpler than stacking and prevents leakage of information in the
    model. The generalizers and the stackers use different datasets.  However, blending
    uses less data and may lead to overfitting.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 混合比堆叠更简单，并且防止模型中的信息泄露。泛化器和堆叠器使用不同的数据集。然而，混合使用的数据较少，可能导致过拟合。
- en: Cross-validation is more solid on stacking than blending. It’s calculated over
    more folds, compared to using a small hold-out dataset in blending.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证在堆叠上比在混合上更可靠。与在混合中使用小的保留数据集相比，堆叠的交叉验证是在更多的折叠上计算的。
- en: Bagging
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自助法
- en: Bagging takes random samples of data, builds learning algorithms, and uses the
    mean to find bagging probabilities. It’s also called *bootstrap aggregating*.
    Bagging aggregates the results from several models in order to obtain a generalized
    result.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 自助法采用随机样本数据，构建学习算法，并使用均值来找出自助法概率。这也称为*自助聚合*。自助法通过整合多个模型的结果来获得一个泛化的结果。
- en: 'The method involves:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法包括：
- en: Creating multiple subsets from the original dataset with replacement,
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从原始数据集中创建多个有替换的子集，
- en: Building a base model for each of the subsets,
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个子集构建一个基础模型，
- en: Running all the models in parallel,
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有模型并行运行，
- en: Combining predictions from all models to obtain final predictions.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有模型的预测结果结合起来以获得最终预测。
- en: Boosting
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升法
- en: Boosting is a machine learning ensemble technique that reduces bias and variance
    by converting weak learners into strong learners. The weak learners are applied
    to the dataset in a sequential manner. The first step is building an initial model
    and fitting it into the training set.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 提升法是一种机器学习集成技术，通过将弱学习者转化为强学习者来减少偏差和方差。弱学习者以顺序方式应用于数据集。第一步是构建初始模型并将其拟合到训练集中。
- en: 'A second model that tries to fix the errors generated by the first model is
    then fitted. Here’s what the entire process looks like:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后拟合一个第二模型，该模型尝试修正第一个模型产生的错误。整个过程如下：
- en: Create a subset from the original data,
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从原始数据中创建一个子集，
- en: Build an initial model with this data,
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这些数据构建初始模型，
- en: Run predictions on the whole data set,
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对整个数据集进行预测，
- en: Calculate the error using the predictions and the actual values,
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预测和实际值计算错误，
- en: Assign more weight to the incorrect predictions,
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对不正确的预测赋予更多权重，
- en: Create another model that attempts to fix errors from the last model,
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建另一个模型，尝试修正上一个模型的错误，
- en: Run predictions on the entire dataset with the new model,
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用新模型对整个数据集进行预测，
- en: Create several models with each model aiming at correcting the errors generated
    by the previous one,
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建多个模型，每个模型旨在修正前一个模型产生的错误，
- en: Obtain the final model by weighting the mean of all the models.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过对所有模型的均值加权来获得最终模型。
- en: Libraries for ensemble learning
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成学习的库
- en: 'With that introduction out of the way, let’s talk about libraries that you
    can use for ensembling. Broadly speaking, there are two categories:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍完这些内容后，我们来谈谈你可以用来进行集成的库。广义上讲，有两个类别：
- en: Bagging algorithms,
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自助法算法，
- en: Boosting algorithms.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升算法。
- en: Bagging algorithms
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自助法算法
- en: Bagging algorithms are based on the bagging technique described above. Let’s
    take a look at a couple of them.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 自助法算法基于上述自助法技术。让我们看看其中的几个。
- en: '**Bagging meta-estimator**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**自助法元估计器**'
- en: Scikit-learn lets us implement a `[BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)`
    and a `[BaggingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html)`.
    The bagging meta-estimator fits each base model on random subsets of the original
    dataset. It then computes the final prediction by aggregating individual base
    model predictions. Aggregation is done by voting or averaging. The method reduces
    the variance of estimators by introducing randomization in their construction
    process.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several flavors of bagging:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Drawing random subsets of the data as random subsets of the samples is referred
    to as *pasting*.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm is referred to as *bagging* when the samples are drawn with replacement.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If random data subsets are taken as random subsets of the feature, the algorithm
    is referred to as *Random Subspaces*.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you create base estimators from subsets of both samples and features, it’s *Random
    Patches*.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a look at how you can create a bagging estimator using Scikit-learn.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'This takes a few steps:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Import the `BaggingClassifier`,
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Import a base estimator – a decision tree classifier,
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create an instance of the `BaggingClassifier`.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The bagging classifier takes several arguments:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: The base estimator – here, a decision tree classifier,
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of estimators you want in the ensemble,
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_samples` to define the number of samples that will be drawn from the training
    set for each base estimator ,'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_features` to dictate the number of features that will be used to train
    each base estimator.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, you can fit this classifier on the training set and score it.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The process will be the same for regression problems, the only difference being
    that you will work with regression estimators.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Forests of randomized trees**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: A [Random Forest](https://neptune.ai/blog/random-forest-regression-when-does-it-fail-and-why)® is
    an ensemble of random decision trees. Each decision tree is created from a different
    sample of the dataset. The samples are drawn with replacement. Every tree produces
    its own prediction.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: In regression, these results are averaged to obtain the final result.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: In classification, the final result can be obtained as the class with the most
    votes.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: The averaging and voting improves the accuracy of the model by preventing overfitting.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: In Scikit-learn a forest of randomized trees can be implemented via `RandomForestClassifier`
    and the `ExtraTreesClassifier`. Similar estimators are available for regression
    problems.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Boosting algorithms
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These algorithms are based on the boosting framework described earlier. Let’s
    look at a few of them.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '**AdaBoost**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost works by fitting a sequence of weak learners. It gives incorrect predictions
    more weight in subsequent iterations, and less weight to correct predictions.
    This forces the algorithm to focus on observations that are harder to predict.
    The final prediction comes from weighing the majority vote or sum.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 通过拟合一系列弱学习器来工作。它在后续迭代中给予错误预测更多的权重，而给予正确预测较少的权重。这迫使算法关注那些更难预测的观测值。最终预测来自加权的多数投票或总和。
- en: AdaBoost can be used for both regression and classification problems. Let’s
    take a moment and look at how you can apply the algorithm to a classification
    problem using Scikit-learn.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 可以用于回归和分类问题。让我们花点时间看看如何使用 Scikit-learn 将算法应用于分类问题。
- en: We use the `AdaBoostClassifier`. `n_estimators` dictates the number of weak
    learners in the ensemble. The contribution of each weak learner to the final combination
    is controlled by the `learning_rate`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `AdaBoostClassifier`。`n_estimators` 决定了集成中的弱学习器数量。每个弱学习器对最终组合的贡献由 `learning_rate`
    控制。
- en: By default, decision trees are used as base estimators. In order to obtain better
    results, the parameters of the decision tree can be tuned. You can also tune the
    number of base estimators.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，决策树被用作基估计器。为了获得更好的结果，可以调整决策树的参数。你还可以调整基估计器的数量。
- en: '[PRE4]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Gradient tree boosting**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度树提升**'
- en: 'Gradient tree boosting also combines a set of weak learners to form a strong
    learner. There are three main items to note, as far as gradient boosting trees
    are concerned:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度树提升还将一组弱学习器组合成一个强学习器。关于梯度提升树，有三项主要事项需要注意：
- en: a differential loss function has to be used,
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须使用差分损失函数，
- en: decision trees are used as weak learners,
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树被用作弱学习器，
- en: it’s an additive model, so trees are added one after the other. Gradient descent
    is used to minimize the loss when adding subsequent trees.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是一个加法模型，因此树是一个接一个地添加的。梯度下降被用来在添加后续树时最小化损失。
- en: You can use Scikit-learn to build a model based on gradient tree boosting.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 Scikit-learn 基于梯度树提升构建模型。
- en: '[PRE5]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**eXtreme Gradient Boosting**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**eXtreme Gradient Boosting**'
- en: eXtreme Gradient Boosting, popularly known as [XGoost](https://neptune.ai/blog/how-to-organize-your-xgboost-machine-learning-ml-model-development-process),
    is a top gradient boosting framework. It’s based on an ensemble of weak decision
    trees. It can do parallel computations on a single computer.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: eXtreme Gradient Boosting，广泛称为[XGoost](https://neptune.ai/blog/how-to-organize-your-xgboost-machine-learning-ml-model-development-process)，是一个顶级的梯度提升框架。它基于一组弱决策树。它可以在单台计算机上进行并行计算。
- en: The algorithm uses regression trees for the base learner. It also has cross-validation
    built-in. Developers love it for its accuracy, efficiency, and feasibility.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法使用回归树作为基学习器。它还内置了交叉验证。开发者喜欢它的准确性、效率和可行性。
- en: '[PRE6]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**LightGBM**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**LightGBM**'
- en: '[LightGBM](https://lightgbm.readthedocs.io/en/latest/) is a gradient boosting
    algorithm based on tree learning. Unlike other tree-based algorithms that use
    depth-wise growth, LightGBM uses leaf-wise tree growth. Leaf-wise growth algorithms
    tend to converge faster than dep-wise-based algorithms.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[LightGBM](https://lightgbm.readthedocs.io/en/latest/) 是一种基于树学习的梯度提升算法。与使用按深度增长的其他基于树的算法不同，LightGBM
    使用按叶子增长的树。叶子增长算法的收敛速度通常比基于深度的算法要快。'
- en: '![Level-wise tree growth](../Images/4d9ff80253d95bfb7b410103ab1659c3.png)[*Source*](https://lightgbm.readthedocs.io/en/latest/Features.html?highlight=dart#other-features)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![按层次增长的树](../Images/4d9ff80253d95bfb7b410103ab1659c3.png)[*来源*](https://lightgbm.readthedocs.io/en/latest/Features.html?highlight=dart#other-features)'
- en: '![Leaf-wise tree growth](../Images/a8823777b46d64bee45eb7773159b426.png)[*Source*](https://lightgbm.readthedocs.io/en/latest/Features.html?highlight=dart#other-features)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![按叶子增长的树](../Images/a8823777b46d64bee45eb7773159b426.png)[*来源*](https://lightgbm.readthedocs.io/en/latest/Features.html?highlight=dart#other-features)'
- en: '![Leaf-wise tree growth](../Images/f8c9ea60502b0df73fed238f77564c07.png)[*Source*](https://lightgbm.readthedocs.io/en/latest/Features.html?highlight=dart#other-features)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![按叶子增长的树](../Images/f8c9ea60502b0df73fed238f77564c07.png)[*来源*](https://lightgbm.readthedocs.io/en/latest/Features.html?highlight=dart#other-features)'
- en: LightGBM can be used for both regression and classification problems by setting
    the appropriate objective.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 可以通过设置适当的目标来用于回归和分类问题。
- en: Here’s how you can apply LightGBM to a binary classification problem.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何将 LightGBM 应用于二分类问题。
- en: '[PRE7]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**CatBoost**'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**CatBoost**'
- en: '[CatBoost](https://github.com/catboost) is a depth-wise gradient boosting library
    developed by [Yandex](https://yandex.com/company/). It grows a balanced tree using
    oblivion decision trees. As you can see in the image below, the same features
    are used when making left and right splits at each level.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[CatBoost](https://github.com/catboost) 是一个由 [Yandex](https://yandex.com/company/)
    开发的深度梯度提升库。它使用遗忘决策树来生长平衡树。正如下面的图像所示，在每一层的左右分裂时使用相同的特征。'
- en: '![Gradient boosting catboost](../Images/4b59e5ba164ef50d380c1c489426a112.png)*[Source](https://heartbeat.fritz.ai/fast-gradient-boosting-with-catboost-38779b0d5d9a)*'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '![梯度提升 CatBoost](../Images/4b59e5ba164ef50d380c1c489426a112.png)*[来源](https://heartbeat.fritz.ai/fast-gradient-boosting-with-catboost-38779b0d5d9a)*'
- en: 'Researchers need Catboost for the following reasons:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员需要 Catboost 的原因如下：
- en: The ability to handle categorical features natively,
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有本地处理分类特征的能力，
- en: Models can be trained on several GPUs,
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型可以在多个 GPU 上进行训练，
- en: It reduces parameter tuning time by providing great results with default parameters,
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过提供默认参数的优秀结果来减少参数调整时间，
- en: Models can be exported to Core ML for on-device inference (iOS),
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型可以导出到 Core ML 进行设备端推理（iOS），
- en: It handles missing values internally,
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它内部处理缺失值，
- en: It can be used for both regression and classification problems.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以用于回归和分类问题。
- en: Here’s how you can apply CatBoost to a classification problem.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何将 CatBoost 应用于分类问题的。
- en: '[PRE8]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Libraries that help you do stacking on base models
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 帮助你对基础模型进行堆叠的库
- en: When stacking, the output of individual models is stacked and a final estimator
    used to compute the final prediction. The estimators are fitted on the whole training
    set. The final estimator is trained on the cross-validated predictions of the
    base estimators.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在堆叠时，单个模型的输出被堆叠，并使用最终估计器来计算最终预测。估计器在整个训练集上进行拟合。最终估计器在基础估计器的交叉验证预测上进行训练。
- en: Scikit-learn can be used to stack estimators. Let’s take a look at how you can
    stack estimators for a classification problem.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 可以用于堆叠估计器。让我们来看看如何为分类问题堆叠估计器。
- en: First, you need to set up the base estimator that you want to use.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要设置要使用的基础估计器。
- en: '[PRE9]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, instantiate the stacking classifier. Its parameters include:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，实例化堆叠分类器。其参数包括：
- en: The estimators defined above,
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上面定义的估计器，
- en: The final estimator that you’d like to use. The logistic regression estimator
    is used by default,
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你希望使用的最终估计器。默认使用逻辑回归估计器，
- en: '`cv` the cross-validation generator. Uses 5 k-fold cross-validation by default,'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cv` 交叉验证生成器。默认使用 5 折交叉验证，'
- en: '`stack_method` to dictate the method to be applied to each estimator. If `auto`,
    it will try `predict_proba`, `decision_function` or `predict`’ in that order.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stack_method` 用于指定应用于每个估计器的方法。如果为 `auto`，它将按顺序尝试 `predict_proba`、`decision_function`
    或 `predict`。'
- en: '[PRE10]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: After that, you can fit the data to the training set and score it on the test
    set.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以将数据拟合到训练集，并在测试集上进行评分。
- en: '[PRE11]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Scikit-learn also lets you implement a voting estimator. It uses the majority
    vote or the average of the probabilities from the base estimators to make the
    final prediction.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 还允许你实现投票估计器。它使用基础估计器的多数投票或概率平均值来做出最终预测。
- en: This can be implemented using the  `VotingClassifier` for classification problems
    and the `VotingRegressor` for regression problems. Just like stacking, you will
    first have to define a set of base estimators.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以使用 `VotingClassifier` 实现分类问题，用 `VotingRegressor` 实现回归问题。就像堆叠一样，你首先需要定义一组基础估计器。
- en: 'Let’s look at how you can implement it for classification problems. The `VotingClassifier`
    lets you select the voting type:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何为分类问题实现它。`VotingClassifier` 让你选择投票类型：
- en: '`soft` means that the average of the probabilities will be used to compute
    the final result,'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`soft` 表示将使用概率的平均值来计算最终结果，'
- en: '`hard` notifies the classifier to use the predicted classes for majority voting.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hard` 通知分类器使用预测的类别进行多数投票。'
- en: '[PRE12]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The voting regressor uses several estimators and returns the final result as
    the average of predicted values.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 投票回归器使用多个估计器，并将最终结果返回为预测值的平均值。
- en: Stacking with Mlxtend
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Mlxtend 进行堆叠
- en: You can also perform stacking using [Mlxtend’s](http://rasbt.github.io/mlxtend/) `[StackingCVClassifier](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/)`.
    The first step is to define a list of base estimators, and then pass the estimators
    to the classifier.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用 [Mlxtend 的](http://rasbt.github.io/mlxtend/) ` [StackingCVClassifier](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/)`
    来进行堆叠。第一步是定义一个基础估算器列表，然后将这些估算器传递给分类器。
- en: You also have to define the final model that will be used to aggregate the predictions.
    In this case, it’s the logistic regression model.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要定义将用于聚合预测的最终模型。在这种情况下，就是逻辑回归模型。
- en: '[PRE13]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: When to use ensemble learning
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 何时使用集成学习
- en: You can employ ensemble learning techniques when you want to improve the performance
    of machine learning models. For example to increase the accuracy of classification
    models or to reduce the mean absolute error for regression models. Ensembling
    also results in a more stable model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想提高机器学习模型的性能时，可以采用集成学习技术。例如，提高分类模型的准确性或减少回归模型的平均绝对误差。集成学习还会导致模型更加稳定。
- en: When your model is overfitting on the training set, you can also employ ensembling
    learning methods to create a more complex model. The models in the ensemble would
    then improve performance on the dataset by combining their predictions.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的模型在训练集上过拟合时，你还可以采用集成学习方法来创建更复杂的模型。集成中的模型将通过结合它们的预测来提高数据集上的性能。
- en: When ensemble learning works best
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成学习最佳的应用时机
- en: Ensemble learning works best when the base models are not correlated. For instance,
    you can train different models such as linear models, decision trees, and neural
    nets on different datasets or features. The less correlated the base models, the
    better.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当基础模型不相关时，集成学习效果最佳。例如，你可以在不同的数据集或特征上训练不同的模型，如线性模型、决策树和神经网络。基础模型的相关性越低，效果越好。
- en: The idea behind using uncorrelated models is that each may be solving a weakness
    of the other. They also have different strengths which, when combined, will result
    in a well-performing estimator. For example, creating an ensemble of just tree-based
    models may not be as effective as combining tree-type algorithms with other types
    of algorithms.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不相关模型的思想在于每个模型可能解决了其他模型的弱点。它们还有不同的优点，当这些优点结合起来时，会形成一个表现良好的估算器。例如，单纯创建树基模型的集成可能不如将树型算法与其他类型算法结合效果更佳。
- en: Final thoughts
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最后的思考
- en: In this article, we explored how to use ensemble learning to improve the performance
    of machine learning models. We’ve also gone through various tools and techniques
    that you can use for ensembling. Your machine learning repertoire has hopefully
    grown.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了如何使用集成学习来提高机器学习模型的性能。我们还介绍了可以用于集成的各种工具和技术。希望你的机器学习技能得到了提升。
- en: Happy ensembling!
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 祝集成学习愉快！
- en: Resources
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 资源
- en: '[Kaggle ensembling guide](https://mlwave.com/kaggle-ensembling-guide/)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kaggle 集成学习指南](https://mlwave.com/kaggle-ensembling-guide/)'
- en: '[Scikit-learn ensembling guide ](https://scikit-learn.org/stable/modules/ensemble.html)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Scikit-learn 集成学习指南](https://scikit-learn.org/stable/modules/ensemble.html)'
- en: '[Notebook used in the article](https://colab.research.google.com/drive/1MEcl4W1Mr9_rRJEPcY2IHWppJq08bgc2?usp=sharing)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文章中使用的笔记本](https://colab.research.google.com/drive/1MEcl4W1Mr9_rRJEPcY2IHWppJq08bgc2?usp=sharing)'
- en: '**Bio: [Derrick Mwiti](https://www.linkedin.com/in/mwitiderrick/)** is a data
    scientist who has a great passion for sharing knowledge. He is an avid contributor
    to the data science community via blogs such as Heartbeat, Towards Data Science,
    Datacamp, Neptune AI, KDnuggets just to mention a few. His content has been viewed
    over a million times on the internet. Derrick is also an author and online instructor.
    He also trains and works with various institutions to implement data science solutions
    as well as to upskill their staff. You might want to check his [Complete Data
    Science & Machine Learning Bootcamp in Python course](https://www.udemy.com/course/data-science-bootcamp-in-python/?referralCode=9F6DFBC3F92C44E8C7F4).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Derrick Mwiti](https://www.linkedin.com/in/mwitiderrick/)** 是一位对分享知识充满热情的数据科学家。他是数据科学社区的积极贡献者，参与了
    Heartbeat、Towards Data Science、Datacamp、Neptune AI、KDnuggets 等多个博客。他的内容在互联网上已经被观看超过一百万次。Derrick
    还是一名作者和在线讲师。他还与各种机构合作，实施数据科学解决方案并提升员工技能。你可以查看他的[完整数据科学与机器学习 Python 课程](https://www.udemy.com/course/data-science-bootcamp-in-python/?referralCode=9F6DFBC3F92C44E8C7F4)。'
- en: '[Original](https://neptune.ai/blog/ensemble-learning-guide). Reposted with
    permission.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://neptune.ai/blog/ensemble-learning-guide)。经许可转载。'
- en: '**Related:**'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[XGBoost: What it is, and when to use it](/2020/12/xgboost-what-when.html)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost：它是什么，何时使用](/2020/12/xgboost-what-when.html)'
- en: '[Gradient Boosted Decision Trees – A Conceptual Explanation](/2021/04/gradient-boosted-trees-conceptual-explanation.html)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[梯度提升决策树 – 概念解释](/2021/04/gradient-boosted-trees-conceptual-explanation.html)'
- en: '[The Best Machine Learning Frameworks & Extensions for Scikit-learn](/2021/03/best-machine-learning-frameworks-extensions-scikit-learn.html)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[最佳机器学习框架及 Scikit-learn 的扩展](/2021/03/best-machine-learning-frameworks-extensions-scikit-learn.html)'
- en: More On This Topic
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[KDnuggets News, April 13: Python Libraries Data Scientists Should…](https://www.kdnuggets.com/2022/n15.html)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，4月13日：数据科学家应使用的 Python 库…](https://www.kdnuggets.com/2022/n15.html)'
- en: '[Ensemble Learning with Examples](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[带示例的集成学习](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习技术：Python 中随机森林的详细讲解](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
- en: '[When Would Ensemble Techniques be a Good Choice?](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[何时选择集成技术？](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
- en: '[Statistics for Machine Learning: What you need to know to become a…](https://www.kdnuggets.com/2024/03/sas-statistics-machine-learning-need-know-become-certified-expert)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的统计学：成为…所需了解的内容](https://www.kdnuggets.com/2024/03/sas-statistics-machine-learning-need-know-become-certified-expert)'
- en: '[Want to Use Your Data Skills to Solve Global Problems? Here’s What…](https://www.kdnuggets.com/2022/04/jhu-want-data-skills-solve-global-problems.html)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[想用你的数据技能解决全球问题？这里有…](https://www.kdnuggets.com/2022/04/jhu-want-data-skills-solve-global-problems.html)'
