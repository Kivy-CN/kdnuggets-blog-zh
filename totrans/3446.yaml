- en: 'Multi-Task Learning in Tensorflow: Part 1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2016/07/multi-task-learning-tensorflow-part-1.html/2](https://www.kdnuggets.com/2016/07/multi-task-learning-tensorflow-part-1.html/2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Alternate Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first solution is particularly suited to situations where you’ll have a
    batch of Task 1 data and then a batch of Task 2 data.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that Tensorflow automatically figures out which calculations are needed
    for the operation you requested, and only conducts those calculations. This means
    that **if we define an optimiser on only one of the tasks, it will only train
    the parameters required to compute that task - and will leave the rest alone**.
    Since Task 1 relies only on the Task 1 and Shared Layers, the Task 2 layer will
    be untouched. Let’s draw another diagram with the desired optimisers at the end
    of each task.
  prefs: []
  type: TYPE_NORMAL
- en: '![Graph with optimisers](../Images/5bd03318ff44c0d9530716319655d482.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can conduct Multi-Task learning by alternately calling each task optimiser,
    which means we can continually transfer some of the information from each task
    to the other. In a loose sense, we are discovering the ‘commonality’ between the
    tasks. The following code implements this for our easy example. If you are following
    along, paste this at the bottom of the previous code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Tips: When is Alternate Training Good?**'
  prefs: []
  type: TYPE_NORMAL
- en: Alternate training is a good idea when you have two different datasets for each
    of the different tasks (for example, translating from English to French and English
    to German). By designing a network in this way, you can improve the performance
    of each of your individual tasks without having to find more task-specific training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Alternate training is the most common situation you’ll find yourself in, because
    there aren’t that many datasets that have two or more outputs. We’ll come on to
    one example, but the clearest examples are where you want to build hierarchy into
    your tasks. For example, in vision, you might want one of your tasks to predict
    the rotation of an object, the other what the object would look like if you changed
    the camera angle. These two tasks are obviously related - in fact the rotation
    probably comes before the image generation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tips: When is Alternate Training Less Good?**'
  prefs: []
  type: TYPE_NORMAL
- en: Alternate training can easily become biased towards a specific task. The first
    way is obvious - if one of your tasks has a far larger dataset than the other,
    then if you train in proportion to the dataset sizes your shared layer will contain
    more information about the more significant task.
  prefs: []
  type: TYPE_NORMAL
- en: The second is less so. If you train alternately, the final task in your model
    will create a bias in the parameters. There isn’t any obvious way that you can
    overcome this problem, but it does mean that in circumstances where you don’t
    have to train alternately, you shouldn’t.
  prefs: []
  type: TYPE_NORMAL
- en: Training at the Same Time - Joint Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When you have a dataset with multiple labels for each input, what you really
    want is to train the tasks at the same time. The question is, how do you preserve
    the independence of the task-specific functions? The answer is surprisingly simple
    - you just add up the loss functions of the individual tasks and optimise on that.
    Below is a diagram that shows a network that can train jointly, with the accompanying
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Joint training](../Images/7e111a03e0943c04ebd6be7be88209f6.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Conclusions and Next Steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this post we’ve gone through the basic principles behind multi-task learning
    in deep neural nets. If you’ve used Tensorflow before, and have your own project,
    then hopefully this has given you enough to get started.
  prefs: []
  type: TYPE_NORMAL
- en: For those of you who want a more meaty, more detailed example of how this can
    be used to improve performance in multiple tasks, then stay tuned for part 2 of
    the tutorial where we’ll delve into Natural Language Processing to build a multi-task
    model for shallow parsing and part of speech tagging.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Jonathan Godwin](https://jg8610.github.io/)** is currently studying
    for a Msc in Machine Learning from UCL with a specialism in deep multi-task learning
    for NLP. He will be finishing in September and will be looking for jobs/research
    roles where he can use this skill set on interesting problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://jg8610.github.io/Multi-Task/). Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Introduction to Recurrent Networks in TensorFlow](/2016/05/intro-recurrent-networks-tensorflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Good, Bad & Ugly of TensorFlow](/2016/05/good-bad-ugly-tensorflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Scikit Flow: Easy Deep Learning with TensorFlow and Scikit-learn](/2016/02/scikit-flow-easy-deep-learning-tensorflow-scikit-learn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[TensorFlow for Computer Vision - Transfer Learning Made Easy](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PyTorch or TensorFlow? Comparing popular Machine Learning frameworks](https://www.kdnuggets.com/2022/02/packt-pytorch-tensorflow-comparing-popular-machine-learning-frameworks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The "Hello World" of Tensorflow](https://www.kdnuggets.com/2022/05/hello-world-tensorflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Guide to Train an Image Classification Model Using Tensorflow](https://www.kdnuggets.com/2022/12/guide-train-image-classification-model-tensorflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Free TensorFlow 2.0 Complete Course](https://www.kdnuggets.com/2023/02/free-tensorflow-20-complete-course.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
