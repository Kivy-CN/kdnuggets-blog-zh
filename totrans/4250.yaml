- en: How to Train a Joint Entities and Relation Extraction Classifier using BERT
    Transformer with spaCy 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/06/train-joint-entities-relation-extraction-classifier-bert-spacy.html](https://www.kdnuggets.com/2021/06/train-joint-entities-relation-extraction-classifier-bert-spacy.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Walid Amamou](https://www.linkedin.com/in/walid-amamou-b65105b9/), Founder
    of UBIAI**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5514c4925ce52d6bb871c2a536d60f5b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [JJ Ying](https://unsplash.com/@jjying?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/science?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most useful applications of NLP technology is information extraction
    from unstructured texts — contracts, financial documents, healthcare records,
    etc. — that enables automatic data query to derive new insights. Traditionally,
    named entity recognition has been widely used to identify entities inside a text
    and store the data for advanced querying and filtering. However, if we want to
    semantically understand the unstructured text, **NER alone is not enough since
    we don’t know how the entities are related to each other**. Performing joint NER
    and relation extraction will open up a whole new way of information retrieval
    through knowledge graphs where you can navigate across different nodes to discover
    hidden relationships. Therefore, performing these tasks jointly will be beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: Building on my [previous article](https://towardsdatascience.com/how-to-fine-tune-bert-transformer-with-spacy-3-6a90bfe57647) where
    we fine-tuned a BERT model for NER using spaCy3, we will now add relation extraction
    to the pipeline using the new Thinc library from spaCy. We train the relation
    extraction model following the steps outlined in [spaCy’s documentation](https://spacy.io/usage/layers-architectures#component-rel).
    We will compare the performance of the relation classifier using transformers
    and tok2vec algorithms. Finally, we will test the model on a job description found
    online.
  prefs: []
  type: TYPE_NORMAL
- en: '**Relation Classification:**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At its core, the relation extraction model is a classifier that predicts a relation **r** for
    a given pair of entity **{e1, e2}**. In case of transformers, this classifier
    is added on top of the output hidden states. For more information about relation
    extraction, please read this excellent [article](https://towardsdatascience.com/bert-s-for-relation-extraction-in-nlp-2c7c3ab487c4) outlining
    the theory of fine tuning transformer model for relation classification.
  prefs: []
  type: TYPE_NORMAL
- en: The pre-trained model that we are going to fine-tune is the roberta-base model
    but you can use any pre-trained model available in huggingface library by simply
    inputting the name in the config file (see below).
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial we are going to extract the relationship between the two entities
    {Experience, Skills} as **Experience_in** and between {Diploma, Diploma_major}
    as **Degree_in**. The goal is to extract the years of experience required in a
    specific skills and the diploma major associated to the required diploma. You
    can of course, train your own relation classifier for your own use case such as
    finding the cause/effect of symptoms in health records or company acquisitions
    in financial documents. The possibilities are limitless…
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will only cover the entity relation extraction part. For
    fine-tuning BERT NER using spaCy 3, please refer to my [previous article](https://towardsdatascience.com/how-to-fine-tune-bert-transformer-with-spacy-3-6a90bfe57647).
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Annotation:**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As in my [previous article](https://towardsdatascience.com/how-to-fine-tune-bert-transformer-with-spacy-3-6a90bfe57647),
    we use [UBIAI](https://ubiai.tools/) text annotation tool to perform the joint
    entity and relation annotation because of its versatile interface that allows
    us to switch between entity and relation annotation easily (see below):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58a68157e72e3adf0c422d9d92851a0a.png)'
  prefs: []
  type: TYPE_IMG
- en: UBIAI’s joint entity and relation annotation interface
  prefs: []
  type: TYPE_NORMAL
- en: For this tutorial, I have only annotated around 100 documents containing entities
    and relations. For production, we will certainly need more annotated data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Preparation:**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we train the model, we need to convert our annotated data to a binary
    spacy file. We first split the annotation generated from UBIAI into training/dev/test
    and save them separately. We modify the [code](https://github.com/explosion/projects/blob/v3/tutorials/rel_component/scripts/parse_data.py/) that
    is provided in spaCy’s tutorial repo to create the binary file for our own annotation
    ([conversion code](https://github.com/walidamamou/relation_extraction_transformer)).
  prefs: []
  type: TYPE_NORMAL
- en: We repeat this step for the training, dev and test dataset to generate three
    binary spacy files ([files available in github](https://github.com/walidamamou/relation_extraction_transformer)).
  prefs: []
  type: TYPE_NORMAL
- en: '**Relation Extraction Model Training:**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For training, we will provide the entities from our golden corpus and train
    the classifier on these entities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new Google Colab project and make sure to select GPU as hardware accelerator
    in the notebook settings. Make sure GPU is enabled by running: !nvidia-smi'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Install spacy-nightly:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the wheel package and clone spacy’s relation extraction repo:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Install transformer pipeline and spacy transformers library:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Change directory to rel_component folder: cd rel_component'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Create a folder with the name “data” inside rel_component and upload the training,
    dev and test binary files into it:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/45fe4a3e6a399e71eb538d052347a94a.png)'
  prefs: []
  type: TYPE_IMG
- en: Training folder
  prefs: []
  type: TYPE_NORMAL
- en: 'Open project.yml file and update the training, dev and test path:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can change the pre-trained transformer model (if you want to use a different
    language, for example), by going to the configs/rel_trf.cfg and entering the name
    of the model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Before we start the training, we will decrease the max_length in configs/rel_trf.cfg
    from the default 100 token to 20 to increase the efficiency of our model. The
    max_length corresponds to the **maximum distance** between two entities above
    which they will not be considered for relation classification. As a result, two
    entities from the same document will be classified, as long as they are within
    a maximum distance (in number of tokens) of each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We are finally ready to train and evaluate the relation extraction model; just
    run the commands below:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You should start seeing the P, R and F score start getting updated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e8e1d34450b135201caee1bbbb64bae.png)'
  prefs: []
  type: TYPE_IMG
- en: Model training in progress
  prefs: []
  type: TYPE_NORMAL
- en: After the model is done training, the evaluation on the test data set will immediately
    start and display the predicted versus golden labels. The model will be saved
    in a folder named “training” along with the scores of our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train the non-transformer model tok2vec, run the following command instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can compare the performance of the two models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The transformer based model’s precision and recall scores are significantly
    better than tok2vec and demonstrate the usefulness of transformers when dealing
    with low amount of annotated data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Joint Entity and Relation Extraction Pipeline:**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Assuming that we have already trained a transformer NER model as in my [previous
    post](https://towardsdatascience.com/how-to-fine-tune-bert-transformer-with-spacy-3-6a90bfe57647),
    we will extract entities from a job description found online (that was not part
    of the training nor the dev set) and feed them to the relation extraction model
    to classify the relationship.
  prefs: []
  type: TYPE_NORMAL
- en: Install spacy transformers and transformer pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Load the NER model and extract entities:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We print the extracted entities:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We have successfully extracted all the skills, number of years of experience,
    diploma and diploma major from the text! Next we load the relation extraction
    model and classify the relationship between the entities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Make sure to copy rel_pipe and rel_model from the scripts folder into
    your main folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9bc4d88f2070b0ed394af5dae32b8732.png)'
  prefs: []
  type: TYPE_IMG
- en: Scripts folder
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we display all the entities having a relationship **Experience_in** with
    confidence score higher than 90%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Remarkably, we were able to extract almost all the years of experience along
    with their respective skills correctly with with no false positives or negatives!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the entities having relationship **Degree_in:**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Again, we successfully extracted all the relationships between diploma and diploma
    major!
  prefs: []
  type: TYPE_NORMAL
- en: This again demonstrates how easy it is to fine tune transformer models to your
    own domain specific case with low amount of annotated data, whether it is for
    NER or relation extraction.
  prefs: []
  type: TYPE_NORMAL
- en: With only a hundred of annotated documents, we were able to train a relation
    classifier with good performance. Furthermore, we can use this initial model to
    auto-annotate hundreds more of unlabeled data with minimal correction. This can
    significantly speed up the annotation process and improve model performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion:**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transformers have truly transformed the domain of NLP and I am particularly
    excited about their application in information extraction. I would like to give
    a shoutout to explosion AI(spaCy developers) and huggingface for providing open
    source solutions that facilitates the adoption of transformers.
  prefs: []
  type: TYPE_NORMAL
- en: If you need data annotation for your project, don’t hesitate to try out [UBIAI](https://ubiai.tools/) annotation
    tool. We provide numerous programmable labeling solutions (such as ML auto-annotation,
    regular expressions, dictionaries, etc…) to minimize hand annotation.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, checkout [this article](https://walidamamou.medium.com/building-a-knowledge-graph-for-job-search-using-bert-transformer-8677c8b3a2e7) to
    learn how to leverage the NER and relation extraction models to build knowledge
    graphs and extract new insights.
  prefs: []
  type: TYPE_NORMAL
- en: If you have any comment, please leave a note below or email at admin@ubiai.tools!
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Walid Amamou](https://www.linkedin.com/in/walid-amamou-b65105b9/)**
    is the Founder of UBIAI, an annotation tool for NLP applications, and holds a
    PhD in Physics.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/how-to-train-a-joint-entities-and-relation-extraction-classifier-using-bert-transformer-with-spacy-49eb08d91b5c).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[How to Fine-Tune BERT Transformer with spaCy 3](/2021/06/fine-tune-bert-transformer-spacy.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a Knowledge Graph for Job Search Using BERT](/2021/06/knowledge-graph-job-search-bert.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fine-Tuning Transformer Model for Invoice Recognition](/2021/06/fine-tuning-transformer-model-invoice-recognition.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How to Build and Train a Transformer Model from Scratch with…](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting Started with spaCy for NLP](https://www.kdnuggets.com/2022/11/getting-started-spacy-nlp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Natural Language Processing with spaCy](https://www.kdnuggets.com/2023/01/natural-language-processing-spacy.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[From Theory to Practice: Building a k-Nearest Neighbors Classifier](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Classifying Long Text Documents Using BERT](https://www.kdnuggets.com/2022/02/classifying-long-text-documents-bert.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Extractive Summarization with LLM using BERT](https://www.kdnuggets.com/extractive-summarization-with-llm-using-bert)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
