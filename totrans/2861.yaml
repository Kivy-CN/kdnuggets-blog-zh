- en: 'Research Guide: Advanced Loss Functions for Machine Learning Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/11/research-guide-advanced-loss-functions-machine-learning-models.html](https://www.kdnuggets.com/2019/11/research-guide-advanced-loss-functions-machine-learning-models.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to good training data and the right model architecture, loss functions
    are one of the most important parts of training an accurate machine learning model.
    For this post, I’d love to give developers an overview of some of the more advanced
    loss functions and how they can be used to improve the accuracy of models—or solve
    entirely new tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For example, [semantic segmentation](https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc) models
    typically use a simple cross-categorical entropy loss function during training,
    but if we want to segment objects with many fine details like hair, adding a gradient
    loss function to the model can vastly improve results.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This is just one example—the following guide explores research centered on a
    variety of advanced loss functions for machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Robust Bi-Tempered Logistic Loss Based on Bregman Divergences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Logistic loss functions don’t perform very well during training when the data
    in question is very noisy. Such noise can be caused by outliers and mislabeled
    data. In this paper, Google Brain authors aim to solve the shortcomings of the
    logistic loss function by replacing the logarithm and exponential functions with
    their corresponding “tempered” versions.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Bi-Tempered Logistic Loss for Training Neural Nets with Noisy Data**](https://ai.googleblog.com/2019/08/bi-tempered-logistic-loss-for-training.html?source=post_page-----aee68ed8a38c----------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: The quality of models produced by machine learning (ML) algorithms directly
    depends on the quality of the training...
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/19c1267393081ac6fa579cfa61d2473b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1906.03361.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The authors introduce a temperature into the exponential function and replace
    the softmax output layer of neural nets with a high-temperature generalization.
    The algorithm used in the log loss is replaced by a low-temperature logarithm.
    The two temperatures are tuned to create loss functions that are nonconvex.
  prefs: []
  type: TYPE_NORMAL
- en: The last neural net layer is replaced with the bi-temperature generalization
    of the logistic loss. This makes the training process more robust to noise. The
    method proposed in this paper is based on [Bregman divergences](https://arxiv.org/abs/1905.11545).
    Its performance can be visualized in the figure below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/4caf99e60120f3be38f29adc6effbc85.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1906.03361.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: For experimentation, the authors added synthetic noise to MNIST and CIFAR-100
    datasets. The results obtained with their bi-temperature loss function was then
    compared to the vanilla logistic loss function. The bi-temperature loss obtains
    an accuracy of 98.56% on MNIST and 62.5% ON CIFAR-100\. The figure below shows
    the performance in detail.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/0ddf5ccd7cecd4074732e8999f64fe16.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1906.03361.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning models are moving closer and closer to edge devices. Fritz
    AI is here to help with this transition. [Explore our suite of developer tools
    that makes it easy to teach devices to see, hear, sense, and think.](https://www.fritz.ai/product/premium.html?utm_campaign=buildmodels4&utm_source=heartbeat)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**GANs Loss Functions**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Discriminator loss aims at maximizing the probability given to real and fake
    images. Minimax loss is used [in the paper that introduced GANs](https://arxiv.org/abs/1406.2661).
    This is a strategy aimed at reducing the worst-case-scenario possible loss. It’s
    simply minimizing the maximum loss. This loss is also used in two-player games
    to reduce the maximum loss for a layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Are GANs Created Equal? A Large-Scale Study**](https://arxiv.org/abs/1711.10337?source=post_page-----aee68ed8a38c----------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial networks (GAN) are a powerful subclass of generative
    models. Despite a very rich research...
  prefs: []
  type: TYPE_NORMAL
- en: In the case of GANs, the two players are the generator and discriminator. This
    involves the minimization of the generator’s loss and maximization of the discriminator’s
    loss. Modification of the discriminator loss forms the non-saturating GAN loss,
    whose aim is to tackle the saturation problem. This involves the generator maximizing
    the log of the discriminator probabilities. It is done for the generated images.
  prefs: []
  type: TYPE_NORMAL
- en: Least squares GAN loss was developed to counter the challenges of binary cross-entropy
    loss that resulted in the generated images being very different from the real
    images. This loss function is adopted for the discriminator. As a result of this,
    GANs using this loss function are able to generate higher quality images than
    regular GANs. A comparison of the two is shown in the next figure.
  prefs: []
  type: TYPE_NORMAL
- en: '[**NIPS 2016 Tutorial: Generative Adversarial Networks**](https://arxiv.org/abs/1701.00160?source=post_page-----aee68ed8a38c----------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: This report summarizes the tutorial presented by the author at NIPS 2016 on
    generative adversarial networks (GANs). The...
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/c45b420599bac33a48cabd76bdca208c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[sources](https://arxiv.org/pdf/1611.04076.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The Wasserstein loss function is dependent on the modification of the GAN architecture,
    where the discriminator doesn’t perform instance classification. Instead, the
    discriminator outputs a number for each instance. It attempts to make the number
    bigger for real instances than for fake ones.
  prefs: []
  type: TYPE_NORMAL
- en: In this loss function, the discriminator attempts to maximize the difference
    between the output on real instances and the output on fake instances. The generator,
    on the other hand, attempts to maximize the discriminator’s output for its fake
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Wasserstein GAN**](https://arxiv.org/abs/1701.07875?source=post_page-----aee68ed8a38c----------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: We introduce a new algorithm named WGAN, an alternative to traditional GAN training.
    In this new model, we show that we...
  prefs: []
  type: TYPE_NORMAL
- en: Here’s an image showing the performance of the GANs using this loss.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/f8204a8d8649657f5330afe2605011be.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1701.07875)'
  prefs: []
  type: TYPE_NORMAL
- en: Focal Loss for Dense Object Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper proposes an improvement to the standard cross-entropy criterion by
    reshaping it such that it down-weights the loss assigned to the well-classified
    examples — focal loss. This loss function is aimed at solving the class imbalance
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Focal loss aims at training on a sparse set of hard examples and prevents easy
    negatives from trouncing the [detector](https://www.fritz.ai/object-detection/) at
    training. For testing, the authors develop RetinaNet — a simple dense detector.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Focal Loss for Dense Object Detection**](https://arxiv.org/abs/1708.02002?source=post_page-----aee68ed8a38c----------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: The highest accuracy object detectors to date are based on a two-stage approach
    popularized by R-CNN, where a...
  prefs: []
  type: TYPE_NORMAL
- en: In this loss function, the cross-entropy loss is scaled with the scaling factors
    decaying at zero as the confidence in the correct classes increases. The scaling
    factor automatically down weights the contribution of easy examples at training
    time and focuses on the hard ones.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/d1f1eb379d7179e323d83d078d643722.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1708.02002)'
  prefs: []
  type: TYPE_NORMAL
- en: Here are the results obtained by the focal loss function on RetinaNet.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/6004f0500be1fb63ba16cca7932c8ae8.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1708.02002)'
  prefs: []
  type: TYPE_NORMAL
- en: Intersection over Union (IoU)-balanced Loss Functions for Single-stage Object
    Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Loss functions adopted by single-stage detectors perform sub-optimally in localization.
    This paper proposes an IoU-based loss function that consists of IoU-balanced classification
    and IoU-balanced localization loss.
  prefs: []
  type: TYPE_NORMAL
- en: '[**IoU-balanced Loss Functions for Single-stage Object Detection**](https://arxiv.org/abs/1908.05641?source=post_page-----aee68ed8a38c----------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Single-stage detectors are efficient. However, we find that the loss functions
    adopted by single-stage detectors are...
  prefs: []
  type: TYPE_NORMAL
- en: The IoU-balanced classification loss focuses on positive scenarios with high
    IoU can increase the correlation between classification and the task of localization.
    The loss aims at decreasing the gradient of the examples with low IoU and increasing
    the gradient of examples with high IoU. This increases the localization accuracy
    of models.
  prefs: []
  type: TYPE_NORMAL
- en: The loss’s performance on the COCO dataset is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/feb8e2a8fb21155b58779ed20e0090c1.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1908.05641.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Boundary loss for highly unbalanced segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This [paper proposes a boundary loss](http://proceedings.mlr.press/v102/kervadec19a/kervadec19a.pdf) for
    highly unbalanced segmentations. The loss takes the form of a distance metric
    on the space of contours and not regions. This is done to tackle the challenges
    of regional losses for highly unbalanced [segmentation](https://www.fritz.ai/image-segmentation/) problems.
    The loss is inspired by discrete optimization techniques for computing gradient
    flows of curve evolution.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/aa561456754ad2ce4e2fe8b004dea4de.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](http://proceedings.mlr.press/v102/kervadec19a/kervadec19a.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The boundary loss uses integrals over the boundary between regions as opposed
    to using unbalanced integrals over the regions. An integral approach for computing
    boundary variations is used. The authors express a non-symmetric L2 distance on
    the space of shapes as a regional integral. This avoids local differential computations
    involving contour points. This then yields a boundary loss that’s expressed as
    the sum of the regional softmax probability outputs of the network. The loss is
    easily combined with regional losses and incorporated in existing deep network
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: The boundary loss was tested on the [Ischemic Stroke Lesion (ISLES)](http://www.isles-challenge.org/) and
    the [White Matter Hyperintensities](https://wmh.isi.uu.nl/) (WMH) benchmark datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/f35d236d1d10b9821c2886c6eb6ff7e0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](http://proceedings.mlr.press/v102/kervadec19a/kervadec19a.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Perceptual Loss Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This loss function is used when images that look similar are being compared.
    The loss function is primarily used for training feedforward neural networks for
    tasks image transformation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/bcc0543cda2af0cdac01c9b6ec18667b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1603.08155.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Perceptual Losses for Real-Time Style Transfer and Super-Resolution**](https://arxiv.org/abs/1603.08155?source=post_page-----aee68ed8a38c----------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: We consider image transformation problems, where an input image is transformed
    into an output image. Recent methods for...
  prefs: []
  type: TYPE_NORMAL
- en: The perceptual loss function works by adding the squared errors in the middle
    of all pixels and calculating the mean.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/ac8b19fcc0168884743ae6615cc84ac5.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1603.08155.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: In [style transfer](https://www.fritz.ai/style-transfer/), perceptual loss enables
    deep learning models to reconstruct finer details better than other loss functions.
    At training time, perceptual losses measure image similarities better than per-pixel
    loss functions. They also enable the transfer of semantic knowledge from the loss
    network to the transformation network.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We should now be up to speed on some of the most common — and a couple of very
    recent — advanced loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: The papers/abstracts mentioned and linked to above also contain links to their
    code implementations. We’d be happy to see the results you obtain after testing
    them.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Derrick Mwiti](https://derrickmwiti.com/)** is a data analyst, a writer,
    and a mentor. He is driven by delivering great results in every task, and is a
    mentor at Lapid Leaders Africa.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://heartbeat.fritz.ai/research-guide-advanced-loss-functions-for-machine-learning-models-aee68ed8a38c).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Research Guide for Neural Architecture Search](/2019/10/research-guide-neural-architecture-search.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Research Guide for Transformers](/2019/10/research-guide-transformers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Research Guide for Video Frame Interpolation with Deep Learning](/2019/10/research-guide-video-frame-interpolation-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Loss Functions: An Explainer](https://www.kdnuggets.com/2022/03/loss-functions-explainer.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Multi-label NLP: An Analysis of Class Imbalance and Loss Function…](https://www.kdnuggets.com/2023/03/multilabel-nlp-analysis-class-imbalance-loss-function-approaches.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Advanced Feature Selection Techniques for Machine Learning Models](https://www.kdnuggets.com/2023/06/advanced-feature-selection-techniques-machine-learning-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3 Research-Driven Advanced Prompting Techniques for LLM Efficiency…](https://www.kdnuggets.com/3-research-driven-advanced-prompting-techniques-for-llm-efficiency-and-speed-optimization)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, August 3: 10 Most Used Tableau Functions • Is…](https://www.kdnuggets.com/2022/n31.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How Activation Functions Work in Deep Learning](https://www.kdnuggets.com/2022/06/activation-functions-work-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
