- en: Random Forest® — A Powerful Ensemble Learning Algorithm
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林® — 一种强大的集成学习算法
- en: 原文：[https://www.kdnuggets.com/2020/01/random-forest-powerful-ensemble-learning-algorithm.html](https://www.kdnuggets.com/2020/01/random-forest-powerful-ensemble-learning-algorithm.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/01/random-forest-powerful-ensemble-learning-algorithm.html](https://www.kdnuggets.com/2020/01/random-forest-powerful-ensemble-learning-algorithm.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引言
- en: In the article [**Decision Tree Algorithm — Explained**](https://towardsdatascience.com/decision-tree-algorithm-explained-83beb6e78ef4)**,** we
    have learned about Decision Tree and how it is used to predict the class or value
    of the target variable by learning simple decision rules inferred from prior data(training
    data).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在文章 [**决策树算法 — 解释**](https://towardsdatascience.com/decision-tree-algorithm-explained-83beb6e78ef4)**中，**我们学习了决策树及其如何通过从先前的数据（训练数据）中推断简单的决策规则来预测目标变量的类别或数值。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: But the common problem with Decision trees, especially having a table full of
    columns, they fit a lot. Sometimes it looks like the tree memorized the training
    data set. If there is no limit set on a decision tree, it will give you 100% accuracy
    on the training data set because in the worse case it will end up making 1 leaf
    for each observation. Thus this affects the accuracy when predicting samples that
    are not part of the training set.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，决策树的一个常见问题是，特别是当表格中有很多列时，它们会过拟合。 有时看起来像是树记住了训练数据集。如果没有对决策树设置限制，它将为训练数据集提供100%准确性，因为在最坏的情况下，它将为每个观察值创建一个叶子。因此，这会影响对不属于训练集的样本进行预测时的准确性。
- en: Random forest is one of several ways to solve this problem of overfitting, now
    let us dive deeper into the working and implementation of this powerful machine
    learning algorithm. But before that, I would suggest you get familiar with the [**Decision
    tree algorithm**](https://towardsdatascience.com/decision-tree-algorithm-explained-83beb6e78ef4)**.**
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是解决过拟合问题的几种方法之一，现在让我们*深入探讨*这一强大的机器学习算法的工作原理和实现方式。在此之前，我建议你熟悉一下 [**决策树算法**](https://towardsdatascience.com/decision-tree-algorithm-explained-83beb6e78ef4)**。**
- en: Random forest is an ensemble learning algorith, so before talking about random
    forest let us first briefly understand what are Ensemble Learning algorithms.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种集成学习算法，所以在讨论随机森林之前，让我们首先简单了解一下什么是集成学习算法。
- en: Ensemble Learning algorithms
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成学习算法
- en: Ensemble learning algorithms are [**meta-algorithms**](https://cs.stackexchange.com/questions/107003/what-is-a-meta-algorithm) that
    combine several machine learning algorithms into one predictive model in order
    to decrease variance, bias or improve predictions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习算法是 [**元算法**](https://cs.stackexchange.com/questions/107003/what-is-a-meta-algorithm) ，它们将多个机器学习算法结合成一个预测模型，以减少方差、偏差或改善预测。
- en: The algorithm can be any [machine learning](https://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer) algorithm
    such as logistic regression, decision tree, etc. These models, when used as inputs
    of ensemble methods, are called ”**base models**”.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 算法可以是任何 [机器学习](https://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer) 算法，如逻辑回归、决策树等。这些模型在作为集成方法的输入时，被称为”**基础模型**”。
- en: '![Figure](../Images/c8cd4a109b77a8574900f3219f03440a.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/c8cd4a109b77a8574900f3219f03440a.png)'
- en: '[Ensemble learning](https://www.commonlounge.com/discussion/1697ade39ac142988861daff4da7f27d)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[集成学习](https://www.commonlounge.com/discussion/1697ade39ac142988861daff4da7f27d)'
- en: Ensemble methods usually produce more accurate solutions than a single model
    would. This has been the case in a number of machine learning competitions, where
    the winning solutions used ensemble methods. In the popular Netflix Competition, [the
    winner used an ensemble method](http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/) to
    implement a powerful collaborative filtering algorithm. Another example is KDD
    2009 where the winner also [used ensemble methods](http://jmlr.org/proceedings/papers/v7/niculescu09/niculescu09.pdf).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法通常比单一模型产生更准确的解决方案。这在许多机器学习竞赛中已得到验证，获胜解决方案通常使用了集成方法。在著名的 Netflix 竞赛中，[获胜者使用了集成方法](http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/)来实现一个强大的协同过滤算法。另一个例子是
    KDD 2009，获胜者也[使用了集成方法](http://jmlr.org/proceedings/papers/v7/niculescu09/niculescu09.pdf)。
- en: 'Ensemble algorithms or methods can be divided into two groups:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 集成算法或方法可以分为两组：
- en: '**Sequential ensemble methods —** where the base learners are generated sequentially
    (e.g. AdaBoost).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺序集成方法**——其中基学习器是顺序生成的（例如，AdaBoost）。'
- en: The basic motivation of sequential methods is to** exploit the dependence between
    the base learners.** The overall performance can be boosted by weighing previously
    mislabeled examples with higher weight.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 顺序方法的基本动机是**利用基学习器之间的依赖性**。通过对之前标记错误的例子加权，整体性能可以得到提升。
- en: '**Parallel ensemble methods — **where the base learners are generated in parallel
    (e.g. Random Forest).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行集成方法**——其中基学习器是并行生成的（例如，随机森林）。'
- en: The basic motivation of parallel methods is to **exploit independence between
    the base learners** since the error can be reduced dramatically by averaging.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 并行方法的基本动机是**利用基学习器之间的独立性**，因为通过平均可以显著减少误差。
- en: Most ensemble methods use a single base learning algorithm to produce homogeneous
    base learners, i.e. learners of the same type, leading to *homogeneous ensembles*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数集成方法使用单一的基学习算法生成同质基学习器，即相同类型的学习器，导致*同质集成*。
- en: There are also some methods that use heterogeneous learners, i.e. learners of
    different types, leading to *heterogeneous ensembles*. In order for ensemble methods
    to be more accurate than any of its individual members, the base learners have
    to be as accurate as possible and as diverse as possible.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一些方法使用异质学习器，即不同类型的学习器，导致*异质集成*。为了使集成方法比任何单一成员更准确，基学习器必须尽可能准确且尽可能多样化。
- en: What is the Random Forest algorithm?
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林算法是什么？
- en: Random forest is a supervised ensemble learning algorithm that is used for both
    classifications as well as regression problems. But however, it is mainly used
    for classification problems. As we know that a forest is made up of trees and
    more trees mean more robust forest. Similarly, the random forest algorithm creates
    decision trees on data samples and then gets the prediction from each of them
    and finally selects the best solution by means of voting. It is an ensemble method
    that is better than a single decision tree because it reduces the over-fitting
    by averaging the result.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种监督集成学习算法，既用于分类问题也用于回归问题。但实际上，它主要用于分类问题。正如我们知道的，森林由树木组成，树木越多，森林越强健。类似地，随机森林算法在数据样本上创建决策树，然后从每棵树中获得预测，并通过投票选出最佳解决方案。它是一种比单一决策树更好的集成方法，因为它通过平均结果来减少过拟合。
- en: '![Figure](../Images/ba9e4638bff4eb8673f4aa3fe4504931.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/ba9e4638bff4eb8673f4aa3fe4504931.png)'
- en: As per majority voting, the final result is ‘Blue’.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 根据多数投票，最终结果是‘蓝色’。
- en: The fundamental concept behind random forest is a simple but powerful one — **the
    wisdom of crowds.**
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的基本概念是一个简单但强大的理念——**众智智慧**。
- en: '**“A large number of relatively uncorrelated models(trees) operating as a committee
    will outperform any of the individual constituent models.”**'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**“大量相对不相关的模型（树）作为一个委员会运作将优于任何单一的构成模型。”**'
- en: The low correlation between models is the key.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 模型之间的低相关性是关键。
- en: The reason why Random forest produces exceptional results is that the trees
    protect each other from their individual errors. While some trees may be wrong,
    many others will be right, so as a group the trees are able to move in the correct
    direction.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林能产生出色结果的原因是树木彼此保护，避免各自的错误。虽然某些树可能出错，但许多其他树会正确，因此作为一个整体，树木能够朝正确的方向发展。
- en: '**Why the name “Random”?**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么叫“随机”？**'
- en: 'Two key concepts that give it the name random:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 两个关键概念赋予了它“随机”这个名字：
- en: A random sampling of training data set when building trees.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在构建树时对训练数据集进行随机抽样。
- en: Random subsets of features considered when splitting nodes.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在拆分节点时考虑的特征随机子集。
- en: '**How is Random Forest ensuring Model diversity?**'
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**随机森林如何确保模型的多样性？**'
- en: 'Random forest ensures that the behavior of each individual tree is not too
    correlated with the behavior of any other tree in the model by using the following
    two methods:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林通过以下两种方法确保每棵树的行为与模型中任何其他树的行为不太相关：
- en: Bagging or Bootstrap Aggregation
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagging或Bootstrap聚合
- en: Random feature selection
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机特征选择
- en: '**Bagging or Bootstrap Aggregation**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**Bagging或Bootstrap聚合**'
- en: Decision trees are very sensitive to the data they are trained on, small changes
    to the training data set can result in a significantly different tree structure.
    The random forest takes advantage of this by allowing each individual tree to **randomly
    sample from the dataset with replacement**, resulting in different trees. This
    process is called Bagging.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树对所训练的数据非常敏感，训练数据集的微小变化可能导致树结构的显著不同。随机森林利用这一点，通过允许每棵树**从数据集中随机抽样（带有替换）**，从而生成不同的树。这一过程称为Bagging。
- en: Note that with bagging we are not subsetting the training data into smaller
    chunks and training each tree on a different chunk. Rather, if we have a sample
    of size **N**, we are still feeding each tree a training set of size **N**. But
    instead of the original training data, we take a random sample of size **N** with
    replacement.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在Bagging中，我们并不是将训练数据子集分成较小的块，并在不同的块上训练每棵树。相反，如果我们有一个大小为**N**的样本，我们仍然给每棵树一个大小为**N**的训练集。但不是原始训练数据，而是带有替换的随机样本。
- en: For example — If our training data is [1,2,3,4,5,6], then we might give one
    of our trees the list [1,2,2,3,6,6] and we can give another tree a list [2,3,4,4,5,6].
    Notice that the lists are of length **6** and some elements are repeated in the
    randomly selected training data we can give to our tree(because we sample with
    replacement).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如——如果我们的训练数据是[1,2,3,4,5,6]，那么我们可能会给一棵树提供列表[1,2,2,3,6,6]，而给另一棵树提供列表[2,3,4,4,5,6]。注意列表的长度是**6**，一些元素在我们提供给树的随机选择的训练数据中被重复（因为我们带有替换地抽样）。
- en: '![Figure](../Images/6bf318832e23fd46e08e0f9a91735ea0.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/6bf318832e23fd46e08e0f9a91735ea0.png)'
- en: '[Bagging](https://medium.com/machine-learning-through-visuals/machine-learning-through-visuals-part-1-what-is-bagging-ensemble-learning-432059568cc8)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[Bagging](https://medium.com/machine-learning-through-visuals/machine-learning-through-visuals-part-1-what-is-bagging-ensemble-learning-432059568cc8)'
- en: The above figure shows how random samples are taken from the dataset with replacement.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了如何从数据集中带有替换地抽取随机样本。
- en: '**Random feature selection**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机特征选择**'
- en: In a normal decision tree, when it is time to split a node, we consider every
    possible feature and pick the one that produces the most separation between the
    observations in the left node vs right node. In contrast, each tree in a random
    forest can pick only from a random subset of features. This forces even more variation
    amongst the trees in the model and ultimately results in low correlation across
    trees and more diversification.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在普通决策树中，当需要拆分节点时，我们考虑所有可能的特征，并选择在左节点与右节点之间产生最大分离的特征。相比之下，随机森林中的每棵树只能从特征的随机子集中选择。这迫使模型中的树之间有更多变化，并最终导致树之间的低相关性和更多的多样化。
- en: So in random forest, we end up with trees that are trained on different sets
    of data and also use different features to make decisions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在随机森林中，我们得到的树是基于不同的数据集训练的，并且使用不同的特征来做决策。
- en: '![Figure](../Images/e67b08d6c0c6c220df6974f3b1aacc30.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/e67b08d6c0c6c220df6974f3b1aacc30.png)'
- en: '[Random feature selection by different trees in random forest.](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[随机森林中不同树的随机特征选择。](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)'
- en: And finally, uncorrelated trees have created that buffer and predict each other
    from their respective errors.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，无关的树创造了缓冲区，并从各自的错误中预测彼此。
- en: 'Random Forest creation pseudocode:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林创建伪代码：
- en: Randomly select “**k**” features from total “**m**” features where **k << m**
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从总共“**m**”个特征中随机选择“**k**”个特征，其中**k << m**
- en: Among the “**k**” features, calculate the node “**d**” using the best split
    point
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在“**k**”个特征中，使用最佳划分点计算节点“**d**”
- en: Split the node into **daughter nodes** using the **best split**
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**最佳划分**将节点拆分为**子节点**
- en: Repeat the 1 **to 3 **steps until “l” number of nodes has been reached
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复1**到3**步，直到达到“l”个节点。
- en: Build forest by repeating steps 1 **to 4 **for “n” number times to create **“n”
    number of trees.**
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过重复步骤 1 **至 4** “n” 次来构建森林，从而创建**“n” 棵树**。
- en: Random Forest classifier Building in Scikit-learn
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Scikit-learn 中构建随机森林分类器
- en: In this section, we are going to build a Gender Recognition classifier using
    the Random Forest algorithm from the voice dataset. The idea is to identify a
    voice as male or female, based upon the acoustic properties of the voice and speech.
    The dataset consists of 3,168 recorded voice samples, collected from male and
    female speakers. The voice samples are pre-processed by acoustic analysis in R
    using the seewave and tuneR packages, with an analyzed frequency range of 0hz-280hz.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用随机森林算法从声音数据集中构建一个性别识别分类器。目的是根据声音和言语的声学属性识别声音为男性或女性。数据集包含 3,168 个录制的声音样本，来自男性和女性发言者。声音样本通过
    R 中的声学分析进行预处理，使用了 seewave 和 tuneR 包，分析频率范围为 0Hz-280Hz。
- en: The dataset can be downloaded from [kaggle](https://www.kaggle.com/primaryobjects/voicegender).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以从 [kaggle](https://www.kaggle.com/primaryobjects/voicegender)下载。
- en: 'The goal is to create a Decision tree and Random Forest classifier and compare
    the accuracy of both the models. The following are the steps that we will perform
    in the process of model building:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是创建一个决策树和随机森林分类器，并比较两个模型的准确性。以下是我们在模型构建过程中将执行的步骤：
- en: '**1\. Importing Various Modules and Loading the Dataset**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 导入各种模块并加载数据集**'
- en: '**2\. Exploratory Data Analysis (EDA)**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 探索性数据分析（EDA）**'
- en: '**3\. Outlier Treatment**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 异常值处理**'
- en: '**4\. Feature Engineering**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 特征工程**'
- en: '**5\. Preparing the Data**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**5\. 数据准备**'
- en: '**6\. Model building**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**6\. 模型构建**'
- en: '**7\. Model optimization**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**7\. 模型优化**'
- en: So let us start.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们开始吧。
- en: '**Step-1: Importing Various Modules and Loading the Dataset**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤-1: 导入各种模块并加载数据集**'
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now load the dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在加载数据集。
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Step-2: Exploratory Data Analysis (EDA)**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤-2: 探索性数据分析（EDA）**'
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Figure](../Images/ca05f41757ed2a156f3fa9e4d28daf21.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/ca05f41757ed2a156f3fa9e4d28daf21.png)'
- en: Dataset
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集
- en: 'The following acoustic properties of each voice are measured and included within
    our data:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 每个声音的以下声学属性被测量并包含在我们的数据中：
- en: '**meanfreq**: mean frequency (in kHz)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**meanfreq**: 平均频率（单位：kHz）'
- en: '**sd**: standard deviation of the frequency'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sd**: 频率的标准差'
- en: '**median**: median frequency (in kHz)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**median**: 中位频率（单位：kHz）'
- en: '**Q25**: first quantile (in kHz)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q25**: 第一四分位数（单位：kHz）'
- en: '**Q75**: third quantile (in kHz)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q75**: 第三四分位数（单位：kHz）'
- en: '**IQR**: interquartile range (in kHz)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IQR**: 四分位数间距（单位：kHz）'
- en: '**skew**: skewness'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**skew**: 偏度'
- en: '**kurt**: kurtosis'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kurt**: 峰度'
- en: '**sp.ent**: spectral entropy'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sp.ent**: 频谱熵'
- en: '**sfm**: spectral flatness'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sfm**: 频谱平坦度'
- en: '**mode**: mode frequency'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**mode**: 模态频率'
- en: '**centroid**: frequency centroid'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**centroid**: 频率质心'
- en: '**peakf**: peak frequency (the frequency with the highest energy)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**peakf**: 峰值频率（能量最高的频率）'
- en: '**meanfun**: the average of fundamental frequency measured across an acoustic
    signal'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**meanfun**: 在声学信号中测量的基频的平均值'
- en: '**minfun**: minimum fundamental frequency measured across an acoustic signal'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**minfun**: 在声学信号中测量的基频的最小值'
- en: '**maxfun**: maximum fundamental frequency measured across an acoustic signal'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**maxfun**: 在声学信号中测量的基频的最大值'
- en: '**meandom**: the average of dominant frequency measured across an acoustic
    signal'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**meandom**: 在声学信号中测量的主频率的平均值'
- en: '**mindom**: minimum of dominant frequency measured across an acoustic signal'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**mindom**: 在声学信号中测量的主频率的最小值'
- en: '**maxdom**: maximum of dominant frequency measured across an acoustic signal'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**maxdom**: 在声学信号中测量的主频率的最大值'
- en: '**dfrange**: the range of dominant frequency measured across an acoustic signal'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**dfrange**: 在声学信号中测量的主频率范围'
- en: '**modindx**: modulation index which is calculated as the accumulated absolute
    difference between adjacent measurements of fundamental frequencies divided by
    the frequency range'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**modindx**: 调制指数，它是通过将相邻的基频测量的绝对差值累积后除以频率范围来计算的'
- en: '**label**: male or female'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**label**: 男性或女性'
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that we have 3168 voice samples and for each sample, 20 different acoustic
    properties are recorded. Finally, the ‘label’ column is the target variable which
    we have to predict which is the gender of the person.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们有 3168 个声音样本，每个样本记录了 20 种不同的声学属性。最后，‘label’ 列是目标变量，我们需要预测的就是该人的性别。
- en: Now our next step is handling the missing values.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们下一步是处理缺失值。
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Figure](../Images/3d4ab01e6f8f8f1ab79de5a677cfb979.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/3d4ab01e6f8f8f1ab79de5a677cfb979.png)'
- en: No missing values in our dataset.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集中没有缺失值。
- en: Now I will perform the univariate analysis. Note that since all of the features
    are ‘numeric’ the most reasonable way to plot them would either be a ‘histogram’
    or a ‘boxplot’.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我将进行单变量分析。请注意，由于所有特征都是“数值型”的，因此最合理的绘制方式是“直方图”或“箱线图”。
- en: Also, univariate analysis is useful for outlier detection. Hence besides plotting
    a boxplot and a histogram for each column or feature, I have written a small utility
    function that tells the remaining no. of observations for each feature if we remove
    its outliers.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，单变量分析对于离群值检测也很有用。因此，除了为每个列或特征绘制箱线图和直方图外，我还编写了一个小工具函数，可以告诉我们如果去除离群值后每个特征剩余的观测数。
- en: To detect the outliers I have used the standard 1.5 InterQuartileRange (IQR)
    rule which states that any observation lesser than ‘first quartile — 1.5 IQR’
    or greater than ‘third quartile +1.5 IQR’ is an outlier.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检测离群值，我使用了标准的1.5四分位数范围（IQR）规则，该规则指出，任何低于“第一个四分位数 — 1.5 IQR”或高于“第三个四分位数 + 1.5
    IQR”的观测值都是离群值。
- en: '[PRE5]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let us plot the first feature i.e. meanfreq.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制第一个特征，即meanfreq。
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/624323449e7fffa3e321bd53d7e8ccfc.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/624323449e7fffa3e321bd53d7e8ccfc.png)'
- en: Inferences made from the above plots —
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从上述图表中得出的结论—
- en: 1) First of all, note that the values are in compliance with that observed from
    describing the method data frame.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 首先，请注意，值与描述方法数据框中观察到的情况一致。
- en: 2) Note that we have a couple of outliers w.r.t. to 1.5 quartile rule (represented
    by a ‘dot’ in the box plot). Removing these data points or outliers leaves us
    with around 3104 values.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 请注意，根据1.5四分位数规则，我们有几个离群值（在箱线图中用“点”表示）。去除这些数据点或离群值后，我们剩下大约3104个值。
- en: 3) Also, from the distplot that the distribution seems to be a bit -ve skewed
    hence we can normalize to make the distribution a bit more symmetric.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 此外，从分布图来看，分布似乎有些负偏，因此我们可以进行归一化，使分布更对称。
- en: 4) LASTLY, NOTE THAT A LEFT TAIL DISTRIBUTION HAS MORE OUTLIERS ON THE SIDE
    BELOW TO Q1 AS EXPECTED AND A RIGHT TAIL HAS ABOVE THE Q3.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 最后，请注意，左尾分布在Q1以下的侧面有更多的离群值，而右尾在Q3以上的侧面则有更多的离群值。
- en: Similar inferences can be made by plotting other features also, I have plotted
    some, you guys can check for all.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 通过绘制其他特征也可以得出类似的结论，我已经绘制了一些，你们可以检查所有的。
- en: '![](../Images/528e6dcf2f3d93b61ac0767cb58b5a48.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/528e6dcf2f3d93b61ac0767cb58b5a48.png)'
- en: '![](../Images/503c1f320cfa9db2122f5771a1353ebe.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/503c1f320cfa9db2122f5771a1353ebe.png)'
- en: '![](../Images/8a4b6927f6c352fec079f57ed0d90983.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a4b6927f6c352fec079f57ed0d90983.png)'
- en: '![](../Images/ed1818a5915e5afd2cd131fb5f3694a6.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed1818a5915e5afd2cd131fb5f3694a6.png)'
- en: Now plot and count the target variable to check if the target class is balanced
    or not.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在绘制和统计目标变量，以检查目标类别是否平衡。
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure](../Images/2556a11af165d6b8342f6329f7fc412e.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/2556a11af165d6b8342f6329f7fc412e.png)'
- en: Plot for Target variable
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 目标变量的绘图
- en: We have the equal number of observations for the ‘males’ and the ‘females’ class
    hence it is a balanced dataset and we don't need to do anything about it.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的“男性”和“女性”类别的观测数量相等，因此这是一个平衡的数据集，我们不需要对此做任何处理。
- en: Now I will perform Bivariate analysis to analyze the correlation between different
    features. To do it I have plotted a ‘heat map’ which clearly visualizes the correlation
    between different features.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我将进行双变量分析，以分析不同特征之间的相关性。为此，我绘制了一个“热图”，该图清晰地可视化了不同特征之间的相关性。
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Figure](../Images/ef81ec187ed9d1a6bdee14801ecae880.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/ef81ec187ed9d1a6bdee14801ecae880.png)'
- en: Heatmap
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 热图
- en: Inferences made from above heatmap plot—
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述热图中得出的结论—
- en: 1) Mean frequency is moderately related to label.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 平均频率与标签的相关性适中。
- en: 2) IQR and label tend to have a strong positive correlation.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 2) IQR 和标签之间通常有很强的正相关关系。
- en: 3) Spectral entropy is also quite highly correlated with the label while sfm
    is moderately related with label.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 频谱熵与标签的相关性也相当高，而sfm与标签的相关性适中。
- en: 4) skewness and kurtosis aren’t much related to label.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 偏度和峰度与标签的相关性不大。
- en: 5) meanfun is highly negatively correlated with the label.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 5) meanfun 与标签的相关性非常负。
- en: 6) Centroid and median have a high positive correlation expected from their
    formulae.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 6) Centroid 和 median 具有根据公式预期的高正相关性。
- en: 7) Also, meanfreq and centroid are exactly the same features as per formulae
    and so are the values. Hence their correlation is perfect 1\. In this case, we
    can drop any of that column.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 7) 此外，meanfreq 和 centroid 是根据公式完全相同的特征，它们的值也是一致的。因此，它们的相关性是完美的1。在这种情况下，我们可以删除其中任何一列。
- en: Note that centroid in general has a high degree of correlation with most of
    the other features so I’m going to drop centroid column.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，质心通常与大多数其他特征有很高的相关性，因此我将删除质心列。
- en: 8) sd is highly positively related to sfm and so is sp.ent to sd.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 8) sd 与 sfm 高度正相关，sp.ent 与 sd 也是如此。
- en: 9) kurt and skew are also highly correlated.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 9) kurt 和 skew 也高度相关。
- en: 10) meanfreq is highly related to the median as well as Q25.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 10) meanfreq 与中位数以及 Q25 高度相关。
- en: 11) IQR is highly correlated to sd.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 11) IQR 与 sd 高度相关。
- en: 12) Finally, self relation ie of a feature to itself is equal to 1 as expected.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 12) 最后，自相关，即特征对自身的关系，等于1，符合预期。
- en: Note that we can drop some highly correlated features as they add redundancy
    to the model but let us keep all the features for now. In the case of highly correlated
    features, we can use dimensionality reduction techniques like Principal Component
    Analysis(PCA) to reduce our feature space.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们可以丢弃一些高度相关的特征，因为它们会给模型带来冗余，但现在我们暂时保留所有特征。对于高度相关的特征，我们可以使用主成分分析（PCA）等降维技术来减少特征空间。
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Step-3: Outlier Treatment**'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤-3: 异常值处理**'
- en: Here we have to deal with the outliers. Note that we discovered the potential
    outliers in the **‘univariate analysis’ **section. Now to remove those outliers
    we can either remove the corresponding data points or impute them with some other
    statistical quantity like median (robust to outliers) etc.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们需要处理异常值。注意，我们在**“单变量分析”**部分发现了潜在的异常值。现在，要去除这些异常值，我们可以选择删除相应的数据点或用其他统计量（如中位数）进行填补（对异常值具有鲁棒性）。
- en: For now, I shall be removing all the observations or data points that are an
    outlier to ‘any’ feature. Doing so substantially reduces the dataset size.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我将移除所有对“任何”特征都是异常值的观测值或数据点。这样做会大大减少数据集的大小。
- en: '[PRE10]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that the new shape is (1636, 20), we are left with 20 features.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，新形状为（1636，20），我们剩下20个特征。
- en: '**Step-4: Feature Engineering**'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤-4: 特征工程**'
- en: Here I have dropped some columns which according to my analysis proved to be
    less useful or redundant.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我丢弃了一些根据我的分析证明不太有用或冗余的列。
- en: '[PRE11]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Figure](../Images/9e3a8003ba3bb2eaa7260f514c0dad23.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/9e3a8003ba3bb2eaa7260f514c0dad23.png)'
- en: Filtered dataset
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤后的数据集
- en: Now let us create some new features. I have done two new things here. Firstly
    I have made ‘meanfreq’, ’median’ and ‘mode’ to comply with the standard relation **3Median=2Mean
    +Mode. **For this, I have adjusted values in the ‘median’ column as shown below.
    You can alter values in any of the other columns say the ‘meanfreq’ column.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一些新特征。我在这里做了两件新事物。首先，我使‘meanfreq’，‘median’ 和 ‘mode’ 符合标准关系**3Median =
    2Mean + Mode**。为此，我调整了‘median’列中的值，如下所示。你可以修改其他列中的值，比如‘meanfreq’列。
- en: '[PRE12]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/db847c3a8ec43122f924de7ca815459e.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db847c3a8ec43122f924de7ca815459e.png)'
- en: The second new feature that I have added is a new feature to measure the ‘skewness’.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我添加的第二个新特征是一个新的特征，用于测量“偏度”。
- en: For this, I have used the ‘[Karl Pearson Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)’
    which is calculated as **Coefficient = (Mean — Mode )/StandardDeviation**
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我使用了‘[卡尔·皮尔森系数](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)’，其计算公式为**系数
    = （均值 — 众数）/标准差**
- en: You can also try some other coefficient also and see how it compared with the
    target i.e. the ‘label’ column.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以尝试一些其他系数，看看它们与目标（即“标签”列）相比如何。
- en: '[PRE13]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/9a3f6b669fd8d37f900fe827d25d2d9c.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a3f6b669fd8d37f900fe827d25d2d9c.png)'
- en: '**Step-5: Preparing the Data**'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤-5: 数据准备**'
- en: The first thing that we’ll do is normalize all the features or basically we’ll
    perform feature scaling to get all the values in a comparable range.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要做的是对所有特征进行标准化，基本上我们会执行特征缩放，以使所有值处于可比较的范围内。
- en: '[PRE14]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Next split your data into train and test set.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将数据拆分为训练集和测试集。
- en: '[PRE15]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Step-6: Model building**'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤-6: 模型构建**'
- en: Now we’ll build two classifiers, decision tree, and random forest and compare
    the accuracies of both of them.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将构建两个分类器，决策树和随机森林，并比较它们的准确率。
- en: '[PRE16]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Put the accuracies in a data frame.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 将准确率放入数据框中。
- en: '[PRE17]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Figure](../Images/c48ef2acaab7d53398af2864b80b12f8.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/c48ef2acaab7d53398af2864b80b12f8.png)'
- en: 'Plot the accuracies:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制准确率：
- en: '![](../Images/fe7c93dfae09b6abfa44b730c4bd5406.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe7c93dfae09b6abfa44b730c4bd5406.png)'
- en: As we have seen, just by using the default parameters for both of our models,
    the random forest classifier outperformed the decision tree classifier(as expected).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，仅仅使用我们模型的默认参数，随机森林分类器的表现优于决策树分类器（如预期）。
- en: '**Step-7: Parameter Tuning with GridSearchCV**'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**第7步：使用GridSearchCV进行参数调优**'
- en: Lastly, let us also tune our random forest classifier using GridSearchCV.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们还使用GridSearchCV调整我们的随机森林分类器。
- en: '[PRE18]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/ffa2c3d7da8a4fdfe128e44e35d6697f.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffa2c3d7da8a4fdfe128e44e35d6697f.png)'
- en: '[PRE19]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/492709261b434d559975c2c3f5cc524f.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/492709261b434d559975c2c3f5cc524f.png)'
- en: After hyperparameter optimization as we can see the results are pretty good
    :)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 经过超参数优化后，我们可以看到结果相当不错 :)
- en: If you want you can also check the Importance of each feature.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意，你还可以检查每个特征的重要性。
- en: '[PRE20]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/9da677f171e6c3452da909f93b05617c.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9da677f171e6c3452da909f93b05617c.png)'
- en: Conclusion
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: Now that you hopefully have the conceptual framework of random forest and this
    article has given you the confidence and understanding needed to start using the
    random forest on your projects. The random forest is a powerful machine learning
    model, but that should not prevent us from knowing how it works. The more we know
    about a model, the better equipped we will be to use it effectively and explain
    how it makes predictions.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在希望你已经掌握了随机森林的概念框架，并且这篇文章已经给了你开始在项目中使用随机森林所需的信心和理解。随机森林是一个强大的机器学习模型，但这不应妨碍我们了解它是如何工作的。我们对模型了解得越多，就越能有效地使用它，并解释它如何做出预测。
- en: You can find the source code in my [Github repository.](https://github.com/nageshsinghc4/Gender-prediction-from-voice-data--random-forest)
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我的 [Github 仓库](https://github.com/nageshsinghc4/Gender-prediction-from-voice-data--random-forest)
    中找到源代码。
- en: Well, that’s all for this article hope you guys have enjoyed reading it and
    I’ll be glad if the article is of any help. Feel free to share your comments/thoughts/feedback
    in the comment section.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，这篇文章就是这些，希望你们喜欢阅读，如果这篇文章对你有帮助，我会很高兴。欢迎在评论区分享你的评论/想法/反馈。
- en: '![Figure](../Images/76b044b507540a50ce22f2596d637aba.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/76b044b507540a50ce22f2596d637aba.png)'
- en: '[Source](http://bestanimations.com/Nature/Flora/Trees/Trees.html)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](http://bestanimations.com/Nature/Flora/Trees/Trees.html)'
- en: Thanks for reading!!!
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 谢谢阅读！！！
- en: '**Bio: [Nagesh Singh Chauhan](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    is a Big data developer at CirrusLabs. He has over 4 years of working experience
    in various sectors like Telecom, Analytics, Sales, Data Science having specialisation
    in various Big data components.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介： [Nagesh Singh Chauhan](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    是CirrusLabs的大数据开发人员。他在电信、分析、销售、数据科学等各个领域拥有超过4年的工作经验，专注于各种大数据组件。'
- en: '[Original](https://towardsdatascience.com/random-forest-a-powerful-ensemble-learning-algorithm-2bf132ba639d).
    Reposted with permission.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/random-forest-a-powerful-ensemble-learning-algorithm-2bf132ba639d)。经许可转载。'
- en: Random Forests® is a registered trademark of Minitab.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林® 是Minitab的注册商标。
- en: '**Related:**'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[A Friendly Introduction to Support Vector Machines](/2019/09/friendly-introduction-support-vector-machines.html)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[支持向量机的友好介绍](/2019/09/friendly-introduction-support-vector-machines.html)'
- en: '[Comparing Decision Tree Algorithms: Random Forest vs. XGBoost](/2019/08/activestate-decision-tree-random-forest-xgboost.html)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[比较决策树算法：随机森林与XGBoost](/2019/08/activestate-decision-tree-random-forest-xgboost.html)'
- en: '[Build an Artificial Neural Network From Scratch: Part 1](/2019/11/build-artificial-neural-network-scratch-part-1.html)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从零开始构建人工神经网络：第1部分](/2019/11/build-artificial-neural-network-scratch-part-1.html)'
- en: More On This Topic
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Does the Random Forest Algorithm Need Normalization?](https://www.kdnuggets.com/2022/07/random-forest-algorithm-need-normalization.html)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机森林算法是否需要归一化？](https://www.kdnuggets.com/2022/07/random-forest-algorithm-need-normalization.html)'
- en: '[Random Forest vs Decision Tree: Key Differences](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机森林与决策树：关键区别](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)'
- en: '[Tuning Random Forest Hyperparameters](https://www.kdnuggets.com/2022/08/tuning-random-forest-hyperparameters.html)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[调整随机森林超参数](https://www.kdnuggets.com/2022/08/tuning-random-forest-hyperparameters.html)'
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习技术：Python中随机森林的操作指南](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
- en: '[Why we will always need humans to train AI — sometimes in real-time](https://www.kdnuggets.com/2021/12/why-we-need-humans-training-ai.html)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么我们总是需要人类来训练AI——有时是实时的](https://www.kdnuggets.com/2021/12/why-we-need-humans-training-ai.html)'
- en: '[Ensemble Learning with Examples](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[带例子的集成学习](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
