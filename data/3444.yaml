- en: 'Multi-Task Learning in Tensorflow: Part 1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tensorflow中的多任务学习：第1部分
- en: 原文：[https://www.kdnuggets.com/2016/07/multi-task-learning-tensorflow-part-1.html](https://www.kdnuggets.com/2016/07/multi-task-learning-tensorflow-part-1.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/07/multi-task-learning-tensorflow-part-1.html](https://www.kdnuggets.com/2016/07/multi-task-learning-tensorflow-part-1.html)
- en: '**By Jonathan Godwin, University College London**.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**由乔纳森·戈德温，伦敦大学学院**。'
- en: A Jupyter notebook accompanies this blog post. Please download [here](https://github.com/jg8610/multi-task-part-1-notebook/tree/master).
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本博客文章附带一个Jupyter笔记本。请下载[这里](https://github.com/jg8610/multi-task-part-1-notebook/tree/master)。
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍
- en: '**Why Multi-Task Learning**'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么选择多任务学习**'
- en: When you think about the way people learn to do new things, they often use their
    experience and knowledge of the world to speed up the learning process. When I
    learn a new language, especially a related one, I use my knowledge of languages
    I already speak to make shortcuts. The process works the other way too - learning
    a new language can help you understand and speak your own better.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当你思考人们如何学习新事物时，他们通常会利用自己的经验和世界知识来加速学习过程。当我学习一门新语言，尤其是相关的语言时，我会利用我已经掌握的语言知识来快速学习。这个过程也反过来有效——学习一门新语言可以帮助你更好地理解和使用自己已知的语言。
- en: Our brains learn to do multiple different tasks at the same time - we have the
    same brain architecture whether we are translating English to German or English
    to French. If we were to use a Machine Learning algorithm to do both of these
    tasks, we might call that ‘multi-task’ learning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大脑能够同时学习多种不同的任务——无论是将英语翻译成德语还是法语，我们的大脑结构都是一样的。如果我们使用机器学习算法同时处理这两个任务，我们可能称之为“多任务”学习。
- en: It’s one of the most interesting and exciting areas of research for Machine
    Learning in coming years, radically reducing the amount of data required to learn
    new concepts. One of the great promises of Deep Learning is that, with the power
    of the models and simple ways to share parameters between tasks, we should be
    able to make significant progress in multi-task learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是未来几年机器学习中最有趣和令人兴奋的研究领域之一，它可以大幅减少学习新概念所需的数据量。深度学习的一个伟大承诺是，利用模型的力量和在任务之间共享参数的简单方法，我们应该能够在多任务学习方面取得显著进展。
- en: As I started to experiment in this area I came across a bit of a road block
    - while it was easy to understand the architecture changes required to implement
    multi-task learning, it was harder to figure out how to implement it in Tensorflow.
    To do anything but standard nets in Tensorflow requires a good understanding of
    how it works, but most of the stock examples don’t provide helpful guidance. I
    hope the following tutorial explains some key concepts simply, and helps those
    who are struggling.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当我开始在这个领域进行实验时，我遇到了一些困难——虽然理解实现多任务学习所需的架构变化很容易，但在Tensorflow中实现它则更为困难。在Tensorflow中进行非标准网络的操作需要对其工作原理有很好的理解，但大多数现成的示例并没有提供有用的指导。我希望下面的教程能够简单地解释一些关键概念，并帮助那些遇到困难的人。
- en: What We Are Going To Do
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们将要做的事情
- en: '**Part 1**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**第1部分**'
- en: '**Understand Tensorflow Computation Graphs With An Example**. Doing multi-task
    learning with Tensorflow requires understanding how computation graphs work -
    skip if you already know.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**通过示例理解Tensorflow计算图**。进行多任务学习需要了解计算图的工作原理——如果你已经了解这些，可以跳过。'
- en: '**Understand How We Can Use Graphs For Multi-Task Learning**. We’ll go through
    an example of how to adapt a simple graph to do Multi-Task Learning.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**了解如何使用图进行多任务学习**。我们将通过一个示例展示如何调整一个简单的图来进行多任务学习。'
- en: '**Part 2**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**第2部分**'
- en: '**Build A Graph for POS Tagging and Shallow Parsing**. We’ll fill in a template
    that trains a net for two related linguistic tasks. Don’t worry, you don’t need
    to know what they are!'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为词性标注和浅层解析构建图**。我们将填写一个模板，以训练一个用于两个相关语言任务的网络。别担心，你不需要知道它们是什么！'
- en: '**Train A Net Jointly and Separately**. We’ll actually train a model in two
    different ways. You should be able to do this on your laptop.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**联合和单独训练网络**。我们将实际以两种不同的方式训练模型。你应该能够在你的笔记本电脑上完成这项任务。'
- en: Understanding Computation Graphs With A Toy Example
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用玩具示例理解计算图
- en: The Computation Graph is the thing that makes Tensorflow (and other similar
    packages) fast. It’s an integral part of machinery of Deep Learning, but can be
    confusing.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图是使Tensorflow（以及其他类似软件包）快速的关键。它是深度学习机制的核心部分，但可能会令人困惑。
- en: There are some neat features of a graph that mean it’s very easy to conduct
    multi-task learning, but first we’ll keep things simple and explain the key concepts.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图具有一些巧妙的特性，使得进行多任务学习非常简单，但首先我们将保持简单，解释关键概念。
- en: '**Definition: Computation Graph**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义：计算图**'
- en: 'The **Computation Graph** is a **template** for computation (re: algorithm)
    you are going to run. It **doesn’t perform any calculations**, but it means that
    your computer can conduct backpropagation far more quickly.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算图**是你将要运行的计算（即算法）的**模板**。它**不执行任何计算**，但这意味着你的计算机可以更快地进行反向传播。'
- en: If you ask Tensorflow for a **result of a calculation** it will **only make
    those calculations required** for the job, **not the whole graph**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你询问 Tensorflow **计算的结果**，它会**仅进行完成工作的计算**，**而不是整个图**。
- en: 'A Toy Example - Linear Transformation: Setting Up The Graph'
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 玩具示例 - 线性变换：设置图
- en: 'We’re going to look at the graph for a simple calculation - a linear transformation
    of our inputs, and taking the square loss:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看一个简单计算的图——对输入进行线性变换，并计算平方损失：
- en: '![Toy Example](../Images/52c75dcdf2f4ae57c1cc42de0b09640c.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![玩具示例](../Images/52c75dcdf2f4ae57c1cc42de0b09640c.png)'
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'There are a few things to emphasis about this graph:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有几点需要强调关于这个图：
- en: '**If we were to run this code right now, we would get no output**. Remember
    that a Computation Graph is just a template - it doesn’t do anything. If we want
    an answer, we have to tell Tensorflow to run the computation using a **Session**.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**如果我们现在运行这段代码，我们不会得到任何输出**。记住，计算图只是一个模板——它不会做任何事情。如果我们想要一个答案，我们必须告诉 Tensorflow
    使用**会话**来运行计算。'
- en: '**We haven’t explicitly created a graph object**. You might expect that we
    would have to create a graph object somewhere in order for Tensorflow to know
    that we wanted to create a graph. In fact, by using the Tensorflow operations,
    we are telling Tensorflow what parts of our code are in the graph.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我们还没有明确创建图对象**。你可能会期望我们需要在某个地方创建一个图对象，以便 Tensorflow 知道我们想要创建一个图。事实上，通过使用
    Tensorflow 操作，我们在告诉 Tensorflow 我们代码中的哪些部分在图中。'
- en: '**Tip: Keep Your Graph Separate**. You’ll typically be doing a fair amount
    of data manipulation and computation outside of the graph, which means keeping
    track of what is and isn’t available inside of python a bit confusing. I like
    to put my graph in a separate file, and often in a separate class to keep concerns
    separated, but this isn’t required.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示：保持图的独立**。你通常会在图之外进行大量的数据操作和计算，这意味着在 python 内部跟踪什么是可用的可能会有些混乱。我喜欢把图放在一个单独的文件中，通常在一个单独的类中，以保持关注点的分离，但这不是必需的。'
- en: 'A Toy Example - Linear Transformation: Getting Results'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 玩具示例 - 线性变换：获取结果
- en: 'Computations on your Graph are conducted inside a Tensorflow **Session**. To
    get results from your session you need to provide it with two things: Target Results
    and Inputs.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图上的计算是在 Tensorflow **会话**内进行的。要从会话中获取结果，你需要提供两样东西：目标结果和输入。
- en: '**Target Results or Operations**. You tell Tensorflow what parts of the graph
    you want to return values for, and it will **automatically figure out what calculations
    within need to be run**. You can also call operations, for example, to initialise
    your variables.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**目标结果或操作**。你告诉 Tensorflow 你希望图的哪些部分返回值，它会**自动找出需要运行的计算**。你还可以调用操作，例如初始化你的变量。'
- en: '**Inputs As Required (‘Feed Dict’)**. In most calculations you will provide
    the input data ad-hoc. In this case, you construct the graph with a**placeholder** for
    this data, and feed it in at computation time. Not all calculations or operations
    will require an input - for many, all the information is already contained in
    the graph.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**按需输入（‘Feed Dict’）**。在大多数计算中，你会临时提供输入数据。在这种情况下，你会为这些数据构建一个带有**占位符**的图，并在计算时输入这些数据。并不是所有计算或操作都需要输入——对于许多计算，所有信息已包含在图中。'
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How To Use Graphs for Multi-Task Learning
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何使用图进行多任务学习
- en: When we create a Neural Net that performs multiple tasks we want to have some
    parts of the network that are shared, and other parts of the network that are
    specific to each individual task. When we’re training, we want information from
    each task to be transferred in the shared parts of the network.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建一个执行多个任务的神经网络时，我们希望有些网络部分是共享的，而其他部分则特定于每个任务。在训练时，我们希望将每个任务的信息转移到网络的共享部分。
- en: So, to start, let’s draw a diagram of a simple two-task network that has a shared
    layer and a specific layer for each individual task. We’re going to feed the outputs
    of this into our loss function with our targets. I’ve labelled where we’re going
    to want to create placeholders in the graph.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，首先，让我们绘制一个简单的两任务网络图，该网络具有一个共享层和每个任务的特定层。我们将把这些输出输入到我们的损失函数中与目标一起计算。我已经标记出我们需要在图中创建占位符的位置。
- en: '![Basic shared graph](../Images/8ae5537858dcd61fd6448f72993af879.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![基本共享图](../Images/8ae5537858dcd61fd6448f72993af879.png)'
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When we are training this network, **we want the parameters of the Task 1 layer
    to not change no matter how wrong we get Task 2**, but **the parameters of the
    shared layer to change with both tasks**. This might seem a little difficult -
    normally you only have one optimiser in a graph, because you only optimise one
    loss function. Thankfully, using the properties of the graph it’s very easy to
    train this sort of model in two ways.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练这个网络时，**我们希望任务 1 的层参数不变，无论任务 2 的结果有多么错误**，但**共享层的参数要随着两个任务而变化**。这可能看起来有点困难——通常你在一个图中只有一个优化器，因为你只优化一个损失函数。幸运的是，利用图的属性，以两种方式训练这种模型是非常简单的。
- en: '* * *'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[TensorFlow for Computer Vision - Transfer Learning Made Easy](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow 在计算机视觉中的应用 - 转移学习变得简单](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
- en: '[PyTorch or TensorFlow? Comparing popular Machine Learning frameworks](https://www.kdnuggets.com/2022/02/packt-pytorch-tensorflow-comparing-popular-machine-learning-frameworks.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 还是 TensorFlow？比较流行的机器学习框架](https://www.kdnuggets.com/2022/02/packt-pytorch-tensorflow-comparing-popular-machine-learning-frameworks.html)'
- en: '[The "Hello World" of Tensorflow](https://www.kdnuggets.com/2022/05/hello-world-tensorflow.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tensorflow 的“Hello World”](https://www.kdnuggets.com/2022/05/hello-world-tensorflow.html)'
- en: '[A Guide to Train an Image Classification Model Using Tensorflow](https://www.kdnuggets.com/2022/12/guide-train-image-classification-model-tensorflow.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Tensorflow 训练图像分类模型指南](https://www.kdnuggets.com/2022/12/guide-train-image-classification-model-tensorflow.html)'
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 TensorFlow 和 Keras 构建和训练你的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
- en: '[Free TensorFlow 2.0 Complete Course](https://www.kdnuggets.com/2023/02/free-tensorflow-20-complete-course.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[免费 TensorFlow 2.0 完整课程](https://www.kdnuggets.com/2023/02/free-tensorflow-20-complete-course.html)'
