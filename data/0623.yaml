- en: A Beginner’s Guide to the CLIP Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CLIP 模型初学者指南
- en: 原文：[https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html](https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html](https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Matthew Brems](https://www.linkedin.com/in/matthewbrems/), Growth Manager
    @ Roboflow**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者 [Matthew Brems](https://www.linkedin.com/in/matthewbrems/)，Roboflow 的增长经理**'
- en: You may have heard about [OpenAI's CLIP model](https://openai.com/blog/clip/).
    If you looked it up, you read that CLIP stands for "Contrastive Language-Image
    Pre-training." That doesn't immediately make much sense to me, so [I read the
    paper](https://arxiv.org/pdf/2103.00020.pdf) where they develop the CLIP model
    – [and the corresponding blog post](https://openai.com/blog/clip/).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能听说过 [OpenAI 的 CLIP 模型](https://openai.com/blog/clip/)。如果你查阅过，你会看到 CLIP 代表“对比语言-图像预训练”。这对我来说并不立即明白，所以我
    [阅读了论文](https://arxiv.org/pdf/2103.00020.pdf) 以及 [相应的博客文章](https://openai.com/blog/clip/)。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT 工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'I''m here to break CLIP down for you in a – hopefully – accessible and fun
    read! In this post, I''ll cover:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里是为了将 CLIP 进行详细讲解，期望让你能轻松有趣地阅读！在这篇文章中，我将涵盖：
- en: what CLIP is,
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLIP 是什么，
- en: how CLIP works, and
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLIP 的工作原理，以及
- en: why CLIP is cool.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLIP 的酷炫之处。
- en: What is CLIP?
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是 CLIP？
- en: CLIP is the first multimodal (in this case, vision and text) model tackling
    computer vision and was recently released [by OpenAI](https://openai.com/blog/clip/) on
    January 5, 2021\. From the [OpenAI CLIP repository](https://github.com/openai/CLIP),
    "CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on
    a variety of (image, text) pairs. It can be instructed in natural language to
    predict the most relevant text snippet, given an image, without directly optimizing
    for the task, similarly to the zero-shot capabilities of GPT-2 and 3."
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 是第一个多模态（在此案例中为视觉和文本）模型，旨在解决计算机视觉问题，最近由 [OpenAI](https://openai.com/blog/clip/)
    于 2021 年 1 月 5 日发布。从 [OpenAI CLIP 仓库](https://github.com/openai/CLIP) 中可以看到，“CLIP（对比语言-图像预训练）是一个在多种（图像，文本）对上进行训练的神经网络。它可以用自然语言指令预测最相关的文本片段，而不直接针对任务进行优化，类似于
    GPT-2 和 GPT-3 的零样本能力。”
- en: Depending on your background, this *may* make sense -- but there's a lot in
    here that may be unfamiliar to you. Let's unpack it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的背景，这 *可能* 有意义——但其中有很多内容可能对你来说比较陌生。我们来解读一下。
- en: CLIP is a neural network model.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLIP 是一个神经网络模型。
- en: It is trained on 400,000,000 (image, text) pairs. An (image, text) pair might
    be a picture and its caption. So this means that there are 400,000,000 pictures
    and their captions that are matched up, and this is the data that is used in training
    the CLIP model.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在 400,000,000 对（图像，文本）数据上进行训练。一个（图像，文本）对可能是一张图片及其标题。也就是说，有 400,000,000 张图片及其标题进行了匹配，这些数据用于训练
    CLIP 模型。
- en: '*"It can predict the most relevant text snippet, given an image."* You can
    input an image into the CLIP model, and it will return for you the likeliest caption
    or summary of that image.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*“它可以根据给定的图像预测最相关的文本片段。”* 你可以将图像输入到 CLIP 模型中，它会为你返回该图像最可能的标题或摘要。'
- en: '*"without directly optimizing for the task, similarly to the zero-shot capabilities
    of GPT-2 and 3."* Most machine learning models learn a specific task. For example,
    an image classifier trained on classifying dogs and cats is expected to do well
    on the task we''ve given it: classifying dogs and cats. We generally would not
    expect a machine learning model trained on dogs and cats to be very good at detecting
    raccoons. However, some models -- including CLIP, GPT-2, and GPT-3 -- tend to
    perform well on tasks they aren''t directly trained to do, which is called "zero-shot
    learning."'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*“不直接优化任务，类似于GPT-2和GPT-3的零样本能力。”* 大多数机器学习模型学习特定任务。例如，一个训练用于分类狗和猫的图像分类器，期望在我们给定的任务上表现良好：分类狗和猫。我们通常不会期望一个训练过狗和猫的机器学习模型在检测浣熊方面表现得很好。然而，一些模型——包括CLIP、GPT-2和GPT-3——在未直接训练的任务上往往表现出色，这被称为“零样本学习”。'
- en: '"Zero-shot learning" is when a model attempts to predict a class it saw zero
    times in the training data. So, using a model trained on exclusively cats and
    dogs to then detect raccoons. A model like CLIP, because of how it uses the text
    information in the (image, text) pairs, tends to do really well with zero-shot
    learning -- even if the image you''re looking at is really different from the
    training images, your CLIP model will likely be able to give a good guess for
    the caption for that image.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “零样本学习”是指模型尝试预测在训练数据中从未见过的类别。因此，例如，使用一个只在猫和狗上训练过的模型来检测浣熊。像CLIP这样的模型，由于它在（图像，文本）对中利用文本信息的方式，通常在零样本学习方面表现得非常出色——即使你看到的图像与训练图像非常不同，你的CLIP模型仍然能够为该图像提供一个不错的标题预测。
- en: 'To put this all together, the CLIP model is:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 综合来看，CLIP模型是：
- en: a neural network model built on hundreds of millions of images and captions,
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个建立在数亿张图像和标题上的神经网络模型，
- en: can return the best caption given an image, and
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在给定图像的情况下返回最佳标题，并且
- en: has impressive "zero-shot" capabilities, making it able to accurately predict
    entire classes it's never seen before!
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有令人印象深刻的“零样本”能力，使其能够准确预测它从未见过的整个类别！
- en: When I wrote my [Introduction to Computer Vision](https://blog.roboflow.com/intro-to-computer-vision/) post,
    I described computer vision as "the ability for a computer to see and understand
    what it sees in manner similar to humans."
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当我写我的[计算机视觉简介](https://blog.roboflow.com/intro-to-computer-vision/)文章时，我将计算机视觉描述为“计算机以类似于人类的方式看到并理解它所见的能力。”
- en: '[When I''ve taught natural language processing](https://github.com/matthewbrems/nlp-fundamentals-python),
    I described NLP in a similar way: "the ability for a computer to understand language
    in manner similar to humans."'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[当我教授自然语言处理时](https://github.com/matthewbrems/nlp-fundamentals-python)，我以类似的方式描述了NLP：“计算机以类似于人类的方式理解语言的能力。”'
- en: '**CLIP is a bridge between computer vision and natural language processing.**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**CLIP是计算机视觉和自然语言处理之间的桥梁。**'
- en: It's not *just* a bridge between computer vision and natural language processing
    -- it's a **very powerful bridge** between the two that has a lot of flexibility
    and a lot of applications.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 它不仅仅是计算机视觉和自然语言处理之间的桥梁——它是一个**非常强大的桥梁**，在这两者之间具有很大的灵活性和应用潜力。
- en: How does CLIP work?
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CLIP是如何工作的？
- en: 'In order for images and text to be connected to one another, they must both
    be [embedded](https://en.wikipedia.org/wiki/Embedding). You''ve worked with embeddings
    before, even if you haven''t thought of it that way. Let''s go through an example.
    Suppose you have one cat and two dogs. You could represent that as a dot on a
    graph, like below:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使图像和文本彼此关联，它们必须都被[嵌入](https://en.wikipedia.org/wiki/Embedding)。即使你没有这样考虑过，你也曾经使用过嵌入。我们来举一个例子。假设你有一只猫和两只狗。你可以将其表示为图表上的一个点，如下所示：
- en: '![](../Images/da688b366c7e92a9420424b780e0ca7a.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da688b366c7e92a9420424b780e0ca7a.png)'
- en: Embedding of "1 cat, 2 dogs." ([Source](https://www.wolframalpha.com/input/?i=%281%2C2%29).)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: “1只猫，2只狗”的嵌入。 ([Source](https://www.wolframalpha.com/input/?i=%281%2C2%29).)
- en: It may not seem very exciting, but we just embedded that information on the
    X-Y grid that you probably learned about in middle school (formally called [Euclidean
    space](https://en.wikipedia.org/wiki/Euclidean_space)). You could also embed your
    friends' pet information on the same graph and/or you could have chosen plenty
    of different ways to represent that information (e.g. put dogs before cats, or
    add a third dimension for raccoons).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来不那么令人兴奋，但我们刚刚将信息嵌入到你可能在初中学到的X-Y坐标系中（正式称为[欧几里得空间](https://en.wikipedia.org/wiki/Euclidean_space)）。你也可以将朋友的宠物信息嵌入到同一个图表中，或者你可以选择多种不同的方法来表示这些信息（例如，将狗放在猫之前，或为浣熊添加第三维度）。
- en: I like to think of embedding as a way to smash information into mathematical
    space. We just took information about dogs and cats and smashed it into mathematical
    space. ***We can do the same thing with text and with images!***
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢把嵌入看作是将信息压缩到数学空间中的一种方式。我们刚刚把关于狗和猫的信息压缩到数学空间中。***我们可以用同样的方法处理文本和图像！***
- en: '**The CLIP model consists of two sub-models called encoders:**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**CLIP模型由两个子模型组成，称为编码器：**'
- en: a text encoder that will embed (smash) text into mathematical space.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个文本编码器将把文本嵌入（压缩）到数学空间中。
- en: an image encoder that will embed (smash) images into mathematical space.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个图像编码器将把图像嵌入（压缩）到数学空间中。
- en: Whenever you fit a [supervised learning model](https://towardsdatascience.com/a-brief-introduction-to-supervised-learning-54a3e3932590),
    you have to find some way to measure the "goodness" or the "badness" of that model
    – the goal is to fit a model that is as "most good" and "least bad" as possible.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你训练一个[监督学习模型](https://towardsdatascience.com/a-brief-introduction-to-supervised-learning-54a3e3932590)时，你必须找到某种方法来衡量该模型的“好坏”——目标是训练一个“最好”和“最少坏”的模型。
- en: 'The CLIP model is no different: the text encoder and image encoder are fit
    to maximize goodness and minimize badness.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP模型也不例外：文本编码器和图像编码器旨在最大化好处并最小化坏处。
- en: '**So, how do we measure "goodness" and "badness?"**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**那么，我们如何衡量“好坏”？**'
- en: In the image below, you'll see a set of purple text cards going into the text
    encoder. The output for each card would be a series of numbers. For example, the
    top card, `pepper the aussie pup` would enter the text encoder – the thing smashing
    it into mathematical space – and come out as a series of numbers like (0, 0.2,
    0.8).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图像中，你会看到一组紫色文本卡片输入到文本编码器中。每张卡片的输出将是一系列数字。例如，最上面的卡片，`pepper the aussie pup`，将进入文本编码器——即将其压缩到数学空间中的东西——并输出一系列数字，如(0,
    0.2, 0.8)。
- en: 'The exact same thing will happen for the images: each image will go into the
    image encoder and the output for each image will also be a series of numbers.
    The picture of, presumably Pepper the Aussie pup, will come out like (0.05, 0.25,
    0.7).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图像将会发生完全相同的情况：每张图像都会输入到图像编码器中，每张图像的输出也将是一系列数字。比如，假设是澳大利亚小狗Pepper的图片，输出可能是(0.05,
    0.25, 0.7)。
- en: '![](../Images/e6be4d3d47d9e227f3975927e4c436de.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6be4d3d47d9e227f3975927e4c436de.png)'
- en: The pre-training phase. ([Source](https://openai.com/blog/clip/).)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练阶段。（[来源](https://openai.com/blog/clip/)）
- en: '**"Goodness" of our model**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们模型的“好处”**'
- en: 'In an ideal world, the series of numbers for the text "pepper the aussie pup"
    will be very close (identical) to the series of numbers for the corresponding
    image. *In fact, this should be the case everywhere*: the series of numbers for
    the text should be very close to the series of numbers for the corresponding image.
    One way for us to measure "goodness" of our model is how close the embedded representation
    (series of numbers) for each text is to the embedded representation for each image.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的情况下，文本“pepper the aussie pup”的数字序列应该非常接近（相同于）对应图像的数字序列。*事实上，这应该是普遍适用的*：文本的数字序列应与对应图像的数字序列非常接近。衡量我们模型的“好处”的一种方法是每个文本的嵌入表示（数字序列）与每个图像的嵌入表示之间的接近程度。
- en: There is a convenient way to calculate the similarity between two series of
    numbers: [the cosine similarity](https://www.machinelearningplus.com/nlp/cosine-similarity/).
    We won't get into the inner workings of that formula here, but rest assured that
    it's a tried and true method of seeing how similar two vectors, or series of numbers,
    are. (Though it isn't the only way!)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种方便的方法来计算两个数字序列之间的相似性：[余弦相似性](https://www.machinelearningplus.com/nlp/cosine-similarity/)。我们在这里不会深入探讨这个公式的内部原理，但可以放心，它是一种经过验证的方法，用于查看两个向量或数字序列之间的相似度。（尽管这不是唯一的方法！）
- en: In the image above, the light blue squares represent where the text and image
    coincide. For example, T1 is the embedded representation of the first text; I1
    is the embedded representation of the first image. We want the cosine similarity
    for I1 and T1 to be as high as possible. We want the same for I2 and T2, and so
    on for all of the light blue squares. **The higher these cosine similarities are,
    the more "goodness" our model has!**
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图像中，浅蓝色方块表示文本和图像的重合位置。例如，T1 是第一个文本的嵌入表示；I1 是第一个图像的嵌入表示。我们希望 I1 和 T1 的余弦相似度尽可能高。对
    I2 和 T2 也希望如此，其他浅蓝色方块也是如此。**这些余弦相似度越高，我们的模型就越“好ness”！**
- en: '**"Badness" of our model**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们模型的“坏ness”**'
- en: At the same time as wanting to maximize the cosine similarity for each of those
    blue squares, there are a lot of grey squares that indicate where the text and
    image don't align. For example, T1 is the text "pepper the aussie pup" but perhaps
    I2 is [an image of a raccoon](https://public.roboflow.com/object-detection/raccoon).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在最大化每一个蓝色方块的余弦相似度的同时，也有很多灰色方块显示文本和图像的不对齐。例如，T1 是文本“pepper the aussie pup”，但也许
    I2 是 [浣熊的图片](https://public.roboflow.com/object-detection/raccoon)。
- en: '![](../Images/733d91cf5addc3335394fbde3eff9d2e.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/733d91cf5addc3335394fbde3eff9d2e.png)'
- en: Picture of a raccoon with bounding box annotation. ([Source](https://public.roboflow.com/object-detection/raccoon).)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 带有边界框注释的浣熊图片。 ([来源](https://public.roboflow.com/object-detection/raccoon).)
- en: Cute though this raccoon is, we want the cosine similarity between this image
    (I2) and the text `pepper the aussie pup` to be pretty small, because this isn't
    Pepper the Aussie pup!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个浣熊很可爱，我们希望这张图像 (I2) 和文本 `pepper the aussie pup` 之间的余弦相似度相当小，因为这不是 Pepper
    the Aussie pup！
- en: While we wanted the blue squares to all have high cosine similarities (as that
    measured "goodness"), we want all of the grey squares to have low cosine similarities,
    because that measures "badness."
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们希望蓝色方块具有高余弦相似度（因为这测量了“好ness”），我们希望所有灰色方块的余弦相似度都很低，因为这测量了“坏ness”。
- en: '![](../Images/5daa99f9af00dfc14023717b1561fad6.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5daa99f9af00dfc14023717b1561fad6.png)'
- en: Maximize cosine similarity of the blue squares; minimize cosine similarity of
    the grey squares. ([Source](https://openai.com/blog/clip/).)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化蓝色方块的余弦相似度；最小化灰色方块的余弦相似度。 ([来源](https://openai.com/blog/clip/).)
- en: '**How do the text and image encoders get fit?**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本和图像编码器如何适配？**'
- en: The text encoder and image encoder get fit at the same time by simultaneously
    maximizing the cosine similarity of those blue squares and minimizing the cosine
    similarity of the grey squares, across all of our text+image pairs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 文本编码器和图像编码器同时通过最大化这些蓝色方块的余弦相似度和最小化灰色方块的余弦相似度来进行适配，这涵盖了我们所有的文本+图像对。
- en: 'Note: this can take a very long time depending on the size of your data. The
    CLIP model trained on 400,000,000 labeled images. The training process took 30
    days across [592 V100 GPUs](https://www.nvidia.com/en-us/data-center/v100/). This
    would have cost $1,000,000 to train on AWS on-demand instances!'
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：根据数据的大小，这可能需要很长时间。CLIP 模型在 400,000,000 张标记图像上进行了训练。训练过程在 [592 个 V100 GPU](https://www.nvidia.com/en-us/data-center/v100/)
    上耗时 30 天。这在 AWS 按需实例上训练将花费 1,000,000 美元！
- en: Once the model is fit, you can pass an image into the image encoder to retrieve
    the text description that best fits the image – or, vice versa, you can pass a
    text description into the model to retrieve an image, as you'll see in some of
    the applications below!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，你可以将图像传递给图像编码器，以检索最符合图像的文本描述——或者，反之，你可以将文本描述传递给模型以检索图像，如下文中的一些应用所示！
- en: '**CLIP is a bridge between computer vision and natural language processing.**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**CLIP 是计算机视觉和自然语言处理之间的桥梁。**'
- en: Why is CLIP cool?
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么 CLIP 很酷？
- en: 'With this bridge between computer vision and natural language processing, CLIP
    has a ton of advantages and cool applications. We''ll focus on the applications,
    but a few advantages to call out:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过建立计算机视觉和自然语言处理之间的桥梁，CLIP 具有许多优势和酷炫的应用。我们将重点关注这些应用，但也有几个优势需要提及：
- en: 'Generalizability: Models are usually super brittle, capable of knowing only
    the very specific thing you trained them to do. CLIP expands knowledge of classification
    models to a wider array of things by leveraging semantic information in text.
    Standard classification models completely discard the semantic meaning of the
    class labels and simply  enumerated numeric classes behind the scenes; CLIP works
    by understanding the meaning of the classes.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泛化能力：模型通常非常脆弱，只能知道你训练它做的非常具体的事情。CLIP 通过利用文本中的语义信息，扩展了分类模型对更多事物的知识。标准分类模型完全忽略了类别标签的语义含义，只是在幕后枚举数字类别；CLIP
    通过理解类别的含义来工作。
- en: 'Connecting text / images better than ever before: CLIP may quite literally
    be the "world''s best caption writer" when considering speed and accuracy together.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比以往更好地连接文本和图像：考虑到速度和准确性，CLIP 可以说是“世界上最好的标题撰写者”。
- en: 'Already-labeled data: CLIP is built on images and captions that were already
    created; other state-of-the-art computer vision algorithms required significant
    additional human time spent labeling.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已标注的数据：CLIP 基于已创建的图像和标题；而其他最先进的计算机视觉算法需要大量额外的人工时间来标注。
- en: Why does [@OpenAI](https://twitter.com/OpenAI?ref_src=twsrc%5Etfw)'s CLIP model
    matter?[https://t.co/X7bnSgZ0or](https://t.co/X7bnSgZ0or)
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么 [@OpenAI](https://twitter.com/OpenAI?ref_src=twsrc%5Etfw) 的 CLIP 模型很重要？[https://t.co/X7bnSgZ0or](https://t.co/X7bnSgZ0or)
- en: ''
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Joseph Nelson (@josephofiowa) [January 6, 2021](https://twitter.com/josephofiowa/status/1346639942571712521?ref_src=twsrc%5Etfw)
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: — Joseph Nelson (@josephofiowa) [2021年1月6日](https://twitter.com/josephofiowa/status/1346639942571712521?ref_src=twsrc%5Etfw)
- en: 'Some of the uses of CLIP so far:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止 CLIP 的一些用途：
- en: '[CLIP has been used to index photos on sites like Unsplash](https://twitter.com/metasemantic/status/1349446585952989186?s=20).'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CLIP 已被用于对 Unsplash 上的照片进行索引](https://twitter.com/metasemantic/status/1349446585952989186?s=20)。'
- en: One Twitter user took celebrities including Elvis Presley, Beyoncé, and Billie
    Eilish, [and used CLIP and StyleGAN to generate portraits in the style of "My
    Little Pony."](https://twitter.com/metasemantic/status/1368713208429764616)
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一位推特用户使用 CLIP 和 StyleGAN 将名人（包括 Elvis Presley、Beyoncé 和 Billie Eilish）[生成了“我的小马驹”风格的肖像](https://twitter.com/metasemantic/status/1368713208429764616)。
- en: Have you played Pictionary? Now you can play online at [paint.wtf, where you'll
    be judged by CLIP](https://paint.wtf/).
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你玩过 Pictionary 吗？现在你可以在 [paint.wtf, where you'll be judged by CLIP](https://paint.wtf/)
    在线玩这个游戏。
- en: CLIP could be used to easily improve NSFW filters!
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLIP 可能被用来轻松改善 NSFW 过滤器！
- en: '[Find photos matching a mood – for example, via a poetry passage](https://twitter.com/metasemantic/status/1349446585952989186).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[找到匹配某种情绪的照片——例如，通过一段诗歌](https://twitter.com/metasemantic/status/1349446585952989186)。'
- en: OpenAI has also created [DALL-E, which creates images from text](https://openai.com/blog/dall-e/).
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 还创建了 [DALL-E, which creates images from text](https://openai.com/blog/dall-e/)。
- en: We hope you'll check out some of the above – or create your own! We've got a [CLIP
    tutorial](https://blog.roboflow.com/how-to-use-openai-clip/) for you to follow.
    If you do something with it, [let us know so we can add it to the above list](https://roboflow.com/contact)!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望你能查看以上内容，或创建你自己的项目！我们为你准备了一个 [CLIP 教程](https://blog.roboflow.com/how-to-use-openai-clip/)。如果你使用了它，[请告诉我们，以便我们将其添加到以上列表中](https://roboflow.com/contact)！
- en: It's important to note that CLIP is *a* bridge between computer vision and natural
    language processing. CLIP is not the only bridge between them. You could build
    those text and image encoders very differently or find other ways of connecting
    the two. **However, CLIP has so far been an exceptionally innovative technique
    that has promoted significant additional innovation.**
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，CLIP 是计算机视觉和自然语言处理之间的 *一个* 桥梁。CLIP 并不是它们之间唯一的桥梁。你可以用非常不同的方式构建这些文本和图像编码器，或者找到其他连接两者的方法。**然而，CLIP
    迄今为止是一种极具创新性的技术，推动了显著的额外创新。**
- en: We're eager to see what you build with CLIP and to see the advancements that
    are built on top of it!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们期待看到你用 CLIP 构建的作品，以及在其基础上取得的进展！
- en: '**Bio: [Matthew Brems](https://www.linkedin.com/in/matthewbrems/)** is Growth
    Manager @ Roboflow.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Matthew Brems](https://www.linkedin.com/in/matthewbrems/)** 是 Roboflow
    的增长经理。'
- en: '[Original](https://blog.roboflow.com/clip-model-eli5-beginner-guide/). Reposted
    with permission.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://blog.roboflow.com/clip-model-eli5-beginner-guide/)。经许可转载。'
- en: '**Related:**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[OpenAI Releases Two Transformer Models that Magically Link Language and Computer
    Vision](/2021/01/openai-transformer-models-link-language-computer-vision.html)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI 发布了两种 Transformer 模型，将语言和计算机视觉神奇地连接起来](/2021/01/openai-transformer-models-link-language-computer-vision.html)'
- en: '[Evaluating Object Detection Models Using Mean Average Precision](/2021/03/evaluating-object-detection-models-using-mean-average-precision.html)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用均值平均精度评估对象检测模型](/2021/03/evaluating-object-detection-models-using-mean-average-precision.html)'
- en: '[Reducing the High Cost of Training NLP Models With SRU++](/2021/03/reducing-high-cost-training-nlp-models-sru.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过 SRU++ 降低训练 NLP 模型的高成本](/2021/03/reducing-high-cost-training-nlp-models-sru.html)'
- en: More On This Topic
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[A Beginner''s Guide to End to End Machine Learning](https://www.kdnuggets.com/2021/12/beginner-guide-end-end-machine-learning.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[端到端机器学习的初学者指南](https://www.kdnuggets.com/2021/12/beginner-guide-end-end-machine-learning.html)'
- en: '[Essential Machine Learning Algorithms: A Beginner''s Guide](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[必备机器学习算法：初学者指南](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)'
- en: '[A Beginner''s Guide to Q Learning](https://www.kdnuggets.com/2022/06/beginner-guide-q-learning.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Q 学习的初学者指南](https://www.kdnuggets.com/2022/06/beginner-guide-q-learning.html)'
- en: '[A Beginner’s Guide to Web Scraping Using Python](https://www.kdnuggets.com/2022/10/beginner-guide-web-scraping-python.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 的网页抓取初学者指南](https://www.kdnuggets.com/2022/10/beginner-guide-web-scraping-python.html)'
- en: '[Beginner''s Guide to Cloud Computing](https://www.kdnuggets.com/2023/01/beginner-guide-cloud-computing.html)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[云计算初学者指南](https://www.kdnuggets.com/2023/01/beginner-guide-cloud-computing.html)'
- en: '[A Beginner''s Guide to Anomaly Detection Techniques in Data Science](https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中异常检测技术的初学者指南](https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html)'
