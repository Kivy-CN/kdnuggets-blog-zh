- en: 'Overview of PEFT: State-of-the-art Parameter-Efficient Fine-Tuning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PEFT 概述：最先进的参数高效微调
- en: 原文：[https://www.kdnuggets.com/overview-of-peft-stateoftheart-parameterefficient-finetuning](https://www.kdnuggets.com/overview-of-peft-stateoftheart-parameterefficient-finetuning)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/overview-of-peft-stateoftheart-parameterefficient-finetuning](https://www.kdnuggets.com/overview-of-peft-stateoftheart-parameterefficient-finetuning)
- en: '![Overview of PEFT: State-of-the-art Parameter-Efficient Fine-Tuning](../Images/aff0878bf10b35d37fe07ff658443928.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![PEFT 概述：最先进的参数高效微调](../Images/aff0878bf10b35d37fe07ff658443928.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: What is PEFT
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 PEFT
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: As large language models (LLMs) such as GPT-3.5, LLaMA2, and PaLM2 grow ever
    larger in scale, fine-tuning them on downstream natural language processing (NLP)
    tasks becomes increasingly computationally expensive and memory intensive.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 GPT-3.5、LLaMA2 和 PaLM2 等大型语言模型（LLMs）规模的不断扩大，在下游自然语言处理（NLP）任务中对它们进行微调变得越来越计算密集且内存占用巨大。
- en: Parameter-Efficient Fine-Tuning (PEFT) methods address these issues by only
    fine-tuning a small number of extra parameters while freezing most of the pretrained
    model. This prevents catastrophic forgetting in large models and enables fine-tuning
    with limited compute.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调（PEFT）方法通过仅微调少量额外参数，同时冻结大部分预训练模型，来解决这些问题。这可以防止大型模型出现灾难性遗忘，并且使得在有限计算资源下也能进行微调。
- en: PEFT has proven effective for tasks like image classification and text generation
    while using just a fraction of the parameters. The small tuned weights can simply
    be added to the original pretrained weights.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT 已被证明在图像分类和文本生成等任务中有效，同时只使用了参数的一小部分。小的调整权重可以简单地添加到原始预训练权重中。
- en: You can even fine tune LLMs on the free version of Google Colab using 4-bit
    quantization and PEFT techniques QLoRA.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以在 Google Colab 的免费版本上使用 4 位量化和 PEFT 技术 QLoRA 来微调 LLMs。
- en: The modular nature of PEFT also allows the same pretrained model to be adapted
    for multiple tasks by adding small task-specific weights, avoiding the need to
    store full copies.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT 的模块化特性还允许通过添加少量任务特定的权重来将相同的预训练模型适应于多个任务，从而避免存储完整的副本。
- en: The [PEFT](https://github.com/huggingface/peft) library integrates popular PEFT
    techniques like LoRA, Prefix Tuning, AdaLoRA, Prompt Tuning, MultiTask Prompt
    Tuning, and LoHa with Transformers and Accelerate. This provides easy access to
    cutting-edge large language models with efficient and scalable fine-tuning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[PEFT](https://github.com/huggingface/peft) 库集成了流行的 PEFT 技术，如 LoRA、前缀调整、AdaLoRA、提示调整、多任务提示调整和
    LoHa，支持 Transformers 和 Accelerate。这提供了对前沿大型语言模型的简便访问，同时实现高效和可扩展的微调。'
- en: What is LoRA
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 LoRA
- en: In this tutorial, we will be using the most popular parameter-efficient fine-tuning
    (PEFT) technique called [LoRA](https://arxiv.org/abs/2106.09685) (Low-Rank Adaptation
    of Large Language Models). LoRA is a technique that significantly speeds up the
    fine-tuning process of large language models while consuming less memory.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用最流行的参数高效微调（PEFT）技术，称为 [LoRA](https://arxiv.org/abs/2106.09685)（大型语言模型的低秩适应）。LoRA
    是一种显著加快大型语言模型微调过程，同时消耗更少内存的技术。
- en: The key idea behind LoRA is to represent weight updates using two smaller matrices
    achieved through low-rank decomposition. These matrices can be trained to adapt
    to new data while minimizing the overall number of modifications. The original
    weight matrix remains unchanged and doesn't undergo any further adjustments. The
    final results are obtained by combining both the original and the adapted weights.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 的关键思想是通过低秩分解使用两个较小的矩阵表示权重更新。这些矩阵可以训练以适应新数据，同时最小化总体修改数量。原始权重矩阵保持不变，不会再进行任何调整。最终结果是通过结合原始权重和适应后的权重得到的。
- en: There are several advantages to using LoRA. Firstly, it greatly enhances the
    efficiency of fine-tuning by reducing the number of trainable parameters. Additionally,
    LoRA is compatible with various other parameter-efficient methods and can be combined
    with them. Models fine-tuned using LoRA demonstrate performance comparable to
    fully fine-tuned models.  Importantly, LoRA doesn't introduce any additional inference
    latency since adapter weights can be seamlessly merged with the base model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LoRA 有几个优势。首先，它通过减少可训练参数的数量大大提高了微调的效率。此外，LoRA 与各种其他高效参数方法兼容，并可以与它们结合使用。使用
    LoRA 微调的模型表现出与完全微调模型相当的性能。重要的是，LoRA 不会引入额外的推理延迟，因为适配器权重可以与基础模型无缝合并。
- en: Use Cases
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用案例
- en: There are many use cases of PEFT, from language models to Image classifiers.
    You can check all of the use case tutorials on official [documentation](https://huggingface.co/docs/peft/task_guides/image_classification_lora).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: PEFT 有许多使用案例，从语言模型到图像分类器。你可以在官方 [文档](https://huggingface.co/docs/peft/task_guides/image_classification_lora)
    中查看所有使用案例教程。
- en: '[StackLLaMA: A hands-on guide to train LLaMA with RLHF](https://huggingface.co/blog/stackllama)'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[StackLLaMA: 训练 LLaMA 的实用指南](https://huggingface.co/blog/stackllama)'
- en: '[Finetune-opt-bnb-peft](https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing)'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Finetune-opt-bnb-peft](https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing)'
- en: '[Efficient flan-t5-xxl training with LoRA and Hugging Face](https://www.philschmid.de/fine-tune-flan-t5-peft)'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[使用 LoRA 和 Hugging Face 高效训练 flan-t5-xxl](https://www.philschmid.de/fine-tune-flan-t5-peft)'
- en: '[DreamBooth fine-tuning with LoRA](https://huggingface.co/docs/peft/task_guides/dreambooth_lora)'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[使用 LoRA 进行 DreamBooth 微调](https://huggingface.co/docs/peft/task_guides/dreambooth_lora)'
- en: '[Image classification using LoRA](https://huggingface.co/docs/peft/task_guides/image_classification_lora)'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[使用 LoRA 的图像分类](https://huggingface.co/docs/peft/task_guides/image_classification_lora)'
- en: Training the LLMs using PEFT
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PEFT 训练 LLMs
- en: In this section, we will learn how to load and wrap our transformer model using
    the `bitsandbytes` and `peft` library. We will also cover loading the saved fine-tuned
    QLoRA model and running inferences with it.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用 `bitsandbytes` 和 `peft` 库加载和包装我们的 transformer 模型。我们还将涵盖加载已保存的微调
    QLoRA 模型并使用它进行推理。
- en: Getting Started
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 入门
- en: First, we will install all the necessary libraries.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将安装所有必要的库。
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then, we will import the essential modules and name the base model ([Llama-2-7b-chat-hf](https://huggingface.co/NousResearch/Llama-2-7b-chat-hf))
    to fine-tune it using the [mlabonne/guanaco-llama2-1k](https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k)
    dataset.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将导入必要的模块，并命名基础模型（[Llama-2-7b-chat-hf](https://huggingface.co/NousResearch/Llama-2-7b-chat-hf)），以使用
    [mlabonne/guanaco-llama2-1k](https://huggingface.co/datasets/mlabonne/guanaco-llama2-1k)
    数据集对其进行微调。
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: PEFT Configuration
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PEFT 配置
- en: Create PEFT configuration that we will use to wrap or train our model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 PEFT 配置，我们将用它来包装或训练我们的模型。
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 4 bit Quantization
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 位量化
- en: Loading LLMs on consumer or Colab GPUs poses significant challenges. However,
    we can overcome this issue by implementing a 4-bit quantization technique with
    an NF4 type configuration using BitsAndBytes. By employing this approach, we can
    effectively load our model, thereby conserving memory and preventing machine crashes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在消费者或 Colab GPU 上加载 LLMs 面临重大挑战。然而，我们可以通过使用 BitsAndBytes 实施 4 位量化技术和 NF4 类型配置来克服这个问题。通过这种方法，我们可以有效地加载模型，从而节省内存并防止机器崩溃。
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Wrapping Base Transformers Model
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 包装基础 Transformers 模型
- en: To make our model parameter efficient, we will wrap the base transformer model
    using `get_peft_model`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的模型在参数上更高效，我们将使用 `get_peft_model` 来包装基础 transformer 模型。
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Our trainable parameters are fewer than those of the base model, allowing us
    to use less memory and fine-tune the model faster.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的可训练参数少于基础模型的参数，这使我们能够使用更少的内存并更快地微调模型。
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The next step is to train the model. You can do that by following the [4-bit
    quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
    guide.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是训练模型。你可以按照 [4 位量化和 QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
    指南进行操作。
- en: Saving the Model
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存模型
- en: After training, you can either save the model adopter locally.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，你可以选择将模型适配器保存到本地。
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Or, push it to the Hugging Face Hub.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，将其推送到 Hugging Face Hub。
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As we can see, the model adopter is just 134MB whereas the base LLaMA 2 7B model
    is around 13GB.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，模型适配器仅为 134MB，而基础的 LLaMA 2 7B 模型约为 13GB。
- en: '![Overview of PEFT: State-of-the-art Parameter-Efficient Fine-Tuning](../Images/980ded1e7c785ce7884c611929a51e7e.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![PEFT 概述：最先进的参数高效微调](../Images/980ded1e7c785ce7884c611929a51e7e.png)'
- en: Loading the Model
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载模型
- en: To run the model Inference, we have to first load the model using 4-bit precision
    quantization and then merge trained PEFT weights with the base (LlaMA 2) model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行模型推理，我们首先需要使用 4 位精度量化加载模型，然后将训练好的 PEFT 权重与基础（LlaMA 2）模型合并。
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Inference
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理
- en: For running the inference we have to write the prompt in guanaco-llama2-1k dataset
    style(*“<s>[INST] {prompt} [/INST]”*). Otherwise you will get responses in different
    languages.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行推理时，我们需要以 guanaco-llama2-1k 数据集风格编写提示(*“<s>[INST] {prompt} [/INST]”*)。否则你将获得不同语言的响应。
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The output seems perfect.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 输出似乎完美。
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Note:** If you are facing difficulties while loading the model in Colab,
    you can check out my notebook: [Overview of PEFT](https://colab.research.google.com/drive/1qh6pAJczmC1GOw0ES-zVjPmFzspikq0m?usp=sharing).'
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 如果你在 Colab 中加载模型时遇到困难，可以查看我的笔记本：[PEFT 概述](https://colab.research.google.com/drive/1qh6pAJczmC1GOw0ES-zVjPmFzspikq0m?usp=sharing)。'
- en: Conclusion
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Parameter-Efficient Fine-Tuning techniques like LoRA enable efficient fine-tuning
    of large language models using only a fraction of parameters. This avoids expensive
    full fine-tuning and enables training with limited compute resources. The modular
    nature of PEFT allows adapting models for multiple tasks. Quantization methods
    like 4-bit precision can further reduce memory usage. Overall, PEFT opens up large
    language model capabilities to a much wider audience.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如 LoRA 这样的参数高效微调技术使得使用少量参数高效微调大型语言模型成为可能。这避免了昂贵的全量微调，并能在有限的计算资源下进行训练。PEFT 的模块化特性允许将模型适配于多个任务。像
    4 位精度这样的量化方法可以进一步减少内存使用。总体而言，PEFT 将大型语言模型的能力拓展到更广泛的受众。
- en: '[](https://www.polywork.com/kingabzpro)****[Abid Ali Awan](https://www.polywork.com/kingabzpro)****
    ([@1abidaliawan](https://www.linkedin.com/in/1abidaliawan)) is a certified data
    scientist professional who loves building machine learning models. Currently,
    he is focusing on content creation and writing technical blogs on machine learning
    and data science technologies. Abid holds a Master''s degree in technology management
    and a bachelor''s degree in telecommunication engineering. His vision is to build
    an AI product using a graph neural network for students struggling with mental
    illness.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.polywork.com/kingabzpro)****[Abid Ali Awan](https://www.polywork.com/kingabzpro)****
    ([@1abidaliawan](https://www.linkedin.com/in/1abidaliawan)) 是一位认证的数据科学专业人士，他喜欢构建机器学习模型。目前，他专注于内容创作和撰写关于机器学习和数据科学技术的技术博客。Abid
    拥有技术管理硕士学位和电信工程学士学位。他的愿景是利用图神经网络为面临心理健康问题的学生开发 AI 产品。'
- en: More On This Topic
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Explainable Forecasting and Nowcasting with State-of-the-art Deep…](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[最先进深度学习的可解释预测和现状预测](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
- en: '[RAG vs Finetuning: Which Is the Best Tool to Boost Your LLM Application?](https://www.kdnuggets.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RAG 与 Finetuning：哪种是提升 LLM 应用的最佳工具？](https://www.kdnuggets.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application)'
- en: '[The Current State of Data Science Careers](https://www.kdnuggets.com/2022/10/current-state-data-science-careers.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学职业的当前状态](https://www.kdnuggets.com/2022/10/current-state-data-science-careers.html)'
- en: '[KDnuggets News, November 2: The Current State of Data Science…](https://www.kdnuggets.com/2022/n43.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，11月2日：数据科学的当前状态…](https://www.kdnuggets.com/2022/n43.html)'
- en: '[Feature Selection: Where Science Meets Art](https://www.kdnuggets.com/2021/12/feature-selection-science-meets-art.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[特征选择：科学与艺术的交汇点](https://www.kdnuggets.com/2021/12/feature-selection-science-meets-art.html)'
- en: '[The Art of Prompt Engineering: Decoding ChatGPT](https://www.kdnuggets.com/2023/06/art-prompt-engineering-decoding-chatgpt.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提示工程的艺术：解码 ChatGPT](https://www.kdnuggets.com/2023/06/art-prompt-engineering-decoding-chatgpt.html)'
