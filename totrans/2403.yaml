- en: Why Use k-fold Cross Validation?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/07/kfold-cross-validation.html](https://www.kdnuggets.com/2022/07/kfold-cross-validation.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Why Use k-fold Cross Validation?](../Images/aacfb45cb2f16b61d7002d768975c272.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Editor
  prefs: []
  type: TYPE_NORMAL
- en: Generalization is a frequent term that is used in conversations about Machine
    Learning. It refers to how able the model can adapt to new, unseen data and how
    it works effectively using various inputs. It is understandable to say that if
    new unseen data is inputted into a model, if this unseen data has similar characteristics
    to the training data, it will perform well.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Generalizing things is easy to us humans, however it can be challenging to Machine
    Learning models. This is where Cross-Validation comes into the picture.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may see cross-validation also being referred to as rotation estimation and/or
    out-of-sample testing. The overall aim of Cross-Validation is to use it as a tool
    to evaluate machine learning models, by training a number of models on different
    subsets of the input data.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation can be used to detect overfitting in a model which infers that
    the model is not effectively generalizing patterns and similarities in the new
    inputted data.
  prefs: []
  type: TYPE_NORMAL
- en: A typical Cross-Validation workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to perform cross-validation, the following steps are typically taken:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the dataset into training data and test data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The parameters will undergo a Cross-Validation test to see which are the best
    parameters to select.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These parameters will then be implemented into the model for retraining
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Final evaluation will occur and this will depend if the cycle has to go again,
    depending on the accuracy and the level of generalization that the model performs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are different types of Cross-Validation techniques
  prefs: []
  type: TYPE_NORMAL
- en: Hold-out
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-folds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leave-one-out
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leave-p-out
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, we will be particularly focusing on K-folds.
  prefs: []
  type: TYPE_NORMAL
- en: K-fold Cross-Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K-fold Cross-Validation is when the dataset is split into a K number of folds
    and is used to evaluate the model's ability when given new data. K refers to the
    number of groups the data sample is split into. For example, if you see that the
    k-value is 5, we can call this a 5-fold cross-validation. Each fold is used as
    a testing set at one point in the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'K-fold Cross-Validation Process:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Choose your k-value
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the dataset into the number of k folds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start off with using your k-1 fold as the test dataset and the remaining folds
    as the training dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model on the training dataset and validate it on the test dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the validation score
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 3 – 5, but changing the value of your k test dataset. So we chose
    k-1 as our test dataset for the first round, we then move onto k-2 as the test
    dataset for the next round.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By the end of it you would have validated the model on every fold that you have.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Average the results that were produced in step 5 to summarize the skill of the
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can easily implement this using sklearn.model_selection.KFold
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Why Should I Use K-fold Cross-Validation?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I am going to break down the reasons below as to why you should use K-fold Cross
    Validation
  prefs: []
  type: TYPE_NORMAL
- en: Make use of your data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have a lot of readily available data that can explain a lot of things and
    help us identify hidden patterns. However, if we only have a small dataset, splitting
    it into a training and test dataset at a 80:20 ratio respectively doesn’t seem
    to do much for us.
  prefs: []
  type: TYPE_NORMAL
- en: However, when using K-fold cross validation, all parts of the data will be able
    to be used as part of the testing data. This way, all of our data from our small
    dataset can be used for both training and testing, allowing us to better evaluate
    the performance of our model.
  prefs: []
  type: TYPE_NORMAL
- en: More available metrics to help evaluate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s refer back to the example of splitting your dataset into training and
    testing as a 80:20 ratio or a train-test split - this provides us with only one
    result to refer to in our evaluation of the model. We are not sure about the accuracy
    of this result, if it’s due to chance, the level of bias, or if it actually performed
    well.
  prefs: []
  type: TYPE_NORMAL
- en: However, when using k-fold cross validation, we have more models that will be
    producing more results. For example, if we chose our k value at 10, we would have
    10 results to use in our evaluation of the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: If we were using accuracy as our measurement; having 10 different accuracy results
    where all of the data was used in the test phase is always going to be better
    and more reliable than using one accuracy result that was produced by a train-test
    split - where all the data wasn’t used in the test phase.
  prefs: []
  type: TYPE_NORMAL
- en: You would have more trust and confidence in your model's performance if the
    accuracy outputs were 94.0, 92.8, 93.0, 97.0, and 94.5 in a 5-fold cross validation
    than a 93.0 accuracy in a train-test split. This proves to us that our algorithm
    is generalizing and actively learning and is providing consistent reliable outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Better performance using dependent data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a random train-test split, we assume that the data inputs are independent.
    Let’s further expand on this. Let’s say we are using a random train-test split
    on a speech recognition dataset for British English speakers who reside from London.
    There are 5 speakers, with 250 recording each. Once the model performs a random
    train-test split, both the training and testing dataset will learn from the same
    speaker, which will be saying the same dialogue.
  prefs: []
  type: TYPE_NORMAL
- en: Yes this does improve the accuracy and boost the performance of the algorithms,
    however, what about when a new speaker is introduced to the model? How does the
    model perform then?
  prefs: []
  type: TYPE_NORMAL
- en: Using K-fold cross validation will allow you to train 5 different models, where
    in each model you are using one of the speakers for the testing dataset and the
    remaining for the training dataset. This way, not only can we evaluate the performance
    of our model, but our model will be able to perform better on new speakers and
    can be deployed in production to produce similar performance for other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Why is 10 Such a Desirable k value?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Choosing the right k-value is important as it can affect your level of accuracy,
    variance, bias and cause you to misinterpreted the overall performance of your
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest way to choose your k value is by equating it to your ‘n’ value,
    also known as leave-one-out cross-validation. n represents the size of the dataset
    which allows for each test sample the opportunity to be used in the test data
    or hold out data.
  prefs: []
  type: TYPE_NORMAL
- en: However, through various experimentations from Data Scientists, Machine Learning
    Engineers, and Researchers they have found that choosing a k-value of 10 has proven
    to provide a low bias and a modest variance.
  prefs: []
  type: TYPE_NORMAL
- en: Kuhn & Johnson spoke about their choice of k value in their book [Applied Predictive
    Modeling](https://link.springer.com/book/10.1007/978-1-4614-6849-3).
  prefs: []
  type: TYPE_NORMAL
- en: '*“The choice of k is usually 5 or 10, but there is no formal rule. As k gets
    larger, the difference in size between the training set and the resampling subsets
    gets smaller. As this difference decreases, the bias of the technique becomes
    smaller (i.e., the bias is smaller for k=10 than k= 5). In this context, the bias
    is the difference between the estimated and true values of performance”*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take this context and put it in an example:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we have a data set where N = 100
  prefs: []
  type: TYPE_NORMAL
- en: If we chose our k value = 2, our subset size = 50 and the difference = 50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we chose our k value k = 4, our subset size = 75 and the difference = 25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we chose our k value = 10, our subset size = 90 and the difference = 10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So therefore, as the value of k increases, the difference between the original
    data set and the cross-validation subsets becomes smaller.
  prefs: []
  type: TYPE_NORMAL
- en: It is also stated that choosing k= 10 is more computationally efficient, as
    the larger the values of k gets it becomes more computationally impractical. Small
    values will also be deemed as computationally efficient, however they pose the
    possibility of high bias.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross-Validation is an important tool that every Data Scientist should be using
    or very proficient in at least. It allows you to make better use of all your data
    as well as providing Data Scientists, Machine Learning Engineers and Researchers
    with a better understanding of the performance of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Having confidence in your model is important to future deployment and trust
    that the model will be effective and perform well.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Data Validation for PySpark Applications using Pandera](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pydantic Tutorial: Data Validation in Python Made Simple](https://www.kdnuggets.com/pydantic-tutorial-data-validation-in-python-made-simple)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MarshMallow: The Sweetest Python Library for Data Serialization and…](https://www.kdnuggets.com/marshmallow-the-sweetest-python-library-for-data-serialization-and-validation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Why the Newest LLMs use a MoE (Mixture of Experts) Architecture](https://www.kdnuggets.com/why-the-newest-llms-use-a-moe-mixture-of-experts-architecture)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3 Reasons Why You Should Use Linear Regression Models Instead of…](https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4 Reasons Why You Shouldn’t Use Machine Learning](https://www.kdnuggets.com/2021/12/4-reasons-shouldnt-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
