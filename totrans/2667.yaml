- en: How to Incorporate Tabular Data with HuggingFace Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何将表格数据与HuggingFace Transformers结合
- en: 原文：[https://www.kdnuggets.com/2020/11/tabular-data-huggingface-transformers.html](https://www.kdnuggets.com/2020/11/tabular-data-huggingface-transformers.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/11/tabular-data-huggingface-transformers.html](https://www.kdnuggets.com/2020/11/tabular-data-huggingface-transformers.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Ken Gu](https://www.linkedin.com/in/ken-gu/), Applied Research Scientist
    Intern at Georgian**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[Ken Gu](https://www.linkedin.com/in/ken-gu/)，Georgian的应用研究科学实习生**。'
- en: Transformer-based models are a game-changer when it comes to using unstructured
    text data. As of September 2020, the top-performing models in the General Language
    Understanding Evaluation (GLUE) benchmark are all BERT transformer-based models.
    At [Georgian](http://georgian.io/), we find ourselves working with supporting
    tabular feature information as well as unstructured text data. We found that by
    using the tabular data in [our models](http://georgian.io/platform/research-at-georgian/),
    we could further improve performance, so we set out to build a toolkit that makes
    it easier for others to do the same.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变换器的模型在使用非结构化文本数据方面是一个游戏规则的改变者。截至2020年9月，通用语言理解评估（GLUE）基准测试中的顶级模型都是BERT变换器模型。在[Georgian](http://georgian.io/)中，我们发现自己在处理支持的表格特征信息以及非结构化文本数据。我们发现，通过在[我们的模型](http://georgian.io/platform/research-at-georgian/)中使用表格数据，可以进一步提高性能，因此我们着手构建一个工具包，使其他人也能更容易做到这一点。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![](../Images/f61083950afa7309c6727bd485c6062b.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f61083950afa7309c6727bd485c6062b.png)'
- en: '*The 9 tasks that are part of the GLUE benchmark.*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*GLUE基准测试中的9项任务。*'
- en: Building on Top of Transformers
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于变换器的模型
- en: The main benefits of using transformers are that they can learn long-range dependencies
    between text and can be trained in parallel (as opposed to sequence to sequence
    models), meaning they can be pre-trained on large amounts of data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用变换器的主要好处是它们可以学习文本之间的长期依赖关系，并且可以并行训练（与序列到序列模型不同），这意味着它们可以在大量数据上进行预训练。
- en: Given these advantages, BERT is now a staple model in many real-world applications.
    Likewise, with libraries such as [HuggingFace Transformers](https://huggingface.co/transformers/),
    it’s easy to build high-performance transformer models on common NLP problems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些优势，BERT 现在是许多现实世界应用中的基础模型。同样，借助像[HuggingFace Transformers](https://huggingface.co/transformers/)这样的库，可以轻松构建在常见NLP问题上表现出色的变换器模型。
- en: Transformer models using unstructured text data are well understood. However,
    in the real-world, text data is often supported by rich structured data or other
    unstructured data like audio or visual information. Each one of these might provide
    signals that one alone would not. We call these different ways of experiencing
    data — audio, visual, or text — modalities.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用非结构化文本数据的变换器模型已经得到很好的理解。然而，在现实世界中，文本数据通常会得到丰富的结构化数据或其他非结构化数据（如音频或视觉信息）的支持。这些每一种可能提供的信息信号是单独一种数据无法提供的。我们称这些不同的数据体验方式——音频、视觉或文本——为模态。
- en: Think about e-commerce reviews as an example. In addition to the review text
    itself, we also have information about the seller, buyer, and product available
    as numerical and categorical features.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以电子商务评论为例。除了评论文本本身外，我们还拥有关于卖家、买家和产品的数值和分类特征信息。
- en: We set out to explore how we could use text and tabular data together to provide
    stronger signals in our projects. We started by exploring the field known as multimodal
    learning, which focuses on how to process different modalities in machine learning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始探索如何将文本和表格数据结合起来，以在我们的项目中提供更强的信号。我们开始探索被称为多模态学习的领域，该领域专注于如何在机器学习中处理不同的模态。
- en: Multimodal Literature Review
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多模态文献综述
- en: The current models for multimodal learning mainly focus on learning from the
    sensory modalities such as audio, visual, and text.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的多模态学习模型主要集中于从感官模态（如音频、视觉和文本）中学习。
- en: Within multimodal learning, there are several branches of research. The MultiComp
    Lab at Carnegie Mellon University provides an excellent [taxonomy](http://multicomp.cs.cmu.edu/research/taxonomy/).
    Our problem falls under what is known as **Multimodal Fusion — **joining information
    from two or more modalities to make a prediction.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在多模态学习中，有几个研究分支。卡内基梅隆大学的MultiComp实验室提供了一个出色的[taxonomy](http://multicomp.cs.cmu.edu/research/taxonomy/)。我们的问题属于被称为**多模态融合**——将来自两种或更多模态的信息结合起来以进行预测。
- en: As text data is our primary modality, our review focues on the literature that
    treats text as the main modality and introduces models that leverage the transformer
    architecture.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文本数据是我们的主要模态，我们的综述集中在将文本视为主要模态并介绍利用变压器架构的模型的文献上。
- en: '**Trivial Solution to Structured Data**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**结构化数据的微不足道的解决方案**'
- en: 'Before we dive into the literature, it’s worth mentioning that there is a simple
    solution that can be used where the structured data is treated as regular text
    and is appended to the standard text inputs. Taking the e-commerce reviews example,
    the input can be structured as follows: Review. Buyer Info. Seller Info. Numbers/Labels.
    Etc. One caveat with this approach, however, is that it is limited by the maximum
    token length that a transformer can handle.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入文献之前，值得一提的是，有一种简单的解决方案可以将结构化数据作为常规文本处理，并附加到标准文本输入中。以电子商务评论为例，输入可以结构化为：评论。买家信息。卖家信息。数字/标签。等等。然而，这种方法的一个警告是，它受限于变压器能够处理的最大标记长度。
- en: Transformer on Images and Text
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像和文本中的变压器
- en: In the last couple of years, transformer extensions for image and text have
    really advanced. [Supervised Multimodal Bitransformers for Classifying Images
    and Text](https://arxiv.org/pdf/1909.02950.pdf) by Kiela et al. (2019) uses pre-trained
    ResNet and pre-trained BERT features on unimodal images and text, respectively,
    and feeds this into a Bidirectional transformer. The key innovation is adapting
    the image features as additional tokens to the transformer model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年中，图像和文本的变压器扩展已经取得了真正的进展。[用于分类图像和文本的监督多模态双变换器](https://arxiv.org/pdf/1909.02950.pdf)（Kiela等，2019年）使用在单模态图像和文本上的预训练ResNet和预训练BERT特征，并将其输入到双向变压器中。关键创新是将图像特征适配为变压器模型的额外标记。
- en: '![](../Images/ef286dead61facdee4957f8ef1ec5b29.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef286dead61facdee4957f8ef1ec5b29.png)'
- en: '*An illustration of the multimodal transformer. This model takes the output
    of ResNet on subregions of the image as input image tokens.*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*多模态变压器的示意图。该模型将ResNet在图像子区域上的输出作为输入图像标记。*'
- en: Additionally, there are models — [ViLBERT](https://arxiv.org/abs/1908.02265) (Lu
    et al. 2019) and [VLBert](https://arxiv.org/pdf/1908.08530.pdf) (Su et al. 2020)
    — that define pretraining tasks for images and text. Both models pre-train on
    the [Conceptual Captions dataset](https://ai.google.com/research/ConceptualCaptions),
    which contains roughly 3.3 million image-caption pairs (web images with captions
    from alt text). In both cases, for any given image, a pre-trained object detection
    model like Faster R-CNN obtains vector representations for regions of the image,
    which count as input token embeddings to the transformer model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些模型——[ViLBERT](https://arxiv.org/abs/1908.02265)（Lu等，2019年）和[VLBert](https://arxiv.org/pdf/1908.08530.pdf)（Su等，2020年）——定义了图像和文本的预训练任务。这些模型都在[Conceptual
    Captions数据集](https://ai.google.com/research/ConceptualCaptions)上进行预训练，该数据集包含约330万对图像-标题对（来自alt文本的网页图像及其标题）。在这两种情况下，对于任何给定的图像，像Faster
    R-CNN这样的预训练对象检测模型会获取图像区域的向量表示，这些表示作为输入标记嵌入到变压器模型中。
- en: '![](../Images/90d0dd095ad4a5abb47417f9c112168a.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90d0dd095ad4a5abb47417f9c112168a.png)'
- en: '*The VLBert model diagram. It takes image regions outputted by Faster R-CNN
    as input image tokens.*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*VLBert模型图。它将Faster R-CNN输出的图像区域作为输入图像标记。*'
- en: 'As an example, ViLBert pre-trains on the following training objectives:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，ViLBert在以下训练目标上进行预训练：
- en: '**Masked multimodal modeling: **Mask input image and word tokens. For the image,
    the model tries to predict a vector capturing image features for the corresponding
    image region, while for text, it predicts the masked text based on the textual
    and visual clues.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**遮蔽多模态建模：** 遮蔽输入图像和词汇标记。对于图像，模型试图预测捕捉图像区域特征的向量，而对于文本，它基于文本和视觉线索预测被遮蔽的文本。'
- en: '**Multimodal alignment: **Whether the image and text pair are actually from
    the same image and caption pair.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**多模态对齐：** 图像和文本对是否确实来自同一图像和标题对。'
- en: '![](../Images/1be6a703b698972f151b8964c46bfb09.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1be6a703b698972f151b8964c46bfb09.png)'
- en: '*The two pre-training tasks for ViLBert.*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*ViLBERT 的两个预训练任务。*'
- en: '![](../Images/f4fa4d30f67670e2cb7bc7237297819d.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4fa4d30f67670e2cb7bc7237297819d.png)'
- en: '![](../Images/e64e06ac18027d7ebda7c21140686ce2.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e64e06ac18027d7ebda7c21140686ce2.png)'
- en: '*An example of masked multimodal learning. Given the image and text, if we
    mask out *dog*, then the model should be able to use the unmasked visual information
    to correctly predict the masked word to be *dog*.*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*一个遮蔽多模态学习的示例。给定图像和文本，如果我们遮蔽掉*dog*，则模型应能够利用未遮蔽的视觉信息正确预测被遮蔽的词为*dog*。*'
- en: All these models use the bidirectional transformer model that is the backbone
    of BERT. The differences are the pre-training tasks the models are trained on
    and slight additions to the transformer. In the case of ViLBERT, the authors also
    introduce a co-attention transformer layer (shown below) to define the attention
    mechanism between the modalities explicitly.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些模型使用双向变压器模型，这是 BERT 的核心。区别在于模型训练的预训练任务以及对变压器的轻微添加。在 ViLBERT 的情况下，作者还引入了一个共注意力变压器层（见下图），以显式定义模态之间的注意力机制。
- en: '![](../Images/3c2cee8e6ff7f49649f4b2b8f2627a38.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c2cee8e6ff7f49649f4b2b8f2627a38.png)'
- en: '*The standard transformer block vs. the co-attention transformer block. The
    co-attention block injects attention-weighted vectors of another modality (linguistic,
    for example) into the hidden representations of the current modality (visual).*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*标准变压器块与共注意力变压器块。共注意力块将另一种模态（例如语言）的注意力加权向量注入当前模态（视觉）的隐藏表示中。*'
- en: Finally, there’s also [LXMERT](https://arxiv.org/abs/1908.07490) (Tan and Mohit
    2019), another pre-trained transformer model that, as of [Transformers version
    3.1.0](https://pypi.org/project/transformers/3.1.0/), is implemented as part of
    the library. The input to LXMERT is the same as ViLBERT and VLBERT. However, LXMERT
    pre-trains on aggregated datasets, which also include visual question answering
    datasets. In total, LXMERT pre-trains on 9.18 million image text pairs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有[LXMERT](https://arxiv.org/abs/1908.07490)（Tan 和 Mohit 2019），另一种预训练的变压器模型，截至[Transformers
    版本 3.1.0](https://pypi.org/project/transformers/3.1.0/)，作为库的一部分实现。LXMERT 的输入与
    ViLBERT 和 VLBERT 相同。然而，LXMERT 在聚合的数据集上进行预训练，这些数据集还包括视觉问答数据集。总的来说，LXMERT 在 918
    万对图像文本上进行预训练。
- en: Transformers on Aligning Audio, Visual, and Text
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对齐音频、视觉和文本的变压器
- en: Beyond transformers for combining image and text, there are multimodal models
    for audio, video, and text modalities in which there is a natural ground truth
    temporal alignment. Papers for this approach include MulT,[ Multimodal Transformer
    for Unaligned Multimodal Language Sequences](https://arxiv.org/pdf/1906.00295.pdf) (Tsai
    et al. 2019), and the Multimodal Adaptation Gate (MAG) from[ Integrating Multimodal
    Information in Large Pretrained Transformers](https://www.aclweb.org/anthology/2020.acl-main.214.pdf) (Rahman
    et al. 2020).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用于结合图像和文本的变压器，还有用于音频、视频和文本模态的多模态模型，其中存在自然的真实时间对齐。相关论文包括 MulT，[Multimodal Transformer
    for Unaligned Multimodal Language Sequences](https://arxiv.org/pdf/1906.00295.pdf)（Tsai
    等 2019），以及来自[Integrating Multimodal Information in Large Pretrained Transformers](https://www.aclweb.org/anthology/2020.acl-main.214.pdf)（Rahman
    等 2020）的多模态适应门（MAG）。
- en: MuIT is similar to ViLBert in which co-attention is used between pairs of modalities.
    MAG, meanwhile, injects other modality information at certain transformer layers
    via a gating mechanism.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: MuIT 与 ViLBERT 类似，在模态对之间使用共注意力。而 MAG 则通过门控机制在某些变压器层中注入其他模态信息。
- en: Transformers with Text and Knowledge Graph Embeddings
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带有文本和知识图谱嵌入的变压器
- en: Some works have also identified knowledge graphs as a vital piece of information
    in addition to text data. [Enriching BERT with Knowledge Graph Embeddings for
    Document Classification](https://arxiv.org/pdf/1909.08402.pdf) (Ostendorff et
    al. 2019) uses features from the author entities in the Wikidata knowledge graph
    in addition to metadata features for book category classification. In this case,
    the model is a simple concatenation of these features and BERT output text features
    of the book title and description before some final classification layers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究还识别了知识图谱作为除文本数据之外的重要信息。[用知识图谱嵌入增强BERT进行文档分类](https://arxiv.org/pdf/1909.08402.pdf)（Ostendorff
    et al. 2019）除了书籍类别分类的元数据特征外，还使用了Wikidata知识图谱中的作者实体特征。在这种情况下，该模型是这些特征和BERT输出的书名和描述的文本特征的简单连接，然后是一些最终分类层。
- en: '![](../Images/36a31b4dd50e91186d80c891e7bdb9d6.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36a31b4dd50e91186d80c891e7bdb9d6.png)'
- en: '*The simple model architecture to incorporate knowledge graph embeddings and
    tabular metadata.*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*简单的模型架构以融入知识图谱嵌入和表格元数据。*'
- en: On the other hand, [ERNIE](https://arxiv.org/abs/1905.07129) (Zhang et al. 2019)
    matches the tokens in the input text with entities in the knowledge graph. They
    fuse these embeddings to produce entity aware text embeddings and text-aware entity
    embeddings with this matching.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，[ERNIE](https://arxiv.org/abs/1905.07129)（Zhang et al. 2019）将输入文本中的令牌与知识图谱中的实体进行匹配。他们融合这些嵌入，以产生具有实体意识的文本嵌入和具有文本意识的实体嵌入。
- en: Key Takeaway
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键要点
- en: The main takeaway for adapting transformers for multimodal data is to ensure
    that there is an attention or weighting mechanism between the different modalities.
    These attention mechanisms can occur at different points of the transformer architecture,
    as encoded input embeddings, injected in the middle, or combined after the transformer
    encodes the text data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 适应变换器以处理多模态数据的主要要点是确保在不同模态之间存在注意力或加权机制。这些注意力机制可以在变换器架构的不同点出现，如编码的输入嵌入、在中间注入，或在变换器编码文本数据后合并。
- en: Multimodal Transformers Toolkit
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多模态变换器工具包
- en: Using what we’ve learned from the literature review and the comprehensive [HuggingFace](https://huggingface.co/)
    library of state-of-the-art transformers, we’ve developed a toolkit. The **multimodal-transformers** package
    extends any HuggingFace transformer for tabular data. To see the code, documentation,
    and working examples, check out [the project repo](https://github.com/georgianpartners/Multimodal-Toolkit).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们从文献回顾和全面的[HuggingFace](https://huggingface.co/)最先进的变换器库中学到的知识，我们开发了一个工具包。**multimodal-transformers**包扩展了任何HuggingFace变换器以适应表格数据。要查看代码、文档和工作示例，请查看[项目仓库](https://github.com/georgianpartners/Multimodal-Toolkit)。
- en: At a high level, the outputs of a transformer model on text data and tabular
    features containing categorical and numerical data are combined in a combining
    module. Since there is no alignment in our data, we choose to combine the text
    features after the transformer’s output. The combining module implements several
    methods for integrating the modalities, including attention and gating methods
    inspired by the literature survey. More details of these methods are available
    [here](https://multimodal-toolkit.readthedocs.io/en/latest/notes/combine_methods.html).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，变换器模型在文本数据和包含分类和数值数据的表格特征上的输出在结合模块中进行合并。由于数据中没有对齐，我们选择在变换器输出后合并文本特征。结合模块实现了几种集成模态的方法，包括受到文献调查启发的注意力和门控方法。这些方法的更多细节请见[这里](https://multimodal-toolkit.readthedocs.io/en/latest/notes/combine_methods.html)。
- en: '![](../Images/eb26b789b45570eadbc807594df6838c.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb26b789b45570eadbc807594df6838c.png)'
- en: '*High-level diagram of multimodal-transformers. The adaptation of transformers
    to incorporate data is all contained in the combining module.*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*多模态变换器的高级图示。变换器适应以融合数据的所有内容都包含在结合模块中。*'
- en: Walkthrough
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 演练
- en: Let’s work through an example where we classify clothing review recommendations.
    We’ll use a simplified version of the example included in the Colab notebook.
    We will use the [Women’s E-Commerce Clothing Reviews](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews) from
    Kaggle, which contains 23,000 customer reviews.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例来进行分类衣物评论推荐。我们将使用Colab笔记本中包含的示例的简化版本。我们将使用Kaggle上的[女性电子商务服装评论](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews)，该数据集包含23,000条客户评论。
- en: '![](../Images/8b90adc7515e33fdd0d1fe2814d20360.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b90adc7515e33fdd0d1fe2814d20360.png)'
- en: '*A sample of the clothing review dataset.*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*衣物评论数据集的一个示例。*'
- en: In this dataset, we have text data in the Title and Review Text columns. We
    also have categorical features from the *Clothing ID, Division Name, Department
    Name, and Class Name* columns and numerical features from the *Rating and Positive
    Feedback Count.*
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，我们在标题和评论文本列中有文本数据。我们还从*Clothing ID, Division Name, Department Name,
    和 Class Name*列中获得类别特征，从*Rating 和 Positive Feedback Count*列中获得数值特征。
- en: Loading The Dataset
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载数据集
- en: We first load our data into a **TorchTabularTextDataset**, which works with
    PyTorch’s data loaders that include the text inputs for HuggingFace Transformers
    and our specified categorical feature columns and numerical feature columns. For
    this, we also need to load our HuggingFace tokenizer.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将数据加载到**TorchTabularTextDataset**中，该数据集与 PyTorch 的数据加载器兼容，包括 HuggingFace
    Transformers 的文本输入，以及我们指定的类别特征列和数值特征列。为此，我们还需要加载 HuggingFace 的分词器。
- en: Loading Transformer with Tabular Model
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用表格模型加载变换器
- en: Now we load our transformer with a tabular model. First, we specify our tabular
    configurations in a **TabularConfig** object. This config is then set as the **tabular_config** member
    variable of a HuggingFace transformer config object. Here, we also specify how
    we want to combine the tabular features with the text features. In this example,
    we will use a weighted sum method.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用表格模型加载变换器。首先，我们在**TabularConfig**对象中指定我们的表格配置。然后将此配置设置为 HuggingFace 变换器配置对象的**tabular_config**成员变量。在这里，我们还指定如何将表格特征与文本特征结合起来。在此示例中，我们将使用加权求和方法。
- en: Once we have the **tabular_config** set, we can load the model using the same
    API as HuggingFace. See the [documentation](https://multimodal-toolkit.readthedocs.io/en/latest/modules/model.html#module-multimodal_transformers.model.tabular_transformers) for
    the list of currently supported transformer models that include the tabular combination
    module.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们设置了**tabular_config**，我们可以使用与 HuggingFace 相同的 API 加载模型。请参阅[文档](https://multimodal-toolkit.readthedocs.io/en/latest/modules/model.html#module-multimodal_transformers.model.tabular_transformers)，以查看目前支持的包含表格组合模块的变换器模型列表。
- en: Training
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练
- en: For training, we can use HuggingFace’s [trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#transformers.Trainer) class.
    We also need to specify the training arguments, and in this case, we will use
    the default.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，我们可以使用 HuggingFace 的[trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer#transformers.Trainer)类。我们还需要指定训练参数，在这种情况下，我们将使用默认值。
- en: Let’s take a look at our models in training!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们正在训练的模型吧！
- en: '![](../Images/36968204ded1b162b45de0c17de61647.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36968204ded1b162b45de0c17de61647.png)'
- en: '*The Tensorboard logs from the above experiment. You can also check out this
    Tensorboard [here](https://tensorboard.dev/experiment/pzG4qrcJTyGhjENIKzE9Dw/#scalars).*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*以上实验的 Tensorboard 日志。您也可以在 [这里](https://tensorboard.dev/experiment/pzG4qrcJTyGhjENIKzE9Dw/#scalars)
    查看这个 Tensorboard。*'
- en: Results
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结果
- en: Using this toolkit, we also ran our experiments on the [Women’s E-Commerce Clothing
    Reviews](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews) dataset
    for recommendation prediction and the [Melbourne Airbnb Open Data](https://www.kaggle.com/tylerx/melbourne-airbnb-open-data) dataset
    for price prediction. The former is a classification task, while the latter is
    a regression task. Our results are in the table below. The **text_only** combine
    method is a baseline that uses only the transformer and is essentially the same
    as a HuggingFace **forSequenceClassification **model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个工具包，我们还在[女性电子商务服装评论](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews)数据集上进行推荐预测实验，以及在[墨尔本
    Airbnb 开放数据](https://www.kaggle.com/tylerx/melbourne-airbnb-open-data)数据集上进行价格预测实验。前者是分类任务，而后者是回归任务。我们的结果见下表。**text_only**组合方法是一个基线，它仅使用变换器，本质上与
    HuggingFace 的**forSequenceClassification**模型相同。
- en: '![](../Images/82486e33dc986cc9db2ee2043bc85266.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82486e33dc986cc9db2ee2043bc85266.png)'
- en: We can see that incorporating tabular features improves performance over the **text_only** method.
    The performance gains depend on how strong the training signals from the tabular
    data are. For example, in the review recommendation case, the **text_only** model
    is already a strong baseline.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，结合表格特征可以提升**text_only**方法的性能。性能提升依赖于表格数据中训练信号的强度。例如，在评论推荐的情况下，**text_only**模型已经是一个强基线。
- en: Next Steps
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下一步
- en: We’ve already used the toolkit successfully in our projects. Feel free to try
    it out on your next machine learning project!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在我们的项目中成功地使用了这个工具包。欢迎在您的下一个机器学习项目中尝试！
- en: Check out the [documentation](https://multimodal-toolkit.readthedocs.io/en/latest/) and
    the included [main](https://github.com/georgianpartners/Multimodal-Toolkit/blob/master/main.py) script
    for how to do evaluation and inference. If you want support for your favorite
    transformer, feel free to add transformer support [here](https://github.com/georgianpartners/Multimodal-Toolkit/blob/master/multimodal_transformers/model/tabular_transformers.py).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 [文档](https://multimodal-toolkit.readthedocs.io/en/latest/) 和包含的 [主](https://github.com/georgianpartners/Multimodal-Toolkit/blob/master/main.py)
    脚本以了解如何进行评估和推理。如果你希望支持你最喜欢的 Transformer，欢迎在 [这里](https://github.com/georgianpartners/Multimodal-Toolkit/blob/master/multimodal_transformers/model/tabular_transformers.py)
    添加对 Transformer 的支持。
- en: Appendix
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录
- en: Readers should check out [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) and
    [The Illustrated BERT](http://jalammar.github.io/illustrated-bert/) for a well-summarized
    overview of transformers and BERT.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 读者应查看 [图解 Transformer](http://jalammar.github.io/illustrated-transformer/) 和
    [图解 BERT](http://jalammar.github.io/illustrated-bert/) 以获得 Transformer 和 BERT
    的概述。
- en: Below, you’ll find a quick taxonomy of papers we reviewed.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们审阅的论文的快速分类。
- en: '**Transformer on Image and Text**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer 在图像和文本上的应用**'
- en: Supervised Multimodal Bitransformers for Classifying Images and Text (Kiela
    et al. 2019)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于分类图像和文本的监督多模态双向 Transformer（Kiela 等，2019）
- en: 'ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language
    Tasks (Lu et al. 2019)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ViLBERT: 预训练任务无关的视觉语言表示用于视觉和语言任务（Lu 等，2019）'
- en: 'VL-BERT: Pretraining of Generic Visual-Linguistic Representations (Su et al.
    ICLR 2020)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'VL-BERT: 预训练的通用视觉语言表示（Su 等，ICLR 2020）'
- en: 'LXMERT: Learning Cross-Modality Encoder Representations from Transformers (Tan
    et al. EMNLP 2019)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LXMERT: 从 Transformer 学习跨模态编码表示（Tan 等，EMNLP 2019）'
- en: '**Transformers on Aligning Audio, Visual, and Text**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer 在音频、视觉和文本对齐方面的应用**'
- en: Multimodal Transformer for Unaligned Multimodal Language Sequences (Tsai et
    al. ACL 2019)
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于非对齐多模态语言序列的多模态 Transformer（Tsai 等，ACL 2019）
- en: Integrating Multimodal Information in Large Pretrained Transformers (Rahman
    et al. ACL 2020)
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大型预训练 Transformer 中集成多模态信息（Rahman 等，ACL 2020）
- en: '**Transformers with Knowledge Graph Embeddings**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer 与知识图谱嵌入**'
- en: Enriching BERT with Knowledge Graph Embeddings for Document Classification (Ostendorff
    et al. 2019)
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过知识图谱嵌入丰富 BERT 以进行文档分类（Ostendorff 等，2019）
- en: 'ERNIE: Enhanced Language Representation with Informative Entities (Zhang et
    al. 2019)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ERNIE: 增强的语言表示与信息实体（张等，2019）'
- en: '[Original](https://medium.com/georgian-impact-blog/how-to-incorporate-tabular-data-with-huggingface-transformers-b70ac45fcfb4).
    Reposted with permission.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/georgian-impact-blog/how-to-incorporate-tabular-data-with-huggingface-transformers-b70ac45fcfb4)。经许可转载。'
- en: '**Bio:** [Ken Gu](https://www.linkedin.com/in/ken-gu/) is an Applied Research
    Intern at Georgian where he is working on various applied machine learning initiatives.
    He received his BS in Computer Science and a concentration in Mathematics from
    University of California Los Angeles. At UCLA, Ken has worked on research projects
    in Graph Deep Learning with a focus on biomedical interaction networks.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：** [Ken Gu](https://www.linkedin.com/in/ken-gu/) 是 Georgian 的应用研究实习生，他正在从事各种应用机器学习项目。他在加州大学洛杉矶分校获得计算机科学学士学位，并辅修数学。在
    UCLA，Ken 曾参与图形深度学习的研究项目，重点关注生物医学交互网络。'
- en: '**Related:**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[A Deep Dive Into the Transformer Architecture – The Development of Transformer
    Models](https://www.kdnuggets.com/2020/08/transformer-architecture-development-transformer-models.html)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深入探讨 Transformer 架构 – Transformer 模型的发展](https://www.kdnuggets.com/2020/08/transformer-architecture-development-transformer-models.html)'
- en: '[Every Complex DataFrame Manipulation, Explained & Visualized Intuitively](https://www.kdnuggets.com/2020/11/dataframe-manipulation-explained-visualized.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每一种复杂的数据框操作，解释与直观展示](https://www.kdnuggets.com/2020/11/dataframe-manipulation-explained-visualized.html)'
- en: '[Research Guide for Transformers](https://www.kdnuggets.com/2019/10/research-guide-transformers.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Transformer 研究指南](https://www.kdnuggets.com/2019/10/research-guide-transformers.html)'
- en: More On This Topic
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Simple NLP Pipelines with HuggingFace Transformers](https://www.kdnuggets.com/2023/02/simple-nlp-pipelines-huggingface-transformers.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 HuggingFace Transformers 的简单 NLP 管道](https://www.kdnuggets.com/2023/02/simple-nlp-pipelines-huggingface-transformers.html)'
- en: '[Data-centric AI and Tabular Data](https://www.kdnuggets.com/2022/09/datacentric-ai-tabular-data.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[以数据为中心的 AI 和表格数据](https://www.kdnuggets.com/2022/09/datacentric-ai-tabular-data.html)'
- en: '[How to Generate Synthetic Tabular Dataset](https://www.kdnuggets.com/2022/03/generate-tabular-synthetic-dataset.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何生成合成的表格数据集](https://www.kdnuggets.com/2022/03/generate-tabular-synthetic-dataset.html)'
- en: '[Answering Questions with HuggingFace Pipelines and Streamlit](https://www.kdnuggets.com/2021/10/simple-question-answering-web-app-hugging-face-pipelines.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 HuggingFace Pipelines 和 Streamlit 回答问题](https://www.kdnuggets.com/2021/10/simple-question-answering-web-app-hugging-face-pipelines.html)'
- en: '[Fine-Tuning BERT for Tweets Classification with HuggingFace](https://www.kdnuggets.com/2022/01/finetuning-bert-tweets-classification-ft-hugging-face.html)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 HuggingFace 对 BERT 进行推特分类微调](https://www.kdnuggets.com/2022/01/finetuning-bert-tweets-classification-ft-hugging-face.html)'
- en: '[A Simple to Implement End-to-End Project with HuggingFace](https://www.kdnuggets.com/a-simple-to-implement-end-to-end-project-with-huggingface)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 HuggingFace 实现的简单端到端项目](https://www.kdnuggets.com/a-simple-to-implement-end-to-end-project-with-huggingface)'
