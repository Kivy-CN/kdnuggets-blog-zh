- en: 'Deep Learning Breakthrough: a sub-linear deep learning algorithm that does
    not need a GPU?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习突破：一种不需要GPU的次线性深度学习算法？
- en: 原文：[https://www.kdnuggets.com/2020/03/deep-learning-breakthrough-sub-linear-algorithm-no-gpu.html](https://www.kdnuggets.com/2020/03/deep-learning-breakthrough-sub-linear-algorithm-no-gpu.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/03/deep-learning-breakthrough-sub-linear-algorithm-no-gpu.html](https://www.kdnuggets.com/2020/03/deep-learning-breakthrough-sub-linear-algorithm-no-gpu.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Anshumali Shrivastava](https://www.cs.rice.edu/~as143/), Rice University**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[Anshumali Shrivastava](https://www.cs.rice.edu/~as143/)，莱斯大学**。'
- en: We all know that [deep neural networks](https://www.kdnuggets.com/2020/02/deep-neural-networks.html)
    (DNNs) are spearheading the current AI revolution. [The backpropagation algorithm](https://www.kdnuggets.com/2019/01/backpropagation-algorithm-demystified.html) is
    a commonly used method for training DNNs. Backpropagation consists offorward and
    backward pass over the network, which can be written as a sequence of matrix multiplications. [GPUs
    (Graphical Processing Units)](https://en.wikipedia.org/wiki/Graphics_processing_unit) have
    a unique advantage here because they have thousands of processing cores optimized
    for Matrix multiplication. They can perform [an order of magnitude faster matrix
    multiplications](https://graphics.stanford.edu/papers/gpumatrixmult/gpumatrixmult.pdf).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都知道 [深度神经网络](https://www.kdnuggets.com/2020/02/deep-neural-networks.html)
    (DNNs) 正在引领当前的AI革命。 [反向传播算法](https://www.kdnuggets.com/2019/01/backpropagation-algorithm-demystified.html)
    是一种常用的训练DNNs的方法。反向传播包括对网络的前向和反向传递，可以写作一系列矩阵乘法。 [GPU (图形处理单元)](https://en.wikipedia.org/wiki/Graphics_processing_unit)
    在这里具有独特的优势，因为它们有成千上万的处理核心，优化用于矩阵乘法。它们可以执行[数量级更快的矩阵乘法](https://graphics.stanford.edu/papers/gpumatrixmult/gpumatrixmult.pdf)。
- en: Wasteful Computations in Backpropagation
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Backpropagation中的浪费计算
- en: Backpropagation is not an efficient algorithm. Prof. Geoffrey Hinton, one of
    the pioneers of backpropagation, has identified several of its vital shortcomings
    in an [interview](https://www.i-programmer.info/news/105-artificial-intelligence/11135--geoffrey-hinton-says-ai-needs-to-start-over.html).
    The core idea behind the SLIDE algorithm is to minimize *wasteful computations* in
    backpropagation. Imagine a vast neural network with hundreds of millions of parameters.
    As training progresses, different neurons specialize and focus (with large activations)
    on very different characteristics of the data. The result is a network where only
    a few neurons are important for any given input, as demonstrated by the famous [cat
    neuron article](https://www.nytimes.com/2012/06/26/technology/in-a-big-network-of-computers-evidence-of-machine-learning.html?auth=login-email&login=email).
    We call this characteristic *adaptive sparsity*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播不是一种高效的算法。反向传播的先驱之一Geoffrey Hinton教授在一次[采访](https://www.i-programmer.info/news/105-artificial-intelligence/11135--geoffrey-hinton-says-ai-needs-to-start-over.html)中指出了它的几个重要缺陷。SLIDE算法的核心思想是最小化*浪费计算*。想象一个具有数亿参数的庞大神经网络。随着训练的进行，不同的神经元专注于数据的非常不同特征（具有大激活值）。结果是一个网络，其中仅有少数几个神经元对任何给定的输入重要，如著名的[猫神经元文章](https://www.nytimes.com/2012/06/26/technology/in-a-big-network-of-computers-evidence-of-machine-learning.html?auth=login-email&login=email)所示。我们称这种特性为*自适应稀疏性*。
- en: Backpropagation is oblivious to adaptive sparsity. Instead, backpropagation
    updates millions of parameters using full matrix multiplication. Most of these
    updates are unnecessary.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播对自适应稀疏性视而不见。相反，反向传播使用完整的矩阵乘法更新数百万个参数。这些更新中的大多数是不必要的。
- en: Since 2013, adaptive sparsity has been exploited in several ways to improve
    generalization. For example, [a NeurIPS 2013paper ](https://papers.nips.cc/paper/5032-adaptive-dropout-for-training-deep-neural-networks.pdf)showed
    we could limit the weight updates to a tiny subset of the neurons in each layer.
    The subset was selected using adaptive sampling. The sampling is adaptive in the
    sense that it favors neurons with large values of activation during sampling.
    The authors argue that sparse adaptive updates support better [generalization ](https://www.kdnuggets.com/2017/06/understanding-deep-learning-rethinking-generalization.html)leading
    to better accuracy.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自2013年以来，自适应稀疏性以几种方式被利用来改善泛化。例如， [一篇NeurIPS 2013论文](https://papers.nips.cc/paper/5032-adaptive-dropout-for-training-deep-neural-networks.pdf) 显示我们可以将权重更新限制到每层中的一个极小子集神经元。该子集是通过自适应采样选择的。采样是自适应的，意味着它在采样期间偏爱具有大激活值的神经元。作者认为，稀疏自适应更新支持更好的 [泛化](https://www.kdnuggets.com/2017/06/understanding-deep-learning-rethinking-generalization.html)，从而提高准确性。
- en: Unfortunately, these sparse updates are far from being computationally efficient.
    We need to compute all the activations for each input to determine which neurons
    to update. Since every neuron must be accessed, the number of operations is *linear* in
    the number of parameters. Thus, if we have one million parameters, we must perform
    one million arithmetic operations for every input.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这些稀疏更新远未实现计算上的高效。我们需要计算每个输入的所有激活值以确定要更新哪些神经元。由于每个神经元都必须被访问，操作的数量在参数的数量上是*线性的*。因此，如果我们有一百万个参数，每个输入必须执行一百万次算术操作。
- en: 'Detour: The Fundamental Puzzle of Inner Product Sampling'
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 绕道：内积抽样的基本谜题
- en: 'Algorithms for efficient sparse updated are locked behind an algorithmic puzzle.
    The puzzle is: “How can we sample neurons with large activations without computing
    all of the activations? A key observation is that the activations are a monotonic
    function of the inner product between neuron weights and the input. Thus, we can
    reduce the problem to: “How can we efficiently sample weight vectors that will
    have a large inner product with the input?”'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 高效稀疏更新的算法被锁在一个算法谜题后面。谜题是：“我们如何在不计算所有激活的情况下抽样具有大激活的神经元？一个关键观察是激活值是神经元权重和输入之间内积的单调函数。因此，我们可以将问题简化为：“我们如何高效地抽样那些与输入有大内积的权重向量？”
- en: The problem seems a little absurd because we need to know the values of the
    inner products to tell which ones are large. The problem is analogous to a similarity
    search problem in disguise, where the inner product is the similarity measure,
    and the input is the query. In web search, it is [well known ](https://en.wikipedia.org/wiki/Search_engine_indexing)that
    we can find documents that are similar (or relevant) to the query without linear
    computations using [indexing](https://en.wikipedia.org/wiki/Web_indexing).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题看起来有点荒谬，因为我们需要知道内积的值才能判断哪些是大的。这个问题类似于伪装的相似性搜索问题，其中内积是相似性度量，输入是查询。在网页搜索中，[我们知道](https://en.wikipedia.org/wiki/Search_engine_indexing)可以在不进行线性计算的情况下找到与查询相似（或相关）的文档，使用的是[indexing](https://en.wikipedia.org/wiki/Web_indexing)。
- en: Hash Tables for Adaptive Sampling
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自适应抽样的哈希表
- en: 'A hash function h(x) maps an input x to an integer, called a hash value or
    fingerprint. Let us assume that h(x) has a special property: given two vectors,
    the hash values of both vectors are the same with probability Pr⁡(h(x)=h(y))=g(⟨x,y⟩),
    where g is a monotonic function. This property describes a [locality sensitive
    hash (LSH)](https://en.wikipedia.org/wiki/Locality-sensitive_hashing) function.
    In simpler terms, if two vectors x and y have a large inner product, then their
    corresponding hash values h(x) and h(y) are very likely to be identical. If the
    vectors have a small inner product, then h(x) and h(y) will most likely be different.
    We showed how to construct a hash function with this property in [this paper](https://papers.nips.cc/paper/5329-asymmetric-lsh-alsh-for-sublinear-time-maximum-inner-product-search-mips.pdf)**,**
    which received the outstanding paper award in NIPS 2014 (now NeurIPS).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希函数h(x)将输入x映射到一个整数，称为哈希值或指纹。假设h(x)具有一个特殊属性：对于两个向量，两个向量的哈希值相同的概率为Pr⁡(h(x)=h(y))=g(⟨x,y⟩)，其中g是一个单调函数。这个属性描述了[局部敏感哈希（LSH）](https://en.wikipedia.org/wiki/Locality-sensitive_hashing)函数。简单来说，如果两个向量x和y的内积很大，那么它们对应的哈希值h(x)和h(y)很可能是相同的。如果向量的内积很小，那么h(x)和h(y)很可能不同。我们在[这篇论文](https://papers.nips.cc/paper/5329-asymmetric-lsh-alsh-for-sublinear-time-maximum-inner-product-search-mips.pdf)**中**展示了如何构造具有这种属性的哈希函数，该论文获得了NIPS
    2014（现NeurIPS）的杰出论文奖。
- en: In early 2016, we showed that ([paper](https://dl.acm.org/doi/10.1145/3097983.3098035))
    our functions lead to an efficient sampling algorithm for neurons. The idea is
    to index all the neurons into a hash table using h, as shown in Figure 2\. We
    store a reference to neuron hash  at the memory location pointed to by its fingerprint
    . After preprocessing the neurons, we can efficiently sample neurons for any input
    x. We simply go to the memory location h(x) and choose neurons from there. This
    process ensures hash collisions, and hence we only select neurons that are likely
    to have large inner products. Our sampling process is provably adaptive to the
    activation values without needing to explicitly compute all activations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2016 年初，我们展示了 ([论文](https://dl.acm.org/doi/10.1145/3097983.3098035)) 我们的函数可以实现高效的神经元采样算法。这个想法是将所有神经元索引到一个哈希表中，使用
    h，如图 2 所示。我们在由指纹指向的内存位置存储神经元哈希的引用。在对神经元进行预处理后，我们可以高效地对任何输入 x 进行神经元采样。我们只需访问内存位置
    h(x) 并从中选择神经元。这个过程确保了哈希碰撞，因此我们只选择可能具有大内积的神经元。我们的采样过程可以证明适应激活值，而不需要显式计算所有激活值。
- en: '![](../Images/030e824cfa522a8537913434e6aa28d2.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/030e824cfa522a8537913434e6aa28d2.png)'
- en: '***Figure 1:** Illustration of Hash tables for sampling neurons. The HASH function
    satisfied the LSH property.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '***图 1:** 哈希表用于采样神经元的示意图。HASH 函数满足 LSH 属性。*'
- en: Subsequent work has shown that probabilistic hash tables can lead to O(1) importance
    sampling for efficient estimation of [partition functions](https://arxiv.org/abs/1703.05160)**,**
    [kernels](https://arxiv.org/abs/1912.02283), and [stochastic gradients](http://papers.nips.cc/paper/9401-fast-and-accurate-stochastic-gradient-estimation.pdf).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 后续工作表明，概率哈希表可以实现 O(1) 的重要性采样，以高效估计 [分区函数](https://arxiv.org/abs/1703.05160)**、**
    [核函数](https://arxiv.org/abs/1912.02283) 和 [随机梯度](http://papers.nips.cc/paper/9401-fast-and-accurate-stochastic-gradient-estimation.pdf)。
- en: '![](../Images/cc9e2e8256466f32241e632ad55f1289.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc9e2e8256466f32241e632ad55f1289.png)'
- en: '***Figure 2:** SLIDE Algorithm Illustration. Every layer has hash tables for
    efficiently identifying neurons with high activation. for every input,  we only
    need hash computation and a few memory probes to obtain a very sparse snapshot
    of the network.*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '***图 2:** SLIDE 算法示意图。每一层都有哈希表用于高效识别激活值高的神经元。对于每个输入，我们只需要进行哈希计算和几个内存探测，就能获得网络的一个非常稀疏的快照。*'
- en: 'The SLIDE Algorithm: Sparse Backpropagation with Hash Tables'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SLIDE 算法：使用哈希表的稀疏反向传播
- en: The SLIDE algorithm (Figure 2) uses hash tables to select neurons during training.
    Complete implementation details are available in our [paper](https://arxiv.org/abs/1903.03129).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: SLIDE 算法（图 2）在训练过程中使用哈希表来选择神经元。完整的实现细节可以在我们的 [论文](https://arxiv.org/abs/1903.03129)
    中找到。
- en: SLIDE begins by initializing the neural network parameters and pre-indexing
    all of the neurons into LSH hash tables.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: SLIDE 从初始化神经网络参数和将所有神经元预索引到 LSH 哈希表开始。
- en: To train the network with a given input, we first compute the LSH hash value
    (H1) of the input. Then, we look up neurons with H1 as the address (memory location)
    and sample neurons from the corresponding bucket. We only compute the activations
    for these neurons, and we set the activations for other neurons to 0\. In the
    next layer, we use the sparse activation output of the previous layer as the input.
    The input is sparse because the only non-zero activations are from the selected
    neurons. We obtain a new hash value H2 for our new input and repeat the sampling
    process for the next layer. For every input, we only need a few hash values and
    memory lookups to select a sparse set of important nodes in the network. We only
    feedforward and backpropagate on the selected neurons. Finally, we update the
    position of the selected neurons in the hash tables using the updated weights.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用给定输入训练网络，我们首先计算输入的 LSH 哈希值 (H1)。然后，我们查找以 H1 为地址（内存位置）的神经元，并从相应的桶中采样神经元。我们只计算这些神经元的激活值，并将其他神经元的激活值设置为
    0。在下一层中，我们将上一层的稀疏激活输出作为输入。输入是稀疏的，因为只有所选神经元的激活值是非零的。我们为新的输入获得一个新的哈希值 H2，并重复下一层的采样过程。对于每个输入，我们只需几个哈希值和内存查找即可选择网络中的一组稀疏的重要节点。我们只在所选神经元上进行前向传播和反向传播。最后，我们使用更新后的权重更新哈希表中所选神经元的位置。
- en: During training, all operations are of the order of active neurons (sub-linear)
    rather than the total number of neurons (linear). In our experiments, we show
    that only 0.5% of the neurons are needed to get the same accuracy as full backpropagation.
    While SLIDE introduces a small overhead to maintain the hash tables, our operations
    are around 200x less expensive when compared to standard backpropagation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，所有操作的数量级是活跃神经元的数量（子线性），而不是神经元的总数量（线性）。在我们的实验中，我们展示了仅需 0.5% 的神经元即可获得与全反向传播相同的准确度。虽然
    SLIDE 引入了少量的开销来维护哈希表，但与标准反向传播相比，我们的操作成本约低 200 倍。
- en: Parallel Gradient Updates
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 并行梯度更新
- en: Since only 0.5% of the nodes are active for a given input, it is unlikely that
    two unrelated inputs x and y will share many selected neurons. As a result, we
    can perform asynchronous gradient updates in parallel. The [HOGWILD paper](https://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.pdf)
    shows that this idea is both theoretically sound and practical for gradient descent.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对给定输入仅有0.5%的节点处于激活状态，因此两个无关的输入 x 和 y 共享许多选择的神经元的可能性较小。因此，我们可以并行执行异步梯度更新。[HOGWILD
    论文](https://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.pdf)表明，这个想法在理论上是合理的，并且对梯度下降是实用的。
- en: To summarize, the SLIDE algorithm requires orders of magnitude fewer arithmetic
    operations on each input. When combined with asynchronous data-parallel gradient
    updates, we have a highly efficient training process. In contrast, standard backpropagation
    on a GPU performs a costly arithmetic operation. GPU parallelization is limited
    to the scaling up matrix multiplications rather than data-parallel processes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，SLIDE 算法在每个输入上需要的算术操作数量减少了几个数量级。当结合异步数据并行梯度更新时，我们有了一个高效的训练过程。相比之下，标准的 GPU
    反向传播需要昂贵的算术操作。GPU 并行化局限于扩展矩阵乘法，而不是数据并行处理。
- en: '** ![](../Images/6d4334dce2e09655789780a566421bed.png)**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '** ![](../Images/6d4334dce2e09655789780a566421bed.png)**'
- en: '***Figure 3:** Training time comparison of SLIDE (in red) against TF-GPU (in
    blue) and TF-CPU (in black). The x-axis is on the log scale. SLIDE was implemented
    in C++ and experiments were done on the same CPU as TF-CPU. TF-CPU used the most
    optimized Intel’s version (MKL-DNN).*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '***图3：** SLIDE（红色）与 TF-GPU（蓝色）和 TF-CPU（黑色）的训练时间比较。x轴为对数刻度。SLIDE 实现于 C++，实验在与
    TF-CPU 相同的 CPU 上进行。TF-CPU 使用了最优化的英特尔版本（MKL-DNN）。*'
- en: Comparison with TensorFlow Backpropagation on CPU and V100 GPU
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与 TensorFlow 在 CPU 和 V100 GPU 上的反向传播比较
- en: SLIDE was compared on large, fully connected neural networks with more than
    100 million parameters, commonly encountered in commercial recommendation engines.
    We highlight one of the training plots from the [paper](https://arxiv.org/abs/1903.03129).
    The code, along with benchmark scripts are publicly available on [Github](https://github.com/keroro824/HashingDeepLearning).  The
    data is publicly available [Amazon-670k dataset](https://www.kaggle.com/c/extreme-classification-amazon/overview) from
    Kaggle.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: SLIDE 在大型、完全连接的神经网络上进行了比较，这些网络通常具有超过1亿个参数，广泛应用于商业推荐引擎。我们突出了来自[论文](https://arxiv.org/abs/1903.03129)的一个训练图表。代码及基准测试脚本已公开发布在[Github](https://github.com/keroro824/HashingDeepLearning)上。数据可以从
    Kaggle 上的[Amazon-670k 数据集](https://www.kaggle.com/c/extreme-classification-amazon/overview)公开获取。
- en: Figure 3\. illustrates the complete progress of training with SLIDE (Red Curve)
    running on a 44-core Intel Xeon CPU (2 sockets of 22 cores each). SLIDE is compared
    with the most optimized implementations of backpropagation available via [TensorFlow](https://www.tensorflow.org/) on
    V100 GPU (TF-GPU) (Blue curve) as well as [Intel optimized TensorFlow](https://github.com/intel/mkl-dnn) on
    the same 44-core CPU (TF-CPU) (Black curve). The running time is on the log scale.
    TF-CPU is slow compared to TF-GPU, indicating the advantages of GPUs for backpropagation
    on this large workload. The exciting part is that a C++ implementation of the
    efficient SLIDE algorithm, on the same CPU, significantly outperforms the GPU
    acceleration. SLIDE converges 3.5x faster than TensorFlow on V100 GPU (1 hour
    vs. 3.5 hours).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 说明了在 44 核 Intel Xeon CPU（2 个 22 核的插槽）上运行 SLIDE（红色曲线）的完整训练进展。SLIDE 与通过[TensorFlow](https://www.tensorflow.org/)
    在 V100 GPU 上的最优化反向传播实现（蓝色曲线）以及在相同的 44 核 CPU 上的[Intel 优化 TensorFlow](https://github.com/intel/mkl-dnn)（黑色曲线）进行比较。运行时间为对数刻度。与
    TF-GPU 相比，TF-CPU 较慢，表明 GPU 在处理这一大型工作负载上的反向传播具有优势。令人兴奋的是，SLIDE 算法的 C++ 实现，在相同的
    CPU 上，显著超越了 GPU 加速。SLIDE 的收敛速度比 TensorFlow 在 V100 GPU 上快 3.5 倍（1 小时对比 3.5 小时）。
- en: '**Bio:** [Anshumali Shrivastava](https://www.linkedin.com/in/shrivastava-anshumali-58098b6/) ([@Anshumali_](https://twitter.com/Anshumali_))
    is an assistant professor in the computer science department at Rice University. 
    His broad research interests include randomized algorithms for large-scale machine
    learning. In 2018, Science news named him one of the Top-10 scientists under 40
    to watch.  He is a recipient of the National Science Foundation CAREER Award,
    a Young Investigator Award from the Air Force Office of Scientific Research, and
    a machine learning research award from Amazon. He has won numerous paper awards,
    including Best Paper Award at NIPS 2014 and Most Reproducible Paper Award at SIGMOD
    2019\. His work has been featured in several media outlets including the New York
    Times, IEEE Spectrum, Science News, Engadget, and ArsTechnica.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：** [Anshumali Shrivastava](https://www.linkedin.com/in/shrivastava-anshumali-58098b6/)
    ([@Anshumali_](https://twitter.com/Anshumali_)) 是莱斯大学计算机科学系的助理教授。他的广泛研究兴趣包括大规模机器学习的随机算法。2018年，《科学新闻》将他评选为值得关注的40岁以下十大科学家之一。他获得了国家科学基金会CAREER奖、美国空军科学研究办公室的年轻研究员奖以及亚马逊的机器学习研究奖。他赢得了许多论文奖项，包括2014年NIPS最佳论文奖和2019年SIGMOD最具可重复性论文奖。他的工作曾在包括《纽约时报》、IEEE
    Spectrum、《科学新闻》、Engadget和ArsTechnica等多个媒体上报道。'
- en: '**Related:**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Here’s how you can accelerate your Data Science on GPU](https://www.kdnuggets.com/2019/07/accelerate-data-science-on-gpu.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何加速您的GPU数据科学](https://www.kdnuggets.com/2019/07/accelerate-data-science-on-gpu.html)'
- en: '[Deep Neural Networks](https://www.kdnuggets.com/2020/02/deep-neural-networks.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度神经网络](https://www.kdnuggets.com/2020/02/deep-neural-networks.html)'
- en: '[Basics of GPU Computing for Data Scientists](https://www.kdnuggets.com/2016/04/basics-gpu-computing-data-scientists.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学家的GPU计算基础](https://www.kdnuggets.com/2016/04/basics-gpu-computing-data-scientists.html)'
- en: '* * *'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您组织中的IT工作'
- en: '* * *'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Building a GPU Machine vs. Using the GPU Cloud](https://www.kdnuggets.com/building-a-gpu-machine-vs-using-the-gpu-cloud)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[建立GPU机器与使用GPU云](https://www.kdnuggets.com/building-a-gpu-machine-vs-using-the-gpu-cloud)'
- en: '[Does the Random Forest Algorithm Need Normalization?](https://www.kdnuggets.com/2022/07/random-forest-algorithm-need-normalization.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机森林算法需要标准化吗？](https://www.kdnuggets.com/2022/07/random-forest-algorithm-need-normalization.html)'
- en: '[What is K-Means Clustering and How Does its Algorithm Work?](https://www.kdnuggets.com/2023/05/kmeans-clustering-algorithm-work.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[K均值聚类是什么，它的算法是如何工作的？](https://www.kdnuggets.com/2023/05/kmeans-clustering-algorithm-work.html)'
- en: '[Machine learning does not produce value for my business. Why?](https://www.kdnuggets.com/2021/12/machine-learning-produce-value-business.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习未能为我的业务创造价值。为什么？](https://www.kdnuggets.com/2021/12/machine-learning-produce-value-business.html)'
- en: '[Naïve Bayes Algorithm: Everything You Need to Know](https://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[朴素贝叶斯算法：你需要知道的一切](https://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html)'
- en: '[KDnuggets News, April 13: Python Libraries Data Scientists Should…](https://www.kdnuggets.com/2022/n15.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets新闻，4月13日：数据科学家应该了解的Python库…](https://www.kdnuggets.com/2022/n15.html)'
