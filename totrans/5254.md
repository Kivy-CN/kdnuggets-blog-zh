# PySpark SQL 备忘单：Python 大数据

> 原文：[https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html/2](https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html/2)

### 过滤

过滤你的 Spark DataFrame 非常简单：你在 DataFrame 上使用 filter() 方法。务必传递你要过滤的条件！

![PySpark 备忘单](../Images/223c3cc3091b988d6c36c70103a7dfd0.png)

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织进行 IT 工作

* * *

### 排序

对 Spark DataFrame 进行排序也很简单：使用 sort() 和 orderBy() 方法，并结合 collect() 检索排序后的 DataFrame。

![PySpark 备忘单](../Images/90419b84e9129e991b74bd6504cd8123.png)

**记住：** 虽然 show() 将前 n 行打印到控制台，collect() 将所有记录作为 [Row](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.Row) 返回。请确保仅在处理小 DataFrame 时使用后者，正如你刚刚阅读的：使用此方法将确保*所有*记录都被返回，并将所有记录从执行器移回到驱动程序。

### 缺失值与替换值

要处理缺失值，你可以替换、填充或删除它们 - 你可以使用 replace()、fill() 和 drop() 方法来完成。

**注意：** replace() 方法需要两个参数，即你要替换的值和你希望用来替换原始值的值。

![PySpark 备忘单](../Images/91c90e62240d7b7ad8f2856aaede2e8a.png)

### 重新分区

重新分区是你会时不时遇到的操作。你可以使用此方法来增加或减少 DataFrame 中的分区数量。然而，你可能要考虑你执行此操作的方式。考虑以下示例：

![PySpark 备忘单](../Images/db874543f832e3f5e449d323028f8813.png)

你会发现 repartition() 和 coalesce() 方法都用于重新分区 DataFrame。

**注意**，尽管 repartition() 方法通过完全数据洗牌创建相等大小的数据分区，coalesce() 方法通过合并现有分区避免了完全洗牌。

### 程序化运行 SQL 查询

![PySpark 备忘单](../Images/fb906355148f1cd2844876634560e38b.png)

**注册 DataFrames 作为视图**

你将 Spark DataFrames 注册为视图，以便可以使用“真实” SQL 查询（而不是你在本文中看到的带有方法的变体）。

使用“真实” SQL 查询 Spark DataFrames 的第一步当然是将 DataFrames 注册为视图。在进行此操作时，重要的是要记住项目的范围 - 正如名称所示，createTempView() 创建一个临时视图，而 createGlobalTempView() 创建一个全局临时视图。后者在所有会话中共享，并在 Spark 应用程序终止之前保持有效，而前者则是会话范围的，如果创建它的会话终止，它将消失。

注意，你还可以使用 createOrReplaceTempView() 来创建一个临时视图（如果没有）或替换一个已有的视图。

**查询视图**

现在你已经创建了一些视图，如上例中的“people”和“customer”视图，你可以在 SparkSession 上使用 sql() 函数以编程方式运行 SQL 查询 - 此操作的结果将是另一个 DataFrame，如上例所示。

### 输出

最后，需要考虑输出：你的分析总会有一些结果，你会想将这些结果保存到另一个文件、Pandas DataFrame 等。这一部分将涵盖你可以使用的一些选项。

![PySpark 备忘单](../Images/fe06a19dff8edd82cc35474174d8623e.png)

**数据结构**

如上所述，你可以将 Spark DataFrame 转换为 RDD、Pandas DataFrame、字符串 RDD 等。虽然将 Spark DataFrame 转换为其他数据结构的可能性很多，但图像中列出的三种结构可能是最常用的。

**写入与保存文件**

写入和保存分析结果在进行数据科学时至关重要 - 当然，当你处理大数据时也是如此。你会想将结果保存到 JSON 或 parquet 文件中。

记住：parquet 文件有一个列式文件格式，适用于 Hadoop 生态系统中的任何项目。不论你使用哪个数据处理框架、数据模型或编程语言！Parquet 文件提供高效的数据压缩和编码方案，具有增强的性能来处理复杂的大数据。而 JSON 或 JavaScript 对象表示法文件则具有一种开放标准的文件格式，使用人类可读的文本来传输由属性-值对和数组数据类型（或任何其他可序列化值）组成的数据对象。

### 停止 SparkSession

在离开之前，别忘了停止你的 SparkSession！记住你已经将 SparkSession 分配给了 spark 变量 - 你将需要这个确切的变量和 stop() 方法来正确停止你的 SparkSession。

![PySpark 备忘单](../Images/20516c91f72dfe0e86f6b1a3d0c02d4f.png)

这只是你与 Spark SQL 旅程的开始。我们希望你在继续学习这个令人兴奋的技术、其应用及其可能性时，别忘了随身携带你的 [备忘单](https://www.datacamp.com/community/blog/bokeh-cheat-sheet-python)。

[**DataCamp**](https://www.datacamp.com/) 是一个在线互动教育平台，专注于为数据科学构建最佳学习体验。我们关于 [R](https://www.datacamp.com/courses?learn=r_programming)、[Python](https://www.datacamp.com/courses?learn=python_programming)、[SQL](https://www.datacamp.com/courses/tech:sql) 和 [数据科学](https://www.datacamp.com/courses) 的课程围绕特定主题构建，结合了视频教学和浏览器内编码挑战，让你通过实践学习。[你可以随时免费开始每个课程](https://www.datacamp.com/courses)，无论何时何地。

[**Karlijn Willems**](https://www.linkedin.com/in/karlijnwillems) 是一名数据科学记者，为 [DataCamp 社区](https://www.datacamp.com/community/authors/karlijn-willems) 撰写文章，专注于数据科学教育、最新新闻和热门趋势。她拥有文学和语言学及信息管理的学位。

**相关**：

+   [Keras 备忘单：Python 中的深度学习](/2017/09/datacamp-keras-cheat-sheet-deep-learning-python.html)

+   [Pandas 备忘单：Python 中的数据科学和数据整理](/2017/01/pandas-cheat-sheet.html)

+   [Bokeh 备忘单：Python 中的数据可视化](/2017/03/bokeh-cheat-sheet.html)

### 更多相关主题

+   [PySpark 数据科学](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)

+   [使用 Pandera 对 PySpark 应用进行数据验证](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)

+   [使用 Python 的数据清洗备忘单](https://www.kdnuggets.com/2023/02/data-cleaning-python-cheat-sheet.html)

+   [构建生成式 AI 应用的最佳 Python 工具备忘单](https://www.kdnuggets.com/2023/08/best-python-tools-generative-ai-cheat-sheet.html)

+   [Python 控制流备忘单](https://www.kdnuggets.com/2022/11/python-control-flow-cheatsheet.html)

+   [KDnuggets 新闻，7月5日：一个糟糕的数据科学项目 • 10 AI…](https://www.kdnuggets.com/2023/n24.html)
