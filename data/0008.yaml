- en: 'Diffusion and Denoising: Explaining Text-to-Image Generative AI'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩散与去噪：解释文本到图像生成 AI
- en: 原文：[https://www.kdnuggets.com/diffusion-and-denoising-explaining-text-to-image-generative-ai](https://www.kdnuggets.com/diffusion-and-denoising-explaining-text-to-image-generative-ai)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/diffusion-and-denoising-explaining-text-to-image-generative-ai](https://www.kdnuggets.com/diffusion-and-denoising-explaining-text-to-image-generative-ai)
- en: '![Diffusion and Denoising: Explaining Text-to-Image Generative AI](../Images/ddf316fd3e2893f67a9d64b6401c6fef.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![扩散与去噪：解释文本到图像生成 AI](../Images/ddf316fd3e2893f67a9d64b6401c6fef.png)'
- en: '## The Concept of Diffusion'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '## 扩散的概念'
- en: Denoising diffusion models are trained to pull patterns out of noise, to generate
    a desirable image. The training process involves showing model examples of images
    (or other data) with varying levels of noise determined according to a noise scheduling
    algorithm, intending to predict what parts of the data are noise. If successful,
    the noise prediction model will be able to gradually build up a realistic-looking
    image from pure noise, subtracting increments of noise from the image at each
    time step.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪扩散模型经过训练以从噪声中提取模式，从而生成期望的图像。训练过程包括向模型展示具有不同噪声水平的图像（或其他数据），噪声水平是根据噪声调度算法确定的，目的是预测数据中的哪些部分是噪声。如果成功，噪声预测模型将能够从纯噪声中逐渐构建出逼真的图像，在每一步中从图像中减去噪声的增量。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![diffusion and denoising process](../Images/e53be5f6aad31b14b122d1a17b57103c.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![扩散和去噪过程](../Images/e53be5f6aad31b14b122d1a17b57103c.png)'
- en: Unlike the image at the top of this section, modern diffusion models don’t predict
    noise from an image with added noise, at least not directly. Instead, they predict
    noise in a latent space representation of the image. Latent space represents images
    in a compressed set of numerical features, the output of an encoding module from
    a variational autoencoder, or [VAE](https://en.wikipedia.org/wiki/Variational_autoencoder).
    This trick put the “latent” in [latent diffusion](https://arxiv.org/pdf/2112.10752.pdf),
    and greatly reduced the time and computational requirements for generating images.
    As reported by the paper authors, latent diffusion speeds up inference by at least
    ~2.7X over direct diffusion and trains about three times faster.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与本节顶部的图像不同，现代扩散模型不会直接从添加噪声的图像中预测噪声。相反，它们在图像的潜在空间表示中预测噪声。潜在空间以压缩的数值特征集表示图像，这是变分自编码器或
    [VAE](https://en.wikipedia.org/wiki/Variational_autoencoder) 编码模块的输出。这个技巧使得“潜在”被包含在
    [潜在扩散](https://arxiv.org/pdf/2112.10752.pdf) 中，并大大减少了生成图像的时间和计算需求。正如论文作者所报告的，潜在扩散使得推理速度比直接扩散快至少约
    2.7 倍，并且训练速度快约三倍。
- en: People working with latent diffusion often talk of using a “diffusion model,”
    but in fact, the diffusion process employs several modules. As in the diagram
    above, a diffusion pipeline for text-to-image workflows typically includes a text
    embedding model (and its tokenizer), a denoise prediction/diffusion model, and
    an image decoder. Another important part of latent diffusion is the scheduler,
    which determines how the noise is scaled and updated over a series of “time steps”
    (a series of iterative updates that gradually remove noise from latent space).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用潜在扩散的人们经常谈到使用“扩散模型”，但实际上，扩散过程涉及几个模块。如上图所示，文本到图像工作流程的扩散管道通常包括一个文本嵌入模型（及其分词器）、一个去噪预测/扩散模型和一个图像解码器。潜在扩散的另一个重要部分是调度器，它决定了噪声如何在一系列“时间步骤”（一系列逐步更新，逐渐从潜在空间中去除噪声）中被缩放和更新。
- en: '![latent diffusion model architecture diagram](../Images/e3eb94eb011d1bc8106e89dc07afa10a.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![潜在扩散模型架构图](../Images/e3eb94eb011d1bc8106e89dc07afa10a.png)'
- en: Latent Diffusion Code Example
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 潜在扩散代码示例
- en: We’ll use CompVis/latent-diffusion-v1-4 for most of our examples. Text embedding
    is handled by a [CLIPTextModel and CLIPTokenizer](https://en.wikipedia.org/wiki/DALL-E#Contrastive_Language-Image_Pre-training_(CLIP).
    Noise prediction uses a ‘[U-Net](https://en.wikipedia.org/wiki/U-Net),’ a type
    of image-to-image model that originally gained traction as a model for applications
    in biomedical images (especially segmentation). To generate images from denoised
    latent arrays, the pipeline uses a variational autoencoder ([VAE](https://en.wikipedia.org/wiki/Variational_autoencoder))
    for image decoding, turning those arrays into images.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在大多数示例中使用 CompVis/latent-diffusion-v1-4。文本嵌入由一个[CLIPTextModel 和 CLIPTokenizer](https://en.wikipedia.org/wiki/DALL-E#Contrastive_Language-Image_Pre-training_(CLIP))
    处理。噪声预测使用一个‘[U-Net](https://en.wikipedia.org/wiki/U-Net)’，这是一种图像到图像的模型，最初在生物医学图像（特别是分割）中获得了广泛应用。为了从去噪的潜在数组生成图像，该管道使用变分自动编码器
    ([VAE](https://en.wikipedia.org/wiki/Variational_autoencoder)) 进行图像解码，将这些数组转换成图像。
- en: We’ll start by building our version of this pipeline from HuggingFace components.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从构建我们自己版本的 HuggingFace 组件管道开始。
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Make sure to check[ pytorch.org](https://pytorch.org/get-started/locally/) to
    ensure the right version for your system if you’re working locally. Our imports
    are relatively straightforward, and the code snippet below suffices for all the
    following demos.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在本地工作，确保检查[ pytorch.org](https://pytorch.org/get-started/locally/)以确保适合你系统的版本。我们的导入相对简单，下面的代码片段足以用于所有后续演示。
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now for the details. Start by defining image and diffusion parameters and a
    prompt.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进入细节。首先定义图像和扩散参数以及提示。
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Initialize your pseudorandom number generator with a seed of your choice for
    reproducing your results.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 用你选择的种子初始化伪随机数生成器，以便复现结果。
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now we can initialize the text embedding model, autoencoder, a U-Net, and the
    time step scheduler.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以初始化文本嵌入模型、自动编码器、U-Net 和时间步调度器。
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Encoding the text prompt as an embedding requires first tokenizing the string
    input. Tokenization replaces characters with integer codes corresponding to a
    vocabulary of semantic units, e.g. via byte pair encoding ([BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding)).
    Our pipeline embeds a null prompt (no text) alongside the textual prompt for our
    image. This balances the diffusion process between the provided description and
    natural-appearing images in general. We’ll see how to change the relative weighting
    of these components later in this article.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本提示编码为嵌入需要首先对字符串输入进行分词。分词将字符替换为对应于语义单元词汇的整数代码，例如通过字节对编码 ([BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding))。我们的管道在图像的文本提示旁边嵌入了一个空提示（无文本）。这在提供的描述和一般自然出现的图像之间平衡了扩散过程。我们将在本文稍后看到如何改变这些组件的相对权重。
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We initialize latent space as random normal noise and scale it according to
    our diffusion time step scheduler.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将潜在空间初始化为随机正态噪声，并根据我们的扩散时间步调度器对其进行缩放。
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Everything is ready to go, and we can dive into the diffusion loop itself. We
    can keep track of images by sampling periodically throughout so we can see how
    noise is gradually decreased.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一切准备就绪，我们可以深入扩散循环本身。我们可以通过周期性采样来跟踪图像，从而查看噪声如何逐渐减少。
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: At the end of the diffusion process, we have a decent rendering of what you
    wanted to generate. Next, we’ll go over additional techniques for greater control.
    As we’ve already made our diffusion pipeline, we can use the streamlined diffusion
    pipeline from HuggingFace for the rest of our examples.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩散过程结束时，我们会得到一个不错的渲染结果。接下来，我们将讨论额外的技术以获得更大的控制力。由于我们已经创建了扩散管道，我们可以使用 HuggingFace
    提供的精简扩散管道来进行接下来的示例。
- en: Controlling the Diffusion Pipeline
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制扩散管道
- en: 'We’ll use a set of helper functions in this section:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这一部分使用一组辅助函数：
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We’ll start with the most well-known and straightforward application of diffusion
    models: image generation from textual prompts, known as text-to-image generation.
    The model we’ll use was released into the wild (of the Hugging Face Hub) by the
    academic[ ](https://ommer-lab.com/)lab that published the latent diffusion paper.
    Hugging Face coordinates workflows like latent diffusion via the convenient pipeline
    API. We want to define what device and what floating point to calculate based
    on if we have or do not have a GPU.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从扩散模型最著名且最直接的应用开始：从文本提示生成图像，即文本到图像生成。我们将使用的模型是由发布了潜在扩散论文的学术[实验室](https://ommer-lab.com/)发布到
    Hugging Face Hub 的。Hugging Face 通过方便的管道 API 协调像潜在扩散这样的工作流。我们需要根据是否拥有 GPU 来定义计算的设备和浮点数。
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Guidance Scale
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引导比例
- en: If you use a very unusual text prompt (very unlike those in the dataset), it’s
    possible to end up in a less-traveled part of latent space. The null prompt embedding
    provides a balance and combining the two according to guidance_scale allows you
    to trade off the specificity of your prompt against common image characteristics.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用非常不寻常的文本提示（与数据集中的提示大相径庭），可能会进入潜在空间的较少涉足部分。空提示嵌入提供了平衡，按照guidance_scale将两者结合起来可以在提示的具体性与常见图像特征之间进行权衡。
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Since we generated the prompt using the 9 guidance coefficients, you can plot
    the prompt and view how the diffusion developed. The default guidance coefficient
    is 0.75 so on the 7th image would be the default image output.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用 9 个引导系数生成了提示，你可以绘制提示并查看扩散是如何发展的。默认引导系数为 0.75，因此第 7 张图像将是默认图像输出。
- en: Negative Prompts
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负面提示
- en: Sometimes latent diffusion really “wants” to produce an image that doesn’t match
    your intentions. In these scenarios, you can use a negative prompt to push the
    diffusion process away from undesirable outputs. For example, we could use a negative
    prompt to make our Martian astronaut diffusion outputs a little less human.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有时潜在扩散确实“想要”产生与您的意图不匹配的图像。在这些情况下，你可以使用负面提示将扩散过程推向不希望的输出。例如，我们可以使用负面提示来使我们的火星宇航员扩散输出看起来稍微少一些人性。
- en: '[PRE11]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You should receive outputs that follow your prompt while avoiding outputting
    the things described in your negative prompt.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该获得遵循提示的输出，同时避免输出负面提示中描述的内容。
- en: Image Variation
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像变体
- en: Text-to-image generation from scratch is not the only application for diffusion
    pipelines. Actually, diffusion is well-suited for image modification, starting
    from an initial image. We’ll use a slightly different pipeline and pre-trained
    model tuned for image-to-image diffusion.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始的文本到图像生成并不是扩散管道的唯一应用。实际上，扩散非常适合从初始图像开始进行图像修改。我们将使用一个稍有不同的管道和为图像到图像扩散调优的预训练模型。
- en: '[PRE12]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: One application of this approach is to generate variations on a theme. A concept
    artist might use this technique to quickly iterate different ideas for illustrating
    an exoplanet based on the latest research.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一种应用是生成主题的变体。概念艺术家可能会使用此技术快速迭代不同的外星行星插图创意，基于最新的研究。
- en: 'We’ll first download a public domain artist’s concept of planet 1e in the TRAPPIST
    system ([credit: NASA/JPL-Caltech](https://photojournal.jpl.nasa.gov/catalog/PIA22093)).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先下载一张公共领域艺术家的 TRAPPIST 系统中 1e 行星的概念图（[来源：NASA/JPL-Caltech](https://photojournal.jpl.nasa.gov/catalog/PIA22093)）。
- en: Then, after downscaling to remove details, we’ll use a diffusion pipeline to
    make several different versions of the exoplanet TRAPPIST-1e.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在降采样以去除细节后，我们将使用扩散流程制作几种不同版本的外星行星 TRAPPIST-1e。
- en: '[PRE13]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![diffusion image variation test](../Images/23eb7c1af72b1ce4acf0e8bc46b4e436.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![扩散图像变体测试](../Images/23eb7c1af72b1ce4acf0e8bc46b4e436.png)'
- en: By feeding the model an example initial image, we can generate similar images.
    You can also use a text-guided image-to-image pipeline to change the style of
    an image by increasing the guidance, adding negative prompts and more such as
    “non-realistic” or “watercolor” or “paper sketch.” Your mile may vary and adjusting
    your prompts will be the easiest way to find the right image you want to create.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向模型提供一个初始图像示例，我们可以生成类似的图像。你还可以使用文本引导的图像到图像流程，通过增加引导、添加负面提示以及更多如“非现实主义”或“水彩”或“纸上素描”等来改变图像的风格。你的结果可能会有所不同，调整提示将是找到你想创建的正确图像的最简单方法。
- en: Conclusions
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Despite the discourse behind diffusion systems and imitating human generated
    art, diffusion models have other more impactful purposes. It has [been](https://github.com/microsoft/foldingdiff)
    [applied to](https://www.pnas.org/doi/10.1073/pnas.0910390107) [protein folding
    prediction](https://arxiv.org/abs/2305.04120) for protein design and drug development.
    Text-to-video is also an [active area](https://arxiv.org/abs/2311.15127) of [research](https://arxiv.org/abs/2211.13221) and
    is offered by several companies (e.g. [Stability AI](https://stability.ai/stable-video),
    [Google](https://imagen.research.google/video/)). Diffusion is also an [emerging
    approach](https://arxiv.org/abs/2305.04120) for [text-to-speech](https://arxiv.org/abs/2304.11750) applications.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管关于扩散系统和模仿人类生成艺术的讨论依然存在，但扩散模型还有其他更具影响力的用途。它已被 [应用于](https://github.com/microsoft/foldingdiff)
    [蛋白质折叠预测](https://www.pnas.org/doi/10.1073/pnas.0910390107)，用于蛋白质设计和药物开发。文本到视频也是一个
    [活跃的领域](https://arxiv.org/abs/2311.15127) 的 [研究](https://arxiv.org/abs/2211.13221)，并由多个公司提供（例如
    [Stability AI](https://stability.ai/stable-video)、[Google](https://imagen.research.google/video/)）。扩散也是一个
    [新兴的方法](https://arxiv.org/abs/2305.04120) 用于 [文本到语音](https://arxiv.org/abs/2304.11750)
    应用。
- en: It’s clear that the diffusion process is taking a central role in the evolution
    of AI and the interaction of technology with the global human environment. While
    the intricacies of copyright, other intellectual property laws, and the impact
    on human art and science are evident in both positive and negative ways. But what
    is truly a positive is the unprecedented capability AI has to understand language
    and generate images. It was AlexNet that had computers analyze an image and output
    text, and only now computers can analyze textual prompts and output coherent images.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，扩散过程在人工智能的演变以及技术与全球人类环境的互动中正发挥着核心作用。虽然版权、其他知识产权法律的复杂性以及对人类艺术和科学的影响既有积极也有消极的表现，但真正积极的是人工智能前所未有的语言理解和图像生成能力。曾经是
    AlexNet 使计算机分析图像并输出文本，而现在计算机可以分析文本提示并输出连贯的图像。
- en: '[Original](https://www.exxactcorp.com/blog/deep-learning/diffusion-and-denoising-explaining-text-to-image-generative-ai).
    Republished with permission.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.exxactcorp.com/blog/deep-learning/diffusion-and-denoising-explaining-text-to-image-generative-ai)。经许可转载。'
- en: '**[Kevin Vu](https://blog.exxactcorp.com/)** manages [Exxact Corp blog](https://blog.exxactcorp.com/)
    and works with many of its talented authors who write about different aspects
    of Deep Learning.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Kevin Vu](https://blog.exxactcorp.com/)** 负责管理 [Exxact Corp 博客](https://blog.exxactcorp.com/)，并与许多才华横溢的作者合作，这些作者写作内容涉及深度学习的不同方面。'
- en: More On This Topic
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关话题
- en: '[Generative AI Playground: Text-to-Image Stable Diffusion with…](https://www.kdnuggets.com/2024/02/intel-generative-ai-playground-text-to-image-stable-diffusion)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[生成型人工智能游乐场：文本到图像的稳定扩散…](https://www.kdnuggets.com/2024/02/intel-generative-ai-playground-text-to-image-stable-diffusion)'
- en: '[Stable Diffusion: Basic Intuition Behind Generative AI](https://www.kdnuggets.com/2023/06/stable-diffusion-basic-intuition-behind-generative-ai.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[稳定扩散：生成型人工智能的基本直觉](https://www.kdnuggets.com/2023/06/stable-diffusion-basic-intuition-behind-generative-ai.html)'
- en: '[Become an AI Artist Using Phraser and Stable Diffusion](https://www.kdnuggets.com/2022/09/become-ai-artist-phraser-stable-diffusion.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Phraser 和稳定扩散成为 AI 艺术家](https://www.kdnuggets.com/2022/09/become-ai-artist-phraser-stable-diffusion.html)'
- en: '[3 Ways to Generate Hyper-Realistic Faces Using Stable Diffusion](https://www.kdnuggets.com/3-ways-to-generate-hyper-realistic-faces-using-stable-diffusion)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用稳定扩散生成超现实面孔的三种方法](https://www.kdnuggets.com/3-ways-to-generate-hyper-realistic-faces-using-stable-diffusion)'
- en: '[Top 7 Diffusion-Based Applications with Demos](https://www.kdnuggets.com/2022/10/top-7-diffusionbased-applications-demos.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7 大基于扩散的应用及演示](https://www.kdnuggets.com/2022/10/top-7-diffusionbased-applications-demos.html)'
- en: '[Explaining Explainable AI for Conversations](https://www.kdnuggets.com/2022/10/explaining-explainable-ai-conversations.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[解释可解释性人工智能在对话中的应用](https://www.kdnuggets.com/2022/10/explaining-explainable-ai-conversations.html)'
