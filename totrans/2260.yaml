- en: 'Making Predictions: A Beginner’s Guide to Linear Regression in Python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/06/making-predictions-beginner-guide-linear-regression-python.html](https://www.kdnuggets.com/2023/06/making-predictions-beginner-guide-linear-regression-python.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Making Predictions: A Beginner''s Guide to Linear Regression in Python](../Images/19b2a9deec2927d6c17364b07dce8b35.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression is the most popular and the first machine learning algorithm
    that a data scientist learns while starting their data science career. It is the
    most important supervised learning algorithm as it sets up the building blocks
    for all the other advanced Machine Learning algorithms. That is why we need to
    learn and understand this algorithm very clearly.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will cover Linear Regression from scratch, its mathematical
    & geometric intuition, and its Python implementation. The only prerequisite is
    your willingness to learn and the basic knowledge of Python syntax. Let’s get
    started.
  prefs: []
  type: TYPE_NORMAL
- en: What is Linear Regression?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear Regression is a [Supervised Machine Learning](https://www.javatpoint.com/supervised-machine-learning)
    algorithm that is used to solve Regression problems. Regression models are used
    to predict a continuous output based on some other factors. E.g., Predicting the
    next month’s stock price of an organization by considering the profit margins,
    total market cap, yearly growth, etc. Linear Regression can also be used in applications
    like predicting the weather, stock prices, sales targets, etc.
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, Linear Regression, it develops a linear relationship between
    two variables. The algorithm finds the best straight line (`y=mx+c`) that can
    predict the dependent variable(y) based on the independent variables(x). The predicted
    variable is called the dependent or target variable, and the variables used to
    predict are called the independent variables or features. If only one independent
    variable is used, then it is called Univariate Linear Regression. Otherwise, it
    is called Multivariate Linear Regression.
  prefs: []
  type: TYPE_NORMAL
- en: To simplify this article, we will only take one independent variable(x) so that
    we can easily visualize it on a 2D plane. In the next section, we will discuss
    its mathematical intuition.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical Intuition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will understand Linear Regression’s geometry and its mathematics. Suppose
    we have a set of sample pairs of X and Y values,
  prefs: []
  type: TYPE_NORMAL
- en: '![Making Predictions: A Beginner''s Guide to Linear Regression in Python](../Images/dcd0e4441bd19099e5d371bf93cd1983.png)'
  prefs: []
  type: TYPE_IMG
- en: We have to use these values to learn a function so that if we give it an unknown
    (x), it can predict a (y) based on the learnings. In regression, many functions
    can be used for prediction, but the linear function is the simplest among all.
  prefs: []
  type: TYPE_NORMAL
- en: '![Making Predictions: A Beginner''s Guide to Linear Regression in Python](../Images/94f7082744b5dfb894054fec9bacea2d.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig.1 Sample Data Points | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The main aim of this algorithm is to find the best-fit line among these data
    points, as indicated in the above figure, which gives the least residual error.
    The residual error is the difference between predicted and actual value.
  prefs: []
  type: TYPE_NORMAL
- en: Assumptions for Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before moving forward, we need to discuss some assumptions of Linear Regression
    that we need to be taken care of to get accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Linearity:** Linearity means that the independent and dependent variables
    must follow a linear relationship. Otherwise, it will be challenging to obtain
    a straight line. Also, the data points must be independent of each other, i.e.
    the data of one observation does not depend on the data of another observation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Homoscedasticity:** It states that the variance of the residual errors must
    be constant. It means that the variance of the error terms should be constant
    and does not change even if the values of the independent variable change. Also,
    the errors in the model must follow a Normal Distribution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**No Multicollinearity:** Multicollinearity means there is a correlation between
    the independent variables. So in Linear Regression, the independent variables
    must not be correlated with each other.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hypothesis Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will hypothesise that a linear relationship will exist between our dependent
    variable(Y) and the independent variable(X). We can represent the linear relationship
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![Making Predictions: A Beginner''s Guide to Linear Regression in Python](../Images/78df40dbe8fbf16f51f3c6c37aab6045.png)'
  prefs: []
  type: TYPE_IMG
- en: We can observe that the straight line depends on the parameters Θ0 and Θ1\.
    So to get the best-fit line, we need to tune or adjust these parameters. These
    are also called the weights of the model. And to calculate these values, we will
    use the loss function, also known as the cost function. It calculates the [**Mean
    Squared Error**](https://en.wikipedia.org/wiki/Mean_squared_error) between the
    predicted and actual values. Our goal is to minimize this cost function. The values
    of Θ0 and Θ1, in which the cost function is minimized, will form our best-fit
    line. The cost function is represented by (J)
  prefs: []
  type: TYPE_NORMAL
- en: '![Making Predictions: A Beginner''s Guide to Linear Regression in Python](../Images/e299133c6521fedec85816679743769b.png)'
  prefs: []
  type: TYPE_IMG
- en: Where,
  prefs: []
  type: TYPE_NORMAL
- en: N is the total number of samples
  prefs: []
  type: TYPE_NORMAL
- en: The squared error function is chosen to handle the negative values (i.e. if
    the predicted value is less than the actual value). Also, the function is divided
    by 2 to ease the differentiation process.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer (Gradient Descent)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Optimizer](https://www.datarobot.com/blog/introduction-to-optimizers/#:~:text=In%20simpler%20terms%2C%20optimizers%20shape,component%20of%20AI%2FML%20governance.)
    is an algorithm that minimises the MSE by iteratively updating the model''s attributes
    like weights or learning rate to achieve the best-fit line. In Linear Regression,
    the Gradient Descent algorithm is used to minimize the cost function by updating
    the values of Θ0 and Θ1.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Making Predictions: A Beginner''s Guide to Linear Regression in Python](../Images/92281635311f1ae1288145354f90dabb.png)'
  prefs: []
  type: TYPE_IMG
- en: is a hyperparameter which is called the learning rate. It determines how much
    our weights are adjusted with respect to the gradient loss. The value of the learning
    rate should be optimal, not too high or too low. If it is too high, it is difficult
    for the model to converge at the global minimum, and if it is too small, it takes
    longer to converge.
  prefs: []
  type: TYPE_NORMAL
- en: We will plot a graph between the Cost function and the Weights to find the optimum
    Θ0 and Θ1.
  prefs: []
  type: TYPE_NORMAL
- en: '![Making Predictions: A Beginner''s Guide to Linear Regression in Python](../Images/1f2d0e2a7b6588b201cb9ed9bccc2d80.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig.2 Gradient Descent Curve | Image by [GeeksForGeeks](https://www.geeksforgeeks.org/ml-linear-regression/)
  prefs: []
  type: TYPE_NORMAL
- en: Initially, we will assign random values to Θ0 and Θ1, then calculate the cost
    function and gradient. For a negative gradient(a derivative of the cost function),
    we need to move in the direction of increasing Θ1 in order to reach the minima.
    And for a positive gradient, we must move backwards to reach the global minima.
    We aim to find a point at which the gradient almost equals zero. At this point,
    the value of the cost function is minimum.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you have understood the working and mathematics of Linear Regression.
    The following section will see how to implement it from scratch using Python on
    a sample dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression Python Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to implement the Linear Regression algorithm
    from scratch only using fundamental libraries like Numpy, Pandas, and Matplotlib.
    We will implement Univariate Linear Regression, which contains only one dependent
    and one independent variable.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset we will use contains about 700 pairs of (X, Y) in which X is the
    independent variable and Y is the dependent variable.  Ashish Jangra contributes
    this dataset, and you can download it from [here](https://github.com/AshishJangra27/Machine-Learning-with-Python-GFG/tree/main/Linear%20Regression).
  prefs: []
  type: TYPE_NORMAL
- en: Importing Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Pandas reads the CSV file and gets the dataframe, while Numpy performs basic
    mathematics and statistics operations. Matplotlib is responsible for plotting
    graphs and curves.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: First, we will get the dataframe `df` and then drop the null values. After that,
    we will split the data into training and testing `x_train`, `y_train`, `x_test`
    and `y_test`.
  prefs: []
  type: TYPE_NORMAL
- en: Building Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We have created a class named `LinearRegression()` in which all the required
    functions are built.
  prefs: []
  type: TYPE_NORMAL
- en: '`__init__` : It is a constructor and will initialize the weights with random
    values when the object of this class is created.'
  prefs: []
  type: TYPE_NORMAL
- en: '`forward_propogation()`: This function will find the predicted output using
    the equation of the straight line.'
  prefs: []
  type: TYPE_NORMAL
- en: '`cost()`: This will calculate the residual error associated with the predicted
    values.'
  prefs: []
  type: TYPE_NORMAL
- en: '`finding_derivatives()`: This function calculates the derivative of the weights,
    which can later be used to update the weights for minimum errors.'
  prefs: []
  type: TYPE_NORMAL
- en: '`train()`: This function will take input from the training data, learning rate
    and the total number of iterations. It will update the weights using back-propagation
    until the specified number of iterations. At last, it will return the weights
    of the best-fit line.'
  prefs: []
  type: TYPE_NORMAL
- en: Training the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can observe that in the 1st iteration, the loss is maximum, and in the subsequent
    iterations, this loss decreases and reaches its minimum value at the end of the
    30th iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '![Making Predictions: A Beginner''s Guide to Linear Regression in Python](../Images/81b74de892ac2c4f5b78600ab589b51b.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig.3 Finding Best-fit Line | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The above gif indicates how the straight line reaches its best-fit line after
    completing the 30th iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Final Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This is the final equation of the best-fit line.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Making Predictions: A Beginner''s Guide to Linear Regression in Python](../Images/9ed8aee0bcefece00a4589665cfb84ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig.4 Actual vs Predicted Output | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The above plot shows the best-fit line (orange) and actual values (blue `+`)
    of the test set. You can also tune the hyperparameters, like the learning rate
    or the number of iterations, to increase the accuracy and precision.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression (Using Sklearn Library)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we have seen how to implement Univariate Linear Regression
    from scratch. But there is also a built-in library by sklearn that can be directly
    used to implement Linear Regression. Let’s briefly discuss how we can do it.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the same dataset, but if you want, you can use a different one also.
    You need to import two extra libraries as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Loading Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Earlier, we had to perform the train-test split using the numpy library manually.
    But now we can use sklearn's `train_test_split()` to directly divide the data
    into the training and testing sets just by specifying the testing size.
  prefs: []
  type: TYPE_NORMAL
- en: Model Training and Predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, we don’t have to write the codes for forward propagation, backward propagation,
    cost function, etc. We can now directly use the `LinearRegression()` class and
    train the model on the input data. Below is the plot obtained on the test data
    from the trained model. The results are similar to when we implemented the algorithm
    on our own.
  prefs: []
  type: TYPE_NORMAL
- en: '![Making Predictions: A Beginner''s Guide to Linear Regression in Python](../Images/e1d697b491adf1c019ea0b03c5a1671f.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig.5 Sklearn Model Output | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GeeksForGeeks:  [ML Linear Regression](https://www.geeksforgeeks.org/ml-linear-regression/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wrapping it Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google Colab Link for the Complete Code - [Linear Regression Tutorial Code](https://colab.research.google.com/drive/1MnLDoDRZPezWMAIkD-QtpviUXc5E9iRc?usp=sharing)
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we have thoroughly discussed what Linear Regression is, its
    mathematical intuition and its Python implementation both from scratch and from
    using sklearn’s library. This algorithm is straightforward and intuitive, so it
    helps beginners to lay a solid foundation as well as helps to gain practical coding
    skills to make accurate predictions using Python.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Aryan Garg](https://www.linkedin.com/in/aryan-garg-1bbb791a3/)** is a B.Tech.
    Electrical Engineering student, currently in the final year of his undergrad.
    His interest lies in the field of Web Development and Machine Learning. He have
    pursued this interest and am eager to work more in these directions.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Comparing Linear and Logistic Regression](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3 Reasons Why You Should Use Linear Regression Models Instead of…](https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear vs Logistic Regression: A Succinct Explanation](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News 22:n12, March 23: Best Data Science Books for…](https://www.kdnuggets.com/2022/n12.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear Regression for Data Science](https://www.kdnuggets.com/2022/07/linear-regression-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear Regression from Scratch with NumPy](https://www.kdnuggets.com/linear-regression-from-scratch-with-numpy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
