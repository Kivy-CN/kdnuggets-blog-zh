- en: What is the difference between Bagging and Boosting?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/11/difference-bagging-boosting.html](https://www.kdnuggets.com/2017/11/difference-bagging-boosting.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png)[comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By xristica, [Quantdare](https://quantdare.com/)**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b1eafba9a76281b988e0532c98b4d3e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Bagging and Boosting are similar in that they are both **ensemble techniques**,
    where a set of weak learners are combined to create a strong learner that obtains
    better performance than a single one. So, let’s start from the beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: What is an ensemble method?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ensemble is a Machine Learning concept in which the idea is to train **multiple
    models** using the same learning algorithm. The ensembles take part in a bigger
    group of methods, called **multiclassifiers,** where a set of hundreds or thousands
    of learners with a common objective are fused together to solve the problem.
  prefs: []
  type: TYPE_NORMAL
- en: The second group of multiclassifiers contain **the hybrid methods**. They use
    a set of learners too, but they can be trained using different learning techniques.
    Stacking is the most well-known. If you want to learn more about Stacking, you
    can read my previous post, “[Dream team combining classifiers](http://quantdare.com/dream-team-combining-classifiers-2/)“.
  prefs: []
  type: TYPE_NORMAL
- en: The main causes of error in learning are due to **noise, bias and variance**.
    Ensemble helps to minimize these factors. These methods are designed to improve
    the stability and the accuracy of Machine Learning algorithms. Combinations of
    multiple classifiers decrease variance, especially in the case of unstable classifiers,
    and may produce a more reliable classification than a single classifier.
  prefs: []
  type: TYPE_NORMAL
- en: To use Bagging or Boosting you must select a base learner algorithm. For example,
    if we choose a classification tree, Bagging and Boosting would consist of a pool
    of trees as big as we want.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/921aafecdda40adb53dd2bdba7c5c20f.png)'
  prefs: []
  type: TYPE_IMG
- en: How do Bagging and Boosting get N learners?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bagging and Boosting get N learners by generating additional data in the training
    stage. N new training data sets are produced by **random sampling with replacement** from
    the original set. By sampling with replacement some observations may be repeated
    in each new training data set.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of Bagging, any element has the same probability to appear in a
    new data set. However, for Boosting the observations are weighted and therefore
    some of them will take part in the new sets more often:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c04ea35b2f2c64b7ac560c54c6b7abbe.png)'
  prefs: []
  type: TYPE_IMG
- en: These multiple sets are used to train the same learner algorithm and therefore
    different classifiers are produced.
  prefs: []
  type: TYPE_NORMAL
- en: Why are the data elements weighted?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'At this point, we begin to deal with the main difference between the two methods.
    While the training stage is parallel for Bagging (i.e., each model is built independently),
    Boosting builds the new learner in a sequential way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d54f29b1e2365a4d2704687ec12e5e62.png)'
  prefs: []
  type: TYPE_IMG
- en: In Boosting algorithms each classifier is trained on data, taking into account
    the previous classifiers’ success. After each training step, the weights are redistributed. M**isclassified
    data increases its weights** to emphasise the most difficult cases. In this way,
    subsequent learners will focus on them during their training.
  prefs: []
  type: TYPE_NORMAL
- en: How does the classification stage work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To predict the class of new data we only need to **apply the N learners to the
    new observations.** In Bagging the result is obtained by averaging the responses
    of the N learners (or majority vote). However, Boosting assigns a second set of
    weights, this time for the N classifiers, in order to take a **weighted average** of
    their estimates.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e374bcb35da6c40895bf7459957d9e9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the Boosting training stage, the algorithm allocates weights to each resulting
    model. A learner with good a classification result on the training data will be
    assigned a higher weight than a poor one. So when evaluating a new learner, Boosting
    needs to keep track of learners’ errors, too. Let’s see the differences in the
    procedures:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9013ae5145570a4c536dd687e1058b44.png)'
  prefs: []
  type: TYPE_IMG
- en: Some of the Boosting techniques include an extra-condition to keep or discard
    a single learner. For example, in AdaBoost, the most renowned, an error less than
    50% is required to maintain the model; otherwise, the iteration is repeated until
    achieving a learner better than a random guess.
  prefs: []
  type: TYPE_NORMAL
- en: The previous image shows the general process of a Boosting method, but several
    alternatives exist with different ways to determine the weights to use in the
    next training step and in the classification stage. Click here if you like to
    go into detail: [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost), [LPBoost](https://en.wikipedia.org/wiki/LPBoost), [XGBoost](http://arxiv.org/pdf/1603.02754v1.pdf), [GradientBoost](https://en.wikipedia.org/wiki/Gradient_boosting), [BrownBoost](https://en.wikipedia.org/wiki/BrownBoost).
  prefs: []
  type: TYPE_NORMAL
- en: Which is the best, Bagging or Boosting?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There’s not an outright winner; it depends on the data, the simulation and the
    circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging and Boosting decrease the variance of your single estimate as they combine
    several estimates from different models. So the result may be a model with **higher
    stability**.
  prefs: []
  type: TYPE_NORMAL
- en: If the problem is that the single model gets a very low performance, Bagging
    will rarely get a **better bias**. However, Boosting could generate a combined
    model with lower errors as it optimises the advantages and reduces pitfalls of
    the single model.
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, if the difficulty of the single model is **over-fitting**, then
    Bagging is the best option. Boosting for its part doesn’t help to avoid over-fitting;
    in fact, this technique is faced with this problem itself. For this reason, Bagging
    is effective more often than Boosting.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| **Similarities** |  | **Differences** |'
  prefs: []
  type: TYPE_TB
- en: '| Both are ensemble methods to get N learners from 1 learner… |  | … but, while
    they are built independently for Bagging, Boosting tries to add new models that
    do well where previous models fail. |'
  prefs: []
  type: TYPE_TB
- en: '| Both generate several training data sets by random sampling… |  | … but only
    Boosting determines weights for the data to tip the scales in favor of the most
    difficult cases. |'
  prefs: []
  type: TYPE_TB
- en: '| Both make the final decision by averaging  the N learners (or taking the
    majority of them)… |  | … but it is an equally weighted average for Bagging and
    a weighted average for Boosting, more weight to those with better performance
    on training data. |'
  prefs: []
  type: TYPE_TB
- en: '| Both are good at reducing variance and provide higher stability… |  | … but
    only Boosting tries to reduce bias. On the other hand, Bagging may solve the over-fitting
    problem, while Boosting can increase it. |'
  prefs: []
  type: TYPE_TB
- en: '[Original](https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Ensemble Learning to Improve Machine Learning Results](/2017/09/ensemble-learning-improve-machine-learning-results.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science Primer: Basic Concepts for Beginners](/2017/08/data-science-primer-basic-concepts-for-beginners.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Must-Know: What is the idea behind ensemble learning?](/2017/05/must-know-ensemble-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[What Is the Difference Between SQL and Object-Relational Mapping (ORM)?](https://www.kdnuggets.com/2022/02/difference-sql-object-relational-mapping-orm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What’s the Difference Between Data Analysts and Data Scientists?](https://www.kdnuggets.com/2022/03/difference-data-analysts-data-scientists.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Efficiency Spells the Difference Between Biological Neurons and…](https://www.kdnuggets.com/2022/11/efficiency-spells-difference-biological-neurons-artificial-counterparts.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Difference Between L1 and L2 Regularization](https://www.kdnuggets.com/2022/08/difference-l1-l2-regularization.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Difference Between Training and Testing Data in Machine Learning](https://www.kdnuggets.com/2022/08/difference-training-testing-data-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[WTF is the Difference Between GBM and XGBoost?](https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
