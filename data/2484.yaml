- en: 'Federated Learning: Collaborative Machine Learning with a Tutorial on How to
    Get Started'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联邦学习：协作机器学习及如何入门教程
- en: 原文：[https://www.kdnuggets.com/2021/12/federated-learning-collaborative-machine-learning-tutorial-get-started.html](https://www.kdnuggets.com/2021/12/federated-learning-collaborative-machine-learning-tutorial-get-started.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/12/federated-learning-collaborative-machine-learning-tutorial-get-started.html](https://www.kdnuggets.com/2021/12/federated-learning-collaborative-machine-learning-tutorial-get-started.html)
- en: '![Federated-Learning-Collaborative-Machine-Learning-blog.jpg](../Images/723734f5fd799f64ecea45bd199a6824.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Federated-Learning-Collaborative-Machine-Learning-blog.jpg](../Images/723734f5fd799f64ecea45bd199a6824.png)'
- en: '**Federated Learning: Privacy, Security, and Data Sovereignty in the Lab and
    in the Wild (with Tutorial)**'
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**联邦学习：实验室和实际应用中的隐私、安全性和数据主权（附教程）**'
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能。'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 为你的组织提供 IT 支持。'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Federated learning, also known as collaborative learning, allows [training models](https://www.exxactcorp.com/Deep-Learning-NVIDIA-GPU-Workstations) at
    scale on data that remains distributed on the devices where they are generated.
    Sensitive data remains with the owners of said data, where training is conducted,
    and a centralized training orchestrator of training only sees the contribution
    of each client through model updates.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习，也称为协作学习，允许在保持数据分布在生成数据的设备上的情况下进行大规模的[模型训练](https://www.exxactcorp.com/Deep-Learning-NVIDIA-GPU-Workstations)。敏感数据保留在数据所有者处，训练过程仅通过模型更新让集中训练协调器看到每个客户端的贡献。
- en: Federated learning doesn’t guarantee privacy on its own (we'll touch on breaking
    and repairing privacy in federated learning systems later on), but it does make
    privacy possible.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习本身并不能保证隐私（稍后我们将讨论如何破解和修复联邦学习系统中的隐私问题），但它使隐私成为可能。
- en: '**Use cases for federated learning:**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**联邦学习的应用场景：**'
- en: Next word prediction for mobile phone keyboards (*e.g. *[McMahan *et al.* 2017](https://arxiv.org/abs/1811.03604), [Hard *et
    al. *2019](https://arxiv.org/abs/1602.05629))
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移动电话键盘的下一个词预测（*例如* [McMahan *等人* 2017](https://arxiv.org/abs/1811.03604)， [Hard *等人* 2019](https://arxiv.org/abs/1602.05629)）
- en: Health research (*e.g.* [Kaissis* et al. *2020](https://www.nature.com/articles/s42256-020-0186-1/), [Sadilek *et
    al.* 2021](https://www.nature.com/articles/s41746-021-00489-2))
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 健康研究（*例如* [Kaissis* 等人* 2020](https://www.nature.com/articles/s42256-020-0186-1/)， [Sadilek *等人* 2021](https://www.nature.com/articles/s41746-021-00489-2)）
- en: Autonomous vehicles (*e.g. *[Zeng* et al. *2021](https://arxiv.org/abs/2102.03401), [OpenMined
    blog post](https://blog.openmined.org/making-autonomous-vehicles-robust-active-learning-federated-learning-v2x/))
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动驾驶汽车（*例如* [Zeng* 等人* 2021](https://arxiv.org/abs/2102.03401)， [OpenMined 博客文章](https://blog.openmined.org/making-autonomous-vehicles-robust-active-learning-federated-learning-v2x/)）
- en: “Smart home” systems (*e.g.* [Matchi* et al.* 2019](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8844592), [Wu* et
    al. *2020](https://arxiv.org/abs/2012.07450))
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “智能家居”系统（*例如* [Matchi* 等人* 2019](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8844592)， [Wu* 等人* 2020](https://arxiv.org/abs/2012.07450)）
- en: … and anywhere else where machine learning predictions would be useful, but
    individuals would rather not give up their personal data if given the choice.
    That covers pretty much every scenario where predictions are made at a resolution
    of the individual.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: … 以及其他任何机器学习预测可能有用的地方，但个人宁愿在选择时不放弃他们的个人数据。这几乎涵盖了所有在个体分辨率下进行预测的场景。
- en: With the public and policy-makers becoming more aware of the data economy, demand
    for privacy-preserving machine learning is on the rise. As a result, data practices
    have been garnering increased scrutiny, and research on privacy-respecting tools
    like federated learning is increasingly active. Ideally, federated learning aims
    to preserve individual and institutional privacy while potentially making collaborations
    between data stakeholders possible where they would normally be impossible due
    to trade secrecy, private health information, or the increased risk of data breaches.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 随着公众和政策制定者对数据经济的认识不断提高，对隐私保护机器学习的需求也在上升。因此，数据实践受到越来越多的审视，对尊重隐私的工具，如联邦学习的研究也越来越活跃。理想情况下，联邦学习旨在保护个人和机构的隐私，同时在因商业秘密、私人健康信息或数据泄露风险增加而通常不可能的情况下，使数据相关方之间的合作成为可能。
- en: Government regulations like the European Union’s[ General Data Protection Regulation](https://en.wikipedia.org/wiki/GDPR) or
    the[ California Consumer Privacy Act](https://www.oag.ca.gov/privacy/ccpa) (among
    others) make privacy preserving strategies like federated learning become a useful
    tool for enterprises that want to remain in legal operation. At the same time,
    attaining a desired degree of privacy and security while maintaining model performance
    and efficiency presents plenty of technical challenges in their own right.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 像欧盟的[通用数据保护条例](https://en.wikipedia.org/wiki/GDPR)或[加州消费者隐私法案](https://www.oag.ca.gov/privacy/ccpa)（以及其他法规）这样的政府规章使得像联邦学习这样的隐私保护策略成为企业保持合法运营的有用工具。同时，在保持模型性能和效率的同时实现所需的隐私和安全程度，仍然面临着许多技术挑战。
- en: Finally, from the everyday perspective of the individual data-producer (such
    as, in all likelihood, yourself, dear reader), it’s nice to know that, at least
    in theory, there’s something that can be placed between your private health and
    financial data and the kind of hodgepodge ecosystem of data brokers that track
    everything else you do online, typically *sans* both moral backbone and security
    competency.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，从普通数据提供者（如很可能的你，亲爱的读者）的日常角度来看，至少在理论上，有一种方法可以在你的私人健康和财务数据与跟踪你在线上所做的所有其他事情的数据经纪人混杂的生态系统之间架起一道屏障，这种生态系统通常*缺乏*道德基础和安全能力，真是令人欣慰。
- en: If any of these issues strike a chord with you, then read on to learn more about
    the intricacies of federated learning and what it can do for machine learning
    on sensitive data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些问题引起了你的兴趣，那么继续阅读，了解联邦学习的复杂性以及它在敏感数据上的机器学习中能做些什么。
- en: '****Federated Learning in a Nutshell****'
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '****联邦学习概述****'
- en: Federated learning aims to train a single model from multiple data sources,
    under the constraint that data stays at the source and is not exchanged by the
    data sources (a.k.a. nodes, clients, or workers) nor by the central server orchestrating
    training, if present.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习旨在从多个数据源训练一个单一模型，前提是数据保持在源头，不被数据源（即节点、客户端或工作者）或协调训练的中央服务器（如果存在的话）交换。
- en: '![Figure](../Images/05695c8aff2696c10ecf1e18dd7473c0.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/05695c8aff2696c10ecf1e18dd7473c0.png)'
- en: '*In a typical federated learning scheme, a central server sends model parameters
    to a population of nodes (also known as clients or workers). The nodes train the
    initial model for some number of updates on local data and send the newly trained
    weights back to the central server, which averages the new model parameters (often
    with respect to the amount of training performed on each node). In this scenario
    the data at any one node is never directly seen by the central server or the other
    nodes, and additional techniques, such as secure aggregation, can further enhance
    privacy.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*在典型的联邦学习方案中，中央服务器将模型参数发送到一组节点（也称为客户端或工作者）。这些节点在本地数据上对初始模型进行若干次更新，并将新训练的权重发送回中央服务器，中央服务器对新的模型参数进行平均（通常考虑到每个节点上进行的训练量）。在这种情况下，任何一个节点上的数据都不会被中央服务器或其他节点直接看到，额外的技术，如安全聚合，能够进一步增强隐私。*'
- en: There are many variations within this framework. For example, we’re mainly concerned
    in this article with federated learning schemes managed by a central server that
    orchestrates training on multiple devices of the same type, each training on their
    own local data and uploading the results to the central server. This is the basic
    scheme described by[ McMahan et al. in 2017](https://arxiv.org/abs/1602.05629).
    However, it’s possible to do away with centralized control of training, and in
    some situations it may be desirable to do so. When individual nodes distribute
    the role of the central manager it becomes decentralized federated learning, an
    attractive approach for[ training](https://www.semanticscholar.org/paper/BrainTorrent%3A-A-Peer-to-Peer-Environment-for-Roy-Siddiqui/aad543a5b7f231f085764ce0258fe8914a15006f) collaborative
    models on[ privileged medical data](https://www.semanticscholar.org/paper/Decentralized-Federated-Learning-for-Electronic-Lu-Zhang/c838937405bce0365b6d4b1dc80f91c22cfa7b31).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个框架内有许多变体。例如，本文主要关注由中央服务器管理的联邦学习方案，该服务器协调多个相同类型设备的训练，每个设备训练自己的本地数据并将结果上传到中央服务器。这是[McMahan
    等人在2017年](https://arxiv.org/abs/1602.05629)描述的基本方案。然而，也可以去掉集中控制的训练，在某些情况下，这可能是值得的。当单个节点分担中央管理的角色时，这就变成了去中心化的联邦学习，这是一种吸引人的方法，用于在[特权医疗数据](https://www.semanticscholar.org/paper/Decentralized-Federated-Learning-for-Electronic-Lu-Zhang/c838937405bce0365b6d4b1dc80f91c22cfa7b31)上[训练](https://www.semanticscholar.org/paper/BrainTorrent%3A-A-Peer-to-Peer-Environment-for-Roy-Siddiqui/aad543a5b7f231f085764ce0258fe8914a15006f)协作模型。
- en: While a typical federated learning scenario might involve a population of mobile
    phones, for example, all with roughly similar computational capabilities and training
    the same model architecture, some schemes, such as a[ HeteroFL](https://arxiv.org/abs/2010.01264) by
    Diao *et al. *2021*, *allow for training a single inference model on a variety
    of devices with vastly different communication and computation capabilities, even
    going so far as to train local models with different architectures and numbers
    of parameters, before aggregating the trained parameters to a global inference
    model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一个典型的联邦学习场景可能涉及到一群移动电话，例如，所有电话具有大致相似的计算能力并训练相同的模型架构，但一些方案，例如Diao *et al. 2021*提出的[HeteroFL](https://arxiv.org/abs/2010.01264)，*允许在具有截然不同通信和计算能力的各种设备上训练单个推理模型，甚至可以训练具有不同架构和参数数量的本地模型，然后将训练好的参数汇聚到一个全局推理模型中。*
- en: The primary advantages of federated learning stem from the fact that data stays
    on the device where it’s generated, and includes, for example, the fact that a
    training dataset is usually substantially larger than the model being trained,
    and sending the latter instead of the former can save on bandwidth. Paramount
    among these advantages is the possibility of privacy, yet it is still possible
    to infer something about the contents of a private dataset from a model parameter
    update alone.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习的主要优势源于数据保持在生成它的设备上，例如，一个训练数据集通常比正在训练的模型大得多，发送后者而不是前者可以节省带宽。这些优势中最重要的是隐私的可能性，但仅凭模型参数更新仍然可能推测出私人数据集的内容。
- en: The simple example used in McMahan *et al.* 2017 to explain the vulnerability
    is a language model trained with a “bag-of-words” input vector, where each input
    vector corresponds specifically to a single word in a large vocabulary. Each non-zero
    gradient update for a corresponding word would give eavesdroppers a clue to the
    presence (and conversely the absence) of the word in the private dataset. More[ sophisticated
    attacks](https://inpher.io/journal-blog/the-privacy-risk-right-under-our-nose-in-federated-learning-part-1/) have
    also been demonstrated. As a result there is a wide spectrum of privacy-enhancing
    techniques that can be incorporated into federated learning, ranging from[ secure
    aggregation](https://dl.acm.org/doi/10.1145/3133956.3133982) of updates to training
    with[ fully homomorphic encryption](https://www.usenix.org/conference/atc20/presentation/zhang-chengliang).
    We’ll briefly touch on the most prominent threats to privacy in federated learning
    and their mitigation in the next section.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: McMahan *et al.* 2017年使用的简单示例来解释漏洞是一个使用“词袋”输入向量训练的语言模型，其中每个输入向量特定对应于大型词汇表中的单个单词。每个非零梯度更新对于相应的单词会给窃听者提供关于该单词在私人数据集中存在（或缺失）的线索。更[复杂的攻击](https://inpher.io/journal-blog/the-privacy-risk-right-under-our-nose-in-federated-learning-part-1/)也已被证明。因此，联邦学习中可以融入各种隐私增强技术，从[安全聚合](https://dl.acm.org/doi/10.1145/3133956.3133982)更新到使用[全同态加密](https://www.usenix.org/conference/atc20/presentation/zhang-chengliang)进行训练。我们将在下一节简要讨论联邦学习中最突出的隐私威胁及其缓解措施。
- en: '****The Ongoing Origin Story of Federated Learning****'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '****联邦学习的持续起源故事****'
- en: State regulation of data privacy is an emergent area of policy, gaining momentum
    about 10 to 20 years after it would have matched the development of large segments
    of the global economy based on personal data collection and analysis. The most
    prominent regulation of personal data belonging to members of the public is the
    European[ General Data Protection Regulation](https://en.wikipedia.org/wiki/General_Data_Protection_Regulation) enacted
    in 2016, more commonly known as GDPR.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 数据隐私的州级监管是一个新兴的政策领域，大约在个人数据收集和分析的全球经济大部分发展之后的10到20年才获得了动量。最突出的个人数据监管是2016年颁布的欧洲[通用数据保护条例](https://en.wikipedia.org/wiki/General_Data_Protection_Regulation)，更常被称为GDPR。
- en: It may come as some surprise, given that similar protections limiting corporate
    surveillance and data collection in the USA are nascent or lacking, but the US
    White House published an extensive report outlining similar principles in 2012
    ([pdf](https://obamawhitehouse.archives.gov/sites/default/files/privacy-final.pdf)),
    including focused collection, data security and transparency, control over which
    data are collected, and an expectation that data collected for one purpose would
    not be used for wildly unrelated purposes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会让人感到惊讶的是，鉴于美国类似的保护措施限制公司监视和数据收集仍处于萌芽阶段或缺失，但美国白宫在2012年发布了一份详细的报告，概述了类似的原则（[pdf](https://obamawhitehouse.archives.gov/sites/default/files/privacy-final.pdf)），包括聚焦收集、数据安全和透明度、对收集数据的控制，以及对收集数据用于一个目的而不会用于无关目的的期望。
- en: The[ California Consumer Privacy Act](https://www.oag.ca.gov/privacy/ccpa) followed
    the EU’s GDPR into law two years later in 2018\. As a state law, the CCPA is significantly
    limited in geographic scope by comparison to GDPR, while the act itself has similar
    aims but a somewhat narrower definition of personal information. Federated learning
    is one machine learning tool that can be used to give privacy a chance.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[加州消费者隐私法](https://www.oag.ca.gov/privacy/ccpa)在2018年跟随欧盟的GDPR成为法律。作为一项州法律，加州消费者隐私法的地理范围相较于GDPR显著有限，尽管该法案有类似的目标，但对个人信息的定义略为狭窄。联邦学习是一种可以用来给隐私带来机会的机器学习工具。'
- en: 'The term *federated learning* was introduced in a[ 2017 paper](https://arxiv.org/abs/1602.05629) by
    McMahan *et al.* to describe the training of a model on decentralized data. The
    authors framed the design strategy for their system under the 2012 White House
    report on consumer data privacy described above. They suggested two primary use
    cases for federated learning: image classification, and language models for voice
    recognition or next word/phrase prediction.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*联邦学习*这一术语在McMahan *et al.* 2017年的一篇[论文](https://arxiv.org/abs/1602.05629)中首次提出，用于描述在分散数据上训练模型。作者根据上述2012年白宫关于消费者数据隐私的报告，为他们的系统设计策略进行了框架构建。他们提出了联邦学习的两个主要应用场景：图像分类，以及用于语音识别或下一个词/短语预测的语言模型。'
- en: It wasn’t long before the potential attack surfaces associated with distributed
    training were demonstrated. Work by [Phong *et al.* 2017](https://ieeexplore.ieee.org/document/8241854) and [Bhowmick
    et al. 2018](https://arxiv.org/abs/1812.00984) among others demonstrated that
    even with access only to the gradient updates or partially trained models returned
    from a federated learning client to the server, some details describing private
    data can be inferred. A summary of privacy concerns and their mitigation can be
    had in [this blog post](https://inpher.io/journal-blog/the-privacy-risk-right-under-our-nose-in-federated-learning-part-1/) at
    inpher.io.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 不久之后，分布式训练相关的潜在攻击面就被展示出来了。[Phong *et al.* 2017](https://ieeexplore.ieee.org/document/8241854)
    和 [Bhowmick et al. 2018](https://arxiv.org/abs/1812.00984)等人的研究表明，即使仅访问来自联邦学习客户端的梯度更新或部分训练模型，也可以推断出描述私有数据的一些细节。有关隐私问题及其缓解的总结可以在[inpher.io的这篇博客文章](https://inpher.io/journal-blog/the-privacy-risk-right-under-our-nose-in-federated-learning-part-1/)中找到。
- en: The balance between privacy, effectiveness, and efficiency in federated learning
    spans a broad spectrum. Communications between the server and clients (or solely
    between decentralized clients) can be encrypted in transport and at rest, but
    there’s an even more robust option where data and models remain encrypted during
    training as well. [Homomorphic encryption](https://en.wikipedia.org/wiki/Homomorphic_encryption) can
    be used to perform computations on encrypted data, so that (ideally) the outputs
    can only be decrypted by the stakeholder with the key. Libraries like OpenMined’s [PySyft](https://github.com/OpenMined/PySyft),
    Microsoft’s [SEAL](https://github.com/Microsoft/SEAL), or [TensorFlow Encrypted](https://github.com/tf-encrypted/tf-encrypted) provide
    tools for encrypted deep learning that can be applied to federated learning systems.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习在隐私、有效性和效率之间的平衡涉及广泛的范围。服务器与客户端之间（或仅在去中心化客户端之间）的通信可以在传输和静态状态下加密，但还有一种更强大的选项，即数据和模型在训练期间也保持加密。[同态加密](https://en.wikipedia.org/wiki/Homomorphic_encryption)可以用于对加密数据进行计算，因此（理想情况下）只有持有密钥的利益相关者才能解密结果。像OpenMined的[PySyft](https://github.com/OpenMined/PySyft)、微软的[SEAL](https://github.com/Microsoft/SEAL)或[TensorFlow
    Encrypted](https://github.com/tf-encrypted/tf-encrypted)这样的库提供了可应用于联邦学习系统的加密深度学习工具。
- en: That’s enough discussion about federated learning, next we'll set up a simple
    federated learning demonstration in the tutorial section.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 关于联邦学习的讨论到此为止，接下来我们将在教程部分设置一个简单的联邦学习演示。
- en: '****Federated ML Tutorial: Federated Learning on the Iris Dataset with the
    Flower Library****'
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '****联邦ML教程：使用Flower库在鸢尾花数据集上进行联邦学习****'
- en: '***If you run into any trouble getting the code to run for this tutorial and
    would like to see a working example, try running it in from your browser at ***[***this
    mybinder link***](https://mybinder.org/v2/gh/riveSunder/fed_ml_flower_demo/201057130524d2e2770afe297faf9fc7a961ab98)*** and
    follow the instructions in the ***[***readme***](https://github.com/riveSunder/fed_ml_flower_demo/commit/201057130524d2e2770afe297faf9fc7a961ab98)*** to
    launch the federated learning demo.***'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '***如果你在运行此教程代码时遇到任何问题，并希望查看一个有效的示例，可以尝试通过浏览器运行[这个mybinder链接](https://mybinder.org/v2/gh/riveSunder/fed_ml_flower_demo/201057130524d2e2770afe297faf9fc7a961ab98)并按照[readme](https://github.com/riveSunder/fed_ml_flower_demo/commit/201057130524d2e2770afe297faf9fc7a961ab98)中的说明启动联邦学习演示。***'
- en: Now that we have an idea of where and why we might want to use federated learning,
    let’s take a hands-on look at how we might do so.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对使用联邦学习的场景和原因有了了解，让我们实际看看如何操作。
- en: There are a number of federated learning libraries to choose from, from the
    more mainstream[ Tensorflow Federated](https://www.tensorflow.org/federated) with
    over 1700 stars on GitHub to the popular and privacy-focused[ PySyft](https://github.com/OpenMined/PySyft) to
    the research oriented[ FedJAX](https://github.com/google/fedjax). Table 1 contains
    a reference list of popular federated learning repositories.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 可供选择的联邦学习库有很多，从更主流的[Tensorflow Federated](https://www.tensorflow.org/federated)（在GitHub上有超过1700个星标）到流行且注重隐私的[PySyft](https://github.com/OpenMined/PySyft)，再到面向研究的[FedJAX](https://github.com/google/fedjax)。表1包含了流行联邦学习库的参考列表。
- en: '| **Name** | **Repository** | **License/Stars** | **Focus** |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| **名称** | **仓库** | **许可证/星标** | **重点** |'
- en: '| TF Federated | [https://github.com/tensorflow/federated](https://github.com/tensorflow/federated)
    | [Apache 2.0 ](https://github.com/tensorflow/federated)/ 1.7k | R&D |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| TF Federated | [https://github.com/tensorflow/federated](https://github.com/tensorflow/federated)
    | [Apache 2.0](https://github.com/tensorflow/federated)/ 1.7k | 研发 |'
- en: '| FedJAX | [https://github.com/google/fedjax](https://github.com/google/fedjax)
    | [Apache 2.0](https://github.com/tensorflow/federated/blob/main/LICENSE) / 130
    | Research |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| FedJAX | [https://github.com/google/fedjax](https://github.com/google/fedjax)
    | [Apache 2.0](https://github.com/tensorflow/federated/blob/main/LICENSE) / 130
    | 研究 |'
- en: '| Flower | [https://github.com/adap/flower](https://github.com/adap/flower)
    | [Apache 2.0](https://github.com/adap/flower/blob/main/LICENSE) / 529 | Usability
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Flower | [https://github.com/adap/flower](https://github.com/adap/flower)
    | [Apache 2.0](https://github.com/adap/flower/blob/main/LICENSE) / 529 | 可用性 |'
- en: '| FedML | [https://github.com/FedML-AI/FedML](https://github.com/FedML-AI/FedML)
    | [Apache 2.0](https://github.com/FedML-AI/FedML/blob/master/LICENSE) / 839 |
    Research |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| FedML | [https://github.com/FedML-AI/FedML](https://github.com/FedML-AI/FedML)
    | [Apache 2.0](https://github.com/FedML-AI/FedML/blob/master/LICENSE) / 839 |
    研究 |'
- en: '| PySyft | [https://github.com/openmined/pysyft](https://github.com/openmined/pysyft)
    | [Apache 2.0](https://github.com/OpenMined/PySyft/blob/main/packages/syft/LICENSE) /
    7.7k | Privacy / R&D |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| PySyft | [https://github.com/openmined/pysyft](https://github.com/openmined/pysyft)
    | [Apache 2.0](https://github.com/OpenMined/PySyft/blob/main/packages/syft/LICENSE) /
    7.7k | 隐私 / 研发 |'
- en: '| IBM federated-learning-lib | [https://github.com/IBM/federated-learning-lib](https://github.com/IBM/federated-learning-lib)
    | [Custom](https://github.com/IBM/federated-learning-lib/blob/main/LICENSE) /
    244 | Enterprise |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| IBM federated-learning-lib | [https://github.com/IBM/federated-learning-lib](https://github.com/IBM/federated-learning-lib)
    | [Custom](https://github.com/IBM/federated-learning-lib/blob/main/LICENSE) /
    244 | 企业 |'
- en: '***Table 1: **Libraries for federated learning.*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '***表 1: **联邦学习的库。*'
- en: For our tutorial we'll use the[ Flower library](https://github.com/adap/flower).
    We chose this library in part because it exemplifies basic federated learning
    concepts in an accessible way and it is framework agnostic, and in part because
    we will be using the “iris” dataset included in SciKit-Learn (and the names match).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的教程，我们将使用[ Flower 库](https://github.com/adap/flower)。我们选择这个库部分是因为它以一种易于理解的方式展示了基本的联邦学习概念，并且它是框架不可知的，部分原因是我们将使用包含在
    SciKit-Learn 中的“iris”数据集（名称也匹配）。
- en: As Flower is agnostic to the deep learning toolkit used to build models (they
    have examples for[ TensorFlow](https://flower.dev/docs/quickstart_tensorflow.html),[ PyTorch](https://flower.dev/docs/quickstart_pytorch.html),[ MXNet](https://flower.dev/docs/quickstart_mxnet.html),
    and[ SciKit-Learn](https://github.com/adap/flower/tree/main/examples/sklearn-logreg-mnist) in
    the documentation), we’ll use PyTorch. From a high-level perspective, we need
    to set up a server and a client, the latter of which we’ll call twice with different
    training datasets. Setting up the server is by far the simpler of the tasks at
    hand, so we’ll start there.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Flower 对用于构建模型的深度学习工具包是不可知的（他们在文档中提供了[ TensorFlow](https://flower.dev/docs/quickstart_tensorflow.html)、[
    PyTorch](https://flower.dev/docs/quickstart_pytorch.html)、[ MXNet](https://flower.dev/docs/quickstart_mxnet.html)
    和[ SciKit-Learn](https://github.com/adap/flower/tree/main/examples/sklearn-logreg-mnist)
    的示例），我们将使用 PyTorch。从高层次来看，我们需要设置一个服务器和一个客户端，后者我们将用不同的训练数据集调用两次。设置服务器无疑是手头任务中最简单的，因此我们将从这里开始。
- en: 'To set up our server, all we need to do is define an evaluation strategy and
    pass it to the default configuration server in Flower. But first let’s make sure
    we have a virtual environment set up that has all the dependencies we’ll need.
    On the Unix command line:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置我们的服务器，我们需要做的只是定义一个评估策略，并将其传递给 Flower 中的默认配置服务器。但首先，让我们确保我们已经设置了一个虚拟环境，其中包含所有我们需要的依赖项。在
    Unix 命令行中：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With our virtual environment up and running, we can write a module for spinning
    up a Flower server to handle federated learning. In the code below we've included
    argparse to make it easier to experiment with different numbers of training rounds
    when calling the server module from the command line. We also define a function
    that generates an evaluation function, which is the only other thing we add to
    the strategy used by the default configuration of the Flower server.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的虚拟环境启动并运行后，我们可以编写一个模块来启动一个 Flower 服务器以处理联邦学习。在下面的代码中，我们包含了 argparse，以便在从命令行调用服务器模块时更容易尝试不同的训练轮数。我们还定义了一个生成评估函数的函数，这是我们对
    Flower 服务器默认配置策略的唯一补充。
- en: 'The contents of our server module file:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的服务器模块文件的内容：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Take notice of the `PTMLPClient` called in the code above. This is used by the
    server module to define the evaluation function, and this class is also the model
    class used for training and doubles as a federated learning client. We'll define
    the `PTMLPClient` next, sub-classing from both Flower's `NumPyClient` class and
    the `torch.nn.Module` class that you’ll already be familiar with if you work with
    PyTorch.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 注意上面代码中调用的`PTMLPClient`。这是服务器模块用来定义评估函数的，同时这个类也是用于训练的模型类，并且还充当联邦学习客户端。接下来我们将定义`PTMLPClient`，它从Flower的`NumPyClient`类和你在使用PyTorch时已经熟悉的`torch.nn.Module`类中子类化。
- en: The `NumPyClient` class handles communication with the server and requires use
    to implement the abstract functions `set_parameters`, `get_parameters`, `fit`,
    and `evaluate`. The `torch.nn.Module` class gives us all the convenient functionality
    of a PyTorch model, mainly the ability to train with the PyTorch Adam optimizer.
    Our `PTMLPClient` class will be just over 100 lines of code, so we’ll go through
    each class function in turn, starting with `__init__`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`NumPyClient`类处理与服务器的通信，并要求我们实现抽象函数`set_parameters`、`get_parameters`、`fit`和`evaluate`。`torch.nn.Module`类为我们提供了PyTorch模型的所有便捷功能，主要是使用PyTorch
    Adam优化器进行训练的能力。我们的`PTMLPClient`类将略超过100行代码，所以我们将逐一讲解每个类函数，从`__init__`开始。'
- en: Notice that we are inheriting from two ancestor classes. Inheriting from nn.Module
    means that we have to make sure to call `__init__` from nn.Module using the super
    command, but Python will let you know right away if you forget to do so. Other
    than that we initialize three dense layers as matrices (`torch.tensor` data types)
    and store some of the information about the training split and model dimensions
    as class variables.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们从两个祖先类中继承。继承自`nn.Module`意味着我们必须确保通过super命令调用`__init__`，但如果你忘记这样做，Python会立即提醒你。除此之外，我们初始化了三个密集层作为矩阵（`torch.tensor`数据类型），并将一些关于训练拆分和模型维度的信息存储为类变量。
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next we'll define the `get_parameters` and `set_parameters` functions of the `PTMLPClient` class.
    These functions concatenate all model parameters as a flattened numpy array, which
    is the data type that Flower's NumPyClient class is expected to return and receive.
    This fits into the federated learning scheme as the server will send initial parameters
    to each client (using `set_parameters`) and expects a set of partially trained
    weights to be returned (from `get_parameters`). This pattern occurs once per round.
    We also initialize the optimizer and loss function in `set_parameters`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义`PTMLPClient`类的`get_parameters`和`set_parameters`函数。这些函数将所有模型参数连接为一个扁平化的numpy数组，这是Flower的NumPyClient类预期返回和接收的数据类型。这适应了联邦学习方案，因为服务器将初始参数发送到每个客户端（使用`set_parameters`），并期望返回一组部分训练的权重（来自`get_parameters`）。这种模式每轮出现一次。我们还在`set_parameters`中初始化优化器和损失函数。
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next we'll define our forward pass and a convenience function for getting a
    loss scalar.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义前向传播和一个用于获取损失标量的便捷函数。
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The last few functions that our client needs are `fit` and `evaluate`. For each
    round, each client initializes its parameters with those supplied to the fit method
    before training for a few epochs (default is 10 in this case). The `evaluate` function
    also sets its parameters before calculating the loss and accuracy on the validation
    split of the training data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们客户端所需的最后几个函数是`fit`和`evaluate`。在每一轮中，每个客户端都会用fit方法提供的参数初始化其参数，然后进行几轮训练（在这种情况下默认是10轮）。`evaluate`函数还会在计算训练数据验证拆分上的损失和准确性之前设置其参数。
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Both `fit` and `evaluate` in our client class call a function get_data which
    is just a wrapper for the SciKit-Learn iris dataset. It also splits the data into
    training and validation sets, and further splits the training dataset in twain
    (which we call ‘alice’ and ‘bob’) to simulate federated learning with clients
    that each have their own data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们客户端类中的`fit`和`evaluate`都调用了一个名为get_data的函数，它只是SciKit-Learn鸢尾花数据集的一个包装器。它还将数据拆分为训练集和验证集，并进一步将训练数据集拆分成两个部分（我们称之为‘alice’和‘bob’），以模拟具有各自数据的客户端的联邦学习。
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now we just need to populate an `if __name__ ==` "`__main__`": method at the
    bottom of our file so that we can run our client code as a module from the command
    line.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只需在文件底部填充一个`if __name__ == "__main__":`方法，以便我们可以从命令行运行我们的客户端代码作为模块。
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Finally, make sure to import everything needed at the top of the client module.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，确保在客户端模块的顶部导入所有需要的内容。
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: That's actually all we have to implement to run a federated training demo with
    Flower!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要实现的全部内容，以运行带有Flower的联邦训练演示！
- en: To start a federated training run, first launch the server in its own command
    line terminal. We saved our server as pt_server.py and our client module as pt_client.py,
    both in the root of the directory we're working in, so to launch a server and
    tell it to train for 40 rounds of federated learning we used the following command.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始联邦训练运行，首先在自己的命令行终端中启动服务器。我们将服务器保存为pt_server.py，将客户端模块保存为pt_client.py，均位于我们正在工作的目录的根目录，因此为了启动服务器并指示其进行40轮联邦学习，我们使用了以下命令。
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next open up a fresh terminal to launch your first client with the “alice”
    training split:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，打开一个新的终端以启动第一个使用“alice”训练集的客户端：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '... and a second terminal for your next client with the “bob” training split.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '... 再打开一个终端以进行下一个使用“bob”训练集的客户端。'
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If everything works you should see training start up and a scroll of info in
    the terminal running the server process.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你应该会看到训练启动并且终端中显示出信息滚动。
- en: '![fed-learning.jpg](../Images/c6c94bae6f2cea3b0ae107931843d1ff.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![fed-learning.jpg](../Images/c6c94bae6f2cea3b0ae107931843d1ff.png)'
- en: 'In our hands this demo achieved just over 96% accuracy in 20 rounds of training.
    The loss and accuracy curves for the training run look like:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的手中，这个演示在20轮训练中达到了96%以上的准确率。训练过程中的损失和准确率曲线如下：
- en: '![fed-learning-loss-of-accuracy.png](../Images/933df64b77c96cabf88ed197d71cfc91.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![fed-learning-loss-of-accuracy.png](../Images/933df64b77c96cabf88ed197d71cfc91.png)'
- en: That’s it! Now you can put “Flower library” on your federated learning resume.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！现在你可以在你的联邦学习简历上添加“Flower library”。
- en: '**Looking to the Future of Federated Learning**'
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**展望联邦学习的未来**'
- en: A casual observer of the modern world might be persuaded to believe that there
    “[is no such thing](https://www.washingtonpost.com/posteverything/wp/2014/07/28/theres-no-such-thing-as-privacy-on-the-internet-anymore/) as [privacy](https://www.fastcompany.com/40561281/survey-most-facebook-users-dont-expect-much-privacy)”
    anymore. These declarations have been primarily directed at the internet (and
    such declarations have been made since [at least 1999](https://www.wired.com/1999/01/sun-on-privacy-get-over-it/))
    but with the rapid adoption of smart home devices and nosy home robots the reasonable
    expectation of privacy, even [within your own home](https://archive.is/0sDwX),
    may be in danger of catastrophic erosion.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现代世界的一个随意观察者可能会被说服相信“[隐私不复存在](https://www.washingtonpost.com/posteverything/wp/2014/07/28/theres-no-such-thing-as-privacy-on-the-internet-anymore/)”。这些声明主要针对互联网（这些声明自[至少1999年](https://www.wired.com/1999/01/sun-on-privacy-get-over-it/)就已存在），但随着智能家居设备和好奇的家庭机器人迅速普及，合理的隐私预期，即使在[你自己的家中](https://archive.is/0sDwX)，也可能面临灾难性的侵蚀。
- en: 'Pay attention to who is making these declarations and you’ll find that many
    of them have a vested financial interest in the easy pilfering of your data, or
    may be beholden to those who do. This sort of “no privacy” defeatist attitude
    is not only wrong, but can be dangerous: loss of privacy allows individuals and
    groups to be subtly manipulated in ways they may not notice or admit, and people
    who know they are being watched [behave differently](https://en.wikipedia.org/wiki/Panopticon).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意谁在做这些声明，你会发现许多人对你数据的轻松盗取有既得经济利益，或者可能依赖于那些有此利益的人。这种“无隐私”的失败主义态度不仅是错误的，而且可能是危险的：隐私的丧失使个人和群体可以被微妙地操控，以至于他们可能没有注意到或承认，而知道自己被监视的人 [行为会有所不同](https://en.wikipedia.org/wiki/Panopticon)。
- en: '**Bio: [Kevin Vu](https://www.kdnuggets.com/author/kevin-vu)** manages Exxact
    Corp blog and works with many of its talented authors who write about different
    aspects of Deep Learning.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**简历：[Kevin Vu](https://www.kdnuggets.com/author/kevin-vu)** 管理Exxact Corp博客，并与许多撰写关于深度学习不同方面的才华横溢的作者合作。'
- en: '[Original](https://www.exxactcorp.com/blog/Deep-Learning/federated-learning-training-models).
    Reposted with permission.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.exxactcorp.com/blog/Deep-Learning/federated-learning-training-models)。经许可转载。'
- en: More On This Topic
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[How to Implement a Federated Learning Project with Healthcare Data](https://www.kdnuggets.com/2023/02/implement-federated-learning-project-healthcare-data.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何用医疗数据实施联邦学习项目](https://www.kdnuggets.com/2023/02/implement-federated-learning-project-healthcare-data.html)'
- en: '[An Intuitive Explanation of Collaborative Filtering](https://www.kdnuggets.com/2022/09/intuitive-explanation-collaborative-filtering.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[协同过滤的直观解释](https://www.kdnuggets.com/2022/09/intuitive-explanation-collaborative-filtering.html)'
- en: '[Open Assistant: Explore the Possibilities of Open and Collaborative…](https://www.kdnuggets.com/2023/04/open-assistant-explore-possibilities-open-collaborative-chatbot-development.html)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开放助理：探索开放和协作的可能性……](https://www.kdnuggets.com/2023/04/open-assistant-explore-possibilities-open-collaborative-chatbot-development.html)'
- en: '[7 Beginner-Friendly Projects to Get You Started with ChatGPT](https://www.kdnuggets.com/2023/08/7-beginnerfriendly-projects-get-started-chatgpt.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7 个适合初学者的 ChatGPT 项目](https://www.kdnuggets.com/2023/08/7-beginnerfriendly-projects-get-started-chatgpt.html)'
- en: '[10 Free Must-Take Data Science Courses to Get Started](https://www.kdnuggets.com/10-free-must-take-data-science-courses-to-get-started)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10 门免费必修的数据科学课程](https://www.kdnuggets.com/10-free-must-take-data-science-courses-to-get-started)'
- en: '[Why Upskilling in Data Vis Matters (& How to Get Started)](https://www.kdnuggets.com/2022/07/sphere-upskilling-data-vis-matters.html)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为何提升数据可视化技能很重要（以及如何入门）](https://www.kdnuggets.com/2022/07/sphere-upskilling-data-vis-matters.html)'
