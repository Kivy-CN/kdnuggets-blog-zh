- en: What is Transfer Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/01/transfer-learning.html](https://www.kdnuggets.com/2022/01/transfer-learning.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![What is Transfer Learning?](../Images/7a6be2553ec0be9a7ea8f0a49778587a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [qimono on Pixabary](https://pixabay.com/users/qimono-1962238/)
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning is a machine learning method where the application of knowledge
    obtained from a model used in one task, can be reused as a foundation point for
    another task.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning algorithms use historical data as their input to make predictions
    and produce new output values. They are typically designed to conduct isolated
    tasks. A source task is a task from which knowledge is transferred to a target
    task. A target task is when improved learning occurs due to the transfer of knowledge
    from a source task.
  prefs: []
  type: TYPE_NORMAL
- en: During transfer learning, the knowledge leveraged and rapid progress from a
    source task is used to improve the learning and development to a new target task.
    The application of knowledge is using the source task’s attributes and characteristics,
    which will be applied and mapped onto the target task.
  prefs: []
  type: TYPE_NORMAL
- en: However, if the transfer method results in a decrease in the performance of
    the new target task, it is called a negative transfer. One of the major challenges
    when working with transfer learning methods is being able to provide and ensure
    the positive transfer between related tasks, whilst avoiding the negative transfer
    between less related tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The What, When, and How of Transfer Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What do we transfer? To understand which parts of the learned knowledge to transfer,
    we need to figure out which portions of knowledge best reflect both the source
    and target. Overall, improving the performance and accuracy of the target task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When do we transfer? Understanding when to transfer is important, as we don’t
    want to be transferring knowledge which could, in turn, make matters worse, leading
    to negative transfer. Our goal is to improve the performance of the target task,
    not make it worse.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we transfer? Now we have a better idea of what we want to transfer and
    when we can then move on to working with different techniques to transfer the
    knowledge efficiently. We will speak more about this later on in the article.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before we dive into the methodology behind transfer learning, it is good to
    know the different forms of transfer learning. We will go through three different
    types of transfer learning scenarios, based on relationships between the source
    task and target task. Below is an overview of the different types of transfer
    learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Different Types of Transfer Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Inductive Transfer Learning**: In this type of transfer learning, the source
    and target task are the same, however, they are still different from one another.
    The model will use inductive biases from the source task to help improve the performance
    of the target task. The source task may or may not contain labeled data, further
    leading onto the model using multitask learning and self-taught learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unsupervised Transfer Learning**: I assume you know what unsupervised learning
    is, however, if you don’t, it is when an algorithm is subjected to being able
    to identify patterns in datasets that have not been labeled or classified. In
    this case, the source and target are similar, however, the task is different,
    where both data is unlabelled in both source and target. Techniques such as dimensionality
    reduction and clustering are well known in unsupervised learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transductive Transfer Learning**: In this last type of transfer learning,
    the source and target tasks share similarities, however, the domains are different.
    The source domain contains a lot of labeled data, whereas there is an absence
    of labeled data in the target domain, further leading onto the model using domain
    adaptation.'
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning vs. Fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fine-tuning is an optional step in transfer learning and is primarily incorporated
    to improve the performance of the model. The difference between Transfer learning
    and Fine-tuning is all in the name.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning is built on adopting features learned from one task and “transferring”
    the leveraged knowledge onto a new task. Transfer learning is usually used on
    tasks where the dataset is too small, to train a full-scale model from scratch.
    Fine-tuning is built on making “fine” adjustments to a process in order to obtain
    the desired output to further improve performance. The parameters of a trained
    model during fine-tuning, are adjusted and tailored precisely and specifically,
    whilst trying to validate the model to achieve the desired outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Why Use Transfer Learning?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Reasons to use transfer learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Not needing a lot of data** - Gaining access to data is always a hindrance
    due to its lack of availability. Working with insufficient amounts of data can
    result in low performance. This is where transfer learning shines as the machine
    learning model can be built with a small training dataset, due to it being pre-trained.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Saving training time** - Machine learning models are difficult to train and
    can take up a lot of time, leading to inefficiency. It requires a long period
    of time to train a deep neural network from scratch on a complex task, so using
    a pre-trained model saves time on building a new one.'
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning Pros
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Better base**: Using a pre-trained model in transfer learning offers you
    a better foundation and starting point, allowing you to perform some tasks without
    even training.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Higher learning rate**: Due to the model already having been trained on a
    similar task beforehand, the model has a higher learning rate.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Higher accuracy rate**: With a better base and higher learning rate, the
    model works at a higher performance, producing more accuracy outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: When Does Transfer Learning *Not* Work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transfer learning should be avoided when the weights trained from your source
    task are different from your target task. For example, if your previous network
    was trained for classifying cats and dogs and your new network is trying to detect
    shoes and socks, there is going to be a problem as the weights transferred from
    your source to the target task will not be able to give you the best of results.
    Therefore, initialising the network with pre-trained weights that correspond with
    similar outputs to the one you are expecting is better than using weights with
    no correlation.
  prefs: []
  type: TYPE_NORMAL
- en: Removing layers from a pre-trained model will cause issues with the architecture
    of the model. If you remove the first layers, your model will have a low learning
    rate as it has to juggle working with low-level features. Removing layers reduces
    the number of parameters that can be trained, which can result in overfitting.
    Being able to use the correct amount of layers is vital in reducing overfitting,
    however, this is also a timely process.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning Cons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Negative transfer learning**: As I mentioned above, negative transfer learning
    is when a previous learning method obstructs the new task. This only occurs if
    the source and target are not similar enough, causing the first round of training
    to be too far off. Algorithms don''t have to always agree with what we deem as
    similar, making it difficult to understand the fundamentals and standards of what
    type of training is sufficient.'
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning in 6 Steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s dive into a better understanding of how transfer learning is implemented
    and the steps taken. There are 6 general steps taken in transfer learning and
    we will go through each of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select Source Task: The first step is selecting a pre-trained model that holds
    an abundance of data, having a relationship between the input and output data
    with your chosen Target task.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a Base Model: Instantiate a base model with pre-trained weights. Pre-trained
    weights can be accessed through architectures such as Xception. This is developing
    your source model, so that it is better than the naive model we started with,
    ensuring some increase in learning rate.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Freeze Layers: To reduce initialising the weights again, freezing the layers
    from the pre-trained model is necessary. It will redeem the knowledge already
    learned and save you from training the model from scratch.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Add new Trainable Layers: Adding new trainable layers on top of the frozen
    layer, will convert old features into predictions on a new dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Train the New Layers: The pre-trained model contains the final output layer
    already. The likelihood that the current output on the pre-trained model and the
    output you want from your model will be different, is high. Therefore, you have
    to train the model with a new output layer. Therefore, adding new dense layers
    and the final dense layer in correspondence to your expected model, will improve
    the learning rate and produce outputs of your desire.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fine-tuning: You can improve the performance of your model by fine-tuning,
    which is done by unfreezing all or parts of the base models and then retraining
    the model with a very low learning rate. It is critical to use a low learning
    rate at this stage, as the model you are training is much larger than it was initially
    in the first round, along with it being a small dataset. As a result, you are
    at risk of overfitting if you apply large weight updates, therefore you want to
    fine-tune in an incremental way. Recompile the model as you have changed the model’s
    behavior and then retrain the model again, monitoring any overfitting feedback.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I hope this article has given you a good introduction and understanding of Transfer
    Learning. Stay tuned, my next article will be me implementing Transfer Learning
    for Image Recognition and Natural Language Processing.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and freelance Technical writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[TensorFlow for Computer Vision - Transfer Learning Made Easy](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Transfer Learning for Image Recognition and Natural Language Processing](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Practical Guide to Transfer Learning using PyTorch](https://www.kdnuggets.com/2023/06/practical-guide-transfer-learning-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Exploring the Potential of Transfer Learning in Small Data Scenarios](https://www.kdnuggets.com/exploring-the-potential-of-transfer-learning-in-small-data-scenarios)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using Transfer Learning to Boost Model Performance](https://www.kdnuggets.com/using-transfer-learning-to-boost-model-performance)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Solid Plan for Learning Data Science, Machine Learning, and Deep Learning](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
