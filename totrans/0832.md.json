["```py\n**- Random variables\n\n- Probability distribution functions (PDFs)\n\n- Mean, Variance, Standard Deviation\n\n- Covariance and Correlation \n\n- Bayes Theorem\n\n- Linear Regression and Ordinary Least Squares (OLS)\n\n- Gauss-Markov Theorem\n\n- Parameter properties (Bias, Consistency, Efficiency)\n\n- Confidence intervals\n\n- Hypothesis testing\n\n- Statistical significance \n\n- Type I & Type II Errors\n\n- Statistical tests (Student's t-test, F-test)\n\n- p-value and its limitations\n\n- Inferential Statistics \n\n- Central Limit Theorem & Law of Large Numbers\n\n- Dimensionality reduction techniques (PCA, FA)**\n```", "```py\nimport numpy as np\nimport math\nx = np.array([1,3,5,6])\nmean_x = np.mean(x)\n# in case the data contains Nan values\nx_nan = np.array([1,3,5,6, math.nan])\nmean_x_nan = np.nanmean(x_nan)\n```", "```py\nx = np.array([1,3,5,6])\nvariance_x = np.var(x)\n\n# here you need to specify the degrees of freedom (df) max number of logically independent data points that have freedom to vary\nx_nan = np.array([1,3,5,6, math.nan])\nmean_x_nan = np.nanvar(x_nan, ddof = 1)\n```", "```py\nx = np.array([1,3,5,6])\nvariance_x = np.std(x)\n\nx_nan = np.array([1,3,5,6, math.nan])\nmean_x_nan = np.nanstd(x_nan, ddof = 1)\n```", "```py\nx = np.array([1,3,5,6])\ny = np.array([-2,-4,-5,-6])\n#this will return the covariance matrix of x,y containing x_variance, y_variance on diagonal elements and covariance of x,y\ncov_xy = np.cov(x,y)\n```", "```py\nx = np.array([1,3,5,6])\ny = np.array([-2,-4,-5,-6])\ncorr = np.corrcoef(x,y)\n```", "```py\n# Random Generation of 1000 independent Binomial samples\nimport numpy as np\nn = 8\np = 0.16\nN = 1000\nX = np.random.binomial(n,p,N)\n# Histogram of Binomial distribution\nimport matplotlib.pyplot as plt\ncounts, bins, ignored = plt.hist(X, 20, density = True, rwidth = 0.7, color = 'purple')\nplt.title(\"Binomial distribution with p = 0.16 n = 8\")\nplt.xlabel(\"Number of successes\")\nplt.ylabel(\"Probability\")\nplt.show()\n```", "```py\n# Random Generation of 1000 independent Poisson samples\nimport numpy as np\nlambda_ = 7\nN = 1000\nX = np.random.poisson(lambda_,N)\n\n# Histogram of Poisson distribution\nimport matplotlib.pyplot as plt\ncounts, bins, ignored = plt.hist(X, 50, density = True, color = 'purple')\nplt.title(\"Randomly generating from Poisson Distribution with lambda = 7\")\nplt.xlabel(\"Number of visitors\")\nplt.ylabel(\"Probability\")\nplt.show()\n```", "```py\n# Random Generation of 1000 independent Normal samples\nimport numpy as np\nmu = 0\nsigma = 1\nN = 1000\nX = np.random.normal(mu,sigma,N)\n\n# Population distribution\nfrom scipy.stats import norm\nx_values = np.arange(-5,5,0.01)\ny_values = norm.pdf(x_values)\n#Sample histogram with Population distribution\nimport matplotlib.pyplot as plt\ncounts, bins, ignored = plt.hist(X, 30, density = True,color = 'purple',label = 'Sampling Distribution')\nplt.plot(x_values,y_values, color = 'y',linewidth = 2.5,label = 'Population Distribution')\nplt.title(\"Randomly generating 1000 obs from Normal distribution mu = 0 sigma = 1\")\nplt.ylabel(\"Probability\")\nplt.legend()\nplt.show()\n```", "```py\n# R code for the graph\ninstall.packages(\"ggplot2\")\ninstall.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\nlibrary(ggplot2)\nView(data(penguins))\nggplot(data = penguins, aes(x = flipper_length_mm,y = body_mass_g))+\n  geom_smooth(method = \"lm\", se = FALSE, color = 'purple')+\n  geom_point()+\n  labs(x=\"Flipper Length (mm)\",y=\"Body Mass (g)\")\n```", "```py\ndef runOLS(Y,X):\n\n   # OLS esyimation Y = Xb + e --> beta_hat = (X'X)^-1(X'Y)\n   beta_hat = np.dot(np.linalg.inv(np.dot(np.transpose(X), X)), np.dot(np.transpose(X), Y))\n\n   # OLS prediction\n   Y_hat = np.dot(X,beta_hat)\n   residuals = Y-Y_hat\n   RSS = np.sum(np.square(residuals))\n   sigma_squared_hat = RSS/(N-2)\n   TSS = np.sum(np.square(Y-np.repeat(Y.mean(),len(Y))))\n   MSE = sigma_squared_hat\n   RMSE = np.sqrt(MSE)\n   R_squared = (TSS-RSS)/TSS\n\n   # Standard error of estimates:square root of estimate's variance\n   var_beta_hat = np.linalg.inv(np.dot(np.transpose(X),X))*sigma_squared_hat\n\n   SE = []\n   t_stats = []\n   p_values = []\n   CI_s = []\n\n   for i in range(len(beta)):\n       #standard errors\n       SE_i = np.sqrt(var_beta_hat[i,i])\n       SE.append(np.round(SE_i,3))\n\n        #t-statistics\n        t_stat = np.round(beta_hat[i,0]/SE_i,3)\n        t_stats.append(t_stat)\n\n        #p-value of t-stat p[|t_stat| >= t-treshhold two sided] \n        p_value = t.sf(np.abs(t_stat),N-2) * 2\n        p_values.append(np.round(p_value,3))\n\n        #Confidence intervals = beta_hat -+ margin_of_error\n        t_critical = t.ppf(q =1-0.05/2, df = N-2)\n        margin_of_error = t_critical*SE_i\n        CI = [np.round(beta_hat[i,0]-margin_of_error,3), np.round(beta_hat[i,0]+margin_of_error,3)]\n        CI_s.append(CI)\n        return(beta_hat, SE, t_stats, p_values,CI_s, \n               MSE, RMSE, R_squared)\n```"]