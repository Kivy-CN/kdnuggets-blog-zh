- en: Best Machine Learning Model For Sparse Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最佳稀疏数据机器学习模型
- en: 原文：[https://www.kdnuggets.com/2023/04/best-machine-learning-model-sparse-data.html](https://www.kdnuggets.com/2023/04/best-machine-learning-model-sparse-data.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/04/best-machine-learning-model-sparse-data.html](https://www.kdnuggets.com/2023/04/best-machine-learning-model-sparse-data.html)
- en: '![Best Machine Learning Model For Sparse Data](../Images/671203cd62e0f4332672e22ce8527e1a.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![最佳稀疏数据机器学习模型](../Images/671203cd62e0f4332672e22ce8527e1a.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Sparse data refers to datasets with many features with zero values. It can cause
    problems in different fields, especially in machine learning.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏数据是指特征值为零的数据集。这会在不同领域造成问题，特别是在机器学习中。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Sparse data can occur as a result of inappropriate feature engineering methods.
    For instance, using a one-hot encoding that creates a large number of dummy variables.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏数据可能由于不适当的特征工程方法而发生。例如，使用一热编码创建大量虚拟变量。
- en: Sparsity can be calculated by taking the ratio of zeros in a dataset to the
    total number of elements. Addressing sparsity will affect the accuracy of your
    machine-learning model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏性可以通过计算数据集中零值的比例与总元素数的比率来衡量。解决稀疏性问题将影响你的机器学习模型的准确性。
- en: Also, we should distinguish sparsity from missing data. Missing data simply
    means that some values are not available. In sparse data, all values are present,
    but most are zero.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们应区分稀疏性和缺失数据。缺失数据仅意味着某些值不可用。在稀疏数据中，所有值都是存在的，但大多数值为零。
- en: Also, sparsity causes unique challenges for machine learning. To be exact, it
    causes overfitting, losing good data, memory problems, and time problems.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，稀疏性对机器学习带来了独特的挑战。具体来说，它会导致过拟合、丢失有用数据、内存问题和时间问题。
- en: This article will explore these common problems related to sparse data. Then
    we will cover the techniques used to handle this issue.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将探讨与稀疏数据相关的这些常见问题。然后，我们将介绍处理这些问题的技术。
- en: Finally, we will apply different machine learning models to the sparse data
    and explain why these models are suitable for sparse data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将应用不同的机器学习模型于稀疏数据，并解释为什么这些模型适用于稀疏数据。
- en: Throughout the article, I will predominantly use the scikit-learn library, and
    if you wish to modify the code and arguments, I will provide the official documentation
    links too.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个文章中，我将主要使用 scikit-learn 库，如果你希望修改代码和参数，我也会提供官方文档链接。
- en: Now let's start with the common problems with sparse data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始讨论稀疏数据的常见问题。
- en: Common Problems With Sparse Data
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏数据的常见问题
- en: Sparse data can pose unique challenges for data analysis. We already mentioned
    that some of the most common issues include overfitting, losing good data, memory
    problems, and time problems.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏数据可能对数据分析提出独特的挑战。我们已经提到了一些最常见的问题，包括过拟合、丢失有用数据、内存问题和时间问题。
- en: Now, let’s have a detailed look at each.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们详细查看每一项。
- en: '![Best Machine Learning Model For Sparse Data](../Images/f8b50e24377d552147b6d62f797db89e.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![最佳稀疏数据机器学习模型](../Images/f8b50e24377d552147b6d62f797db89e.png)'
- en: Image by Author
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Overfitting
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合
- en: Overfitting occurs when a model becomes too complex and starts to capture noise
    in the data instead of the underlying patterns.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合发生在模型变得过于复杂，开始捕捉数据中的噪声而不是潜在模式时。
- en: In sparse data, there may be a large number of features, but only a few of them
    are actually relevant to the analysis. This can make it difficult to identify
    which features are important and which ones are not.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在稀疏数据中，可能有大量特征，但只有少数特征与分析实际相关。这会使得识别哪些特征重要、哪些不重要变得困难。
- en: As a result, a model may overfit to noise in the data and perform poorly on
    new data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可能导致模型对数据中的噪声过拟合，并在新数据上表现不佳。
- en: If you are new to machine learning or want to know more, you can do that in
    the [scikit-learn documentation about overfitting](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对机器学习不熟悉或想了解更多，你可以参考[scikit-learn关于过拟合的文档](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html)。
- en: Losing Good Data
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 丢失有用数据
- en: One of the biggest problems with sparse data is that it can lead to the loss
    of potentially useful information.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏数据的一个最大问题是它可能导致潜在有用信息的丢失。
- en: When we have very limited data, it becomes more difficult to identify meaningful
    patterns or relationships in that data. This is because the noise and randomness
    inherent to any data set can more easily obscure essential features when the data
    is sparse.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们拥有非常有限的数据时，识别数据中的有意义模式或关系变得更加困难。这是因为任何数据集固有的噪声和随机性在数据稀疏时更容易掩盖重要特征。
- en: Furthermore, because the amount of data available is limited, there is a higher
    chance that we will miss out on some of the truly valuable patterns or relationships
    in the data. This is especially true in cases where the data is sparse due to
    a lack of sampling, as opposed to simply being missing. In such cases, we may
    not even be aware of the missing data points and thus may not realize we are losing
    valuable information.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于可用数据的数量有限，我们更有可能错过数据中一些真正有价值的模式或关系。特别是在由于采样不足而导致数据稀疏的情况下，这一点尤其真实。在这种情况下，我们甚至可能未意识到丢失了数据点，因此可能无法意识到我们正在丢失有价值的信息。
- en: That’s why if too many features are removed, or the data is compressed too much,
    important information may be lost, resulting in a less accurate model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么如果移除过多特征，或者数据压缩过度，可能会丢失重要信息，从而导致模型准确性降低。
- en: Memory Problem
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存问题
- en: Memory problems can arise due to the large size of the dataset. Sparse data
    often results in many features, and storing this data can be computationally expensive.
    This can limit the amount of data that can be processed at once or require significant
    computing resources.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 内存问题可能由于数据集的巨大规模而出现。稀疏数据通常导致许多特征，存储这些数据可能需要计算上的高开销。这可能限制一次能处理的数据量，或者需要大量计算资源。
- en: '[Here](https://scikit-learn.org/0.15/modules/scaling_strategies.html) you can
    see different strategies to scale your data by using scikit-learn.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[这里](https://scikit-learn.org/0.15/modules/scaling_strategies.html)你可以查看使用
    scikit-learn 进行数据缩放的不同策略。'
- en: Time Problem
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间问题
- en: The time problem can also occur due to the large size of the dataset. Sparse
    data may require longer processing times, especially when dealing with a large
    number of features. This can limit the speed at which data can be processed, which
    can be problematic in time-sensitive applications.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 时间问题也可能由于数据集的巨大规模而发生。稀疏数据可能需要更长的处理时间，特别是当处理大量特征时。这可能限制数据处理的速度，在时间敏感的应用中可能会有问题。
- en: What Are the Methods for Working With Sparse Features?
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理稀疏特征的方法有哪些？
- en: '![Best Machine Learning Model For Sparse Data](../Images/1943174e04fa31dcc50b815eb278f98a.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![适用于稀疏数据的最佳机器学习模型](../Images/1943174e04fa31dcc50b815eb278f98a.png)'
- en: Image by Author
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Sparse data poses a challenge in data analysis due to its low occurrence of
    non-zero values. However, there are several methods available to mitigate this
    issue.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏数据在数据分析中带来了挑战，因为其非零值出现频率较低。然而，有几种方法可以缓解这个问题。
- en: One common approach is removing the feature causing sparsity in the dataset.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的方法是移除导致数据集稀疏的特征。
- en: Another option is to use Principal Component Analysis (PCA) to reduce the dimensionality
    of the dataset while retaining important information.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是使用主成分分析（PCA）来减少数据集的维度，同时保留重要信息。
- en: Feature hashing is another technique that can be employed, which involves mapping
    features to a fixed-length vector.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 特征哈希是一种可以采用的技术，它涉及将特征映射到固定长度的向量。
- en: T-Distributed Stochastic Neighbor Embedding (t-SNE) is another useful method
    that can be utilized to visualize high-dimensional datasets.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: T-分布随机邻域嵌入（t-SNE）是另一种有用的方法，可以用来可视化高维数据集。
- en: In addition to these techniques, selecting a suitable machine learning model
    that can handle sparse data, such as SVM or logistic regression, is crucial.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些技术，选择一个适合处理稀疏数据的机器学习模型，如 SVM 或逻辑回归，也是至关重要的。
- en: By implementing these strategies, one can effectively address the challenges
    associated with sparse data in data analysis.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实施这些策略，可以有效解决数据分析中与稀疏数据相关的挑战。
- en: Now let’s start with the tactics used to reduce sparse data first, then we will
    go deeper into the models.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们先从减少稀疏数据的策略开始，然后我们将深入探讨模型。
- en: Remove it!
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 移除它！
- en: When working with sparse data, one approach is to remove features that contain
    mostly zero values. This can be done by setting a threshold on the percentage
    of non-zero values in each feature. Any feature that falls below this threshold
    can be removed from the dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理稀疏数据时，一种方法是移除包含大多数零值的特征。这可以通过设置每个特征中非零值的百分比阈值来完成。任何低于该阈值的特征都可以从数据集中移除。
- en: This approach can help reduce the dimensionality of the dataset and improve
    the performance of certain machine learning algorithms.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以帮助减少数据集的维度，并提高某些机器学习算法的性能。
- en: Code Example
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码示例
- en: In this example, we set the dimensions of the dataset, as well as the sparsity
    level, which determines how many values in the dataset will be zero.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们设置了数据集的维度以及稀疏度，稀疏度决定了数据集中有多少值为零。
- en: Then, we generate random data with the specified sparsity level to check whether
    our method works or not. At this step, we calculate the sparsity to compare afterward.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们生成具有指定稀疏度的随机数据，以检查我们的方法是否有效。在此步骤中，我们计算稀疏度以供之后比较。
- en: Next, the code sets the number of zeros to remove and randomly removes a specific
    number of zeros from the dataset. Then we recalculate the sparsity of the modified
    dataset to check whether our method works or not.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，代码设置要移除的零的数量，并随机移除数据集中特定数量的零。然后，我们重新计算修改后的数据集的稀疏度，以检查我们的方法是否有效。
- en: Finally, we recalculate the sparsity to see the changes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们重新计算稀疏度以查看变化。
- en: Here is the code.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这是代码。
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here is the output.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果。
- en: '![Best Machine Learning Model For Sparse Data](../Images/1f7e7f923e68f352886c4fdd2a123ecf.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![最佳稀疏数据机器学习模型](../Images/1f7e7f923e68f352886c4fdd2a123ecf.png)'
- en: PCA
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA
- en: PCA is a popular technique for dimensionality reduction. It identifies the principal
    components of the data, which are the directions in which the data varies the
    most.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种流行的降维技术。它识别数据的主成分，这些主成分是数据变化最大的方向。
- en: These principal components can then be used to represent the data in a lower-dimensional
    space.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以使用这些主成分在较低维度的空间中表示数据。
- en: In the context of sparse data, PCA can be used to identify the most important
    features that contain the most variation in the data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在稀疏数据的背景下，PCA可以用来识别数据中包含最多变异的最重要特征。
- en: By selecting only these features, we can reduce the dimensionality of the dataset
    while still retaining most of the important information.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仅选择这些特征，我们可以减少数据集的维度，同时保留大部分重要信息。
- en: You can implement PCA by using the sci-kit learn library, as we will do it next
    in the code example. [Here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)
    is the official documentation if you want to learn more about it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用scikit-learn库来实现PCA，我们将在接下来的代码示例中这样做。[这里](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)是官方文档，如果你想了解更多信息。
- en: Code Example
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码示例
- en: To apply PCA to sparse data, we can use the scikit-learn library in Python.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要对稀疏数据应用PCA，我们可以使用Python中的scikit-learn库。
- en: The library provides a PCA class that we can use to fit a PCA model to the data
    and transform it into lower-dimensional space.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 该库提供了一个PCA类，我们可以使用它来拟合PCA模型并将数据转换为较低维度的空间。
- en: In the first section of the following code, we create a dataset as we did in
    the previous section, with a given dimension and sparsity.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的代码的第一部分，我们创建一个数据集，如同在前一部分中一样，具有给定的维度和稀疏度。
- en: In the second section, we will apply PCA to reduce the dimension of the dataset
    to 10\. After that, we will recalculate the sparsity.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分，我们将应用PCA将数据集的维度减少到10。之后，我们将重新计算稀疏度。
- en: Here is the code.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这是代码。
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here is the output.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果。
- en: '![Best Machine Learning Model For Sparse Data](../Images/c722563f5d7f093ec75871b17dfc8296.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![最佳稀疏数据机器学习模型](../Images/c722563f5d7f093ec75871b17dfc8296.png)'
- en: Feature Hashing
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征哈希
- en: Another method for working with sparse data is called feature hashing. This
    approach converts each feature into a fixed-length array of values using a hashing
    function.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 处理稀疏数据的另一种方法叫做特征哈希。这种方法通过哈希函数将每个特征转换为固定长度的值数组。
- en: The hashing function maps each input feature to a set of indices in the fixed-length
    array. The values are summed together if multiple input features are mapped to
    the same index. Feature hashing can be useful for large datasets where storing
    a large feature dictionary may not be feasible.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希函数将每个输入特征映射到固定长度数组中的一组索引。如果多个输入特征映射到相同的索引，则将这些值相加。特征哈希对于存储大型特征字典可能不可行的大型数据集非常有用。
- en: We will cover this together in the next section, yet if you want to dig deeper
    into it, [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html)
    you can see the official documentation of the feature hasher in the scikit-learn
    library.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一部分一起讨论这个内容，但如果你想深入了解，[这里](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html)可以查看scikit-learn库中特征哈希器的官方文档。
- en: Code Example
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码示例
- en: Here, we again use the same method in dataset creation.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们再次使用相同的方法来创建数据集。
- en: Then we apply feature hashing to the dataset using the FeatureHasher class from
    scikit-learn.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用scikit-learn的FeatureHasher类对数据集应用特征哈希。
- en: We specify the number of output features with the **n_features** parameter and
    the input type as a dictionary with the **input_type** parameter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用**n_features**参数指定输出特征的数量，并使用**input_type**参数指定输入类型为字典。
- en: We then transform the input data into hashed arrays using the transform method
    of the FeatureHasher object.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用FeatureHasher对象的transform方法将输入数据转换为哈希数组。
- en: Finally, we calculate the sparsity of the resulting dataset.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算结果数据集的稀疏度。
- en: Here is the code.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是代码。
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here is the output.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果。
- en: '![Best Machine Learning Model For Sparse Data](../Images/9415c1a75c52d781b730b48385a8e84b.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![最佳稀疏数据机器学习模型](../Images/9415c1a75c52d781b730b48385a8e84b.png)'
- en: t-SNE Embedding
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: t-SNE 嵌入
- en: t-SNE (t-Distributed Stochastic Neighbor Embedding) is a non-linear dimensionality
    reduction technique used to visualize high-dimensional data. It reduces the dimensionality
    of the data while preserving its global structure and has become a popular tool
    in machine learning for visualizing and clustering high-dimensional data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE（t-分布随机邻域嵌入）是一种非线性降维技术，用于可视化高维数据。它在减少数据的维度的同时保持其全局结构，已成为机器学习中可视化和聚类高维数据的热门工具。
- en: t-SNE is particularly useful for working with sparse data because it can effectively
    reduce the dimensionality of the data while maintaining its structure. The t-SNE
    algorithm works by calculating pairwise distances between data points in high-
    and low-dimensional spaces. It then minimizes the difference between these distances
    in high- and low-dimensional space.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE特别适用于处理稀疏数据，因为它可以有效地降低数据的维度，同时保持其结构。t-SNE算法通过计算高维和低维空间中数据点之间的成对距离来工作。然后，它最小化这些距离在高维和低维空间中的差异。
- en: To use t-SNE with sparse data, the data must first be converted into a dense
    matrix. This can be done using various techniques, such as PCA or feature hashing.
    Once the data has been converted, t-SNE can be high-x to obtain a low-dimensional
    embedding of the data.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在稀疏数据上使用t-SNE，数据必须首先转换为密集矩阵。这可以通过各种技术完成，如PCA或特征哈希。数据转换完成后，t-SNE可以用来获得数据的低维嵌入。
- en: Also, if you are curious about t-SNE, [here](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)
    is the official documentation of the scikit-learn to see more.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你对t-SNE感兴趣，[这里](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)是scikit-learn的官方文档，可以查看更多信息。
- en: Code Example
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码示例
- en: The following code first sets the dimensions of the dataset and the sparsity
    level, generates random data with the specified sparsity level, and calculates
    the sparsity of the dataset before t-SNE is applied, as we did in the previous
    examples.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码首先设置数据集的维度和稀疏度级别，生成具有指定稀疏度级别的随机数据，并计算应用t-SNE之前数据集的稀疏度，正如我们在前面的示例中所做的那样。
- en: It then applies t-SNE to the dataset with 3 components and calculates the sparsity
    of the resulting t-SNE embedding. Finally, it prints out the sparsity of the t-SNE
    embedding.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 它然后对具有3个组件的数据集应用t-SNE，并计算t-SNE嵌入的稀疏度。最后，它打印出t-SNE嵌入的稀疏度。
- en: Here is the code.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这是代码。
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here is the output.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果。
- en: '![Best Machine Learning Model For Sparse Data](../Images/5f5d3328773ed82df3f8582ace6c31da.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![最佳稀疏数据机器学习模型](../Images/5f5d3328773ed82df3f8582ace6c31da.png)'
- en: Best Machine Learning Model for Sparse Data
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏数据的最佳机器学习模型
- en: Now that we have addressed the challenges of working with sparse data, we can
    explore machine learning models specifically designed to perform well with sparse
    data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经解决了处理稀疏数据的挑战，我们可以探索专门为稀疏数据设计的机器学习模型。
- en: These models can handle the unique characteristics of sparse data, such as a
    high number of features with many zeros and limited information, which can make
    it challenging to achieve accurate predictions with traditional models.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型可以处理稀疏数据的独特特征，例如大量特征中有许多零和有限的信息，这使得传统模型很难实现准确预测。
- en: By using models designed explicitly for sparse data, we can ensure that our
    predictions are more precise and reliable.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用专门为稀疏数据设计的模型，我们可以确保预测结果更加精准可靠。
- en: Now let’s talk about the models good for sparse data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来谈谈适合稀疏数据的模型。
- en: SVC (Support Vector Classifier)
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SVC（支持向量分类器）
- en: SVC (Support Vector Classifier) with the linear kernel can perform well with
    sparse data because it uses a subset of training points, known as support vectors,
    to make predictions. This means it can handle high-dimensional, sparse data efficiently.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: SVC（支持向量分类器）与线性核可以很好地处理稀疏数据，因为它使用训练点的子集，即支持向量，进行预测。这意味着它可以高效地处理高维稀疏数据。
- en: You can use Support Vector for regression, too.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用支持向量进行回归。
- en: I explained the [Support Vector Machine here](https://www.stratascratch.com/blog/machine-learning-algorithms-explained-support-vector-machine/?utm_source=blog&utm_medium=click&utm_campaign=kdn+sparse+data)
    if you want to learn more about the Support Vector algorithm, both classification
    and regression.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于支持向量算法的内容，包括分类和回归，我在 [这里解释了支持向量机](https://www.stratascratch.com/blog/machine-learning-algorithms-explained-support-vector-machine/?utm_source=blog&utm_medium=click&utm_campaign=kdn+sparse+data)。
- en: Logistic Regression
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: This can also work well with sparse data because logistic regression uses a
    regularization term to control the model complexity, which can help prevent overfitting
    on sparse datasets.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以很好地处理稀疏数据，因为逻辑回归使用正则化项来控制模型复杂性，这有助于防止在稀疏数据集上的过拟合。
- en: 'If you want to learn more about logistic regression and also for other classification
    algorithms, here is the [Overview of Machine Learning Algorithms: Classification](https://www.stratascratch.com/blog/overview-of-machine-learning-algorithms-classification/?utm_source=blog&utm_medium=click&utm_campaign=kdn+sparse+data).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于逻辑回归以及其他分类算法的信息，请查看 [机器学习算法概述：分类](https://www.stratascratch.com/blog/overview-of-machine-learning-algorithms-classification/?utm_source=blog&utm_medium=click&utm_campaign=kdn+sparse+data)。
- en: KNeighboursClassifier
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KNeighboursClassifier
- en: This algorithm can work well with sparse data since it computes distances between
    data points and can handle high-dimensional data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法可以很好地处理稀疏数据，因为它计算数据点之间的距离，并且能够处理高维数据。
- en: You can see KNN and other [machine learning algorithms here](https://www.stratascratch.com/blog/machine-learning-algorithms-you-should-know-for-data-science/?utm_source=blog&utm_medium=click&utm_campaign=kdn+sparse+data)
    that you should know for data science.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [这里的机器学习算法](https://www.stratascratch.com/blog/machine-learning-algorithms-you-should-know-for-data-science/?utm_source=blog&utm_medium=click&utm_campaign=kdn+sparse+data)
    查看KNN和其他你需要了解的数据科学算法。
- en: MLPClassifier
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLPClassifier
- en: The MLPClassifier can perform well with sparse data when the input data is standardized,
    as it uses gradient descent for optimization.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: MLPClassifier在输入数据标准化时可以很好地处理稀疏数据，因为它使用梯度下降进行优化。
- en: '[Here](https://www.stratascratch.com/blog/here-is-how-chatgpt-will-help-you-be-a-better-data-scientist/?utm_source=blog&utm_medium=click&utm_campaign=kdn+sparse+data)
    you can see the implementation of MLP Classifier, along witha  bunch of other
    algorithms, with the help of ChatGPT.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[这里](https://www.stratascratch.com/blog/here-is-how-chatgpt-will-help-you-be-a-better-data-scientist/?utm_source=blog&utm_medium=click&utm_campaign=kdn+sparse+data)你可以看到MLP分类器的实现，以及其他算法，借助ChatGPT的帮助。'
- en: DecisionTreeClassifier
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树分类器
- en: It can work well with sparse data when the number of features is small. If you
    do not know about decision trees, I explained [decision trees and random forests
    here](http://stratascratch.com/blog/decision-tree-and-random-forest-algorithm-explained/?utm_source=blog&utm_medium=click&utm_campaign=kdn+sparse+data),
    which will be our final model for analyzing the models for sparse data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当特征数量较少时，它可以很好地处理稀疏数据。如果你对决策树不了解，我在 [这里解释了决策树和随机森林](http://stratascratch.com/blog/decision-tree-and-random-forest-algorithm-explained/?utm_source=blog&utm_medium=click&utm_campaign=kdn+sparse+data)，这是我们分析稀疏数据模型的最终模型。
- en: RandomForestClassifier
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RandomForestClassifier
- en: The RandomForestClassifier can work well with sparse data when the number of
    features is small.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: RandomForestClassifier 在特征较少时可以很好地处理稀疏数据。
- en: '![Best Machine Learning Model For Sparse Data](../Images/5b2bb7e52165ba96f0ff593375d81136.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![处理稀疏数据的最佳机器学习模型](../Images/5b2bb7e52165ba96f0ff593375d81136.png)'
- en: Image by Author
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Now, I will show you how these models perform on the generated data. But, I
    will add another algorithm to see whether these algorithms will outperform this
    algorithm (which is typically not good for sparse data) or not.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我将展示这些模型在生成的数据上的表现。但是，我会添加另一个算法，以查看这些算法是否会超越这个通常不适合稀疏数据的算法。
- en: Code Example
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码示例
- en: In this section, we will test multiple machine learning models on a sparse dataset,
    which is a dataset with a lot of empty or zero values.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将测试多个机器学习模型在稀疏数据集上的表现，这是一种包含大量空值或零值的数据集。
- en: We will calculate the sparsity of the dataset and evaluate the models using
    the F1 score.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将计算数据集的稀疏性，并使用 F1 分数评估模型。
- en: Then, we will create a data frame with the F1 scores for each model to compare
    their performance. Also, we will filter out any warnings that may appear during
    the evaluation process.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将创建一个包含每个模型 F1 分数的数据框，以比较它们的表现。此外，我们将过滤掉在评估过程中可能出现的任何警告。
- en: '[PRE4]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here is the output.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果。
- en: '![Best Machine Learning Model For Sparse Data](../Images/e3a1ac872b9e24830ac4b1b0935ba9cb.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![处理稀疏数据的最佳机器学习模型](../Images/e3a1ac872b9e24830ac4b1b0935ba9cb.png)'
- en: By now, you might catch an algorithm that is not well-suited for the sparse
    data. Yes, the answer is the KMeans. But why?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能会发现一个不适合稀疏数据的算法。没错，答案是 KMeans。但为什么呢？
- en: KMeans is typically not well suited for sparse data because it is based on distance
    measures, which can be problematic with high-dimensional, sparse data.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: KMeans 通常不适合稀疏数据，因为它基于距离度量，这在高维稀疏数据中可能会出现问题。
- en: There are also some algorithms that we can’t even try. For instance, if you
    try to include the GaussianNB classifier in this list, you will get an error.
    It suggests that the GaussianNB classifier expects dense data instead of sparse
    data. This is because the GaussianNB classifier assumes that the input data follows
    Gaussian distribution and is unsuitable for sparse data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一些算法我们甚至不能尝试。例如，如果你试图将 GaussianNB 分类器包含在这个列表中，你会遇到错误。它提示 GaussianNB 分类器期望的是密集数据而不是稀疏数据。这是因为
    GaussianNB 分类器假设输入数据遵循高斯分布，因此不适合稀疏数据。
- en: Conclusion
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, working with sparse data can be challenging due to various problems
    like overfitting, losing good data, memory, and time problems.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，处理稀疏数据可能会面临各种问题，比如过拟合、丢失有效数据、内存和时间问题。
- en: However, several methods are available for working with sparse features, including
    removing features, using PCA, and feature hashing.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有几种方法可以处理稀疏特征，包括移除特征、使用 PCA 和特征哈希。
- en: Moreover, certain machine learning models like SVM, Logistic Regression, Lasso,
    Decision Tree, Random Forest, MLP, and k-nearest neighbors are well-suited for
    handling sparse data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，某些机器学习模型如 SVM、Logistic Regression、Lasso、Decision Tree、Random Forest、MLP 和
    k-最近邻适合处理稀疏数据。
- en: These models have been designed to handle high-dimensional and sparse data efficiently,
    making them the best choices for sparse data problems. Using these methods and
    models can improve your model's accuracy and save time and resources.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型已被设计为高效处理高维和稀疏数据，使它们成为处理稀疏数据问题的最佳选择。使用这些方法和模型可以提高模型的准确性，节省时间和资源。
- en: '**[Nate Rosidi](https://www.stratascratch.com)** is a data scientist and in
    product strategy. He''s also an adjunct professor teaching analytics, and is the
    founder of [StrataScratch](https://www.stratascratch.com/), a platform helping
    data scientists prepare for their interviews with real interview questions from
    top companies. Connect with him on [Twitter: StrataScratch](https://twitter.com/StrataScratch)
    or [LinkedIn](https://www.linkedin.com/in/nathanrosidi/).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Nate Rosidi](https://www.stratascratch.com)** 是一位数据科学家和产品战略专家。他还担任分析学兼职教授，并且是
    [StrataScratch](https://www.stratascratch.com/) 的创始人，该平台帮助数据科学家准备面试，提供来自顶级公司的真实面试问题。可以通过
    [Twitter: StrataScratch](https://twitter.com/StrataScratch) 或 [LinkedIn](https://www.linkedin.com/in/nathanrosidi/)
    与他联系。'
- en: More On This Topic
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多
- en: '[Working With Sparse Features In Machine Learning Models](https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在机器学习模型中处理稀疏特征](https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html)'
- en: '[Sparse Matrix Representation in Python](https://www.kdnuggets.com/2020/05/sparse-matrix-representation-python.html)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 中的稀疏矩阵表示](https://www.kdnuggets.com/2020/05/sparse-matrix-representation-python.html)'
- en: '[Generalized and Scalable Optimal Sparse Decision Trees(GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[广义可扩展的最优稀疏决策树（GOSDT）](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
- en: '[Segment Anything Model: Foundation Model for Image Segmentation](https://www.kdnuggets.com/2023/07/segment-anything-model-foundation-model-image-segmentation.html)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Segment Anything 模型：图像分割的基础模型](https://www.kdnuggets.com/2023/07/segment-anything-model-foundation-model-image-segmentation.html)'
- en: '[Best Practices to Use OpenAI GPT Model](https://www.kdnuggets.com/2023/08/best-practices-openai-gpt-model.html)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 OpenAI GPT 模型的最佳实践](https://www.kdnuggets.com/2023/08/best-practices-openai-gpt-model.html)'
- en: '[How To Use Synthetic Data To Overcome Data Shortages For Machine…](https://www.kdnuggets.com/2022/03/synthetic-data-overcome-data-shortages-machine-learning-model-training.html)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何利用合成数据克服机器学习模型训练中的数据短缺问题](https://www.kdnuggets.com/2022/03/synthetic-data-overcome-data-shortages-machine-learning-model-training.html)'
