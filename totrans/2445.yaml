- en: Risk Management Framework for AI/ML Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/03/risk-management-framework-aiml-models.html](https://www.kdnuggets.com/2022/03/risk-management-framework-aiml-models.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Risk Management Framework for AI/ML Models](../Images/7bd3035887aff786c512488ea06ba5fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Business vector created by jcomp — [www.freepik.com](https://www.freepik.com)
    (illustrating the make or break concept of managing AI Risk and producing successful
    AI models)'
  prefs: []
  type: TYPE_NORMAL
- en: 'AI/ML models like any other mathematical and financial models come with their
    set of risks and require awareness to mitigate them beforehand. The intent of
    this article is very well summarised in the [definition of risk](https://en.wikipedia.org/wiki/Risk):'
  prefs: []
  type: TYPE_NORMAL
- en: Risk is the possibility of something bad happening. Risk involves uncertainty
    about the effects/implications of activity with respect to something that humans
    value (such as health, well-being, wealth, property, or the environment), often
    focusing on negative, undesirable consequences.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Risk needs to be accounted for well in advance, right at the design stage. One
    can not wait for the consequences to show up, in order to do the damage control.
    That is where “Risk Management” helps by designing a forward-looking approach,
    pulling levers to assess the worst-case scenario, and timely acting upon it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Risk Management Framework for AI/ML Models](../Images/960f56f00412ad589eef949fec84836f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: Key measures to manage risks include identifying the sources of risk and checking
    how they are handled in current processes. This analysis surfaces the gap where
    current checks may prove insufficient to the unforeseen risks.
  prefs: []
  type: TYPE_NORMAL
- en: The framework by principle involves brainstorming on what can possibly go wrong — one
    can not wait for a self-driving car to get stuck in the dilemma of whether to
    save an old man vs a child when on road. There is no one ideal answer to this
    problem but the worst-case scenario has to be thought through and should be acceptable
    before implementing the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Risks by nature are not self-contained and have cascading effects by amplifying
    the nuisance all across. Clear metrics of how to define and measure risks are
    crucial to assure how risk is capped within acceptable limits. The metrics also
    prove the effectiveness of risk mitigation measures through pre and post-analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The risks associated with AI are relatively unknown to measure, plan and capture
    in advance due to its recent technological advancements as against other mature
    industries like finance and medicine. There are a few lessons that can be directly
    learned and imported into the AI risk management tool:'
  prefs: []
  type: TYPE_NORMAL
- en: Documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Integrity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regulations and Compliance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explainability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Documentation**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML models may end up solving a revised problem than the one originally conceived.
    The project starts with a certain expectation (read magic) from the ML model.
    However, the model’s performance gets restricted due to the lack of relevant attributes
    or good quality data providing statistical signals for model learning. A number
    of experiments are performed to calibrate the features, algorithms, data, or even
    the problem statement itself to assess how the model can best suit the exact or
    tweaked business task and yet add value.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the business requirement is to optimize resource allocation
    in a service center to attend to the bulk queries and calls received throughout
    the day. The problem can be framed as a multi-class classification where the target
    variable i.e. the expected number of calls can be divided into 3 categories — low,
    medium, and high. This implies that businesses will estimate resources as per
    the ML model’s prediction of the low, medium, or high workload (or the number
    of expected calls).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the business could be more interested in knowing the expected
    workload over the next week from the ML model and would apply its own post-processing
    logic to decide how the resources should be allocated.
  prefs: []
  type: TYPE_NORMAL
- en: We have discussed two ways of framing a problem statement — regression and classification.
  prefs: []
  type: TYPE_NORMAL
- en: While there is no one rule that fits all — it needs to be discussed with the
    business and well-documented. The assumptions, data availability, the history
    of the data, any attributes that are only available in real-time and are not shown
    to model during the learning phase need to be captured in great detail and signed-off
    with the client before diving deep into the data modeling.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Integrity**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Covid has drastically changed the dynamics of how the world runs and rendered
    a lot of ML models becoming irrelevant. No model/AI solution provider would have
    been able to foresee it considering the black swan effect of this pandemic. But,
    there are other innumerable ways the model output can deviate from the expected
    outcome, one of the key reasons being the drift between the training data and
    inference data characteristics. Now, this is in developers' hands and can be controlled
    using various statistical methods. One such metric is the [KS statistic](https://www.statisticshowto.com/kolmogorov-smirnov-test/).
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about how to maintain data quality to build successful
    machine learning solutions, I would recommend a prior read of this [article](/2022/03/significance-data-quality-making-successful-machine-learning-model.html).
    It explains how seamless data integration and consistency is crucial to maintaining
    data pipelines and underlines the importance of adopting data culture.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reputational, Regulation, and Compliance Risks**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can artificially intelligent robots gain sentience and harm human safety by
    taking over the world? Such questions predominantly occupy our thoughts and immediately
    make AI the villain of the wave of digital transformation.
  prefs: []
  type: TYPE_NORMAL
- en: But AI is just a technology that can not do harm on its own unless it is designed
    and constructed to do so. This leads to an important and often ignored concept—
    intent.
  prefs: []
  type: TYPE_NORMAL
- en: '[Echoing Dave Waters](https://www.supplychaintoday.com/artificial-intelligence-machine-learning-quotes-top-minds/),
    “The potential benefits of artificial intelligence are huge, so are the dangers.”'
  prefs: []
  type: TYPE_NORMAL
- en: '![Risk Management Framework for AI/ML Models](../Images/077af6302e53bfcfb7f19f8944495d64.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: People vector created by pch.vector — [www.freepik.com](https://www.freepik.com),
    with inputs from Author on good and bad intent with examples'
  prefs: []
  type: TYPE_NORMAL
- en: AI has huge untapped potential to improve human lives but comes with its own
    risks.
  prefs: []
  type: TYPE_NORMAL
- en: The moment risks enter the scene, follows the regulations. Yes, regulations
    are needed to put checks and balances in place and to make sure that they provide
    the right framework that does not deter the flight of AI benefits but controls
    the possible harms.
  prefs: []
  type: TYPE_NORMAL
- en: Societal and human-friendly applications that do not compromise or misuse personal
    data and respect user privacy can only succeed and make their way to market. Regulations
    like GDPR aim at giving control and rights to individuals over how they allow
    their personal data to be used.
  prefs: []
  type: TYPE_NORMAL
- en: Timely intervention and updates of such regulation frameworks ensure that our
    society can continue to reap the benefits of good intent of AI while deterring
    the mal-intended AI products to hit the ground.
  prefs: []
  type: TYPE_NORMAL
- en: '**Explainability**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can the AI model explain the predictions. It is imperative for the model developer
    to explain the internal working of the algorithm without trading off with the
    model accuracy. There are two ways it can be achieved — interpretable models or
    auxiliary tools.
  prefs: []
  type: TYPE_NORMAL
- en: '![Risk Management Framework for AI/ML Models](../Images/0c891159999ab0413218577fddfa7734.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Education vector created by freepik — [www.freepik.com](https://www.freepik.com)'
  prefs: []
  type: TYPE_NORMAL
- en: The words interpretability and explainability are often used interchangeably.
    Models like decision trees, linear regression come under the umbrella of interpretable
    models due to their inherent nature of interpreting the predictions. While black-box
    models like deep neural networks need the help of auxiliary tools and techniques
    like LIME and SHAP framework to explain the model output.
  prefs: []
  type: TYPE_NORMAL
- en: The illustration shown below captures the true state the data science community
    should strive to achieve where we are able to explain why a model outputs a particular
    class X and not class Y. When does the model succeed and can be trusted with a
    greater degree of confidence? What changes in the input data would have led to
    the flip in predictions etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![Risk Management Framework for AI/ML Models](../Images/0bc4baa19aa3551af1012306a34a6147.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: https://[www.darpa.mil](https://www.darpa.mil/)/program/explainable-artificial-intelligence'
  prefs: []
  type: TYPE_NORMAL
- en: There are various tools that help keep the risk in check, including but not
    limited to model interpretability, bias detection, and mitigation, model monitoring,
    and maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: I will elaborate more on each of these in my upcoming articles. So stay tuned.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Vidhi Chugh](https://vidhi-chugh.medium.com/)** is an award-winning AI/ML
    innovation leader and an AI Ethicist. She works at the intersection of data science,
    product, and research to deliver business value and insights. She is an advocate
    for data-centric science and a leading expert in data governance with a vision
    to build trustworthy AI solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Introduction to Giskard: Open-Source Quality Management for AI Models](https://www.kdnuggets.com/2023/11/giskard-introduction-giskard-opensource-quality-management-ai-models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Social User Authentication in Django Framework](https://www.kdnuggets.com/2023/01/social-user-authentication-django-framework.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Only Prompting Framework for Every Use](https://www.kdnuggets.com/the-only-prompting-framework-for-every-use)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Free 4 Week Data Science Course on AI Quality Management](https://www.kdnuggets.com/2022/02/truera-free-4-week-data-science-course-ai-quality-management.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, May 18: 5 Free Hosting Platform For Machine…](https://www.kdnuggets.com/2022/n20.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Things You Need To Know About Data Management And Why It Matters…](https://www.kdnuggets.com/2022/05/6-things-need-know-data-management-matters-computer-vision.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
