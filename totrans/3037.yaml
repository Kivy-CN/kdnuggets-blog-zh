- en: Common mistakes when carrying out machine learning and data science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/12/common-mistakes-data-science.html](https://www.kdnuggets.com/2018/12/common-mistakes-data-science.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Jekaterina Kokatjuhha](https://www.linkedin.com/in/jekaterina-kokatjuhha/),
    Research Engineer at Zalando.**'
  prefs: []
  type: TYPE_NORMAL
- en: This is part two of this series, find part one here - [**How to build a data
    science project from scratch**](https://www.kdnuggets.com/2018/12/build-data-science-project-from-scratch.html).
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: After scraping or getting the data, there are many steps to accomplish **before**
    applying a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: You need to visualize each of the variables to see distributions, find the outliers,
    and understand why there are such outliers.
  prefs: []
  type: TYPE_NORMAL
- en: What can you do with missing values in certain features?
  prefs: []
  type: TYPE_NORMAL
- en: What would be the best way to convert categorical features into numerical ones?
  prefs: []
  type: TYPE_NORMAL
- en: There are many such questions, but I will give some details on the ones where
    the majority of beginners encounter mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Visualization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Firstly, you should visualize the distribution of the continuous features to
    get a feeling if there are many outliers, what the distribution would be, and
    if it makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to visualize it, for example [box plots](https://www.khanacademy.org/math/statistics-probability/summarizing-quantitative-data/box-whisker-plots/a/box-plot-review), [histograms](https://en.wikipedia.org/wiki/Histogram), [cumulative
    distribution functio](https://en.wikipedia.org/wiki/Cumulative_distribution_function)ns,
    and [violin plots](https://en.wikipedia.org/wiki/Violin_plot). However, one should
    pick the plot that will give the most information about the data.
  prefs: []
  type: TYPE_NORMAL
- en: To see the distribution (if it is [normal](https://en.wikipedia.org/wiki/Normal_distribution),
    or [bimodal](https://en.wikipedia.org/wiki/Multimodal_distribution)), the histograms
    will be the most helpful. Although histograms are a good starting point, the box
    plots might be superior in identifying the number of outliers and seeing where
    the median quartiles lie.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the plots, the most interesting question would be: **do you see what
    you expected to see? **Answering this question will help you either in finding
    insights or finding bugs in the data.
  prefs: []
  type: TYPE_NORMAL
- en: To get inspired and understand what plot will give the most value, I frequently
    referred to the [Python’s seaborn gallery](https://seaborn.pydata.org/examples/index.html).
    Another good source of inspiration for the visualization and finding insights
    are kernels on Kaggle. [Here is my kaggle kernel](https://www.kaggle.com/jkokatjuhha/in-depth-visualisations-simple-methods) of
    the in-depth visualization of the titanic dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of rental prices, I plotted the histograms of each continuous
    feature and expected to see a long right tail in the distribution of the rent
    without bills and total area.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c89bc5e86afd341fed4b789799b00190.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Histograms of continuous features**'
  prefs: []
  type: TYPE_NORMAL
- en: Box plots helped me see the number of outliers for each of the features. In
    fact, most of the outliers apartments based on the rent without bills were either
    the ateliers for the small shops with more than 200m2 or the student dormitories
    with very low rent.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/404bdbb751bfd3f901a5b02fdb627afd.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Boxplots of continuous features**'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Do I impute the values based on the whole dataset?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sometimes there will be missing values, due to various reasons. If we exclude
    every observation with at least one missing value, we can end up with a very reduced
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There are [many ways of imputing](https://www.iriseekhout.com/missing-data/missing-data-methods/imputation-methods/) the
    values, mean, or median. It is up to you how to do it **but** make sure to calculate
    the imputation statistics **only on the training data to avoid **[**data leakage**](https://machinelearningmastery.com/data-leakage-machine-learning/) of
    your test set.
  prefs: []
  type: TYPE_NORMAL
- en: In the rental data, I also extracted a description of the apartment. Whenever
    the quality, condition, or type of apartment was missing, I would impute it from
    the description if the description contained this information.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. How do I transform categorical variables?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some algorithms, depending on the implementation, wouldn’t work directly with
    the categorical data, so one would need to somehow transform them into numerical
    values.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways of transforming categorical variables into numerical features,
    such as [Label Encoder, One Hot Encoding, bin encoding](http://pbpython.com/categorical-encoding.html),
    and hashing encoding. However, most people use the Label Encoding **incorrectly**when
    the One Hot Encoding should have been used instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume, in our rental data, that we have an apartment-type column with the
    following values: [ground floor, loft, maisonette, loft, loft, ground floor].
    LabelEncoder can turn this into [3,2,1,2,2,1], introducing ordinality, which means
    that ground_floor >loft > maisonette. For some algorithms like decision trees,
    and its deviations, this type of encoding for this feature would be fine, but
    applying regressions and SVM might not make that much sense.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the rental price dataset, the **condition** is encoded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: new:1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: renovated:2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'needs renovation: 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'and the **quality** as:'
  prefs: []
  type: TYPE_NORMAL
- en: Luxus:1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'better than normal: 2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'normal: 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'simple: 4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'unknown: 5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4\. Do I need to standardize variables?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Standardization brings all continuous variables to the same scale, meaning if
    one variable has values from 1K to 1M and another from 0.1 to 1, after standardization
    they will have the same range.
  prefs: []
  type: TYPE_NORMAL
- en: '[L1 or L2 regularizations](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c) are
    the common way of reducing [overfitting](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/) and
    can be used within many regression algorithms. However, it is important to apply
    feature standardization **before** L1 or L2.'
  prefs: []
  type: TYPE_NORMAL
- en: The rental price is in Euros so the fitted coefficient would be approximately
    100 times larger than the fitted coefficient if the price was in cents. L1 and
    L2 penalize the larger coefficients more, meaning it will penalize the features
    in smaller scales more. To prevent this, the features should be standardized before
    applying L1 or L2.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason to standardize is that if you or the your algorithm use gradient
    descent, gradient descent converges much faster with feature scaling.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Do I need to derive the logarithm of the target variable?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It took me a while to understand that there is **no universal answer**.
  prefs: []
  type: TYPE_NORMAL
- en: 'It depends on many factors:'
  prefs: []
  type: TYPE_NORMAL
- en: whether you want fractional or absolute error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: which algorithm you use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: what residual plots and changes in the metrics tell you
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In regression, firstly [pay attention to the residual plots](http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/) and
    the metric. Sometimes the logarithmization of the target variable leads to a better
    model and the results of the model would still be easy to understand. However,
    there are still other transformations that could be of interest, such as to taking
    the square root.
  prefs: []
  type: TYPE_NORMAL
- en: There are many answers on Stack Overflow regarding this question, and I think [Residual
    Plots and RMSE on raw and log target variable](https://stats.stackexchange.com/questions/319880/non-linear-regression-residual-plots-and-rmse-on-raw-and-log-target-variable) explains
    it very well.
  prefs: []
  type: TYPE_NORMAL
- en: For the rental data, I derived the logarithm of the price as the residual plots
    looked a bit better.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe1f5cc65c6786934dee94174e02a61f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Residual plots of the logarithms (left) and untransformed data (right) of
    rent not including the bills variable. The right plot exhibits “heteroscedasticity” — the
    residuals get larger as the prediction moves from small to large.**'
  prefs: []
  type: TYPE_NORMAL
- en: Some more important stuff
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some algorithms, such as regressions, will suffer from [collinearities](https://en.wikipedia.org/wiki/Multicollinearity) in
    the data because the coefficients become very unstable ([more math](http://www.stat.cmu.edu/~larry/=stat401/lecture-17.pdf)). [SVM](https://en.wikipedia.org/wiki/Support_vector_machine) [might
    or might not suffer](https://stats.stackexchange.com/questions/149662/is-support-vector-machine-sensitive-to-the-correlation-between-the-attributes) from
    collinearity due to the choice of kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Decision-based algorithms will not suffer from multicollinearity as they could
    use features interchangeably in different trees without it affecting the performance.
    However, the interpretation of feature importance then gets more difficult as
    the correlated variable may not appear to be as important as it is.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After you have familiarized yourself with data and cleaned out the outliers,
    it is the perfect time to get the hang of machine learning. There are many algorithms
    you could use for this supervised machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: There were three different algorithms I wanted to explore, comparing characterstics
    such as performance differences and speed. These three were gradient boosted trees
    with different implementations (XGBoost and LightGMB), Random Forest (FR, scikit-learn)
    and 3-layer Neuronal Networks (NN, Tensorflow). I selected RMSLE (root mean squared
    logarithm error) to be the metric for the optimization of the process. I used
    RMSLE because I derived the logarithm of the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost and LigthGBM performed comparably, RF slightly worse, whereas NN was
    the worst.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/baa8f77c7d797cc5f8898e8441cd8cf0.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Performance (RMSLE) of the algorithms on the test set.**'
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree-based algorithms are very good at interpreting features. For example,
    they produce a feature importance score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature importance: finding the drivers of the rental price'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After fitting a decision tree-based model, you can see what features are the
    most valuable for the price prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance provides a score that indicates how informative each feature
    was in the construction of the decision trees within the model. One of the ways
    to calculate this score is to count how many times a feature is used to split
    the data across all trees. This score can be computed in [different ways](https://datascience.stackexchange.com/questions/12318/how-do-i-interpret-the-output-of-xgboost-importance).
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance can reveal other insights about the main price drivers.
  prefs: []
  type: TYPE_NORMAL
- en: For the rental price prediction, it isn’t surprising that total area is the
    most important driver of the price. Interestingly, some features that were engineered
    with external API are also in the top most important features.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/818ee95c8e2f3873c5cbbad1f9d44e12.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Feature importance calculated by split (above) and by gain (below)**'
  prefs: []
  type: TYPE_NORMAL
- en: However, as mentioned in [“Interpretable Machine Learning with XGBoost”](https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27),
    there can be inconsistencies in feature importance depending on the attribution
    option. The author of the linked blogpost, and SHAP NIPS paper, proposes a new
    way of calculating feature importance that will be both accurate and consistent.
    This uses the [shap Python library](https://github.com/slundberg/shap). SHAP values
    represent the responsibility of a feature for a change in the model output.
  prefs: []
  type: TYPE_NORMAL
- en: The output of the analysis on the rental price data is shown in the figure below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba1d5ac0a74b69d880a8d7afc3cd0aae.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Each apartment has one dot on each row. The x position of the dot is the
    impact of that feature on the model’s prediction for the customer, and the color
    of the dot represents the value of that feature for the apartment**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The figure incorporates a lot of valuable information (features are sorted
    by mean (|Tree SHAP|)). Small disclaimer: data is from the beginning of 2018;
    the district can evolve and therefore the price-dependent factors could change.'
  prefs: []
  type: TYPE_NORMAL
- en: the proximity to the city center (kilometers till U-Bahn Stadtmitte by car and
    duration of a train trip to S-Bahn Friedrichstrasse) increases the predicted rental
    apartment price
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: total area as the strongest driver of the rental price
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if the apartment owner requires you to have a low income certificate (WBS in
    German), the predicted price is lower
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'renting an apartment in these districts would also increase the rental price:
    Mitte, Prenzlauer Berg, Wilmersdorf, Charlottenburg, Zehlendorf and Friedrichshain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'districts with lower prices would be: Spandau, Tempelhof, Wedding and Reinickendorf'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: obviously, an apartment in better condition — the lower value is the better — of
    better quality — the lower value is better — with furniture, a built-in kitchen,
    and elevator will cost more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interesting are the impacts of following features:'
  prefs: []
  type: TYPE_NORMAL
- en: duration to the nearest metro station
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: number of stations within 1 km.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Duration to the nearest metro station: **'
  prefs: []
  type: TYPE_NORMAL
- en: It seems that, for some apartments, the high value of this feature indicates
    the higher price. The reason for this is that these apartments are situated in
    very wealthy residential areas outside of Berlin.
  prefs: []
  type: TYPE_NORMAL
- en: 'One can also see that the proximity to the metro station has two directions:
    it lowers **and** it increases the price for some apartments. The reason could
    be that the apartments that are very close to metro station would also suffer
    from underground noise or vibrations caused by trains but, on the other hand,
    they would be well-connected to the public transportation. However, one could
    investigate a bit more into this feature as it shows the proximity only to the
    nearest metro stations and not tram/bus stations.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of stations within 1 km: **'
  prefs: []
  type: TYPE_NORMAL
- en: The same applies to the number of stations within one kilometer from the apartment.
    Many metro stations around would, in general, increase the rental price. However,
    it also had a negative effect — more noise.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble averaging
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After playing around with different models and comparing performance, you could
    just combine the results of each of the model and build an ensemble!
  prefs: []
  type: TYPE_NORMAL
- en: Bagging is the machine learning ensemble model that utilizes the predictions
    of several algorithms to calculate the final aggregated predictions. It is designed
    to prevent overfitting and reduces the variance of the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0acefa672b5eefe0d69f4a84bc74bd4.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Advantage of using ensembles: The red model performs better in the lower
    left box, however, the blue model performs better in the upper right box. By combining
    the predictions from both models, it could improve the overall performance. Fig
    taken from [here.](https://burakhimmetoglu.com/2016/12/01/stacking-models-for-improved-predictions/)**'
  prefs: []
  type: TYPE_NORMAL
- en: As I already had predictions from the above mentioned algorithms, I combined
    all four models in all possible ways and picked the seven best single and ensemble
    models based on the RMSLE of the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: Then the RMSLE of those seven models was calculated on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/54e522617bfcb8d870e87a119d1cebbc.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Test RMSLE of the algorithms.**'
  prefs: []
  type: TYPE_NORMAL
- en: The ensemble of three decision-tree based algorithms performed the best compared
    to each single model.
  prefs: []
  type: TYPE_NORMAL
- en: You could also produce a weighted ensemble, assigning more weight to a better
    single model. The reasoning behind it is that other models could overrule the
    best model only if they collectively agree on an alternative.
  prefs: []
  type: TYPE_NORMAL
- en: In reality, one would never know if an averaged ensemble would be better than
    the single model without just trying it out.
  prefs: []
  type: TYPE_NORMAL
- en: Stacked models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An averaged or weighted ensemble is not the only way to combine the predictions
    of different models. You could also stack the models in very different ways!
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind stacked models is to create several base models and a meta model
    on top of the results from the base models in order to produce final predictions.
    However, it is not so obvious how to train the meta model because it can be biased
    towards the best of the base models. A very good explanation of how to do it correctly
    can be found in the post [“Stacking models for improved predictions”](https://burakhimmetoglu.com/2016/12/01/stacking-models-for-improved-predictions/).
  prefs: []
  type: TYPE_NORMAL
- en: For the rental price case, stacked models didn’t improve the RMSLE at all — they
    even increased the metrics. There might be several reasons for this — either I
    coded it incorrectly ;) or there was just too much noise introduced by stacking.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to explore more of the ensemble and stacked model articles, the [Kaggle
    Ensemble Guide](https://mlwave.com/kaggle-ensembling-guide/) explains many different
    kinds of ensembling with the performance comparison and referrals on how such
    stacked models got to the top of Kaggle’s competitions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Final thoughts**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: listen to what people talk about around you; their complaining can serve as
    a good starting point for solving something big
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: let people find their own insights by providing interactive dashboards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: don’t restrict yourself to common feature engineering as multiplying two variables.
    Try to find additional sources of data or explanations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: try out ensembles and stacked models as those methods could improve the performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**And please, provide the date of the data you display!**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sources of figures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.pinterest.de/minimalcouture/paris-apartments/](https://www.pinterest.de/minimalcouture/paris-apartments/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.theodysseyonline.com/the-struggles-of-moving-into-your-first-apartment](https://www.theodysseyonline.com/the-struggles-of-moving-into-your-first-apartment)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.fashionbeans.com/content/the-worlds-10-smallest-apartments-are-downright-shocking/](https://www.fashionbeans.com/content/the-worlds-10-smallest-apartments-are-downright-shocking/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio**: [Jekaterina Kokatjuhha](https://www.linkedin.com/in/jekaterina-kokatjuhha/)
    is a Research Engineer at Zalando, focusing on scalable machine learning for fraud
    prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.freecodecamp.org/how-to-build-a-data-science-project-from-scratch-dc4f096a62a1).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resources:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Data Science Projects Employers Want To See: How To Show A Business Impact](https://www.kdnuggets.com/2018/12/data-science-projects-business-impact.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Common Mistakes That Lead to Bad Data Visualization](https://www.kdnuggets.com/2017/10/5-common-mistakes-bad-data-visualization.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10 Mistakes to Avoid When Adopting Advanced Analytics](https://www.kdnuggets.com/2018/07/tdwi-10-mistakes-avoid-advanced-analytics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
