- en: The Most Complete Guide to PyTorch for Data Scientists
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学家使用 PyTorch 的最完整指南
- en: 原文：[https://www.kdnuggets.com/2020/09/most-complete-guide-pytorch-data-scientists.html](https://www.kdnuggets.com/2020/09/most-complete-guide-pytorch-data-scientists.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/09/most-complete-guide-pytorch-data-scientists.html](https://www.kdnuggets.com/2020/09/most-complete-guide-pytorch-data-scientists.html)
- en: '[comments](#comments)![Header](../Images/43c438dc46265ffdd4bc06ff3d0f68a3.png)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[comments](#comments)![Header](../Images/43c438dc46265ffdd4bc06ff3d0f68a3.png)'
- en: '***PyTorch*** has sort of became one of the de facto standards for creating
    Neural Networks now, and I love its interface. Yet, it is somehow a little difficult
    for beginners to get a hold of.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '***PyTorch*** 现在已成为创建神经网络的事实标准之一，我喜欢它的接口。然而，对初学者来说，它还是有些难以掌握。'
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: I remember picking PyTorch up only after some extensive experimentation a couple
    of years back. To tell you the truth, it took me a lot of time to pick it up but
    am I glad that I moved from [Keras to PyTorch](https://towardsdatascience.com/moving-from-keras-to-pytorch-f0d4fff4ce79).* With
    its high customizability and pythonic syntax, *PyTorch is just a joy to work with,
    and I would recommend it to anyone who wants to do some heavy lifting with Deep
    Learning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我记得在几年前经过一些广泛的实验后才开始使用 PyTorch。说实话，我花了很多时间才掌握它，但我很高兴我从 [Keras 迁移到 PyTorch](https://towardsdatascience.com/moving-from-keras-to-pytorch-f0d4fff4ce79)。*凭借其高度自定义和
    Python 风格的语法，*PyTorch 的确是一种愉悦的工作工具，我会推荐给任何想要进行深度学习重载的人。
- en: So, in this PyTorch guide, ***I will try to ease some of the pain with PyTorch
    for starters*** and go through some of the most important classes and modules
    that you will require while creating any Neural Network with Pytorch.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本 PyTorch 指南中，***我会尝试减轻初学者在使用 PyTorch 时的痛苦***，并讲解创建任何神经网络所需的一些最重要的类和模块。
- en: But, that is not to say that this is aimed at beginners only as ***I will also
    talk about the*** ***high customizability PyTorch provides and will talk about
    custom Layers, Datasets, Dataloaders, and Loss functions***.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，这并不意味着本教程仅面向初学者，因为***我还会讨论*** ***PyTorch 提供的高度自定义功能，并讲解自定义层、数据集、数据加载器和损失函数***。
- en: So let’s get some coffee ☕ ️and start it up.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们喝杯咖啡 ☕️ 开始吧。
- en: Tensors
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Tensors
- en: '[Tensors](https://www.kdnuggets.com/2018/05/wtf-tensor.html) are the basic
    building blocks in PyTorch and put very simply, they are NumPy arrays but on GPU.
    In this part, I will list down some of the most used operations we can use while
    working with Tensors. This is by no means an exhaustive list of operations you
    can do with Tensors, but it is helpful to understand what tensors are before going
    towards the more exciting parts.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[Tensors](https://www.kdnuggets.com/2018/05/wtf-tensor.html) 是 PyTorch 中的基本构建块，简单来说，它们是
    GPU 上的 NumPy 数组。在这一部分，我将列出一些在处理 Tensors 时最常用的操作。这绝不是你可以对 Tensors 执行操作的详尽列表，但在深入更有趣的部分之前，了解
    tensors 是很有帮助的。'
- en: 1\. Create a Tensor
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1\. 创建 Tensor
- en: We can create a PyTorch tensor in multiple ways. This includes converting to
    tensor from a NumPy array. Below is just a small gist with some examples to start
    with, but you can do a whole lot of [more things](https://pytorch.org/docs/stable/tensors.html) with
    tensors just like you can do with NumPy arrays.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过多种方式创建 PyTorch tensor，其中包括从 NumPy 数组转换。以下只是一些入门示例，但你可以做很多其他事情，[更多内容](https://pytorch.org/docs/stable/tensors.html)
    就像你可以用 NumPy 数组做的那样。
- en: '![Image for post](../Images/be8c3218e6ad5ab93ddbc41fa18b206d.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Image for post](../Images/be8c3218e6ad5ab93ddbc41fa18b206d.png)'
- en: 2\. Tensor Operations
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2\. Tensor 操作
- en: Again, there are a lot of operations you can do on these tensors. The full list
    of functions can be found [here](https://pytorch.org/docs/stable/torch.html?highlight=mm#math-operations).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你可以对这些张量进行很多操作。所有函数的完整列表可以在 [这里](https://pytorch.org/docs/stable/torch.html?highlight=mm#math-operations)找到。
- en: '![Image for post](../Images/ba8d46e45d52acf8c3009404823db6fd.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![Image for post](../Images/ba8d46e45d52acf8c3009404823db6fd.png)'
- en: '**Note: **What are PyTorch Variables? In the previous versions of Pytorch,
    Tensor and Variables used to be different and provided different functionality,
    but now the Variable API is [deprecated](https://pytorch.org/docs/stable/autograd.html#variable-deprecated),
    and all methods for variables work with Tensors. So, if you don’t know about them,
    it’s fine as they re not needed, and if you know them, you can forget about them.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** PyTorch 变量是什么？在 PyTorch 的早期版本中，Tensor 和 Variables 是不同的并且提供了不同的功能，但现在
    Variable API 已经 [弃用](https://pytorch.org/docs/stable/autograd.html#variable-deprecated)，所有变量的方法都适用于
    Tensors。所以，如果你不了解它们，也没关系，因为它们已经不需要了，如果你知道它们，你可以忘记它们。'
- en: The nn.Module
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`nn.Module`'
- en: '![Figure](../Images/cbff37d276c1915bc74c150f60bef3d3.png)Photo by [Fernand
    De Canne](https://unsplash.com/@fernanddecanne?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/cbff37d276c1915bc74c150f60bef3d3.png)照片由 [Fernand De Canne](https://unsplash.com/@fernanddecanne?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: Here comes the fun part as we are now going to talk about some of the most used
    constructs in Pytorch while creating deep learning projects. nn.Module lets you
    create your Deep Learning models as a class. You can inherit from `nn.Module`
    to define any model as a class. Every model class necessarily contains an `__init__` procedure
    block and a block for the `forward` pass.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在到了有趣的部分，我们将讨论在创建深度学习项目时 PyTorch 中一些最常用的构造。`nn.Module` 允许你将深度学习模型作为类创建。你可以从
    `nn.Module` 继承来将任何模型定义为类。每个模型类必然包含一个 `__init__` 过程块和一个 `forward` 过程块。
- en: In the `__init__` part, the user can define all the layers the network is going
    to have but doesn't yet define how those layers would be connected to each other.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `__init__` 部分，用户可以定义网络将拥有的所有层，但尚未定义这些层将如何相互连接。
- en: In the `forward` pass block, the user defines how data flows from one layer
    to another inside the network.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `forward` 过程块中，用户定义数据如何在网络的各层之间流动。
- en: 'So, put simply, any network we define will look like:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们定义的任何网络都将如下所示：
- en: Here we have defined a very simple Network that takes an input of size 784 and
    passes it through two linear layers in a sequential manner. But the thing to note
    is that we can define any sort of calculation while defining the forward pass,
    and that makes PyTorch highly customizable for research purposes. For example,
    in our crazy experimentation mode, we might have used the below network where
    we arbitrarily attach our layers. Here we send back the output from the second
    linear layer back again to the first one after adding the input to it(skip connection)
    back again(I honestly don’t know what that will do).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个非常简单的网络，它接收大小为 784 的输入，并将其依次通过两个线性层。但需要注意的是，在定义前向传递时，我们可以定义任何计算，这使得
    PyTorch 在研究中高度可定制。例如，在我们的疯狂实验模式中，我们可能使用了下面的网络，其中我们任意地附加了我们的层。在这里，我们将第二个线性层的输出再次发送回第一个层，并将输入加到它上面（跳跃连接）（老实说，我不知道这会有什么效果）。
- en: We can also check if the neural network forward pass works. I usually do that
    by first creating some random input and just passing that through the network
    I have created.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以检查神经网络前向传递是否有效。我通常通过首先创建一些随机输入，然后将其通过我创建的网络来检查。
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A word about Layers
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于层的说明
- en: Pytorch is pretty powerful, and you can actually create any new experimental
    layer by yourself using `nn.Module`. For example, rather than using the predefined
    Linear Layer `nn.Linear` from Pytorch above, we could have created our **custom
    linear layer**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 功能强大，你实际上可以使用 `nn.Module` 自行创建任何新的实验层。例如，我们本可以创建我们自己的 **自定义线性层**，而不是使用上述的
    PyTorch 预定义线性层 `nn.Linear`。
- en: 'You can see how we wrap our weights tensor in `nn.Parameter.` This is done
    to make the tensor to be considered as a model parameter. From PyTorch [docs](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '你可以看到我们是如何将权重张量包装在`nn.Parameter.`中的。这是为了让张量被视为模型参数。来自 PyTorch 的 [文档](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter):'
- en: Parameters are `[*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)` subclasses,
    that have a very special property when used with `*Module*` - when they’re assigned
    as Module attributes they are automatically added to the list of its parameters,
    and will appear in `*parameters()*` iterator
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参数是 `[*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)`
    子类，这些子类在与 `*Module*` 一起使用时具有一个非常特殊的属性——当它们被分配为模块属性时，它们会自动添加到参数列表中，并会出现在 `*parameters()*`
    迭代器中。
- en: As you will later see, the `model.parameters()` iterator will be an input to
    the optimizer. But more on that later.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如你稍后将看到的，`model.parameters()` 迭代器将作为优化器的输入。但更多细节稍后再谈。
- en: Right now, we can now use this custom layer in any PyTorch network, just like
    any other layer.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以像使用其他层一样在任何 PyTorch 网络中使用这个自定义层。
- en: 'But then again, Pytorch would not be so widely used if it didn’t provide a
    lot of ready to made layers used very frequently in wide varieties of Neural Network
    architectures. Some examples are: `[nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)`,
    `[nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)`,
    `[nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)`,
    `[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)`, `[nn.BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d)`,
    `[nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout)`,
    `[nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding)`, `[nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU)/[nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM)`,
    `[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax)`, `[nn.LogSoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax)`,
    `[nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention)`,
    `[nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder)`,
    `[nn.TransformerDecoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder)`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，如果 PyTorch 没有提供大量在各种神经网络架构中经常使用的现成层，它就不会被如此广泛使用。一些示例包括：[nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)、[nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)、[nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)、[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)、`[nn.BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d)`、[nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout)、[nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding)、`[nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU)/[nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM)`、[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax)、`[nn.LogSoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax)`、[nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention)、[nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder)、[nn.TransformerDecoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder)。
- en: I have linked all the layers to their source where you could read all about
    them, but to show how I usually try to understand a layer and read the docs, I
    would try to look at a very simple convolutional layer here.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我已将所有层链接到其来源，你可以在这些来源中阅读所有相关信息，但为了展示我通常如何理解一层并阅读文档，我将尝试查看一个非常简单的卷积层。
- en: '![Image for post](../Images/c6dbe2d59cfe06a6d60c85ce6574c285.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![Image for post](../Images/c6dbe2d59cfe06a6d60c85ce6574c285.png)'
- en: So, a Conv2d Layer needs as input an Image of height H and width W, with `Cin` channels.
    Now, for the first layer in a convnet, the number of `in_channels` would be 3
    (RGB), and the number of `out_channels` can be defined by the user. The `kernel_size`
    mostly used is 3x3, and the `stride` normally used is 1.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`Conv2d` 层需要一个高度为 H 和宽度为 W 的图像作为输入，并具有`Cin`通道。对于 convnet 的第一层，`in_channels`
    的数量通常是 3（RGB），`out_channels` 的数量可以由用户定义。最常用的 `kernel_size` 是 3x3，而通常使用的 `stride`
    是 1。
- en: 'To check a new layer which I don’t know much about, I usually try to see the
    input as well as output for the layer like below where I would first initialize
    the layer:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查我不太了解的新层，我通常会尝试查看该层的输入和输出，如下所示，我会首先初始化该层：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: And then pass some random input through it. Here 100 is the batch size.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过它传递一些随机输入。在这里，100是批量大小。
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: So, we get the output from the convolution operation as required, and I have
    sufficient information on how to use this layer in any Neural Network I design.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们根据需要从卷积操作中获得输出，并且我有足够的信息来了解如何在我设计的任何神经网络中使用这个层。
- en: Datasets and DataLoaders
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集和DataLoader
- en: 'How would we pass data to our Neural nets while training or while testing?
    We can definitely pass tensors as we have done above, but Pytorch also provides
    us with pre-built Datasets to make it easier for us to pass data to our neural
    nets. You can check out the complete list of datasets provided at [torchvision.datasets](https://pytorch.org/docs/stable/torchvision/datasets.html) and [torchtext.datasets](https://pytorch.org/text/datasets.html).
    But, to give a concrete example for datasets, let’s say we had to pass images
    to an Image Neural net using a folder which has images in this structure:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练或测试时如何将数据传递给我们的神经网络？我们可以像上面那样传递张量，但Pytorch还提供了预构建的数据集，方便我们将数据传递给神经网络。你可以查看[torchvision.datasets](https://pytorch.org/docs/stable/torchvision/datasets.html)和[torchtext.datasets](https://pytorch.org/text/datasets.html)提供的完整数据集列表。但为了给出一个具体的数据集例子，假设我们需要通过一个包含以下结构的文件夹将图像传递给图像神经网络：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can use `torchvision.datasets.ImageFolder` dataset to get an example image
    like below:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`torchvision.datasets.ImageFolder`数据集来获取如下例图片：
- en: '![Image for post](../Images/8f36e3e7f1066b9c820066cbac26ddbf.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![文章图片](../Images/8f36e3e7f1066b9c820066cbac26ddbf.png)'
- en: 'This dataset has 847 images, and we can get an image and its label using an
    index. Now we can pass images one by one to any image neural network using a for
    loop:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集有847张图片，我们可以使用索引来获取图片及其标签。现在我们可以使用for循环将图片逐一传递给任何图像神经网络：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***But that is not optimal. We want to do batching. ***We can actually write
    some more code to append images and labels in a batch and then pass it to the
    Neural network. But Pytorch provides us with a utility iterator `torch.utils.data.DataLoader` to
    do precisely that. Now we can simply wrap our `train_dataset` in the Dataloader,
    and we will get batches instead of individual examples.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '***但这不是最优的。我们想要进行批量处理。***我们实际上可以编写更多代码，将图片和标签附加到一个批次中，然后将其传递给神经网络。但Pytorch为我们提供了一个实用的迭代器`torch.utils.data.DataLoader`来准确完成这个任务。现在我们可以简单地将我们的`train_dataset`包装在Dataloader中，这样我们将获得批次，而不是单独的示例。'
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can simply iterate with batches using:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用批量迭代：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'So actually, the whole process of using datasets and Dataloaders becomes:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 所以实际上，使用数据集和DataLoader的整个过程变成了：
- en: You can look at this particular example in action in my previous blogpost on
    Image classification using Deep Learning [here](https://towardsdatascience.com/end-to-end-pipeline-for-setting-up-multiclass-image-classification-for-data-scientists-2e051081d41c).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我之前的博客文章中查看这个特定的例子，关于使用深度学习的图像分类[在这里](https://towardsdatascience.com/end-to-end-pipeline-for-setting-up-multiclass-image-classification-for-data-scientists-2e051081d41c)。
- en: This is great, and Pytorch does provide a lot of functionality out of the box.
    But the main power of Pytorch comes with its immense customization. We can also
    create our own custom datasets if the datasets provided by PyTorch don’t fit our
    use case.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常好，Pytorch确实提供了许多开箱即用的功能。但Pytorch的主要优势在于其巨大的自定义能力。如果Pytorch提供的数据集不适合我们的用例，我们还可以创建自己的自定义数据集。
- en: Understanding Custom Datasets
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解自定义数据集
- en: To write our custom datasets, we can make use of the abstract class `torch.utils.data.Dataset` provided
    by Pytorch. We need to inherit this `Dataset` class and need to define two methods
    to create a custom Dataset.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要编写自定义数据集，我们可以利用Pytorch提供的抽象类`torch.utils.data.Dataset`。我们需要继承这个`Dataset`类，并定义两个方法来创建自定义数据集。
- en: '`__len__` : a function that returns the size of the dataset. This one is pretty
    simple to write in most cases.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__len__`：一个函数，返回数据集的大小。在大多数情况下，这个函数编写起来相当简单。'
- en: '`__getitem__`: a function that takes as input an index `i` and returns the
    sample at index `i`.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__getitem__`：一个函数，接收一个索引`i`作为输入，返回索引`i`处的样本。'
- en: For example, we can create a simple custom dataset that returns an image and
    a label from a folder. See that most of the tasks are happening in `__init__` part
    where we use `glob.glob` to get image names and do some general preprocessing.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以创建一个简单的自定义数据集，从文件夹中返回图像和标签。请注意，大多数任务都发生在`__init__`部分，在这里我们使用`glob.glob`来获取图像名称并进行一些通用预处理。
- en: Also, note that we open our images one at a time in the` __getitem__` method
    and not while initializing. This is not done in `__init__` because we don't want
    to load all our images in the memory and just need to load the required ones.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请注意，我们在`__getitem__`方法中逐个打开图像，而不是在初始化时。这么做是因为我们不想将所有图像加载到内存中，只需加载需要的图像即可。
- en: We can now use this dataset with the utility `Dataloader` just like before.
    It works just like the previous dataset provided by PyTorch but without some utility
    functions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以像之前一样使用这个数据集与`Dataloader`。它的工作方式和PyTorch之前提供的数据集一样，只是没有一些实用函数。
- en: Understanding Custom DataLoaders
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解自定义 DataLoaders
- en: '**This particular section is a little advanced and can be skipped going through
    this post as it will not be needed in a lot of situations.** But I am adding it
    for completeness here.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**这一部分稍微复杂一些，通常情况下可以跳过，因为在很多情况下并不需要。** 但我在这里添加是为了完整性。'
- en: So let’s say you are looking to provide batches to a network that processes
    text input, and the network could take sequences with any sequence size as long
    as the size remains constant in the batch. For example, we can have a BiLSTM network
    that can process sequences of any length. It’s alright if you don’t understand
    the layers used in it right now; just know that it can process sequences with
    variable sizes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说你希望为处理文本输入的网络提供批次，该网络可以处理任何序列长度，只要批次中的大小保持一致。例如，我们可以有一个BiLSTM网络，它可以处理任何长度的序列。如果你现在不理解其中使用的层也没关系；只需知道它可以处理变长序列即可。
- en: This network expects its input to be of shape (`batch_size`, `seq_length`) and
    works with any `seq_length`. We can check this by passing our model two random
    batches with different sequence lengths(10 and 25).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络期望输入形状为(`batch_size`, `seq_length`)，并且可以处理任何`seq_length`。我们可以通过将模型传入两个不同序列长度（10和25）的随机批次来检查这一点。
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, we want to provide tight batches to this model, such that each batch has
    the same sequence length based on the max sequence length in the batch to minimize
    padding. This has an added benefit of making the neural net run faster. It was,
    in fact, one of the methods used in the winning submission of the Quora Insincere
    challenge in Kaggle, where running time was of utmost importance.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们希望为模型提供紧凑的批次，使得每个批次的序列长度相同，基于批次中的最大序列长度来最小化填充。这还有一个附加好处，就是让神经网络运行得更快。实际上，这是Kaggle
    Quora Insincere挑战赛获胜提交中使用的方法之一，当时运行时间至关重要。
- en: So, how do we do this? Let’s write a very simple custom dataset class first.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何做到这一点呢？我们先编写一个非常简单的自定义数据集类。
- en: Also, let’s generate some random data which we will use with this custom Dataset.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们来生成一些随机数据，用于这个自定义数据集。
- en: '![Figure](../Images/09c6e3ad648b1c216c3dffeec8a2eb70.png)Example of one random
    sequence and label. Each integer in the sequence corresponds to a word in the
    sentence.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure](../Images/09c6e3ad648b1c216c3dffeec8a2eb70.png)一个随机序列和标签的示例。序列中的每个整数对应句子中的一个词。'
- en: 'We can use the custom dataset now using:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用自定义数据集：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If we now try to use the Dataloader on this dataset with `batch_size`>1, we
    will get an error. Why is that?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试在这个数据集上使用`batch_size`>1的Dataloader，我们将得到一个错误。为什么会这样？
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Image for post](../Images/f16ed32a2e9e8eb50c11b791e59b59ff.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![Image for post](../Images/f16ed32a2e9e8eb50c11b791e59b59ff.png)'
- en: This happens because the sequences have different lengths, and our data loader
    expects our sequences of the same length. Remember that in the previous image
    example, we resized all images to size 224 using the transforms, so we didn’t
    face this error.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为序列的长度不同，而我们的数据加载器期望序列长度相同。记住，在前面的图像示例中，我们使用变换将所有图像调整为224的大小，因此我们没有遇到这个错误。
- en: '***So, how do we iterate through this dataset so that each batch has sequences
    with the same length, but different batches may have different sequence lengths?***'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '***那么，我们如何遍历这个数据集，使得每个批次的序列长度相同，但不同批次可能有不同的序列长度？***'
- en: We can use `collate_fn` parameter in the DataLoader that lets us define how
    to stack sequences in a particular batch. To use this, we need to define a function
    that takes as input a batch and returns (`x_batch`, `y_batch` ) with padded sequence
    lengths based on `max_sequence_length` in the batch. The functions I have used
    in the below function are simple NumPy operations. Also, the function is properly
    commented so you can understand what is happening.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在DataLoader中使用`collate_fn`参数，这让我们可以定义如何在特定批次中堆叠序列。要使用它，我们需要定义一个函数，该函数以批次作为输入，并根据批次中的`max_sequence_length`返回(`x_batch`，`y_batch`)及其填充的序列长度。我在下面的函数中使用的函数是简单的NumPy操作。此外，函数已适当注释，以便你理解发生了什么。
- en: 'We can now use this `collate_fn` with our Dataloader as:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将这个`collate_fn`与我们的Dataloader一起使用，如下所示：
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Figure](../Images/071eb46c3a18b3d3e7c289a3cb5d1471.png)See that the batches
    have different sequence lengths now'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图](../Images/071eb46c3a18b3d3e7c289a3cb5d1471.png)现在可以看到批次具有不同的序列长度'
- en: It will work this time as we have provided a custom `collate_fn.` And see that
    the batches have different sequence lengths now. Thus we would be able to train
    our BiLSTM using variable input sizes just like we wanted.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这次会有效，因为我们提供了自定义的`collate_fn`。现在可以看到批次具有不同的序列长度。因此，我们将能够使用可变输入大小来训练我们的BiLSTM，就像我们想要的那样。
- en: Training a Neural Network
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练一个神经网络
- en: 'We know how to create a neural network using `nn.Module.` But how to train
    it? Any neural network that has to be trained will have a training loop that will
    look something similar to below:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道如何使用`nn.Module`创建神经网络。但是如何训练它呢？任何需要训练的神经网络都会有一个训练循环，其形式大致如下：
- en: 'In the above code, we are running five epochs and in each epoch:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们运行了五个训练周期，并且在每个周期中：
- en: We iterate through the dataset using a data loader.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用数据加载器遍历数据集。
- en: In each iteration, we do a forward pass using `model(x_batch)`
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每次迭代中，我们使用`model(x_batch)`进行前向传播。
- en: We calculate the Loss using a `loss_criterion`
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`loss_criterion`计算损失。
- en: We back-propagate that loss using `loss.backward()` call. We don't have to worry
    about the calculation of the gradients at all, as this simple call does it all
    for us.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`loss.backward()`调用反向传播损失。我们无需担心梯度计算，因为这个简单的调用为我们完成了所有工作。
- en: Take an optimizer step to change the weights in the whole network using `optimizer.step()`.
    This is where weights of the network get modified using the gradients calculated
    in `loss.backward()` call.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`optimizer.step()`进行优化器步进，以更改整个网络中的权重。这是网络权重根据`loss.backward()`调用计算的梯度进行修改的地方。
- en: We go through the validation data loader to check the validation score/metrics.
    Before doing validation, we set the model to eval mode using `model.eval().`Please
    note we don't back-propagate losses in eval mode.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过验证数据加载器检查验证得分/指标。在进行验证之前，我们使用`model.eval()`将模型设置为评估模式。请注意，在评估模式下，我们不会进行损失反向传播。
- en: Till now, we have talked about how to use `nn.Module` to create networks and
    how to use Custom Datasets and Dataloaders with Pytorch. So let's talk about the
    various options available for Loss Functions and Optimizers.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了如何使用`nn.Module`创建网络以及如何在Pytorch中使用自定义数据集和数据加载器。接下来我们来谈谈损失函数和优化器的各种选择。
- en: Loss functions
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失函数
- en: Pytorch provides us with a variety of [loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) for
    our most common tasks, like Classification and Regression. Some most used examples
    are `[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`,
    `[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)`,
    `[nn.KLDivLoss](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss)`
    and `[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss).`
    You can read the documentation of each loss function, but to explain how to use
    these loss functions, I will go through the example of `[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)`
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch为我们提供了各种[损失函数](https://pytorch.org/docs/stable/nn.html#loss-functions)，适用于最常见的任务，比如分类和回归。一些常用的例子有`[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`、`[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)`、`[nn.KLDivLoss](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss)`和`[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)`。你可以阅读每个损失函数的文档，但为了解释如何使用这些损失函数，我将以`[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)`为例。
- en: '![Image for post](../Images/cfb45e3ebca92aab225660e6ddccde92.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![Image for post](../Images/cfb45e3ebca92aab225660e6ddccde92.png)'
- en: 'The documentation for NLLLoss is pretty succinct. As in, this loss function
    is used for Multiclass classification, and based on the documentation:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: NLLLoss 的文档非常简洁。即，这个损失函数用于多类别分类，文档中说明：
- en: the input expected needs to be of size (`batch_size` x `Num_Classes` ) — These
    are the predictions from the Neural Network we have created.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 期望的输入大小应为 (`batch_size` x `Num_Classes`)——这些是我们创建的神经网络的预测结果。
- en: We need to have the log-probabilities of each class in the input — To get log-probabilities
    from a Neural Network, we can add a `LogSoftmax` Layer as the last layer of our
    network.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要在输入中获得每个类别的对数概率——为了从神经网络中获取对数概率，我们可以在网络的最后一层添加一个 `LogSoftmax` 层。
- en: The target needs to be a tensor of classes with class numbers in the range(0,
    C-1) where C is the number of classes.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标需要是一个包含类编号的张量，类编号范围在 (0, C-1) 之间，其中 C 是类别数。
- en: So, we can try to use this Loss function for a simple classification network.
    Please note the `LogSoftmax` layer after the final linear layer. If you don't
    want to use this `LogSoftmax` layer, you could have just used `[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以尝试将这个损失函数用于一个简单的分类网络。请注意，在最后一个线性层后面的 `LogSoftmax` 层。如果你不想使用这个 `LogSoftmax`
    层，你可以直接使用 `[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`。
- en: 'Let’s define a random input to pass to our network to test it:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个随机输入以传递给网络进行测试：
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And pass it through the model to get predictions:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 并通过模型进行预测：
- en: '[PRE12]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can now get the loss as:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以得到损失值：
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Custom Loss Function
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义损失函数
- en: Defining your custom loss functions is again a piece of cake, and you should
    be okay as long as you use tensor operations in your loss function. For example,
    here is the `customMseLoss`
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义损失函数的定义也很简单，只要在损失函数中使用张量操作就可以了。例如，这里是 `customMseLoss`。
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You can use this custom loss just like before. But note that we don’t instantiate
    the loss using criterion this time as we have defined it as a function.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像以前一样使用这个自定义损失函数。但请注意，这次我们没有使用 criterion 实例化损失函数，因为我们已将其定义为函数。
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If we wanted, we could have also written it as a class using `nn.Module` ,
    and then we would have been able to use it as an object. Here is an NLLLoss custom
    example:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们愿意，也可以将其写成一个类，使用 `nn.Module`，然后可以将其作为对象使用。这里是一个 NLLLoss 自定义示例：
- en: Optimizers
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化器
- en: 'Once we get gradients using the `loss.backward()` call, we need to take an
    optimizer step to change the weights in the whole network. Pytorch provides a
    variety of different ready to use optimizers using the `torch.optim` module. For
    example: `[torch.optim.Adadelta](https://pytorch.org/docs/stable/optim.html#torch.optim.Adadelta)`,
    `[torch.optim.Adagrad](https://pytorch.org/docs/stable/optim.html#torch.optim.Adagrad)`,
    `[torch.optim.RMSprop](https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop)`
    and the most widely used `[torch.optim.Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam).`
    To use the most used Adam optimizer from PyTorch, we can simply instantiate it
    with:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '一旦我们使用 `loss.backward()` 调用获得梯度，我们需要进行优化器步骤来改变整个网络中的权重。Pytorch 提供了多种现成的优化器，使用
    `torch.optim` 模块。例如: `[torch.optim.Adadelta](https://pytorch.org/docs/stable/optim.html#torch.optim.Adadelta)`、`[torch.optim.Adagrad](https://pytorch.org/docs/stable/optim.html#torch.optim.Adagrad)`、`[torch.optim.RMSprop](https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop)`
    和最广泛使用的 `[torch.optim.Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)`。要使用
    Pytorch 中最常用的 Adam 优化器，我们可以简单地实例化它：'
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: And then use `optimizer**.**zero_grad()` and `optimizer.step()` while training
    the model.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在训练模型时使用 `optimizer**.**zero_grad()` 和 `optimizer.step()`。
- en: I am not discussing how to write custom optimizers as it is an infrequent use
    case, but if you want to have more optimizers, do check out the [pytorch-optimizer](https://pytorch-optimizer.readthedocs.io/en/latest/) library,
    which provides a lot of other optimizers used in research papers. Also, if you
    anyhow want to create your own optimizers, you can take inspiration using the
    source code of implemented optimizers in [PyTorch](https://github.com/pytorch/pytorch/tree/master/torch/optim) or [pytorch-optimizers](https://github.com/jettify/pytorch-optimizer/tree/master/torch_optimizer).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会讨论如何编写自定义优化器，因为这是一个不常见的用例，但如果你想要更多的优化器，请查看 [pytorch-optimizer](https://pytorch-optimizer.readthedocs.io/en/latest/)
    库，它提供了许多在研究论文中使用的其他优化器。此外，如果你想要创建自己的优化器，可以参考 [PyTorch](https://github.com/pytorch/pytorch/tree/master/torch/optim)
    或 [pytorch-optimizers](https://github.com/jettify/pytorch-optimizer/tree/master/torch_optimizer)
    中已实现优化器的源代码来获取灵感。
- en: '![Figure](../Images/8fa38ecb11772198ed87d804bc7c312e.png)Other optimizers from [pytorch-optimizer](https://github.com/jettify/pytorch-optimizer) library'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![图](../Images/8fa38ecb11772198ed87d804bc7c312e.png)来自 [pytorch-optimizer](https://github.com/jettify/pytorch-optimizer)
    库的其他优化器'
- en: Using GPU/Multiple GPUs
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 GPU/多个 GPU
- en: Till now, whatever we have done is on the CPU. If you want to use a GPU, you
    can put your model to GPU using `model.to('cuda')`. Or if you want to use multiple
    GPUs, you can use `nn.DataParallel`. Here is a utility function that checks the
    number of GPUs in the machine and sets up parallel training automatically using `DataParallel` if
    needed.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所做的都是在 CPU 上进行的。如果你想使用 GPU，你可以通过 `model.to('cuda')` 将模型转移到 GPU 上。或者，如果你想使用多个
    GPU，你可以使用 `nn.DataParallel`。这里有一个实用函数，可以检查机器中的 GPU 数量，并在需要时使用 `DataParallel` 自动设置并行训练。
- en: The only thing that we will need to change is that we will load our data to
    GPU while training if we have GPUs. It’s as simple as adding a few lines of code
    to our training loop.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们唯一需要改变的就是在训练时如果有 GPU，就将数据加载到 GPU 上。这只需在训练循环中添加几行代码即可完成。
- en: Conclusion
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: Pytorch provides a lot of customizability with minimal code. While at first,
    it might be hard to understand how the whole ecosystem is structured with classes,
    in the end, it is simple Python. In this post, I have tried to break down most
    of the parts you might need while using Pytorch, and I hope it makes a little
    more sense for you after reading this.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch 提供了大量的自定义选项，且代码量最小。虽然一开始，理解整个生态系统如何通过类来构建可能比较困难，但最终，它还是简单的 Python。在这篇文章中，我尝试分解你在使用
    Pytorch 时可能需要的大部分内容，希望阅读后你能更好地理解这些内容。
- en: You can find the code for this post here on my [GitHub](https://github.com/MLWhiz/data_science_blogs/tree/master/pytorch_guide) repo,
    where I keep codes for all my blogs.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我的 [GitHub](https://github.com/MLWhiz/data_science_blogs/tree/master/pytorch_guide)
    仓库中找到这篇文章的代码，我在那里保存了所有博客的代码。
- en: If you want to learn more about Pytorch using a course based structure, take
    a look at the [Deep Neural Networks with PyTorch](https://www.coursera.org/learn/deep-neural-networks-with-pytorch?ranMID=40328&ranEAID=lVarvwc5BD0&ranSiteID=lVarvwc5BD0-Mh_whR0Q06RCh47zsaMVBQ&siteID=lVarvwc5BD0-Mh_whR0Q06RCh47zsaMVBQ&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0) course
    by IBM on Coursera. Also, if you want to know more about Deep Learning, I would
    like to recommend this excellent course on [Deep Learning in Computer Vision](https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0) in
    the [Advanced machine learning specialization](https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想通过课程结构学习更多关于 Pytorch 的内容，可以查看 IBM 在 Coursera 上提供的 [深度神经网络与 PyTorch](https://www.coursera.org/learn/deep-neural-networks-with-pytorch?ranMID=40328&ranEAID=lVarvwc5BD0&ranSiteID=lVarvwc5BD0-Mh_whR0Q06RCh47zsaMVBQ&siteID=lVarvwc5BD0-Mh_whR0Q06RCh47zsaMVBQ&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0)
    课程。另外，如果你想了解更多关于深度学习的内容，我推荐这个 [计算机视觉中的深度学习](https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0)
    课程，它属于 [高级机器学习专修](https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0)。
- en: Thanks for the read. I am going to be writing more beginner-friendly posts in
    the future too. Follow me up at [**Medium**](https://medium.com/@rahul_agarwal?source=post_page---------------------------) or
    Subscribe to my [**blog**](http://eepurl.com/dbQnuX?source=post_page---------------------------) to
    be informed about them. As always, I welcome feedback and constructive criticism
    and can be reached on Twitter [**@mlwhiz**](https://twitter.com/MLWhiz?source=post_page---------------------------).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读。我将来也会写更多适合初学者的文章。关注我在 [**Medium**](https://medium.com/@rahul_agarwal?source=post_page---------------------------) 或订阅我的 [**博客**](http://eepurl.com/dbQnuX?source=post_page---------------------------) 以获取最新信息。正如往常一样，我欢迎反馈和建设性的批评，可以通过Twitter联系我 [**@mlwhiz**](https://twitter.com/MLWhiz?source=post_page---------------------------)。
- en: Also, a small disclaimer — There might be some affiliate links in this post
    to relevant resources, as sharing knowledge is never a bad idea.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，简单的免责声明——这篇文章中可能包含一些关联链接，分享知识从来没有坏处。
- en: '**Bio: [Rahul Agarwal](https://www.linkedin.com/in/rahulagwl/)** is Senior
    Statistical Analyst at WalmartLabs. Follow him on Twitter [@mlwhiz](https://twitter.com/MLWhiz).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介：[Rahul Agarwal](https://www.linkedin.com/in/rahulagwl/)** 是WalmartLabs的高级统计分析师。关注他的Twitter [@mlwhiz](https://twitter.com/MLWhiz)。'
- en: '[Original](https://towardsdatascience.com/minimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b).
    Reposted with permission.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/minimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b)。已获授权转载。'
- en: '**Related:**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[6 bits of advice for Data Scientists](/2019/09/advice-data-scientists.html)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[给数据科学家的6条建议](/2019/09/advice-data-scientists.html)'
- en: '[The Hitchhiker’s Guide to Feature Extraction](/2019/06/hitchhikers-guide-feature-extraction.html)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[特征提取的指南](/2019/06/hitchhikers-guide-feature-extraction.html)'
- en: '[The 5 Classification Evaluation Metrics Every Data Scientist Must Know](/2019/10/5-classification-evaluation-metrics-every-data-scientist-must-know.html)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学家必须知道的5种分类评估指标](/2019/10/5-classification-evaluation-metrics-every-data-scientist-must-know.html)'
- en: More On This Topic
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[The Complete Free PyTorch Course for Deep Learning](https://www.kdnuggets.com/2022/10/complete-free-pytorch-course-deep-learning.html)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[完全免费的PyTorch深度学习课程](https://www.kdnuggets.com/2022/10/complete-free-pytorch-course-deep-learning.html)'
- en: '[How to Build a Data Science Enablement Team: A Complete Guide](https://www.kdnuggets.com/2022/10/build-data-science-enablement-team-complete-guide.html)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何建立数据科学赋能团队：完整指南](https://www.kdnuggets.com/2022/10/build-data-science-enablement-team-complete-guide.html)'
- en: '[Geocoding in Python: A Complete Guide](https://www.kdnuggets.com/2022/11/geocoding-python-complete-guide.html)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python中的地理编码：完整指南](https://www.kdnuggets.com/2022/11/geocoding-python-complete-guide.html)'
- en: '[A Complete Guide To Decision Tree Software](https://www.kdnuggets.com/2022/08/complete-guide-decision-tree-software.html)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[决策树软件的完整指南](https://www.kdnuggets.com/2022/08/complete-guide-decision-tree-software.html)'
- en: '[A Practical Guide to Transfer Learning using PyTorch](https://www.kdnuggets.com/2023/06/practical-guide-transfer-learning-pytorch.html)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用PyTorch进行迁移学习的实用指南](https://www.kdnuggets.com/2023/06/practical-guide-transfer-learning-pytorch.html)'
- en: '[A Beginner''s Guide to PyTorch](https://www.kdnuggets.com/a-beginners-guide-to-pytorch)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学者的PyTorch指南](https://www.kdnuggets.com/a-beginners-guide-to-pytorch)'
