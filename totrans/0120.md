# **教科书就是你所需的一切**：一种颠覆性的 AI 培训方法

> 原文：[https://www.kdnuggets.com/2023/07/textbooks-all-you-need-revolutionary-approach-ai-training.html](https://www.kdnuggets.com/2023/07/textbooks-all-you-need-revolutionary-approach-ai-training.html)

![教科书就是你所需的一切：一种颠覆性的 AI 培训方法](../Images/0d8f5143e382734bcb1df502380dac67.png)

图片由作者使用 Midjourney 创建

# 介绍

研究人员总是寻求更好、更创新的方式来培训人工智能模型。一篇 [微软最近的论文](https://arxiv.org/abs/2306.11644) 提出了一个有趣的方法——使用合成教科书来教导模型，而不是通常使用的大量数据集。

论文介绍了一种称为 Phi-1 的模型，该模型完全基于定制的教科书进行训练。研究人员发现，这与在大量数据上训练的更大模型在某些任务中同样有效。

标题“**教科书就是你所需的一切**”巧妙地参考了 AI 中著名的概念“**注意力即一切**”。但这里他们颠覆了这个想法——与其关注模型架构本身，他们展示了像教科书中那样的高质量、精心策划的训练数据的价值。

关键见解是，一个经过深思熟虑、设计良好的数据集可以与庞大、无焦点的数据堆一样有用。因此，研究人员精心编制了一本合成教科书，以细致地提供模型所需的知识。

基于教科书的方法是一种引人入胜的新方向，可以有效地培训 AI 模型，使其在特定任务中表现出色。这突出了训练数据的策划和质量的重要性，而不仅仅是数据量的暴力方式。

# 关键点

+   尽管 Phi-1 模型显著小于 GPT-3 等模型，但在 Python 编码任务中表现优异。这表明，模型的大小并非决定 AI 模型表现的唯一因素。

+   研究人员使用了合成教科书进行培训，强调了高质量、精心策划的数据的重要性。这种方法可能会颠覆我们对 AI 模型培训的思考方式。

+   当对 Phi-1 模型进行合成练习和解答的微调时，其性能显著提高，这表明有针对性的微调可以增强模型在特定任务之外的能力。

# 讨论

Phi-1 模型拥有 13 亿个参数，相较于拥有 1750 亿个参数的 GPT-3 模型来说相对较小。尽管存在尺寸差异，Phi-1 在 Python 编码任务中表现出色。这一成就突显了训练数据的质量可能与模型的大小一样重要，甚至更为重要。

研究人员使用合成教材来训练 Phi-1 模型。这本教材是利用 GPT-3.5 生成的，包含了 Python 语言文本和练习。使用合成教材强调了高质量、精心策划的数据在 AI 模型训练中的重要性。这种方法可能会将 AI 训练的重点从创建更大的模型转移到策划更好的训练数据上。

有趣的是，Phi-1 模型在用合成练习和解决方案进行微调后，其性能显著提高。这种改进不仅限于它专门训练的任务。例如，尽管训练数据中未包含 pygame 这类外部库，模型在使用这些库的能力上有所提升。这表明微调可以增强模型在特定任务之外的能力。

# 研究问答

**问：Phi-1 模型在多样性方面与较大的模型相比如何？**

答：Phi-1 模型专注于 Python 编程，这限制了它的多样性，相比于多语言模型，其在特定 API 编程或使用不常见的包方面的领域知识也较少。

**问：Phi-1 模型如何处理提示中的风格变化或错误？**

答：由于数据集的结构化性质以及语言和风格的缺乏多样性，Phi-1 模型对风格变化或提示中的错误的鲁棒性较差。如果提示中存在语法错误，模型的表现会下降。

**问：Phi-1 模型的性能是否可以通过使用 GPT-4 生成合成数据来提高？**

答：是的，研究人员认为通过使用 GPT-4 生成合成数据而非 GPT-3.5，可以取得显著的成果。然而，GPT-4 目前使用起来较慢且费用更高。

**问：Phi-1 模型的训练方法与传统方法有何不同？**

答：传统方法通常关注于增加模型的规模和数据量。与此不同，Phi-1 模型强调数据的质量，并使用合成教材进行训练。这种方法可能会将 AI 训练的重点从创建更大的模型转移到策划更好的训练数据上。

# 研究总结

微软研究的“教材即一切”在训练 AI 模型方面有一个相当新颖的想法。他们没有像通常那样将大量数据抛给模型，而是创建了一本合成教材来教模型。

他们仅使用这本定制教材训练了一个名为 Phi-1 的较小模型，相比于像 GPT-3 这样的庞大模型，它的效果令人震惊。这表明，即使数据集规模较小，只要设计得当、质量高，也能训练出非常有效的 AI。

关键在于花时间策划优秀的训练数据，就像你在教科书中看到的一样，而不仅仅是将模型喂入数TB的随机杂乱数据。这一切都是关于质量，而不是数量。

这可能会改变人们对未来AI训练的看法。与其追求需要巨大数据集的越来越大模型，不如更多地关注创造最佳的训练教材，即使它们更小。这是一个引人入胜的想法，关键在于教材，而不仅仅是模型的扩展。

**[马修·梅奥](https://www.linkedin.com/in/mattmayo13/)** ([**@mattmayo13**](https://twitter.com/mattmayo13)) 是一名数据科学家，也是KDnuggets的总编辑，KDnuggets是重要的数据科学和机器学习在线资源。他的兴趣包括自然语言处理、算法设计与优化、无监督学习、神经网络以及自动化机器学习方法。马修拥有计算机科学硕士学位和数据挖掘研究生文凭。他可以通过editor1 at kdnuggets[dot]com联系。

### 更多相关话题

+   [Synapse CoR：具有革命性转折的ChatGPT](https://www.kdnuggets.com/synapse-cor-chatgpt-with-a-revolutionary-twist)

+   [今天所有营销分析和数据科学专业人士需要的5项技能](https://www.kdnuggets.com/2023/08/mads-5-skills-marketing-analytics-data-science-pros-need-today.html)

+   [如果你想掌握生成性AI，忽略所有（但两个）工具](https://www.kdnuggets.com/if-you-want-to-master-generative-ai-ignore-all-but-two-tools)

+   [我们不需要数据科学家，我们需要数据工程师](https://www.kdnuggets.com/2021/02/dont-need-data-scientists-need-data-engineers.html)

+   [如何一步步成为数据科学家指南](https://www.kdnuggets.com/2021/05/guide-become-data-scientist.html)

+   [支持向量机：直观方法](https://www.kdnuggets.com/2022/08/support-vector-machines-intuitive-approach.html)
