# 超越独热编码：分类变量的探讨

> 原文：[https://www.kdnuggets.com/2015/12/beyond-one-hot-exploration-categorical-variables.html](https://www.kdnuggets.com/2015/12/beyond-one-hot-exploration-categorical-variables.html)

![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)

**作者：Will McGinnis**。

在机器学习中，数据为王。用于数据预测的算法和模型很重要且非常有趣，但机器学习仍然受限于“垃圾进，垃圾出”的理念。考虑到这一点，让我们看看这些输入数据中的一个小子集：分类变量。

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业的快车道。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的 IT 部门

* * *

![mushrooms-cars](../Images/56c0fdcb94cc15a810d93943d1ca52f4.png)

分类变量（[wiki](https://en.wikipedia.org/wiki/Categorical_variable)）是那些表示固定数量可能值的变量，而不是连续的数值。每个值将测量结果分配到这些有限的组或类别之一。它们与有序变量的区别在于，从一个类别到另一个类别的距离应当是相等的，不论类别的数量，而有序变量则具有某种内在的排序。举个例子：

1.  有序变量：低、中、高

1.  分类变量：乔治亚州、阿拉巴马州、南卡罗来纳州、……、纽约

我们稍后将使用的机器学习算法倾向于需要数字而非字符串作为输入，因此我们需要某种编码方法将其转换。

简短插曲：在这篇文章中还有一个经常出现的概念，那就是维度的概念。简单来说，它只是数据集中列的数量，但对最终模型有重要的影响。在极端情况下，“[维度灾难](https://en.wikipedia.org/wiki/Curse_of_dimensionality)”的概念讨论了在高维空间中，一些东西可能无法正常工作。即使在相对低维的问题中，维度更多的数据集也需要更多的参数来让模型理解，这意味着需要更多的行来可靠地学习这些参数。如果数据集中的行数固定，增加额外的维度而不增加更多的模型学习信息可能对最终模型的准确性产生负面影响。

回到当前的问题：我们希望将类别变量编码为数字，但我们担心这种维度问题。显而易见的答案是直接为每个类别分配一个整数（我们假设我们事先知道所有可能的类别）。这被称为序数编码。它不会增加任何维度，但暗示了一个实际可能不存在的变量顺序。

#### 方法论

为了了解这效果如何，我编写了一个简单的 Python 脚本来测试不同的编码方法在常见数据集上的表现。首先是过程概述：

+   我们收集一个有类别变量的分类问题的数据集。

+   我们使用某种编码方法将 X 数据集转换为数值。

+   我们使用 scikit-learn 的交叉验证分数和 BernoulliNB() 分类器生成数据集的分数。每个数据集重复 10 次，然后使用所有分数的均值。

+   我们存储数据集的维度、均分和编码数据及生成分数的时间。

这些内容在 UCI 数据集库中的几个不同数据集上重复出现：

+   [汽车评估](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation)

+   [蘑菇](https://archive.ics.uci.edu/ml/datasets/Mushroom)

+   [剪接位点](http://archive.ics.uci.edu/ml/machine-learning-databases/molecular-biology/splice-junction-gene-sequences/)

我尝试了 7 种不同的编码方法（4-7 的描述取自 [statsmodel’s docs](http://statsmodels.sourceforge.net/devel/contrasts.html)）：

1.  序数：如上所述

1.  One-Hot：每个类别一列，每个单元格中为该列类别的行填入1或0。

1.  Binary：首先将类别编码为序数，然后将这些整数转换为二进制代码，再将二进制字符串中的数字拆分为单独的列。这种方法将数据编码在比 One-Hot 更少的维度中，但会对距离产生一定的扭曲。

1.  Sum：将给定级别的因变量的均值与所有级别的因变量的总体均值进行比较。也就是说，它使用每个前 k-1 级别与级别 k 之间的对比。在这个例子中，级别 1 与所有其他级别进行比较，级别 2 与所有其他级别进行比较，级别 3 与所有其他级别进行比较。

1.  多项式：多项式编码对于 k=4 级别所采用的系数是类别变量中的线性、二次和三次趋势。这里假设类别变量由一个底层的、等间距的数值变量表示。因此，这种编码仅用于具有等间距的有序类别变量。

1.  后向差异：将一个级别的因变量均值与前一个级别的因变量均值进行比较。这种编码方法可能对名义变量或序数变量有用。

1.  Helmert：对某一水平的因变量均值与所有前面水平的因变量均值进行比较。因此，有时会使用“反向”这个名称来区分于前向 Helmert 编码。

### 结果

#### 蘑菇

|  | 编码 | 维度 | 平均得分 | 用时 |
| --- | --- | --- | --- | --- |
| 0 | 顺序编码 | 22 | 0.81 | 3.65 |
| 1 | 独热编码 | 117 | 0.81 | 8.19 |
| 6 | Helmert 编码 | 117 | 0.84 | 5.43 |
| 5 | 反向差分编码 | 117 | 0.85 | 7.83 |
| 3 | 总和编码 | 117 | 0.85 | 4.93 |
| 4 | 多项式编码 | 117 | 0.86 | 6.14 |
| 2 | 二进制编码 | 43 | 0.87 | 3.95 |

#### 汽车

|  | 编码 | 维度 | 平均得分 | 用时 |
| --- | --- | --- | --- | --- |
| 10 | 总和编码 | 21 | 0.55 | 1.46 |
| 13 | Helmert 编码 | 21 | 0.58 | 1.46 |
| 7 | 顺序编码 | 6 | 0.64 | 1.47 |
| 8 | 独热编码 | 21 | 0.65 | 1.39 |
| 11 | 多项式编码 | 21 | 0.67 | 1.49 |
| 12 | 反向差分编码 | 21 | 0.70 | 1.50 |
| 9 | 二进制编码 | 9 | 0.70 | 1.44 |

#### 拼接

|  | 编码 | 维度 | 平均得分 | 用时 |
| --- | --- | --- | --- | --- |
| 14 | 顺序编码 | 61 | 0.68 | 5.11 |
| 17 | 总和编码 | 3465 | 0.92 | 25.90 |
| 16 | 二进制编码 | 134 | 0.94 | 3.35 |
| 15 | 独热编码 | 3465 | 0.95 | 2.56 |

### 结论

这绝不是一项详尽的研究，但似乎二进制编码在一定的一致性下表现良好，而维度增加并不显著。 顺序编码，如预期的那样，一直表现不佳。

如果你想查看源代码、添加或建议新的数据集或新的编码方法，我已将所有内容（包括数据集）上传到 github: [https://github.com/wdm0006/categorical_encoding](https://github.com/wdm0006/categorical_encoding)。

请随时在那儿直接贡献，或在此评论提出建议。

[原文](http://willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/ ).

**相关：**

+   [数据挖掘医疗数据 – 我们能发现什么？](/2014/04/data-mining-medicare-data.html)

+   [机器学习的 5 个部落 – 问题与答案](/2015/11/domingos-5-tribes-machine-learning-questions-answers.html)

+   [欺诈检测解决方案](/solutions/fraud-detection.html)

### 更多相关内容

+   [如何处理机器学习中的分类数据](https://www.kdnuggets.com/2021/05/deal-with-categorical-data-machine-learning.html)

+   [使用 MultiLabelBinarizer 编码分类特征](https://www.kdnuggets.com/2023/01/encoding-categorical-features-multilabelbinarizer.html)

+   [构建视觉搜索引擎 - 第 1 部分：数据探索](https://www.kdnuggets.com/2022/02/building-visual-search-engine-part-1.html)

+   [ChatGPT 驱动的数据探索：揭示数据集中的隐藏洞察](https://www.kdnuggets.com/2023/07/chatgptpowered-data-exploration-unlock-hidden-insights-dataset.html)

+   [超越编码：为什么人工触感很重要](https://www.kdnuggets.com/beyond-coding-why-the-human-touch-matters)

+   [超越管道：图作为 Scikit-Learn 元估计器](https://www.kdnuggets.com/2022/09/graphs-scikitlearn-metaestimators.html)
