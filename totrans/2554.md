# 如何创建无偏见的机器学习模型

> 原文：[https://www.kdnuggets.com/2021/07/create-unbiased-machine-learning-models.html](https://www.kdnuggets.com/2021/07/create-unbiased-machine-learning-models.html)

[评论](#comments)

**由[Deepchecks](https://deepchecks.com/)的联合创始人兼首席执行官Philip Tannor**。

![图示](../Images/f109257a81bf17a819ae1929ce210afe.png)

*图片由* [*Clker-Free-Vector-Images*](https://pixabay.com/users/clker-free-vector-images-3736/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=307248) *提供* [*Pixabay*](https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=307248)

* * *

## 我们的三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升您的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持您的组织的IT工作

* * *

AI系统在许多行业变得越来越受欢迎并且核心。它们决定谁可能从银行获得贷款，一个人是否应该被定罪，我们甚至可能在不久的将来将我们的生命托付给诸如自动驾驶车辆等系统。因此，越来越需要机制来控制这些系统，以确保它们按预期行为。

近年来一个重要的问题是*公平性*。虽然通常机器学习模型是根据准确度等指标来评估的，但公平性的理念是我们必须确保我们的模型在性别、种族和其他选定属性方面不带有偏见。

一个经典的种族偏见AI系统案例是由Northpointe开发的COMPAS软件系统，它旨在协助美国法院评估被告成为惯犯的可能性。Propublica发布了一篇[文章](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)，声称该系统对黑人存在偏见，给他们更高的风险评级。

![图示](../Images/286f789c76493394720b3c8cbe64ca84.png)

*机器学习系统对非裔美国人的偏见？ (*[*来源*](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)*)*

在这篇文章中，我们将尝试了解机器学习模型中的偏见来源，并探索创建无偏见模型的方法。

### 偏见来源于哪里？

> “人类是最薄弱的环节”
> 
> —布鲁斯·施奈尔

在网络安全领域，常有人说“人类是最薄弱的环节”（Schneier）。这个观点在我们的情况中也适用。偏见实际上是由人类无意中引入到机器学习模型中的。

请记住，机器学习模型的好坏仅能取决于其训练数据，因此如果训练数据包含偏见，我们可以预期我们的模型也会模仿这些偏见。一些代表性的例子可以在自然语言处理中的词嵌入领域找到。词嵌入是学习到的词语密集向量表示，旨在捕捉词语的语义信息，然后可以将其输入到机器学习模型中用于不同的下游任务。因此，具有相似含义的词语的嵌入预计应该是“接近”的。

![图](../Images/226ab20e7f0444a5cf42625469fff5d4.png)

*词嵌入可以捕捉词语的语义含义。 (*[*source*](https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/)*)*

事实证明，嵌入空间可以用来提取词语之间的关系，并找到类比。一个经典的例子是[著名的](https://www.technologyreview.com/2015/09/17/166211/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/) king-man+woman=queen 等式。然而，如果我们将“doctor”替换成“king”，我们会得到“nurse”作为“doctor”的女性对应词。这一不理想的结果简单地反映了我们社会和历史中存在的性别偏见。如果在大多数可用的文本中，医生通常是男性，而护士通常是女性，那么我们的模型也会这样理解。

[PRE0]

*代码示例：* *man* *对* *doctor* *的类比就像* *woman* *对* *nurse* *的类比，依据 gensim word2vec* *(*[*source*](https://colab.research.google.com/drive/165qN7RfKByFDlWB6m-E5gRvcSEEodGV0?usp=sharing)*)*

### 文化特定的倾向

目前，互联网使用最多的语言是[英语](https://www.statista.com/statistics/262946/share-of-the-most-common-languages-on-the-internet/#:~:text=As%20of%20January%202020%2C%20English,percent%20of%20global%20internet%20users.)。数据科学和机器学习领域的大部分研究和产品也使用英语。因此，许多用于创建巨大语言模型的“自然”数据集往往反映了美国的思维和文化，并可能对其他国籍和文化存在偏见。

![图](../Images/3a6bd2ee7625df70901f1fe33257b7ca.png)

*文化偏见：GPT-2需要主动引导才能生成符合给定提示的积极段落。 (*[*source*](https://blog.einstein.ai/gedi/)*)*

### 合成数据集

数据中的一些偏见可能是在数据集构建过程中无意中产生的。在构建和评估过程中，人们更容易注意到和关注他们熟悉的细节。一个著名的图像分类错误的例子是，当谷歌照片[错误地将黑人分类为猩猩](https://www.wsj.com/articles/BL-DGB-42522)。尽管这种单一的错误分类可能不会对整体评估指标产生强烈影响，但这是一个敏感问题，可能对产品及其客户关系产生重大影响。

![图示](../Images/05cedf7c97a4e086de2b0fa7ba670e26.png)

*种族主义AI算法？将黑人误分类为猩猩。（*[*来源*](https://www.wsj.com/articles/BL-DGB-42522)*)*

总之，没有数据集是完美的。无论数据集是手工制作的还是“自然”的，它都可能反映出创建者的偏见，因此最终模型也会包含相同的偏见。

### 创建公平的机器学习模型

有多种提出的方法来创建公平的机器学习模型，这些方法通常分为以下几个阶段之一。

### 预处理

一种简单的创建无敏感属性偏见的机器学习模型的方法是简单地从数据中删除这些属性，使模型无法使用这些属性进行预测。然而，将属性划分为明确的类别并不总是简单的。例如，一个人的名字可能与他们的性别或民族相关，但我们不一定希望将这个属性视为敏感属性。更复杂的方法尝试使用降维技术来消除敏感属性。

### 在训练时

创建[无偏见机器学习模型](https://deepchecks.com/how-to-create-unbiased-ml-models/)的一种优雅方法是使用对抗去偏见技术。在这种方法中，我们同时训练两个模型。对抗模型被训练以预测给定预测者预测或隐藏表示的受保护属性。预测者则被训练以在原始任务中成功，同时使对抗者失败，从而最小化偏见。

![图示](../Images/ac04940d304eced71c619b269d1c2468.png)

*对抗去偏见示意图：预测者损失函数由两个部分组成，预测者损失和对抗损失。（*[*来源*](https://arxiv.org/pdf/1801.07593.pdf)*)*

这种方法可以在不“丢弃”输入数据的情况下实现很好的去偏见效果，然而，它可能会遇到在训练对抗网络时通常出现的困难。

### 后处理

在后处理阶段，我们获得模型的预测概率，但我们仍然可以根据这些输出选择如何行动，例如，我们可以为不同的群体调整决策阈值，以满足我们的公平性要求。

确保模型公平性的一种方法是在所有组的 ROC 曲线下面积的交集。交集代表了可以同时为所有类别实现的 TPR 和 FPR。请注意，为了满足所有类别 TPR 和 FPR 的平等结果，可能需要故意选择在某些类别上取得较差的结果。

![图](../Images/1bd7e256c87e70aaceaf57dc23972269.png)

*彩色区域是实现公平性分离性标准时所能达到的范围。(*[*来源*](https://fairmlbook.org/classification.html)*)*

另一种在后处理阶段去偏见的方法涉及对每个类别的预测进行 *校准*。*校准* 是确保分类模型的概率输出确实反映正标签匹配比例的方法。形式上，分类模型被称为已校准，如果对于每个 r 值：

![方程](../Images/4261c701bf4517a4b0357b5fd7d1157d.png)

当模型得到适当校准时，错误率将在不同保护属性值之间相似。

### 结论

总结一下，我们讨论了 ML 世界中的 *偏见* 和 *公平性* 概念，看到模型偏见通常反映了社会中现有的偏见。我们可以通过各种方式来强制执行和测试模型的公平性，希望使用这些方法能带来全球 AI 辅助系统中更公正的决策。

### 进一步阅读

[词嵌入中的性别偏见](https://towardsdatascience.com/gender-bias-word-embeddings-76d9806a0e17)

[Propublica 文章](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)

Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, & Hanna Wallach. (2018). 一种公平分类的减少方法。

Brian Hu Zhang, Blake Lemoine, & Margaret Mitchell. (2018). 通过对抗学习减轻不必要的偏见。

Solon Barocas, Moritz Hardt, & Arvind Narayanan (2019). *公平性与机器学习*。fairmlbook.org。

Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, & Aram Galstyan. (2019). 关于机器学习中的偏见和公平性的调查。

**简介：Philip Tannor** 是 [Deepchecks](https://deepchecks.com/) 的联合创始人兼首席执行官。

**相关：**

+   [保证隐私所需的数据保护技术](/2020/10/data-protection-techniques-guarantee-privacy.html)

+   [什么让人工智能值得信赖？](/2021/05/what-makes-ai-trustworthy.html)

+   [人工智能中的伦理、公平性和偏见](/2021/06/ethics-fairness-ai.html)

### 更多相关话题

+   [构建数据管道以创建大型语言模型应用程序](https://www.kdnuggets.com/building-data-pipelines-to-create-apps-with-large-language-models)

+   [如何为机器学习创建数据集](https://www.kdnuggets.com/2022/02/create-dataset-machine-learning.html)

+   [如何为你的数据项目制定采样计划](https://www.kdnuggets.com/2022/11/create-sampling-plan-data-project.html)

+   [使用 Tableau 创建高效的组合数据源](https://www.kdnuggets.com/2022/05/create-efficient-combined-data-sources-tableau.html)

+   [利用你的数据科学技能创造 5 种收入来源](https://www.kdnuggets.com/2023/03/data-science-skills-create-5-streams-income.html)

+   [创建时间序列比率分析仪表盘](https://www.kdnuggets.com/2023/06/wolfer-create-time-series-ratio-analysis-dashboard.html)
