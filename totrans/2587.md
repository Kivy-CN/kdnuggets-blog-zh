# XGBoost 解释：少于200行Python代码实现DIY XGBoost库

> 原文：[https://www.kdnuggets.com/2021/05/xgboost-explained-diy-xgboost-library-200-lines-python.html](https://www.kdnuggets.com/2021/05/xgboost-explained-diy-xgboost-library-200-lines-python.html)

[评论](#comments)

**由 [Guillaume Saupin](https://www.linkedin.com/in/guillaume-saupin-5802aa31/) 供稿，Verteego的CTO**

![](../Images/2af80546d15b24dbb95b3e7f06ad2b1d.png)

图片由 [Jens Lelie](https://unsplash.com/@madebyjens?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

XGBoost可能是数据科学中最广泛使用的库之一。世界各地的许多数据科学家都在使用它。这是一个非常通用的算法，可以用于分类、回归以及置信区间，如在这篇[文章](https://towardsdatascience.com/confidence-intervals-for-xgboost-cac2955a8fde)中所示。但有多少人真正理解其基本原理呢？

你可能认为像XGBoost这样的机器学习算法会使用非常复杂和先进的数学方法。你可能会想象它是一个软件工程的杰作。

你说得对。XGBoost库确实很复杂，但如果仅考虑应用于决策树的梯度提升的数学公式，其实并不复杂。

你将在下面详细看到如何使用梯度提升方法训练**回归**决策树，代码少于200行。

### 决策树

在进入数学细节之前，让我们先回顾一下决策树。其原理相当简单：将一个给定特征集的值与遍历二叉树相关联。每个二叉树节点都附带一个条件；叶子节点包含值。

如果条件为真，我们继续使用左节点进行遍历，如果条件为假，则使用右节点。一旦到达叶子节点，我们就得到了预测结果。

正如常见的那样，一张图片胜过千言万语：

![](../Images/f681ccf5b863618aa3ba615f5f7ed042.png)

三层决策树。图片由作者提供。

附加到节点的条件可以视为一个决策，因此得名**决策树**。

这种结构在计算机科学历史中相当久远，并且成功地使用了几十年。基本实现如下：

### 堆叠多个树

尽管决策树在一些应用中，如专家系统（在AI寒冬之前）取得了一定的成功，但它仍然是一个非常基础的模型，无法处理通常在实际数据中遇到的复杂性。

我们通常将这种类型的估计器称为***弱模型***。

为了克服这一局限性，九十年代出现了将多个*弱模型*结合起来创建*强模型*的想法：****集成学习****。

这种方法可以应用于任何类型的模型，但由于决策树是简单、快速、通用且易于解释的模型，因此它们被广泛使用。

可以采用各种策略来组合模型。例如，我们可以使用每个模型预测的加权和。更好的是，使用贝叶斯方法根据学习来组合它们。

XGBoost 和所有**提升**方法使用另一种方法：每个新模型试图弥补前一个模型的错误。

### 优化决策树

正如我们上面所看到的，使用决策树进行预测是直接的。使用*集成学习*时，工作也不会更复杂：我们只需将每个模型的贡献相加即可。

实际上复杂的是构建树本身！我们如何找到在训练数据集的每个节点上应用的最佳条件？这就是数学发挥作用的地方。完整的推导可以在 [XGBoost 文档](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)中找到。在这里，我们将只关注本文感兴趣的公式。

正如在机器学习中总是要做的那样，我们希望设置模型参数，使得模型在训练集上的预测最小化给定的目标：

![](../Images/1c6ea3b60063b729682ac9a687e59dd2.png)

目标公式。公式由作者提供。

请注意，这个目标由两个项组成：

+   一种用于衡量预测误差的方法。它是著名的损失函数*l(y, y_hat)*。

+   另一个*omega*，用于控制模型复杂度。

正如 XGBoost [文档](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)中所述，复杂度是目标的一个非常重要的部分，它允许我们调整偏差/方差的权衡。可以使用许多不同的函数来定义这个正则化项。XGBoost 使用：

![](../Images/0ce54f0ee55300ffbfdaa119f4139291.png)

正则化项。公式由作者提供。

这里* T* 是叶子的总数，而*w_j* 是附加到每个叶子的权重。这意味着大的权重和大量的叶子会受到惩罚。

由于误差通常是复杂的非线性函数，我们使用二阶泰勒展开对其进行线性化：

![](../Images/30f9a52c0856023e980b2ed196a90f7a.png)

损失的二阶展开。公式由作者提供。

其中：

![](../Images/d15672d7944ba013a374896777a30f45.png)

高斯和赫西恩公式。公式由作者提供。

线性化是相对于预测项计算的，因为我们想估计当预测发生变化时误差如何变化。线性化是必需的，因为它将简化误差的最小化。

我们希望通过梯度提升实现的是找到最优的*delta_y_i*，以最小化损失函数，即我们希望找到如何修改现有的树，以便这种修改能够改进预测。

处理树模型时，需要考虑两种参数：

+   定义树本身的部分：每个节点的条件，树的深度

+   附加到每个树叶子的值。这些值就是预测值本身。

探索每种树的配置将会过于复杂，因此梯度树提升方法只考虑将一个节点拆分成两个叶子。这意味着我们需要优化三个参数：

+   分裂值：我们根据什么条件来分裂数据

+   附加到左叶子的值

+   附加到右叶子的值

在 XGBoost 文档中，相对于目标的叶子 *j* 的最佳值由以下公式给出：

![](../Images/30ba452986e0c2f8e110335381fe9963.png)

相对于目标的最佳叶子值。公式由作者提供。

其中 *G_j* 是附加到节点 *j* 的训练点梯度的总和，*H_j* 是附加到节点 *j* 的训练点海森矩阵的总和。

使用这个最佳参数得到的目标函数减少量是：

![](../Images/92074bf0fd43651aab5b4918039c0764.png)

使用最佳权重时的目标改进。公式由作者提供。

选择正确的分裂值是使用穷举法：我们计算每个分裂值的改进，并保留最佳的一个。

我们现在拥有了所有所需的数学信息，可以通过添加新的叶子来提升初始树的性能，以达到给定的目标。

在具体实施之前，让我们花点时间理解这些公式的含义。

### 理解梯度提升

让我们尝试了解权重是如何计算的，以及 *G_j* 和 *H_i* 等于多少。由于它们分别是损失函数对预测值的梯度和海森矩阵，我们必须选择一个损失函数。

我们将关注平方误差，它是常用的，并且是 XGBoost 的默认目标：

![](../Images/bd91ae15b1e00e8ed1dd2fc927f73301.png)

平方误差损失函数。公式由作者提供。

这是一个相当简单的公式，其梯度为：

![](../Images/9941cb63747e203b1bc9de0f40549606.png)

损失函数的梯度。公式由作者提供。

以及海森矩阵：

![](../Images/4821077093599e50bb41001008817d81.png)

损失函数的海森矩阵。公式由作者提供。

因此，如果我们记得最大化误差减少的最佳权重的公式：

![](../Images/30ba452986e0c2f8e110335381fe9963.png)

相对于目标的最佳叶子值。公式由作者提供。

我们意识到，最佳权重，*即*我们在之前的预测中添加的值，正好是之前预测与实际值之间的平均误差的相反数（当正则化被禁用时，即 *lambda = 0*）。使用平方损失训练带有梯度提升的决策树最终只是将预测值与每个新节点的平均误差进行更新。

我们还可以看到，*lambda* 具有预期的效果，即确保权重不会过大，因为权重与 *lambda* 成反比。

### 训练决策树

现在是简单的部分。假设我们有一个现有的决策树，它能以给定的误差进行预测。我们想通过分裂一个节点来减少误差并改善附带的目标。

实现这一点的算法非常直接：

+   选择一个感兴趣的特征。

+   根据所选特征的值对当前节点附加的数据点进行排序。

+   选择一个可能的分裂值。

+   将数据点低于此分裂值的放入右节点，高于此值的放入左节点。

+   计算父节点、右节点和左节点的目标减少。

+   如果左节点和右节点的目标减少总和大于父节点的目标，则将分裂值保留为最佳值。

+   对每个分裂值进行迭代。

+   如果有最佳分裂值，则使用该值，并添加两个新节点。

+   如果没有分裂能改善目标，则不要添加子节点。

生成的代码创建了一个DecisionTree类，该类由一个目标、一个估计器数量（即树的数量）和一个最大深度进行配置。

如承诺，代码少于200行：

训练的核心代码在函数*_find_best_split*中。它基本上遵循上述详细步骤。

请注意，为了支持任何目标，而无需手动计算梯度和海森矩阵，我们使用自动微分和[jax](https://jax.readthedocs.io/en/latest/index.html)库来自动化计算。

最初，我们从一个只有一个节点的树开始，其叶子值为零。由于我们模仿XGBoost，我们还使用了一个基础分数，该分数设为待预测值的均值。

另外，请注意，在第126行，如果达到初始化树时定义的最大深度，我们将停止树的构建。我们还可以使用其他条件，如每个叶子的最小样本数量或新权重的最小值。

另一个非常重要的点是选择用于分裂的特征。在这里，为了简单起见，特征是随机选择的，但我们本可以使用更智能的策略，例如选择具有最大方差的特征。

### 结论

在本文中，我们了解了梯度提升如何训练决策树。为了进一步提高我们的理解，我们编写了训练决策树集合并用于预测所需的最小行数。

深刻理解我们用于机器学习的算法至关重要。这不仅有助于我们构建更好的模型，更重要的是使我们能够根据需要调整这些模型。

例如，在梯度提升的情况下，调整损失函数是提高预测精度的一个极好方法。

**个人简介：[Guillaume Saupin](https://www.linkedin.com/in/guillaume-saupin-5802aa31/)** 是Verteego的CTO。

[原文](https://towardsdatascience.com/diy-xgboost-library-in-less-than-200-lines-of-python-69b6bf25e7d9)。经许可转载。

**相关：**

+   [梯度提升决策树 – 概念解释](/2021/04/gradient-boosted-trees-conceptual-explanation.html)

+   [XGBoost：它是什么，以及何时使用](/2020/12/xgboost-what-when.html)

+   [三人行必有我师：集成学习的案例](/2019/09/ensemble-learning.html)

* * *

## 我们的三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持组织的 IT 部门

* * *

### 更多相关内容

+   [少于 15 行代码实现多模态深度学习](https://www.kdnuggets.com/2023/01/predibase-multi-modal-deep-learning-less-15-lines-code.html)

+   [KDnuggets 新闻，7 月 20 日：机器学习算法解释…](https://www.kdnuggets.com/2022/n29.html)

+   [每个机器学习算法在不到 1 分钟内解释](/2022/07/machine-learning-algorithms-explained-less-1-minute.html)

+   [在不到 6 个月内成为商业智能分析师](https://www.kdnuggets.com/become-a-business-intelligence-analyst-in-less-than-6-months)

+   [使用 Streamlit 自制自动化机器学习](/2021/11/diy-automated-machine-learning-app.html)

+   [黑色星期五特惠 - 用 DataCamp 以更低的价格精通机器学习](https://www.kdnuggets.com/2022/11/datacamp-black-friday-deal-master-machine-learning-less-datacamp.html)
