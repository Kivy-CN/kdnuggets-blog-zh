- en: 'Comparing Clustering Techniques: A Concise Technical Overview'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2016/09/comparing-clustering-techniques-concise-technical-overview.html](https://www.kdnuggets.com/2016/09/comparing-clustering-techniques-concise-technical-overview.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Clustering is used for analyzing data which does not include pre-labeled classes.
    Data instances are grouped together using the concept of maximizing intraclass
    similarity and minimizing the similarity between differing classes. This translates
    to the clustering algorithm identifying and grouping instances which are very
    similar, as opposed to ungrouped instances which are much less-similar to one
    another. As clustering does not require the pre-labeling of classes, it is a form
    of unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering](../Images/42a053780bfea2e58ce12ff265cb28cb.png)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '*k*-means Clustering is perhaps the most well-known example of a clustering
    algorithm. However, it is not the only one. Completely different schemes exist,
    such hierarchical clustering, fuzzy clustering, and density clustering, as do
    different takes on centroid-style clustering, such as using the median (as opposed
    to the mean), or ensuring that the centroid is a cluster member (read on for some
    context).'
  prefs: []
  type: TYPE_NORMAL
- en: Below is a brief technical overview of the *k*-means clustering algorithm, as
    well as Expectation-Maximization (EM), a Gaussian distribution-based clustering
    method. Both approaches attempt to maximize intraclass similarity and minimize
    inter-class similarity, yet take different approaches in doing so.
  prefs: []
  type: TYPE_NORMAL
- en: '***k*-means Clustering**'
  prefs: []
  type: TYPE_NORMAL
- en: '*k*-means is a simple, yet often effective, approach to clustering. *k* points
    are randomly chosen as cluster centers, or centroids, and all training instances
    are plotted and added to the closest cluster. After all instances have been added
    to clusters, the centroids, representing the mean of the instances of each cluster
    are re-calculated, with these re-calculated centroids becoming the new centers
    of their respective clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, all cluster membership is reset, and all instances of the training
    set are re-plotted and -added to their closest, possibly re-centered, cluster.
    This iterative process continues until there is no change to the centroids or
    their membership, and the clusters are considered settled.
  prefs: []
  type: TYPE_NORMAL
- en: '![k-means Algorithm](../Images/826561ae313b869f1d4d0b7041d2d089.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Fig.1: *k*-means Clustering Algorithm.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Convergence is achieved once the re-calculated centroids match the previous
    iteration’s centroids, or are within some preset margin. The measure of distance
    is generally Euclidean in *k*-means, which, given 2 points in the form of *(x,
    y)*, can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/5496763be70b3ffcee728278cd3945e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Of technical note, especially in the era of parallel computing, iterative clustering
    in *k*-means is serial in nature; however, the distance calculations within an
    iteration need not be. Therefore, for sets of a significant size, distance calculations
    are a worthy target for parallelization in the *k*-means clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '**Expectation-Maximization**'
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic clustering aims to determine the most likely set of clusters,
    given a set of data. EM is a probabilistic clustering algorithm, and, as such,
    involves determining the probabilities that instances belong to particular clusters.
    EM ”approaches maximum likelihood or maximum a posteriori estimates of parameters
    in statistical models” ([Han, Kamber & Pei](http://hanj.cs.illinois.edu/bk3/)).
    The EM process begins with a set of parameters, iterating until clustering is
    maximized, with respect to k clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'As its name suggests, EM consists of 2 distinct steps: expectation and maximization.
    The expectation step (E-step) assigns particular objects to clusters based on
    parameters. This can also be referred to as the cluster probability calculation
    step, the cluster probabilities being the ”expected” class values. The maximization
    step (M-step) calculates the distribution parameters, maximizing expected likelihood.'
  prefs: []
  type: TYPE_NORMAL
- en: The parameter estimation equations express the fact that cluster probabilities
    are known for each cluster, as opposed to the clusters themselves. The mean for
    a cluster is calculated as
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/e5eedf278af22418280caf14e82c84ee.png)'
  prefs: []
  type: TYPE_IMG
- en: and the standard deviation of a cluster is determined by
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/c95f31494efd75e7934b9cde5f7e0af3.png)'
  prefs: []
  type: TYPE_IMG
- en: where *w[i]* is the probability an instance *i* is a member of cluster *C*,
    and *x[i]* are all of the dataset’s instances.
  prefs: []
  type: TYPE_NORMAL
- en: When given a new instance to cluster, its cluster membership probability is
    calculated and compared to each cluster. The instance becomes a member of the
    cluster with the highest membership probability. These steps are repeated until
    the inter-cluster delta is below a predefined threshold. In practice, iteration
    should continue until the log-likelihood increase is negligible, and the log-likelihood
    typically increases dramatically during the first number of iterations and converges
    to this negligible point quite quickly.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that EM can also be used for fuzzy clustering, as opposed
    to probabilistic clustering.
  prefs: []
  type: TYPE_NORMAL
- en: While it is clear that *k*-means and Expectation-maximization take different
    approaches to getting there, they are both clustering techniques. This difference
    should be a good hint to the variance which exists between the wide array of clustering
    techniques which are available [and in use today](/2016/09/poll-algorithms-used-data-scientists.html).
  prefs: []
  type: TYPE_NORMAL
- en: '**Related**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[MDL Clustering: Unsupervised Attribute Ranking, Discretization, and Clustering](/2016/08/mdl-clustering-unsupervised-attribute-ranking-discretization-clustering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Tutorial on the Expectation Maximization (EM) Algorithm](/2016/08/tutorial-expectation-maximization-algorithm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Support Vector Machines: A Concise Technical Overview](/2016/09/support-vector-machines-concise-technical-overview.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Clustering Unleashed: Understanding K-Means Clustering](https://www.kdnuggets.com/2023/07/clustering-unleashed-understanding-kmeans-clustering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT](https://www.kdnuggets.com/comparing-natural-language-processing-techniques-rnns-transformers-bert)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Write Engaging Technical Blogs](https://www.kdnuggets.com/2022/04/write-engaging-technical-blogs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[With Data Privacy learn to implement technical privacy solutions…](https://www.kdnuggets.com/2022/04/manning-data-privacy-learn-implement-technical-privacy-solutions-tools-scale.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hidden Technical Debts Every AI Practitioner Should be Aware of](https://www.kdnuggets.com/2022/07/hidden-technical-debts-every-ai-practitioner-aware.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ChatGPT vs Google Bard: A Comparison of the Technical Differences](https://www.kdnuggets.com/2023/03/chatgpt-google-bard-comparison-technical-differences.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
