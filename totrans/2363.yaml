- en: How to Correctly Select a Sample From a Huge Dataset in Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/05/sample-huge-dataset-machine-learning.html](https://www.kdnuggets.com/2019/05/sample-huge-dataset-machine-learning.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![How to correctly select a sample from a huge dataset in machine learning](../Images/09979ee53c1210a7e83d719a441a6d55.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Lukas](https://www.pexels.com/@goumbik?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)
    from [Pexels](https://www.pexels.com/photo/analytics-blur-close-up-commerce-590020/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)
  prefs: []
  type: TYPE_NORMAL
- en: 'In machine learning, we often need to train a model with a **very large** dataset
    of thousands or even millions of records. The higher the size of a dataset, the
    higher its **statistical significance** and the information it carries, but we
    rarely ask ourselves: is such a huge dataset **really useful**? Or we could reach
    a satisfying result with a smaller, much more manageable one? Selecting a reasonably
    small dataset carrying the good amount of information can really make us **save
    time** and money.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make a simple mental experiment. Imagine that we are in a **library**
    and want to learn Dante Alighieri’s *Divina Commedia* word by word.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have two options:'
  prefs: []
  type: TYPE_NORMAL
- en: Grab the first edition we find and start studying from it
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Grab as many editions as possible and study from all of them
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The right answer is very clear. Why would we want to study from different books
    when **just one** of them is enough?
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is the same thing. We have a model that **learns something**
    and, exactly like us, it needs some time. What we want is the **minimum amount**
    of information that is required to learn properly from the phenomenon, without
    wasting our time. Information redundancy doesn’t contain any business value for
    us.
  prefs: []
  type: TYPE_NORMAL
- en: But how can we be sure that our edition isn’t corrupted or incomplete? We must
    perform some kind of **high-level comparison** with the population made by the
    other editions. For example, we could check the number of *canti* and *cantiche*.
    If our book has three *cantiche* and each one of them has 33 *canti*, maybe it’s
    complete and we can safely learn from it.
  prefs: []
  type: TYPE_NORMAL
- en: What we are doing is learn from a sample (the single *Divina Commedia* edition)
    and check its **statistical significance** (the macro comparison with the other
    books).
  prefs: []
  type: TYPE_NORMAL
- en: The same, exact concept can be applied in machine learning. Instead of learning
    from a huge population of many records, we can make a **sub-sampling** of it keeping
    all the **statistics intact**.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical Framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to take a small, easy to handle dataset, we must be sure we **don’t
    lose** statistical significance with respect to the population. A too small dataset
    won’t carry enough information to learn from, a too huge dataset can be time-consuming
    to analyze. So how can we choose the good **compromise** between size and information?
  prefs: []
  type: TYPE_NORMAL
- en: Statistically speaking, we want that our sample keeps the **probability distribution**
    of the population under a reasonable **significance level**. In other words, if
    we take a look at the **histogram** of the sample, it must be the same as the
    histogram of the population.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to accomplish this goal. The simplest thing to do is taking
    a random sub-sample with **uniform distribution** and check if it’s significant
    or not. If it’s reasonably significant, we’ll keep it. If it’s not, we’ll take
    another sample and repeat the procedure until we get a good significance level.
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate vs. Multiple Univariate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we have a dataset made by *N* variables, it can be clustered in a *N*-variate
    histogram and so can be every sub-sample we can take from it.
  prefs: []
  type: TYPE_NORMAL
- en: This operation, although academically correct, can be really difficult to perform
    in reality, especially if our dataset mixes numerical and categorical variables.
  prefs: []
  type: TYPE_NORMAL
- en: That’s why I prefer a simpler approach, that usually introduces an acceptable
    approximation. What we are going to do is consider each variable **independently**
    from the others. If each one of the single, univariate histograms of the sample
    columns is **comparable** with the correspondent histogram of the population columns,
    we can **assume** that the sample is **not biased**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The comparison between sample and population is then made this way:'
  prefs: []
  type: TYPE_NORMAL
- en: Take one variable from the sample
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare its probability distribution with the probability distribution of the
    same variable of the population
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat with all the variables
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some of you could think that we are forgetting the **correlation** between variables.
    It’s not completely true, in my opinion, if we select our sample **uniformly**.
    It’s widely known that selecting a sub-sample uniformly will produce, with large
    numbers, the **same probability distribution** of the original population. Powerful
    resampling methods like **bootstrap** are built around this concept (see my [previous
    article](https://medium.com/data-science-journal/the-bootstrap-the-swiss-army-knife-of-any-data-scientist-acd6e592be13)
    for more information).
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Sample and Population
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As I said before, for each variable we must compare its probability distribution
    on the sample with the probability distribution on the population.
  prefs: []
  type: TYPE_NORMAL
- en: The histograms of categorical variables can be compared using **Pearson’s chi-square
    test**, while the cumulative distribution functions of the numerical variables
    can be compared using the **Kolmogorov-Smirnov test**.
  prefs: []
  type: TYPE_NORMAL
- en: Both statistical tests work under the null hypothesis that the sample has the
    **same distribution** of the population. Since a sample is made by many columns
    and we want all of them to be **significative**, we can reject the null hypothesis
    if the p-value of **at least one** of the tests is lower than the usual **5% confidence
    level**. In other words, we want **every column** to pass the significance test
    in order to accept the sample as valid.
  prefs: []
  type: TYPE_NORMAL
- en: R Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s move from theory to practice. As usual, I’ll use an example in R language.
    What I’m going to show you is how the statistical tests can **give us a warning**
    when sampling is not done properly.
  prefs: []
  type: TYPE_NORMAL
- en: Data simulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s simulate some (huge) data. We’ll create a data frame with 1 million records
    and 2 columns. The first one has 500.000 records taken from a normal distribution,
    while the other 500.000 records are taken from a uniform distribution. This variable
    is **clearly biased** and it will help me explain the concepts of statistical
    significance later.
  prefs: []
  type: TYPE_NORMAL
- en: The other field is a **factor variable** created by using the first 10 letters
    from the alphabet uniformly distributed.
  prefs: []
  type: TYPE_NORMAL
- en: Here follows the code to create such a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Create a sample and check its significance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we can try to create a sample made by 10.000 records from the original dataset
    and check its significance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember: numerical variables must be checked with the **Kolmogorov-Smirnov**
    test, while categorical variables (i.e. factors in R) need **Pearson’s chi-square**
    test.'
  prefs: []
  type: TYPE_NORMAL
- en: For each test, we’ll store its p-value in a named list for the final check.
    If **all the p-values** are greater than 5%, we can say that the sample is not
    biased.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The p-values are, then:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to Correctly Select a Sample From a Huge Dataset in Machine Learning](../Images/436dc733ebd7412f7383722e384d8945.png)'
  prefs: []
  type: TYPE_IMG
- en: Each one of them is **greater than 5%**, so we can say that the sample is statistically
    significant.
  prefs: []
  type: TYPE_NORMAL
- en: What happens if we take the first 10.000 records instead of taking them randomly?
    We know that the first half of the X1 variable of the dataset has a different
    distribution than the total, so we expect that such a sample can’t be representative
    of the whole population.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we repeat the tests, these are the p-values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to Correctly Select a Sample From a Huge Dataset in Machine Learning](../Images/3de7151fc14e6a869ec2ec3b2d63cd60.png)'
  prefs: []
  type: TYPE_IMG
- en: As expected, X1 has a too low p-value due to the bias of the population. In
    this case, we must **keep generating** random samples until all the p-values are
    greater than the minimum allowed confidence level.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I’ve shown you that a proper sample can be statistically significant
    to represent the whole population. This may help us in machine learning because
    a small dataset can make us train models **more quickly** than a larger one, carrying
    the same amount of information.
  prefs: []
  type: TYPE_NORMAL
- en: However, everything is strongly related to the **significance level** we choose.
    For certain kinds of problems, it can be useful to raise the confidence level
    or discard those variables that don’t show a suitable p-value. As usual, a proper
    data discovery before training can help us decide how to perform a sample correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/data-science-journal/how-to-correctly-select-a-sample-from-a-huge-dataset-in-machine-learning-24327650372c).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Gianluca Malato](http://www.gianlucamalato.it/)** is a writer and blogger,
    but first and foremost a passionate reader. Gianluca writes novels and short stories
    of fantasy, horror and science fiction genres.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How to Select Rows and Columns in Pandas Using [ ], .loc, iloc, .at…](https://www.kdnuggets.com/2019/06/select-rows-columns-pandas.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Create a Dataset for Machine Learning](https://www.kdnuggets.com/2022/02/create-dataset-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unsupervised Disentangled Representation Learning in Class…](https://www.kdnuggets.com/2023/01/unsupervised-disentangled-representation-learning-class-imbalanced-dataset-elastic-infogan.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Choosing the Right Clustering Algorithm for Your Dataset](https://www.kdnuggets.com/2019/10/right-clustering-algorithm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Generate Synthetic Tabular Dataset](https://www.kdnuggets.com/2022/03/generate-tabular-synthetic-dataset.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ChatGPT-Powered Data Exploration: Unlock Hidden Insights in Your Dataset](https://www.kdnuggets.com/2023/07/chatgptpowered-data-exploration-unlock-hidden-insights-dataset.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
