- en: Learn how to use PySpark in under 5 minutes (Installation + Tutorial)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/08/learn-pyspark-installation-tutorial.html](https://www.kdnuggets.com/2019/08/learn-pyspark-installation-tutorial.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Georgios Drakos](https://gdcoder.com/author/), Data Scientist at TUI**'
  prefs: []
  type: TYPE_NORMAL
- en: I’ve found that is a little difficult to get started with Apache Spark (this
    will focus on PySpark) and install it on local machines for most people. With
    this simple tutorial you’ll get there really fast!
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Apache Spark**](http://spark.apache.org/)** is a must for Big data’s lovers **as
    it is a fast, easy-to-use general engine for big data processing with built-in
    modules for streaming, SQL, machine learning and graph processing. This technology
    is an in-demand skill for data engineers, but also data scientists can benefit
    from learning Spark when doing Exploratory Data Analysis (EDA), feature extraction
    and, of course, ML. But please remember that Spark is only truly realized when
    it is run on a cluster with a large number of nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark definition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark Application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install PySpark on Mac
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open Jupyter Notebook with PySpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Launching a SparkSession
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclussion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure-name](../Images/35cf5da02d3e664136e0ff5dee351c8a.png)'
  prefs: []
  type: TYPE_IMG
- en: Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Apache Spark is one of the hottest and largest open source project in data
    processing framework with rich high-level APIs for the programming languages like
    Scala, Python, Java and R. It realizes the potential of bringing together both
    Big Data and machine learning. This is because:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark is fast (up to 100x faster than traditional [Hadoop MapReduce](https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm))
    due to in-memory operation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It offers robust, distributed, fault-tolerant data objects (called [RDDs](https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It integrates beautifully with the world of machine learning and graph analytics
    through supplementary packages like [MLlib](https://spark.apache.org/mllib/) and [GraphX](https://spark.apache.org/graphx/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark is implemented on [Hadoop/HDFS](https://www.bernardmarr.com/default.asp?contentID=1080) and
    written mostly in [Scala](https://www.scala-lang.org/), a functional programming
    language.However, for most beginners, Scala is not a great first language to learn
    when venturing into the world of data science.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, Spark provides a wonderful Python API called [PySpark](https://spark.apache.org/docs/latest/api/python/index.html).
    This allows Python programmers to interface with the Spark framework — letting
    you manipulate data at scale and work with objects over a distributed file system.
    So, Spark is not a new programming language that you have to learn but a framework
    working on top of HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: This presents new concepts like nodes, lazy evaluation, and the transformation-action
    (or ‘map and reduce’) paradigm of programming.In fact, Spark is versatile enough
    to work with other file systems than Hadoop — like Amazon S3 or Databricks (DBFS).
  prefs: []
  type: TYPE_NORMAL
- en: Internet powerhouses such as Netflix, Yahoo, and eBay have deployed Spark at
    massive scale, collectively processing multiple petabytes of data on clusters
    of over 8,000 nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Typically when you think of a computer you think about one machine sitting on
    your desk at home or at work. This machine works perfectly well for applying machine
    learning on small dataset . However, when you have huge dataset(in tera bytes
    or giga bytes), there are some things that your computer is not powerful enough
    to perform. One particularly challenging area is data processing. Single machines
    do not have enough power and resources to perform computations on huge amounts
    of information (or you may have to wait for the computation to finish).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/c36231c95b1b58e1c053a9d632f4efad.png)'
  prefs: []
  type: TYPE_IMG
- en: A cluster, or group of machines, pools the resources of many machines together
    allowing us to use all the cumulative resources as if they were one. Now a group
    of machines alone is not powerful, you need a framework to coordinate work across
    them. **Spark is a tool for just that, managing and coordinating the execution
    of tasks on data across a cluster of computers.**
  prefs: []
  type: TYPE_NORMAL
- en: Spark Application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A Spark Application consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: Driver
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executors (set of distributed worker processes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Driver
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Driver runs the main() method of our application having the following duties:'
  prefs: []
  type: TYPE_NORMAL
- en: Runs on a node in our cluster, or on a client, and schedules the job execution
    with a cluster manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Responds to user’s program or input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzes, schedules, and distributes work across the executors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An executor is a distributed process responsible for the execution of tasks.
    Each Spark Application has its own set of executors, which stay alive for the
    life cycle of a single Spark application.
  prefs: []
  type: TYPE_NORMAL
- en: Executors perform all data processing of a Spark job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stores results in memory, only persisting to disk when specifically instructed
    by the driver program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns results to the driver once they have been completed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each node can have anywhere from 1 executor per node to 1 executor per core
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '** Node is single entity machine or server .'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/ce110256af4c8d0e3e7b93dde3d91146.png)'
  prefs: []
  type: TYPE_IMG
- en: Spark’s Application Workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you submit a job to Spark for processing, there is a lot that goes on behind
    the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: Our Standalone Application is kicked off, and initializes its SparkContext.
    Only after having a SparkContext can an app be referred to as a Driver
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our Driver program asks the Cluster Manager for resources to launch its executors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Cluster Manager launches the executors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our Driver runs our actual Spark code
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executors run tasks and send their results back to the driver
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SparkContext is stopped and all executors are shut down, returning resources
    back to the cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install Spark on Mac (locally)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '***First Step: Install Brew***'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need to install brew if you have it already skip this step:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. open terminal on your mac. You can go to spotlight and type terminal to
    find it easily (alternative you can find it on /Applications/Utilities/).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Enter the command bellow.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Hit Return and the script will run. It will output to your terminal a log
    of what is going to install. Hit Return to continue or any other key to abort.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. It might ask for **sudo **privileges. If this happens you will have to type
    your admin password and hit Return again.
  prefs: []
  type: TYPE_NORMAL
- en: '**Notes:** Command line tools (Apple''s XCode) will be installed after this
    guide.'
  prefs: []
  type: TYPE_NORMAL
- en: The installation will look like as the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/0561cfe8388405c80924de741a51e719.png)Installation
    of Homebrew through command line'
  prefs: []
  type: TYPE_NORMAL
- en: When the installation finishes successfully it will look  as the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/e72a72372711566e135617971f063ce3.png)caption'
  prefs: []
  type: TYPE_NORMAL
- en: By default Homebrew is sending anonymous data and analytics. You can find additional
    information [here](https://docs.brew.sh/Analytics). You can choose to opt-out
    by running the command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '***Second Step: Install Anaconda***'
  prefs: []
  type: TYPE_NORMAL
- en: In the same terminal just simple type: `$ brew cask install anaconda`. Please
    see resources section in case you face any issue in that step.
  prefs: []
  type: TYPE_NORMAL
- en: '***Third final Step: Install PySpark***'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. ona terminal type `$ brew install apache-spark`
  prefs: []
  type: TYPE_NORMAL
- en: 2\. if you see this error message, enter `$ brew cask install caskroom/versions/java8`to
    install Java8, you will not see this error if you have it already installed.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/c41b2ed8a71ba60c5e55ad5b6b5f89fd.png)'
  prefs: []
  type: TYPE_IMG
- en: '3\. check if pyspark is properly install by typing on the terminal `$ pyspark`.
    If you see the below it means that it has been installed properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/48936f60f3da3b12e50bc0a205db863c.png)'
  prefs: []
  type: TYPE_IMG
- en: Open Jupyter Notebook with PySpark Ready
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section assumes that PySpark has been installed properly and no error appear
    when typing on a terminal `$ pyspark`. At this step, I present the steps you have
    to follow in order create Jupyter Notebooks automatically initialised with SparkContext.
  prefs: []
  type: TYPE_NORMAL
- en: In order to create a global profile for your terminal session, you will need
    to create or modify your .bash_profile or .bashrc file. Here, I will use .bash_profile
    as my example
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Check if you have .bash_profile in your system `$ ls -a`, if you don't have
    one, create one using `$ touch ~/.bash_profile`
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Find Spark path by running `$ brew info apache-spark`
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/0ed7b859ff7ba04f678b5c67416b5240.png)'
  prefs: []
  type: TYPE_IMG
- en: '3\. If you already have a .bash_profile, open it by `$ vim ~/.bash_profile`,
    press `I` in order to insert, and paste the following codes in any location (DO
    NOT delete anything in your file):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 4\. Press `ESC` to exit insert mode, enter `:wq` to exit VIM. *You could fine
    more VIM commands here.*
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Refresh terminal profile by `$ source ~/.bash_profile`
  prefs: []
  type: TYPE_NORMAL
- en: My favourite way to use PySpark in a Jupyter Notebook is by installing [findSpark](https://github.com/minrk/findspark)package
    which allow me to make a Spark Context available in my code.
  prefs: []
  type: TYPE_NORMAL
- en: '*findSpark package is not specific to Jupyter Notebook, you can use this trick
    in your favorite IDE too.*'
  prefs: []
  type: TYPE_NORMAL
- en: Install findspark by running the following command on a terminal
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Launch a regular Jupyter Notebook and run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure-name](../Images/272c85c537c5ad986fc8099a97520f23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Please note that with Spark 2.2 a lot of people recommend just to simply do `pip
    install pyspark` .I try using `pip` to install `pyspark` but I couldn’t get the `pyspark`cluster
    to get started properly. Reading several answers on Stack Overflow and the [official
    documentation](https://pypi.org/project/pyspark/2.2.0/), I came across this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The Python packaging for Spark is not intended to replace all of the other
    use cases. This Python packaged version of Spark is suitable for interacting with
    an existing cluster (be it Spark standalone, YARN, or Mesos) - but does not contain
    the tools required to setup your own standalone Spark cluster. You can download
    the full version of Spark from the [Apache Spark downloads page](http://spark.apache.org/downloads.html).*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Therefore, I would suggest to follow the steps that I described above.
  prefs: []
  type: TYPE_NORMAL
- en: Launching a SparkSession
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Well, it’s the main entry point for Spark functionality: it represents the
    connection to a Spark cluster and you can use it to create RDDs and to broadcast
    variables on that cluster. When you’re working with Spark, everything starts and
    ends with this SparkSession. **Note** that SparkSession is a new feature of Spark
    2.0 which minimize the number of concepts to remember or construct. (before Spark
    2.0.0, the three main connection objects were SparkContext, SqlContext and HiveContext).'
  prefs: []
  type: TYPE_NORMAL
- en: In interactive environments, a SparkSession will already be created for you
    in a variable named spark. For consistency, you should use this name when you
    create one in your own application.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create a new SparkSession through a Builder pattern which uses a "fluent
    interface" style of coding to build a new object by chaining methods together.
    Spark properties can be passed in, as shown in these examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of your application, please remember to call `spark.stop()`  in
    order to end the SparkSession. Let''s understand the various settings that we
    define above:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**master**`**: **Sets the Spark master URL to connect to, such as “local”
    to run locally, “local[4]” to run locally with 4 cores, or “spark://master:7077”
    to run on a Spark standalone cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**config**`**:**Sets a config option by specifying a (key, value) pair.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**appName**`**: **Sets a name for the application, if no name is set, a randomly
    generated name will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**getOrCreate**`**:**Gets an existing [`**SparkSession**`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?#pyspark.sql.SparkSession) or,
    if there is no existing one, creates a new one based on the options set in this
    builder. In case an existing SparkSession is returned, the config options specified
    in this builder affecting the `SQLContext` configuration will applied. As `SparkContext` configuration
    cannot be modified on runtime (you have to stop existing context first) while`SQLContext` configuration
    can be modified on runtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark has seen immense growth over the past several years. Hundreds of contributors
    working collectively have made Spark an amazing piece of technology powering the
    de facto standard for big data processing and data sciences across all industries.
    But please remember to use it for manipulations of **huge dataset **when facing
    performance issues otherwise it may have opposite effects. For small datasets
    (few gigabytes) it is advisable instead to use [Pandas](https://pandas.pydata.org/).
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading and I am looking forward to hearing your questions :)
  prefs: []
  type: TYPE_NORMAL
- en: '*Stay tuned and Happy Machine Learning.*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[https://medium.com/ayuth/install-anaconda-on-macos-with-homebrew-c94437d63a37](https://medium.com/ayuth/install-anaconda-on-macos-with-homebrew-c94437d63a37)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://picocoder.io/how-to-install-homebrew-on-mac/](https://picocoder.io/how-to-install-homebrew-on-mac/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://stackoverflow.com/](https://stackoverflow.com/questions/46387574/installing-pyspark-on-macbook)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.oreilly.com/library/view/learning-spark/9781449359034/](https://www.oreilly.com/library/view/learning-spark/9781449359034/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bio: [Georgios Drakos](https://gdcoder.com/author/)** is a Data Scientist
    with a BSc and MSc in Electrical and Computer Engineering (National Technical
    University of Athens) as well as a MSc from Imperial College London, currently
    working as a Data Scientist for TUI in the travel industry. He specialised in
    Computational Intelligence for Decision Support, Data Engineering, Sophisticated
    Analytics and Technological Innovation Management. Highly interested in Cloud-based
    technologies, Distributed Computing, Big Data and the business applications of
    Data Science & Machine Learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://gdcoder.com/learn-how-to-use-pyspark-in-under-5-minutes/).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Spark NLP: Getting Started With The World’s Most Widely Used NLP Library In
    The Enterprise](/2019/06/spark-nlp-getting-started-with-worlds-most-widely-used-nlp-library-enterprise.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Scalable Python Code with Pandas UDFs: A Data Science Application](/2019/06/scalable-python-code-pandas-udfs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Analyzing Tweets with NLP in Minutes with Spark, Optimus and Twint](/2019/05/analyzing-tweets-nlp-spark-optimus-twint.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[FastAPI Tutorial: Build APIs with Python in Minutes](https://www.kdnuggets.com/fastapi-tutorial-build-apis-with-python-in-minutes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PySpark for Data Science](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Validation for PySpark Applications using Pandera](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Bias-Variance Trade-Off in 3 Minutes](https://www.kdnuggets.com/2020/09/understanding-bias-variance-trade-off-3-minutes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Build a Machine Learning Web App in 5 Minutes](https://www.kdnuggets.com/2022/03/build-machine-learning-web-app-5-minutes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Build a Web Scraper with Python in 5 Minutes](https://www.kdnuggets.com/2022/02/build-web-scraper-python-5-minutes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
