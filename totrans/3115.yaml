- en: Deep learning scaling is predictable, empirically
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/05/deep-learning-scaling-predictable-empirically.html](https://www.kdnuggets.com/2018/05/deep-learning-scaling-predictable-empirically.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: '[Deep learning scaling is predictable, empirically](https://arxiv.org/abs/1712.00409) Hestness
    et al., *arXiv, Dec.2017*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '*With thanks to Nathan Benaich for highlighting this paper in his excellent [summary
    of the AI world in 1Q18](https://www.getrevue.co/profile/nathanbenaich/issues/your-guide-to-ai-in-q1-2018-by-nathan-ai-100379)*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a really wonderful study with far-reaching implications that could
    even impact company strategies in some cases. It starts with a simple question:
    “how can we improve the state of the art in deep learning?” We have three main
    lines of attack:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: We can search for improved *model architectures*.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can *scale computation*.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can create *larger training data sets*.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As DL application domains grow, we would like a deeper understanding of the
    relationships between training set size, computational scale, and model accuracy
    improvements to advance the state-of-the-art.
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Finding better model architectures often depends on ‘unreliable epiphany,’
    and as the results show, has limited impact compared to increasing the amount
    of data available. We’ve known this for some time of course, including from the
    2009 Google paper, [‘The unreasonable effectiveness of data](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf).’
    The results from today’s paper help us to quantify the data advantage across a
    range of deep learning applications. The key to understanding is captured in the
    following equation:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![\displaystyle \epsilon(m) \sim \alpha m^{\beta_g} + \gamma](../Images/fdc3d545505be6fa9d5f9b5ca22b8968.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: Which says that the generalisation error ![\epsilon](../Images/a97aa34ff2e2a0e8c415f52f0e96e2e2.png) as
    a function of the amount of training data ![m](../Images/152a02f7d270706469057ec5bc183e60.png),
    follows a *power-law* with exponent ![\beta_g](../Images/dd7a8a5ee65d76f35eff545a1f01f234.png).
    Empirically, ![\beta_g](../Images/4e84cad13f39dbf8713190b80b67cf2c.png) usually
    seems to be in the range -0.07 and -0.35\. With error of course, lower is better,
    hence the negative exponents. We normally plot power-laws on log-log graphs, where
    they result in straight lines with the gradient indicating the exponent.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Making *better models* can move the y-intercept down (until we reach the irreducible
    error level), but doesn’t seem to impact the power law coefficient. On the other
    hand, *more data* puts us on an power law path of improvement.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd2828eb8b5f8b54ff75d2a9596763f7.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: The three learning zones
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The learning curves for real applications can be broken down into three regions:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a498412270207a1e2a0024c1231235b8.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: The **small data region** is where models struggle to learn from insufficient
    data and models can only perform as well as ‘best’ or ‘random’ guessing.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The middle region is the **power-law region**, where the power-law exponent
    defines the steepness of the curve (slope on a log-log scale). The exponent is
    an *indicator of the difficulty for models to represent the data generating function*.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “*Results in this paper indicate that the power-law exponent is unlikely to
    be easily predicted with prior theory and **probably dependent on aspects of the
    problem domain or data distribution.***”
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: The **irreducible error region** is the non-zero lower-bound error past which
    models will be unable to improve. With sufficiently large training sets, models
    saturate in this region.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On model size
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We expect the number of model parameters to fit a data set should follow ![s(m)
    \sim \alpha m^{\beta_p}](../Images/24d8f00443ba3e4ce18240cfff2d93e0.png) where ![s(m)](../Images/2203762900df00c7411cf70a847bfdda.png) is
    the required model size to fin a training set of size ![m](../Images/152a02f7d270706469057ec5bc183e60.png),
    and ![\beta_p \in [0.5,1]](../Images/c5436a709bbb30c80b8049944cde8cd7.png).
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Best-fit models grow sublinearly in training shard size. Higher values of ![\beta_p](../Images/c98dfbc3e9d18694997557bb9e9c1e2b.png)indicate
    models that make less effective use of extra parameters on larger data sets. Despite
    model size scaling differences though, “*for a given model architecture, we can
    accurately predict the model size that will best fit increasingly larger data
    sets.*”
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Implications of the power-law
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Predictable learning curves and model size scaling indicate some significant
    implications on how DL could proceed. For machine learning practitioners and researchers,
    predictable scaling can aid model and optimization debugging and iteration time,
    and offer a way to estimate the most impactful next steps to improve model accuracy.
    Operationally, predictable curves can guid decision making about whether or how
    to grow data sets or computation.
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'One interesting consequence is that *model exploration* can be done on smaller
    data sets (and hence faster / cheaper). The data set needs to be large enough
    to show accuracy in the power-law region of the curve. Then the most promising
    models can be scaled to larger data sets to ensure proportional accuracy gains.
    This works because growing training sets and models is likely to result in the
    same relative gains across models. When building a company we look for product-market
    fit before scaling the business. In deep learning it seems the analogy is to look
    for *model-problem fit* before scaling, a search then scale strategy:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1f261ca02c750e1ec53a7d399af0955.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: 'If you need to make a business decision about the return on investment, or
    likely accuracy improvement, from investing in the collection of more data, the
    power-law can help you predict returns:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/957ad0a1cf564181bfeeadc305e9c9e9.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: 'Conversely, if generalisation error within the power-law region drifts from
    the power-law predictions, it’s a clue that increasing model size might help,
    or a more extensive hyperparameter search (i.e., more compute), might help:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/241cec1bf74a0100d141caba41031176.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: …predictable learning and model size curves may offer a way to project the compute
    requirements to reach a particular accuracy level.
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Can you beat the power law?
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model architecture improvements (e.g., increasing model depth) seem only to
    shift learning curves down, but not improve the power-law exponent.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: We have yet to find factors that affect the power-law exponent. To beat the
    power-law as we increase data set size, models would need to learn more concepts
    with successively less data. In other words, models must successively extract
    more marginal information from each additional training sample.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If we *can* find ways to improve the power-law exponent though, then the potential
    accuracy improvements in some problem domains are ‘immense.’
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: The empirical data
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The empirical data to back all this up was collected by testing various training
    data sizes (in powers of two) with state-of-the-art deep learning models in a
    number of different domains.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Here are the neural machine translation learning curves. On the right you can
    see results for the best-fit model at each training set size. As training set
    sizes grow, the empirical error tends away from the power-law trend, and a more
    exhaustive hyperparameter search would be needed to bring it back in line.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/162e60cbdae4d8249de4838b19dd9cba.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: The next domain is word language models. A variety of model architectures are
    used, and although they differ appreciably, they all show the same learning curve
    profile as characterised by the power-law exponent.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a62fa308c7a0a853d9170cc7826d1761.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: 'And here are the results for character language models:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c583cb656be9cbf924c71afd78ff0b29.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: 'With image classification, we see the ‘small data region’ appear on the plots
    when there is insufficient data to learn a good classifier:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab3deb189ed0a1f1f331582c99ad0b29.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: 'Finally, here’s speech recognition:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ad30be4abd103ec4c201536b32f984f.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
- en: 'We empirically validate that DL model accuracy improves as a power-law as we
    grow training sets for state-of-the-art (SOTA) model architectures in four machine
    learning domains: machine translation, language modeling, image processing, and
    speech recognition. These power-law learning curves exist across all tested domains,
    model architectures, optimizers, and loss functions.'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Original](https://blog.acolyer.org/2018/03/28/deep-learning-scaling-is-predictable-empirically/).
    Reposted with permission.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[How to Make AI More Accessible](/2018/04/make-ai-more-accessible.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Are High Level APIs Dumbing Down Machine Learning?](/2018/04/high-level-apis-dumbing-down-machine-learning.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Derivation of Convolutional Neural Network from Fully Connected Network Step-By-Step](/2018/04/derivation-convolutional-neural-network-fully-connected-step-by-step.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[逐步推导卷积神经网络与全连接网络的关系](/2018/04/derivation-convolutional-neural-network-fully-connected-step-by-step.html)'
- en: '* * *'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速入门网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 工作'
- en: '* * *'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Scaling Data Management Through Apache Gobblin](https://www.kdnuggets.com/2023/01/scaling-data-management-apache-gobblin.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过 Apache Gobblin 扩展数据管理](https://www.kdnuggets.com/2023/01/scaling-data-management-apache-gobblin.html)'
- en: '[Data Scaling with Python](https://www.kdnuggets.com/2023/07/data-scaling-python.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 进行数据扩展](https://www.kdnuggets.com/2023/07/data-scaling-python.html)'
- en: '[Things You Should Know When Scaling Your Web Data-Driven Product](https://www.kdnuggets.com/2023/08/things-know-scaling-web-datadriven-product.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[扩展你的网络数据驱动产品时需要知道的事情](https://www.kdnuggets.com/2023/08/things-know-scaling-web-datadriven-product.html)'
- en: '[A Solid Plan for Learning Data Science, Machine Learning, and Deep Learning](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学、机器学习和深度学习的扎实计划](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能、分析、机器学习、数据科学、深度学习……](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
- en: '[15 Free Machine Learning and Deep Learning Books](https://www.kdnuggets.com/2022/10/15-free-machine-learning-deep-learning-books.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15 本免费的机器学习和深度学习书籍](https://www.kdnuggets.com/2022/10/15-free-machine-learning-deep-learning-books.html)'
