- en: Top 10 Machine Learning Algorithms for Beginners
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初学者的十大机器学习算法
- en: 原文：[https://www.kdnuggets.com/2017/10/top-10-machine-learning-algorithms-beginners.html](https://www.kdnuggets.com/2017/10/top-10-machine-learning-algorithms-beginners.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/10/top-10-machine-learning-algorithms-beginners.html](https://www.kdnuggets.com/2017/10/top-10-machine-learning-algorithms-beginners.html)
- en: '![](../Images/c9c27633e28613cfa8923640c7a1149f.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9c27633e28613cfa8923640c7a1149f.png)'
- en: I. Introduction
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I. 介绍
- en: The study of [ML algorithms](https://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html)
    has gained immense traction post the Harvard Business Review [article](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century)
    terming a ‘Data Scientist’ as the ‘Sexiest job of the 21st century’. So, for those
    starting out in the field of ML, we decided to do a reboot of our immensely popular
    Gold blog [The 10 Algorithms Machine Learning Engineers need to know](/2016/08/10-algorithms-machine-learning-engineers.html)
    - albeit this post is targetted towards beginners.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 对[机器学习算法](https://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html)的研究在《哈佛商业评论》[文章](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century)称数据科学家为“21世纪最性感的职业”之后，获得了极大的关注。因此，为了帮助那些刚入门机器学习领域的人，我们决定重新编排我们极受欢迎的金牌博客[机器学习工程师需要了解的10种算法](/2016/08/10-algorithms-machine-learning-engineers.html)——尽管这篇文章面向的是初学者。
- en: ML algorithms are those that can learn from data and improve from experience,
    without human intervention. Learning tasks may include learning the function that
    maps the input to the output, learning the hidden structure in unlabeled data;
    or ‘instance-based learning’, where a class label is produced for a new instance
    by comparing the new instance (row) to instances from the training data, which
    were stored in memory. ‘Instance-based learning’ does not create an abstraction
    from specific instances.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法是能够从数据中学习并通过经验改进的算法，无需人工干预。学习任务可能包括学习将输入映射到输出的函数，学习无标记数据中的隐藏结构；或者是‘基于实例的学习’，在这种学习中，通过将新的实例（行）与存储在内存中的训练数据实例进行比较，为新的实例生成类别标签。‘基于实例的学习’不会从特定实例中创建抽象。
- en: II. Types of ML algorithms
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II. 机器学习算法的类型
- en: 'There are 3 types of ML algorithms:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法有3种类型：
- en: '**1\. Supervised learning:**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 监督学习：**'
- en: 'Supervised learning can be explained as follows: use labeled training data
    to learn the mapping function from the input variables (X) to the output variable
    (Y).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习可以解释为：使用标记的训练数据来学习从输入变量（X）到输出变量（Y）的映射函数。
- en: '*Y = f (X)*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y = f (X)*'
- en: 'Supervised learning problems can be of two types:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习问题可以分为两类：
- en: '*a. Classification*: To predict the outcome of a given sample where the output
    variable is in the form of categories. Examples include labels such as male and
    female, sick and healthy.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*a. 分类*：预测给定样本的结果，其中输出变量以类别的形式出现。例子包括性别（男性和女性）、健康状况（生病和健康）。'
- en: '*b. Regression*: To predict the outcome of a given sample where the output
    variable is in the form of real values. Examples include real-valued labels denoting
    the amount of rainfall, the height of a person.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*b. 回归*：预测给定样本的结果，其中输出变量以实际值的形式出现。例子包括表示降雨量、人的身高的实际值标签。'
- en: The 1st 5 algorithms that we cover in this blog– Linear Regression, Logistic
    Regression, CART, Naïve Bayes, KNN are examples of supervised learning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本博客中讨论的前5种算法——线性回归、逻辑回归、CART、朴素贝叶斯、KNN是监督学习的例子。
- en: Ensembling is a type of supervised learning. It means combining the predictions
    of multiple different weak ML models to predict on a new sample. Algorithms 9-10
    that we cover– Bagging with Random Forests, Boosting with XGBoost are examples
    of ensemble techniques.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习是一种监督学习。它指的是将多个不同的弱机器学习模型的预测结果结合起来，以对新的样本进行预测。我们讨论的算法9-10——使用随机森林的袋装方法、使用XGBoost的提升方法就是集成技术的例子。
- en: '**2\. Unsupervised learning:**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 无监督学习：**'
- en: Unsupervised learning problems possess only the input variables (X) but no corresponding
    output variables. It uses unlabeled training data to model the underlying structure
    of the data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习问题只有输入变量（X），但没有相应的输出变量。它使用未标记的训练数据来建模数据的潜在结构。
- en: 'Unsupervised learning problems can be of two types:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习问题可以分为两类：
- en: '*a. Association*: To discover the probability of the co-occurrence of items
    in a collection. It is extensively used in market-basket analysis. Example: If
    a customer purchases bread, he is 80% likely to also purchase eggs.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*a. 关联*：发现集合中项的共现概率。这在市场篮子分析中被广泛使用。例如：如果一个顾客购买了面包，他有80%的可能性也会购买鸡蛋。'
- en: '*b. Clustering*: To group samples such that objects within the same cluster
    are more similar to each other than to the objects from another cluster.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*b. 聚类*：将样本分组，使得同一簇内的对象彼此之间的相似度高于与其他簇中的对象之间的相似度。'
- en: '*c. Dimensionality Reduction*: True to its name, Dimensionality Reduction means
    reducing the number of variables of a dataset while ensuring that important information
    is still conveyed. Dimensionality Reduction can be done using Feature Extraction
    methods and Feature Selection methods. Feature Selection selects a subset of the
    original variables. Feature Extraction performs data transformation from a high-dimensional
    space to a low-dimensional space. Example: PCA algorithm is a Feature Extraction
    approach.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*c. 降维*：顾名思义，降维意味着减少数据集中的变量数量，同时确保重要信息仍然被传达。降维可以通过特征提取方法和特征选择方法来完成。特征选择从原始变量中选择一个子集。特征提取则将数据从高维空间转换到低维空间。例如：PCA算法是一种特征提取方法。'
- en: Algorithms 6-8 that we cover here - Apriori, K-means, PCA are examples of unsupervised
    learning.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里讨论的算法6-8 - Apriori、K-means、PCA 是无监督学习的例子。
- en: '**3\. Reinforcement learning:**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 强化学习：**'
- en: Reinforcement learning is a type of machine learning algorithm that allows the
    agent to decide the best next action based on its current state, by learning behaviours
    that will maximize the reward.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一种机器学习算法，它允许智能体根据当前状态决定最佳的下一步行动，通过学习能够最大化奖励的行为。
- en: Reinforcement algorithms usually learn optimal actions through trial and error.
    They are typically used in robotics – where a robot can learn to avoid collisions
    by receiving negative feedback after bumping into obstacles, and in video games
    – where trial and error reveals specific movements that can shoot up a player’s
    rewards. The agent can then use these rewards to understand the optimal state
    of game play and choose the next action.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 强化算法通常通过试错法学习最佳行动。它们通常用于机器人技术——机器人可以通过在碰到障碍物后获得负反馈来学习避免碰撞；以及视频游戏——在游戏中通过试错法可以揭示能够提升玩家奖励的特定动作。智能体可以使用这些奖励来了解游戏的最佳状态并选择下一步行动。
- en: III. Quantifying the popularity of ML algorithms
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III. 量化机器学习算法的受欢迎程度
- en: Survey papers [such as these](http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf)
    have quantified the 10 most popular data mining algorithms. However, such lists
    are subjective and as in the case of the quoted paper, the sample size of the
    polled participants is very narrow and consists of advanced practitioners of data
    mining. The persons polled were the winners of the ACM KDD Innovation Award, the
    IEEE ICDM Research Contributions Award; the Program Committee members of the KDD-06,
    ICDM’06 and SDM’06; and the 145 attendees of the ICDM’06.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 调查论文[如这些](http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf)已经量化了10种最受欢迎的数据挖掘算法。然而，这些列表是主观的，正如引用的论文所示，参与者的样本量非常狭窄，主要是数据挖掘的高级从业者。受访者包括ACM
    KDD创新奖和IEEE ICDM研究贡献奖的获奖者；KDD-06、ICDM’06和SDM’06的程序委员会成员；以及ICDM’06的145名与会者。
- en: The Top 10 algorithms in this blog are meant for beginners and are primarily
    those that I learnt from the ‘Data Warehousing and Mining’ (DWM) course during
    my Bachelor’s degree in Computer Engineering at the University of Mumbai. The
    DWM course is a great introduction to the field of ML algorithms. I have especially
    included the last 2 algorithms (ensemble methods) based on their [prevalence to
    win Kaggle competitions](http://www.datasciencecentral.com/profiles/blogs/want-to-win-at-kaggle-pay-attention-to-your-ensembles)
    . Hope you enjoy the article!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本博客中的前10种算法旨在为初学者提供帮助，主要是我在孟买大学计算机工程本科阶段从‘数据仓储与挖掘’（DWM）课程中学到的。这门课程是了解机器学习算法领域的绝佳入门。最后两种算法（集成方法）是根据它们在[Kaggle竞赛中获胜的普遍性](http://www.datasciencecentral.com/profiles/blogs/want-to-win-at-kaggle-pay-attention-to-your-ensembles)特别列出的。希望你喜欢这篇文章！
- en: IV. Supervised learning algorithms
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV. 监督学习算法
- en: '**1\. Linear Regression**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 线性回归**'
- en: In ML, we have a set of input variables (x) that are used to determine the output
    variable (y). A relationship exists between the input variables and the output
    variable. The goal of ML is to quantify this relationship.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们有一组输入变量（x），用于确定输出变量（y）。输入变量和输出变量之间存在一种关系。机器学习的目标是量化这种关系。
- en: '![](../Images/a1ae8980334e4a697589004f968e799e.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1ae8980334e4a697589004f968e799e.png)'
- en: 'Figure 1: Linear Regression is represented as a line in the form of y = a +
    bx. [Source](http://bhagyeshvikani.blogspot.ca/2015/10/linear-regression.html)In
    Linear Regression, the relationship between the input variables (x) and output
    variable (y) is expressed as an equation of the form y = a + bx. Thus, the goal
    of linear regression is to find out the values of coefficients a and b. Here,
    a is the intercept and b is the slope of the line.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：线性回归表示为形式为 y = a + bx 的直线。[来源](http://bhagyeshvikani.blogspot.ca/2015/10/linear-regression.html)
    在线性回归中，输入变量（x）和输出变量（y）之间的关系被表示为形式为 y = a + bx 的方程。因此，线性回归的目标是找出系数 a 和 b 的值。这里，a
    是截距，b 是直线的斜率。
- en: Figure 1 shows the plotted x and y values for a dataset. The goal is to fit
    a line that is nearest to most of the points. This would reduce the distance (‘error’)
    between the y value of a data point and the line.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1 显示了数据集中 x 和 y 值的散点图。目标是拟合一条与大多数点最接近的直线。这将减少数据点的 y 值与直线之间的距离（“误差”）。
- en: '**2\. Logistic Regression**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 逻辑回归**'
- en: Linear regression predictions are continuous values (rainfall in cm),logistic
    regression predictions are discrete values (whether a student passed/failed) after
    applying a transformation function.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归预测的是连续值（如降雨量，以 cm 为单位），而逻辑回归预测的是离散值（如学生是否及格），这是在应用转换函数后得到的。
- en: 'Logistic regression is best suited for binary classification (datasets where
    y = 0 or 1, where 1 denotes the default class. Example: In predicting whether
    an event will occur or not, the event that it occurs is classified as 1\. In predicting
    whether a person will be sick or not, the sick instances are denoted as 1). It
    is named after the transformation function used in it, called the logistic function
    h(x)= 1/ (1 + e^x), which is an S-shaped curve.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归最适合于二分类问题（数据集中 y = 0 或 1，其中 1 表示默认类别。例如：在预测事件是否发生时，事件发生被分类为 1。在预测一个人是否会生病时，生病的情况被标记为
    1）。它以其使用的转换函数命名，即逻辑函数 h(x)= 1/ (1 + e^x)，这是一个 S 形曲线。
- en: In logistic regression, the output is in the form of probabilities of the default
    class (unlike linear regression, where the output is directly produced). As it
    is a probability, the output lies in the range of 0-1\. The output (y-value) is
    generated by log transforming the x-value, using the logistic function h(x)= 1/
    (1 + e^ -x) . A threshold is then applied to force this probability into a binary
    classification.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中，输出是默认类别的概率（与线性回归直接产生输出不同）。由于它是概率，输出范围在 0-1 之间。输出（y 值）是通过对 x 值进行对数转换，使用逻辑函数
    h(x)= 1/ (1 + e^ -x) 得到的。然后应用阈值将这个概率强制转换为二分类。
- en: '![](../Images/4c145ea1c19f29d1a5af2933df8d96f1.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c145ea1c19f29d1a5af2933df8d96f1.png)'
- en: 'Figure 2: Logistic Regression to determine if a tumour is malignant or benign.
    Classified as malignant if the probability h(x)>= 0.5\. [Source](https://athemathmo.github.io/2016/03/07/rusty-machine.html)In
    Figure 2, to determine whether a tumour is malignant or not, the default variable
    is y=1 (tumour= malignant) ; the x variable could be a measurement of the tumour,
    such as the size of the tumour. As shown in the figure, the logistic function
    transforms the x-value of the various instances of the dataset, into the range
    of 0 to 1\. If the probability crosses the threshold of 0.5 (shown by the horizontal
    line), the tumour is classified as malignant.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：逻辑回归用于判断肿瘤是恶性还是良性。如果概率 h(x)>= 0.5，则分类为恶性。[来源](https://athemathmo.github.io/2016/03/07/rusty-machine.html)
    在图 2 中，为了判断肿瘤是否恶性，默认变量为 y=1（肿瘤=恶性）；x 变量可以是肿瘤的测量值，比如肿瘤的大小。如图所示，逻辑函数将数据集中各种实例的 x
    值转换到 0 到 1 的范围内。如果概率超过 0.5 的阈值（由水平线表示），则将肿瘤分类为恶性。
- en: The logistic regression equation *P(x) = e ^ (b0 +b1*x) / (1 + e^(b0 + b1*x))*
    can be transformed into *ln(p(x) / 1-p(x)) = b0 + b1*x*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归方程 *P(x) = e ^ (b0 +b1*x) / (1 + e^(b0 + b1*x))* 可以转换为 *ln(p(x) / 1-p(x))
    = b0 + b1*x*。
- en: The goal of logistic regression is to use the training data to find the values
    of coefficients b0 and b1 such that it will minimize the error between the predicted
    outcome and the actual outcome. These coefficients are estimated using the technique
    of Maximum Likelihood Estimation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的目标是利用训练数据来寻找系数 b0 和 b1 的值，以最小化预测结果与实际结果之间的误差。这些系数使用最大似然估计技术来估算。
- en: '**3\. CART**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**3. CART**'
- en: Classification and Regression Trees (CART) is an implementation of Decision
    Trees, among others such as ID3, C4.5.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 分类与回归树（CART）是一种决策树的实现，还有其他如 ID3、C4.5。
- en: 'The non-terminal nodes are the root node and the internal node. The terminal
    nodes are the leaf nodes. Each non-terminal node represents a single input variable
    (x) and a splitting point on that variable; the leaf nodes represent the output
    variable (y). The model is used as follows to make predictions: walk the splits
    of the tree to arrive at a leaf node and output the value present at the leaf
    node.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 非终端节点是根节点和内部节点。终端节点是叶子节点。每个非终端节点代表一个输入变量（x）及其在该变量上的分裂点；叶子节点代表输出变量（y）。模型的使用方法是：沿着树的分裂路径走到一个叶子节点，并输出该叶子节点上的值。
- en: 'The decision tree in Figure3 classifies whether a person will buy a sports
    car or a minivan depending on their age and marital status. If the person is over
    30 years and is not married, we walk the tree as follows : ‘over 30 years?’ ->
    yes -> ’married?’ -> no. Hence, the model outputs a sportscar.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3 中的决策树根据一个人的年龄和婚姻状况来分类其是否会购买跑车或小型货车。如果该人超过 30 岁且未婚，我们按以下方式走树：‘超过 30 岁？’ ->
    是 -> ‘已婚？’ -> 否。因此，模型输出的是跑车。
- en: '![](../Images/b6949b5859022506961fc0ced7a7b482.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6949b5859022506961fc0ced7a7b482.png)'
- en: 'Figure 3: Parts of a decision tree. [Source](http://www.hypertextbookshop.com/dataminingbook/public_version/contents/chapters/chapter001/section002/green/page001.html)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：决策树的部分。 [来源](http://www.hypertextbookshop.com/dataminingbook/public_version/contents/chapters/chapter001/section002/green/page001.html)
- en: '**4\. Naïve Bayes**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**4. 朴素贝叶斯**'
- en: 'To calculate the probability that an event will occur, given that another event
    has already occurred, we use Bayes’ Theorem. To calculate the probability of an
    outcome given the value of some variable, that is, to calculate the probability
    of a hypothesis(h) being true, given our prior knowledge(d), we use Bayes’ Theorem
    as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算一个事件发生的概率，给定另一个事件已经发生，我们使用贝叶斯定理。为了计算某个变量值下的结果概率，即计算假设 h 为真的概率，给定我们的先验知识
    d，我们使用贝叶斯定理如下：
- en: '*P(h|d)= (P(d|h) * P(h)) / P(d)*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(h|d) = (P(d|h) * P(h)) / P(d)*'
- en: where
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: where
- en: P(h|d) = Posterior probability. The probability of hypothesis h being true,
    given the data d, where P(h|d)= P(d1| h)* P(d2| h)*....*P(dn| h)* P(d)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(h|d) = 后验概率。给定数据 d，假设 h 为真的概率，其中 P(h|d) = P(d1|h) * P(d2|h) * .... * P(dn|h)
    * P(d)
- en: P(d|h) = Likelihood. The probability of data d given that the hypothesis h was
    true.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(d|h) = 似然性。给定假设 h 为真的情况下，数据 d 的概率。
- en: P(h) = Class prior probability. The probability of hypothesis h being true (irrespective
    of the data)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(h) = 类别先验概率。假设 h 为真的概率（与数据无关）
- en: P(d) = Predictor prior probability. Probability of the data (irrespective of
    the hypothesis)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(d) = 预测先验概率。数据的概率（与假设无关）
- en: This algorithm is called ‘naive’ because it assumes that all the variables are
    independent of each other, which is a naive assumption to make in real-world examples.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法被称为‘naive’（朴素的），因为它假设所有变量相互独立，这在实际例子中是一个朴素的假设。
- en: '![](../Images/f2dc03a423162be40bd2e9876237bb27.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f2dc03a423162be40bd2e9876237bb27.png)'
- en: 'Figure 4: Using Naive Bayes to predict the status of ‘play’ using the variable
    ‘weather’.Using Figure 4 as an example, what is the outcome if weather=’sunny’?'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：使用朴素贝叶斯预测‘play’的状态，变量为‘weather’。以图 4 为例，如果天气 = ‘sunny’，结果是什么？
- en: To determine the outcome play= ‘yes’ or ‘no’ given the value of variable weather=’sunny’,
    calculate P(yes|sunny) and P(no|sunny) and choose the outcome with higher probability.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定结果是 play = ‘yes’ 还是 ‘no’，给定变量 weather = ‘sunny’，计算 P(yes|sunny) 和 P(no|sunny)，并选择概率较高的结果。
- en: ->P(yes|sunny)= (P(sunny|yes) * P(yes)) /  P(sunny)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: -> P(yes|sunny) = (P(sunny|yes) * P(yes)) / P(sunny)
- en: = (3/9  * 9/14 ) / (5/14)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: = (3/9 * 9/14) / (5/14)
- en: = 0.60
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: = 0.60
- en: -> P(no|sunny)=  (P(sunny|no) * P(no)) /  P(sunny)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: -> P(no|sunny) = (P(sunny|no) * P(no)) / P(sunny)
- en: = (2/5  * 5/14 ) / (5/14)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: = (2/5 * 5/14) / (5/14)
- en: = 0.40
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: = 0.40
- en: Thus, if the weather =’sunny’, the outcome is play= ‘yes’.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果天气 = ‘sunny’，结果是 play = ‘yes’。
- en: '**5\. KNN**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**5. KNN**'
- en: The k-nearest neighbours algorithm uses the entire dataset as the training set,
    rather than splitting the dataset into a trainingset and testset.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: K 最近邻算法使用整个数据集作为训练集，而不是将数据集拆分为训练集和测试集。
- en: When an outcome is required for a new data instance, the KNN algorithm goes
    through the entire dataset to find the k-nearest instances to the new instance,
    or the k number of instances most similar to the new record, and then outputs
    the mean of the outcomes (for a regression problem) or the mode (most frequent
    class) for a classification problem. The value of k is user-specified.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要为一个新的数据实例生成结果时，KNN 算法会遍历整个数据集，以找到与新实例最接近的 k 个实例，或者与新记录最相似的 k 个实例，然后输出结果的均值（对于回归问题）或众数（最频繁的类别，对于分类问题）。k
    的值由用户指定。
- en: The similarity between instances is calculated using measures such as Euclidean
    distance and Hamming distance.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 实例之间的相似性通过欧氏距离和汉明距离等度量来计算。
- en: '* * *'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT 部门'
- en: '* * *'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多相关话题
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[建立一个扎实的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用管道编写干净的 Python 代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学统计学习的最佳资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学去寻找目的，并找到目的去…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每位初学者数据科学家应掌握的 6 个预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一个 90 亿美元的 AI 失败，深入分析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
