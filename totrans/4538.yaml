- en: Build Pipelines with Pandas Using pdpipe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/12/build-pipelines-pandas-pdpipe.html](https://www.kdnuggets.com/2019/12/build-pipelines-pandas-pdpipe.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/35dc4b78af29432b47da1a6c94187323.png)'
  prefs: []
  type: TYPE_IMG
- en: Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pandas is an amazing library in the Python ecosystem for data analytics and
    machine learning. They form the perfect bridge between the data world, where Excel/CSV
    files and SQL tables live, and the modeling world where Scikit-learn or TensorFlow
    perform their magic.
  prefs: []
  type: TYPE_NORMAL
- en: A data science flow is most often a sequence of steps — datasets must be cleaned,
    scaled, and validated before they can be ready to be used by that powerful machine
    learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: These tasks can, of course, be done with many single-step functions/methods
    that are offered by packages like Pandas but a more elegant way is to use a pipeline.
    In almost all cases, a pipeline reduces the chance of error and saves time by
    automating repetitive tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the data science world, great examples of packages with pipeline features
    are — [dplyr in R language](https://dplyr.tidyverse.org/), and [Scikit-learn in
    the Python ecosystem](https://scikit-learn.org/stable/modules/compose.html).
  prefs: []
  type: TYPE_NORMAL
- en: A data science flow is most often a sequence of steps — datasets must be cleaned,
    scaled, and validated before they can be ready to be used
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Following is a great article about their use in a machine-learning workflow.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Managing Machine Learning Workflows with Scikit-learn Pipelines Part 1:
    A Gentle Introduction**](https://www.kdnuggets.com/2017/12/managing-machine-learning-workflows-scikit-learn-pipelines-part-1.html?source=post_page-----cade6128cd31----------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Are you familiar with Scikit-learn Pipelines? They are an extremely simple yet
    very useful tool for managing machine ...
  prefs: []
  type: TYPE_NORMAL
- en: Pandas also offer a `**.pipe**` method which can be used for similar purposes
    with user-defined functions. However, in this article, we are going to discuss
    a wonderful little library called [**pdpipe**](https://github.com/shaypal5/pdpipe),
    which specifically addresses this pipelining issue with Pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: In almost all cases, a pipeline reduces the chance of error and saves time by
    automating repetitive tasks
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Pipelining with Pandas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The example [Jupyter notebook can be found here in my Github repo](https://github.com/tirthajyoti/Machine-Learning-with-Python/blob/master/Pandas%20and%20Numpy/pdpipe-example.ipynb).
    Let’s see how we can build useful pipelines with this library.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the demonstration purpose, we will use a [dataset of US Housing prices](https://www.kaggle.com/vedavyasv/usa-housing) (downloaded
    from Kaggle). We can load the dataset in Pandas and show its summary statistics
    as follows,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fec6e833a0de5a47dadd841113509ca2.png)'
  prefs: []
  type: TYPE_IMG
- en: However, the dataset also has an ‘Address’ field which contains text data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/82af45ead8d94494050e8834814a9587.png)'
  prefs: []
  type: TYPE_IMG
- en: Adding a size qualifier column
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the demo, we add a column to the dataset qualifying the size of the house,
    with the following code,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bec265b90a72104255320186bb0880b7.png)'
  prefs: []
  type: TYPE_IMG
- en: The dataset looks like following after this,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0600cabbbf97f788eecefe6f6485a709.png)'
  prefs: []
  type: TYPE_IMG
- en: The simplest pipeline — one operation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We start with the simplest possible pipeline, consisting of just one operation
    (don’t worry, we will add complexity soon enough).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say the machine learning team and the domain experts say that they think
    we can safely ignore the `Avg. Area House Age` data for modeling. Therefore, we
    will drop this column from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For this task, we create a pipeline object `drop_age` with the `ColDrop` method
    from [**pdpipe**](https://github.com/shaypal5/pdpipe) and pass the DataFrame to
    this pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The resulting DataFrame, as expected, looks like following,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98a36d37e0343974e7ee173153763177.png)'
  prefs: []
  type: TYPE_IMG
- en: Chain stages of pipeline simply by adding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pipelines are useful and practical only when we are able to multiple stages.
    There are multiple methods by which you can do that in [**pdpipe**](https://github.com/shaypal5/pdpipe).
    However, the simplest and most intuitive approach is to use the + operator. It
    is like hand-joining to pipes!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say, apart from dropping the age column, we also want to one-hot-encode
    the `House_size` column so that a classification or regression algorithm can be
    run on the dataset easily.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: So, we created a pipeline object first with the `ColDrop` method to drop the `Avg.
    Area House Age` column. Thereafter, we just simply added the `OneHotEncode` method
    to this pipeline object with the usual Python `+=` syntax.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting DataFrame looks like the following. Note the additional indicator
    columns `House_size_Medium` and `House_size_Small` created from the one-hot-encoding
    process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f28f16ec5fc6c724925932d3fa92e28b.png)'
  prefs: []
  type: TYPE_IMG
- en: Drop some rows based on their values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we may want to remove rows of data based on their values. Specifically,
    we may want to drop all the data where the house price is less than 250,000\.
    We have the`ApplybyCol` method to apply any user-defined function to the DataFrame
    and also a method `ValDrop` to drop rows based on a specific value. We can easily
    chain these methods to our pipeline to selectively drop rows (we are still adding
    to our existing `pipeline` object which already does the other jobs of column
    dropping and one-hot-encoding).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The first method tags the rows based on the value in the `Price` column by applying
    the user-defined function `price_tag()`,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/160d29fd4df1f23cf5a9212239a4768f.png)'
  prefs: []
  type: TYPE_IMG
- en: The second method looks for the string `drop` in the `Price_tag` column and
    drops those rows that match. And finally, the third method removes the `Price_tag` column,
    cleaning up the DataFrame. After all, this `Price_tag` column was only needed
    temporarily, to tag specific rows, and should be removed after it served its purpose.
  prefs: []
  type: TYPE_NORMAL
- en: All of this is done by simply chaining stages of operations on the same pipeline!
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we can look back and see what our pipeline does to the DataFrame
    right from the beginning,
  prefs: []
  type: TYPE_NORMAL
- en: drops a specific column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: one-hot-encodes a categorical data column for modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tags data based on a user-defined function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: drops rows based on the tag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: drops the temporary tagging column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of this — using the following five lines of code,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Scikit-learn and NLTK stages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many more useful and intuitive DataFrame manipulation methods available
    for DataFrame manipulation. However, we just wanted to show that even some operations
    from Scikit-learn and NLTK package are included in [**pdpipe**](https://github.com/shaypal5/pdpipe) for
    making awesome pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling estimator from Scikit-learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most common tasks for building machine learning models is the scaling
    of the data. Scikit-learn offers a few different types of scaling such as Min-Max
    scaling, or Standardization based scaling (where mean of a data set is subtracted
    followed by division by standard deviation).
  prefs: []
  type: TYPE_NORMAL
- en: We can directly chain such scaling operations in a pipeline. Following code
    demonstrates the use,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here we applied the `[StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)`[ estimator](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) from
    the Scikit-learn package to transform the data for clustering or neural network
    fitting. We can selectively exclude columns which do not need such scaling as
    we have done here for the indicator columns `House_size_Medium` and `House_size_Small`.
  prefs: []
  type: TYPE_NORMAL
- en: And voila! We get the scaled DataFrame,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c395fb9f3a24f89c8681e0bdd1c6a89.png)'
  prefs: []
  type: TYPE_IMG
- en: Tokenizer from NLTK
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We note that the Address field in our DataFrame is pretty useless right now.
    However, if we can extract zip code or State from those strings, they might be
    useful for some kind of visualization or machine learning task.
  prefs: []
  type: TYPE_NORMAL
- en: We can use a [Word Tokenizer](https://www.guru99.com/tokenize-words-sentences-nltk.html) for
    this purpose. NLTK is a popular and powerful Python library for text mining and
    natural language processing (NLP) and offers a range of tokenizer methods. Here,
    we can use one such tokenizer to split up the text in the address field and extract
    the name of the state from that. We recognize that the name of the state is the
    penultimate word in the address string. Therefore, following chained pipeline
    will do the job for us,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The resulting DataFrame looks like following,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/423cabcab7c4c503e70de4a239380d78.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we summarize all the operations shown in this demo, it looks like the following,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc42570defa14208de7f4f03da36a377.png)'
  prefs: []
  type: TYPE_IMG
- en: All of these operations may be used frequently on similar types of datasets
    and it will be wonderful to have a simple set of sequential code blocks to execute
    as a pre-processing operation before the dataset is ready for the next level of
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelining is the key to achieve that uniform set of sequential code blocks.
    Pandas is the most widely used Python library for such data pre-processing tasks
    in a machine learning/data science team and [**pdpipe**](https://github.com/shaypal5/pdpipe) provides
    a simple yet powerful way to build pipelines with Pandas-type operations which
    can be directly applied to the Pandas DataFrame objects.
  prefs: []
  type: TYPE_NORMAL
- en: '[Explore this library on your own](https://github.com/shaypal5/pdpipe) and
    build more powerful pipelines for your specific data science task.'
  prefs: []
  type: TYPE_NORMAL
- en: If you have any questions or ideas to share, please contact the author at [**tirthajyoti[AT]gmail.com**](mailto:tirthajyoti@gmail.com).
    Also, you can check the author’s [**GitHub**](https://github.com/tirthajyoti?tab=repositories)** repositories **for
    code, ideas, and resources in machine learning and data science. If you are, like
    me, passionate about AI/machine learning/data science, please feel free to [add
    me on LinkedIn](https://www.linkedin.com/in/tirthajyoti-sarkar-2127aa7/) or [follow
    me on Twitter](https://twitter.com/tirthajyotiS).
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/https-medium-com-tirthajyoti-build-pipelines-with-pandas-using-pdpipe-cade6128cd31).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[How to Speed up Pandas by 4x with one line of code](/2019/11/speed-up-pandas-4x.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Great New Features in Latest Scikit-learn Release](/2019/12/5-features-scikit-learn-release-highlights.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Pipelines, Luigi, Airflow: Everything you need to know](/2019/03/data-pipelines-luigi-airflow-everything-need-know.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
