- en: How to Evaluate the Performance of Your Machine Learning Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何评估你的机器学习模型的性能
- en: 原文：[https://www.kdnuggets.com/2020/09/performance-machine-learning-model.html](https://www.kdnuggets.com/2020/09/performance-machine-learning-model.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/09/performance-machine-learning-model.html](https://www.kdnuggets.com/2020/09/performance-machine-learning-model.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Saurabh Raj](https://www.linkedin.com/in/saurabh-raj-445821153/), IIT
    Jammu**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者 [Saurabh Raj](https://www.linkedin.com/in/saurabh-raj-445821153/)，印度理工学院贾穆**。'
- en: Why evaluation is necessary?
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么评估是必要的？
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你在组织中的IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Let me start with a very simple example.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我从一个非常简单的例子开始。
- en: '*Robin and Sam both started preparing for an entrance exam for engineering
    college. They both shared a room and put an equal amount of hard work while solving
    numerical problems. They both studied almost the same hours for the entire year
    and appeared in the final exam. Surprisingly, Robin cleared, but Sam did not.
    When asked, we got to know that there was one difference in their strategy of
    preparation, “test series.” Robin had joined a test series, and he used to test
    his knowledge and understanding by giving those exams and then further evaluating
    where is he lagging. But Sam was confident, and he just kept training himself.*'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*罗宾和萨姆都开始为工程学院的入学考试做准备。他们共享一个房间，并在解决数值问题时付出了相同的努力。他们几乎每天都学习相同的小时数，并参加了最终考试。令人惊讶的是，罗宾通过了，而萨姆没有。经过询问，我们了解到他们准备策略中有一个不同点，“测试系列。”罗宾参加了一个测试系列，他通过这些考试来测试自己的知识和理解，然后进一步评估自己的不足之处。但萨姆很自信，他只是继续训练自己。*'
- en: In the same fashion, as discussed above, a machine learning model can be trained
    extensively with many parameters and new techniques, but as long as you are skipping
    its evaluation, you cannot trust it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以同样的方式，如上所述，一个机器学习模型可以通过许多参数和新技术进行广泛的训练，但只要你忽视其评估，你就无法信任它。
- en: '**How to read the Confusion Matrix?**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何阅读混淆矩阵？**'
- en: A **confusion matrix** is a correlation between the predictions of a model and
    the actual class labels of the data points.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个**混淆矩阵**是模型预测与数据点实际类别标签之间的关联。
- en: '![](../Images/30f2fc2e52391c8308b3603b4a398421.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30f2fc2e52391c8308b3603b4a398421.png)'
- en: '*Confusion Matrix for a Binary Classification.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*二分类的混淆矩阵。*'
- en: 'Let’s say you are building a model that detects whether a person has diabetes
    or not. After the train-test split, you got a test set of length 100, out of which
    70 data points are labeled positive (1), and 30 data points are labelled negative
    (0). Now let me draw the matrix for your test prediction:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在构建一个模型来检测一个人是否患有糖尿病。在训练-测试分割后，你得到了一个长度为100的测试集，其中70个数据点标记为正例（1），30个数据点标记为负例（0）。现在，让我绘制一下你的测试预测矩阵：
- en: '![](../Images/07a2cf18b521e3d4d6f037e3e3b3c416.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07a2cf18b521e3d4d6f037e3e3b3c416.png)'
- en: Out of 70 actual positive data points, your model predicted 64 points as positive
    and 6 as negative. Out of 30 actual negative points, it predicted 3 as positive
    and 27 as negative.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在70个实际正例数据点中，你的模型预测了64个点为正例，6个点为负例。在30个实际负例数据点中，它预测了3个点为正例，27个点为负例。
- en: '**Note:** In the notations, **True Positive, True Negative, False Positive,
    & False Negative**, notice that the second term (Positive or Negative) is denoting
    your prediction, and the first term denotes whether you predicted right or wrong.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 在这些符号中，**真正例、真负例、假正例和假负例**，注意第二个术语（正例或负例）表示你的预测，而第一个术语表示你预测的正确性。'
- en: 'Based on the above matrix, we can define some very important ratios:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述矩阵，我们可以定义一些非常重要的比率：
- en: '**TPR (True Positive Rate) = ( True Positive / Actual Positive )**'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TPR（真正阳性率）=（真正阳性 / 实际阳性）**'
- en: '**TNR (True Negative Rate) = ( True Negative/ Actual Negative)**'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TNR（真正负性率）=（真正负性 / 实际负性）**'
- en: '**FPR (False Positive Rate) = ( False Positive / Actual Negative )**'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FPR（假阳性率）=（假阳性 / 实际负例）**'
- en: '**FNR (False Negative Rate) = ( False Negative / Actual Positive )**'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FNR（假阴性率）=（假阴性 / 实际阳性）**'
- en: 'For our case of diabetes detection model, we can calculate these ratios:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们糖尿病检测模型的情况，我们可以计算这些比例：
- en: TPR = 91.4%
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: TPR = 91.4%
- en: TNR = 90%
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: TNR = 90%
- en: FPR = 10%
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: FPR = 10%
- en: FNR = 8.6%
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: FNR = 8.6%
- en: If you want your model to be smart, then your model has to predict correctly.
    This means your **True Positives **and**True Negatives** *should be as high as
    possible*, and at the same time, you need to minimize your mistakes for which
    your **False Positives **and**False Negatives** *should be as low as possible.*Also
    in terms of ratios, your **TPR & TNR** *should be very high*whereas**FPR & FNR**
    *should be very low***,**
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想让你的模型变得智能，你的模型必须准确预测。这意味着你的**真正阳性**和**真正负性** *应该尽可能高*，同时，你需要尽量减少你的错误，即**假阳性**和**假阴性**
    *应该尽可能低*。同时，在比例方面，你的**TPR & TNR** *应该非常高*，而**FPR & FNR** *应该非常低*。
- en: '**A smart model:** TPR ↑ , TNR ↑, FPR ↓, FNR ↓'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**一个智能模型：** TPR ↑，TNR ↑，FPR ↓，FNR ↓'
- en: '**A dumb model:** Any other combination of TPR, TNR, FPR, FNR'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**一个糟糕的模型：** 任何其他组合的TPR、TNR、FPR、FNR。'
- en: One may argue that it is not possible to take care of all four ratios equally
    because, at the end of the day, no model is perfect. Then what should we do?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能会争辩说，不可能对所有四个比例进行平等处理，因为最终没有模型是完美的。那么我们应该怎么做？
- en: Yes, it is true. So that is why we build a model keeping the domain in our mind.
    There are certain domains that demand us to keep a specific ratio as the main
    priority, even at the cost of other ratios being poor. For example, in cancer
    diagnosis, we cannot miss any positive patient at any cost. So we are supposed
    to keep TPR at the maximum and FNR close to 0\. Even if we predict any healthy
    patient as diagnosed, it is still okay as he can go for further check-ups.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这是真的。这就是为什么我们在构建模型时要考虑领域的原因。有些领域要求我们将特定的比例作为主要优先级，即使其他比例较差也是如此。例如，在癌症诊断中，我们不能以任何代价漏掉任何一个阳性患者。因此，我们应该将TPR保持在最大值，而FNR接近0。即使我们将任何健康患者预测为已诊断，仍然可以接受，因为他们可以进行进一步检查。
- en: Accuracy
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准确率
- en: Accuracy is what its literal meaning says, a measure of how accurate your model
    is.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率就是其字面意义上所说的，衡量你的模型有多准确。
- en: '**Accuracy = Correct Predictions / Total Predictions**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**准确率 = 正确预测 / 总预测**'
- en: '**By using confusion matrix, Accuracy = (TP + TN)/(TP+TN+FP+FN)**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过使用混淆矩阵，准确率 = （TP + TN）/（TP + TN + FP + FN）**'
- en: Accuracy is one of the simplest performance metrics we can use. But let me warn
    you, accuracy can sometimes lead you to false illusions about your model, and
    hence you should first know your data set and algorithm used then only decide
    whether to use accuracy or not.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是我们可以使用的最简单的性能指标之一。但我需要警告你，准确率有时会让你对模型产生错误的幻想，因此你应该首先了解你的数据集和使用的算法，然后决定是否使用准确率。
- en: 'Before going to the failure cases of accuracy, let me introduce you with two
    types of data sets:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论准确率的失败案例之前，让我先介绍两种数据集：
- en: '**Balanced:**A data set that contains almost equal entries for all labels/classes.
    E.g., out of 1000 data points, 600 are positive, and 400 are negative.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**平衡数据集：** 一个包含几乎所有标签/类别都有相等条目的数据集。例如，在1000个数据点中，600个是正例，400个是负例。'
- en: '**Imbalanced:**A data set that contains a biased distribution of entries towards
    a particular label/class. E.g., out of 1000 entries, 990 are positive class, 10
    are negative class.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**不平衡数据集：** 一个包含偏向特定标签/类别的条目的数据集。例如，在1000个条目中，990个是正例，10个是负例。'
- en: '**Very Important: Never use accuracy as a measure when dealing with imbalanced
    test set.**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**非常重要：处理不平衡测试集时，永远不要使用准确率作为衡量标准。**'
- en: Why?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么？
- en: Suppose you have an imbalanced test set of 1000 entries with **990 (+ve)** and **10
    (-ve)**. And somehow, you ended up creating a poor model which always predicts
    “+ve” due to the imbalanced train set. Now when you predict your test set labels,
    it will always predict “+ve.” So out of 1000 test set points, you get 1000 “+ve”
    predictions. Then your accuracy would come,
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个不平衡的测试集，包含1000个条目，其中**990 (+ve)**和**10 (-ve)**。假如你创建了一个糟糕的模型，由于不平衡的训练集，该模型总是预测“+ve”。现在，当你预测测试集标签时，它将总是预测“+ve”。所以，在1000个测试集点中，你得到1000个“+ve”预测。然后你的准确率将是，
- en: 990/1000 = 99%
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 990/1000 = 99%
- en: Whoa! Amazing! You are happy to see such an awesome accuracy score.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！太棒了！你很高兴看到如此出色的准确度评分。
- en: But, you should know that your model is really poor because it always predicts
    “+ve” label.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，你应该知道你的模型实际上很差，因为它总是预测“+ve”标签。
- en: '**Very Important: Also, we cannot compare two models that return probability
    scores and have the same accuracy.**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**非常重要：另外，我们不能比较两个返回概率分数并且具有相同准确度的模型。**'
- en: 'There are certain models that give the probability of each data point for belonging
    to a particular class like that in Logistic Regression. Let us take this case:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有些模型提供每个数据点属于特定类别的概率，就像逻辑回归中的那样。让我们来看看这种情况：
- en: '![](../Images/05861a474f677b329bfc620bf8dc6030.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05861a474f677b329bfc620bf8dc6030.png)'
- en: '*Table 1.*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*表1。*'
- en: As you can see, **If P(Y=1) > 0.5, it predicts class 1.** When we calculate
    accuracy for both M1 and M2, it comes out the same, but it is quite evident that** M1
    is a much better model than M2 by taking a look at the probability scores.**
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，**如果P(Y=1) > 0.5，它预测类别1。** 当我们计算M1和M2的准确度时，结果是相同的，但很明显**M1是比M2更好的模型，看看概率分数就可以看出来。**
- en: This issue is beautifully dealt with by **Log Loss**, which I explain later
    in the blog.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**Log Loss**很好地解决了这个问题，我将在博客中进一步解释。'
- en: Precision & Recall
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精确度与召回率
- en: '**Precision:** It is the ratio of True Positives (TP) and the total positive
    predictions. Basically, it tells us how many times your positive prediction was
    actually positive.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**精确度：** 它是实际正例（TP）与所有正例预测的比例。基本上，它告诉我们你的正例预测中有多少次实际上是正例。'
- en: '![](../Images/cf09000fabffe30a8aad38e173e9a92c.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf09000fabffe30a8aad38e173e9a92c.png)'
- en: '**Recall :** It is nothing but TPR (True Positive Rate explained above). It
    tells us about out of all the positive points how many were predicted positive.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**召回率：** 这就是上面解释的TPR（真正阳性率）。它告诉我们所有正例中有多少被预测为正例。'
- en: '![](../Images/b57cd45c6855c191615d9787865b7b00.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b57cd45c6855c191615d9787865b7b00.png)'
- en: '**F-Measure:** Harmonic mean of precision and recall.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**F-Measure：** 精确度和召回率的调和平均数。'
- en: '![](../Images/22ffa6e5026b2bb424756c20378fb9fb.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22ffa6e5026b2bb424756c20378fb9fb.png)'
- en: 'To understand this, let’s see this example: When you ask a query in google,
    it returns 40 pages, but only 30 were relevant. But your friend, who is an employee
    at Google, told you that there were 100 total relevant pages for that query. So
    it’s precision is 30/40 = 3/4 = 75% while it’s recall is 30/100 = 30%. So, in
    this case, precision is “how useful the search results are,” and recall is “how
    complete the results are.”'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，我们来看一个例子：当你在Google中查询时，它返回了40页结果，但只有30页是相关的。然而，你的朋友，一个Google的员工，告诉你总共有100页相关的结果。所以，它的精确度是30/40
    = 3/4 = 75%，而召回率是30/100 = 30%。所以，在这个例子中，精确度是“搜索结果的有用性”，而召回率是“结果的完整性”。
- en: ROC & AUC
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ROC与AUC
- en: '**Receiver Operating Characteristic Curve (ROC):**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收者操作特征曲线（ROC）：**'
- en: It is a **plot between TPR (True Positive Rate) and FPR (False Positive Rate)** calculated
    by taking multiple threshold values from the reverse sorted list of probability
    scores given by a model.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个**TPR（真正阳性率）与FPR（假阳性率）的绘图**，通过从模型给出的概率分数的逆序列表中取多个阈值计算得出。
- en: '![](../Images/a8cecf6aa0a8e5bd17da5c886e1826a6.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8cecf6aa0a8e5bd17da5c886e1826a6.png)'
- en: '*A typical ROC curve.*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*典型的ROC曲线。*'
- en: Now, how do we plot ROC?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何绘制ROC曲线？
- en: To answer this, let me take you back to Table 1 above. Just consider the M1
    model. You see, for all x values, we have a probability score. In that table,
    we have assigned the data points that have a score of more than 0.5 as class 1\.
    Now sort all the values in descending order of probability scores and one by one
    take threshold values equal to all the probability scores. Then we will have threshold
    values = [0.96,0.94,0.92,0.14,0.11,0.08]. Corresponding to each threshold value,
    predict the classes, and calculate TPR and FPR. You will get 6 pairs of TPR &
    FPR. Just plot them, and you will get the ROC curve.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，让我带你回到上面的表1。只考虑M1模型。你会看到，对于所有x值，我们都有一个概率分数。在这个表中，我们将分数大于0.5的数据点分配为类别1。现在按概率分数的降序排列所有值，一一取等于所有概率分数的阈值。然后我们会得到阈值
    = [0.96,0.94,0.92,0.14,0.11,0.08]。对应每个阈值，预测类别，并计算TPR和FPR。你将得到6对TPR和FPR。将它们绘制出来，你将得到ROC曲线。
- en: '**Note: Since the maximum TPR and FPR value is 1, the area under the curve
    (AUC) of ROC lies between 0 and 1.**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：由于最大TPR和FPR值为1，ROC曲线下的面积（AUC）介于0和1之间。**'
- en: The area under the blue dashed line is 0.5\. AUC = 0 means very poor model,
    AUC = 1 means perfect model. As long as your model’s AUC score is more than 0.5\.
    your model is making sense because even a random model can score 0.5 AUC.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝色虚线下的面积是 0.5。AUC = 0 表示模型很差，AUC = 1 表示完美模型。只要你的模型的 AUC 分数超过 0.5，模型就是有意义的，因为即使是随机模型也可以获得
    0.5 的 AUC。
- en: '**Very Important:** You can get very high AUC even in a case of a dumb model
    generated from an imbalanced data set. So always be careful while dealing with
    imbalanced data set.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**非常重要：** 即使在不平衡数据集生成的简单模型中，你也可以获得很高的 AUC。因此，处理不平衡数据集时一定要小心。'
- en: '**Note:** AUC had nothing to do with the numerical values probability scores
    as long as the order is maintained. AUC for all the models will be the same as
    long as all the models give the same order of data points after sorting based
    on probability scores.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 只要顺序保持一致，AUC 与数值概率分数无关。只要所有模型在基于概率分数排序后给出相同的数据点顺序，所有模型的 AUC 都将相同。'
- en: Log Loss
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对数损失
- en: This performance metric checks the deviation of probability scores of the data
    points from the cut-off score and assigns a penalty proportional to the deviation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个性能指标检查数据点的概率分数与临界分数的偏差，并赋予一个与偏差成比例的惩罚。
- en: For each data point in a binary classification, we calculate it’s log loss using
    the formula below,
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二分类中的每个数据点，我们使用下面的公式计算它的对数损失，
- en: '![](../Images/f33815e4c2d63cbb613945cbbc3c354f.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f33815e4c2d63cbb613945cbbc3c354f.png)'
- en: '*Log Loss formula for a Binary Classification.*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*二分类对数损失公式。*'
- en: where p = probability of the data point to belong to class 1 and y is the class
    label (0 or 1).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 p = 数据点属于类别 1 的概率，y 是类别标签（0 或 1）。
- en: Suppose if p_1 for some x_1 is 0.95 and p_2 for some x_2 is 0.55 and cut off
    probability for qualifying for class 1 is 0.5\. Then both qualify for class 1,
    but the log loss of p_2 will be much more than the log loss of p_1.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 假设某些 x_1 的 p_1 是 0.95，某些 x_2 的 p_2 是 0.55，且类别 1 的合格概率是 0.5。那么两者都符合类别 1，但 p_2
    的对数损失将远高于 p_1 的对数损失。
- en: '![](../Images/2c6427275f7d1f6c3954f8c53d9769ba.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c6427275f7d1f6c3954f8c53d9769ba.png)'
- en: As you can see from the curve, the range of log loss is [0, infinity).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从曲线中可以看到，对数损失的范围是 [0, 无穷)。
- en: For each data point in multi-class classification, we calculate it’s log loss
    using the formula below,
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多分类中的每个数据点，我们使用下面的公式计算它的对数损失，
- en: '![](../Images/9ae498217f3f357a2cc9d20c32d61779.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ae498217f3f357a2cc9d20c32d61779.png)'
- en: '*Log Loss formula for multi-class classification.*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*多分类对数损失公式。*'
- en: where y(o,c) = 1 if x(o,c) belongs to class 1\. The rest of the concept is the
    same.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 y(o,c) = 1 如果 x(o,c) 属于类别 1。其余的概念是相同的。
- en: Coefficient of Determination
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决定系数
- en: It is denoted by *R².* While predicting target values of the test set, we encounter
    a few errors (e_i), which is the difference between the predicted value and actual
    value.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 它用*R²*表示。在预测测试集的目标值时，我们遇到了一些误差（e_i），这是预测值与实际值之间的差异。
- en: Let’s say we have a test set with n entries. As we know, all the data points
    will have a target value, say [y1,y2,y3…….yn]. Let us take the predicted values
    of the test data be [f1,f2,f3,……fn].
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含 n 个条目的测试集。我们知道，所有的数据点都有一个目标值，比如 [y1,y2,y3…….yn]。让我们将测试数据的预测值取为 [f1,f2,f3,……fn]。
- en: Calculate the **Residual Sum of Squares,** which is the sum of all the errors
    (e_i) squared,by using this formula where fi is the predicted target value by
    a model for i’th data point.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 计算**残差平方和**，这是所有误差（e_i）平方的总和，使用这个公式，其中 fi 是模型为第 i 个数据点预测的目标值。
- en: '![](../Images/e190eeb6f2c5cb8c62d7d67a0f9349ce.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e190eeb6f2c5cb8c62d7d67a0f9349ce.png)'
- en: '*Total Sum of Squares.*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*总平方和。*'
- en: 'Take the mean of all the actual target values:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 取所有实际目标值的均值：
- en: '![](../Images/96866a341b0907ede51ac3a55e06ca01.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96866a341b0907ede51ac3a55e06ca01.png)'
- en: 'Then calculate the **Total Sum of Squares,** which is proportional to the variance
    of the test set target values:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后计算**总平方和**，它与测试集目标值的方差成正比：
- en: '![](../Images/96e43b6be85e7379ea598a204a3d62ae.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96e43b6be85e7379ea598a204a3d62ae.png)'
- en: If you observe both the formulas of the sum of squares, you can see that the
    only difference is the 2nd term, i.e., y_bar and fi. The total sum of squares
    somewhat gives us an intuition that it is the same as the residual sum of squares
    only but with predicted values as [ȳ, ȳ, ȳ,…….ȳ ,n times]. Yes, your intuition
    is right. Let’s say there is a very simple mean model that gives the prediction
    of the average of the target values every time irrespective of the input data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你观察平方和的两个公式，你会发现唯一的区别是第二项，即 y_bar 和 fi。总平方和在某种程度上给了我们一个直觉，即它与残差平方和是一样的，只不过预测值为
    [ȳ, ȳ, ȳ, ……ȳ ,n 次]。是的，你的直觉是对的。假设有一个非常简单的均值模型，它每次都给出目标值的平均预测，无论输入数据是什么。
- en: 'Now we formulate R² as:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将 R² 公式化为：
- en: '![](../Images/d59ceef42c369d77947ff9d2aa943379.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d59ceef42c369d77947ff9d2aa943379.png)'
- en: 'As you can see now, R² is a metric to compare your model with a very simple
    mean model that returns the average of the target values every time irrespective
    of input data. The comparison has 4 cases:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，现在 R² 是用来将你的模型与每次返回目标值平均值的非常简单均值模型进行比较的指标。比较有 4 种情况：
- en: '**case 1: SS_R = 0**'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例 1: SS_R = 0**'
- en: (R² = 1) Perfect model with no errors at all.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: (R² = 1) 完美模型，没有任何错误。
- en: '**case 2: SS_R > SS_T**'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例 2: SS_R > SS_T**'
- en: (R² < 0) Model is even worse than the simple mean model.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: (R² < 0) 模型甚至比简单均值模型还要差。
- en: '**case 3: SS_R = SS_T**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例 3: SS_R = SS_T**'
- en: (R² = 0) Model is same as the simple mean model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: (R² = 0) 模型与简单均值模型相同。
- en: '**case 4: SS_R < SS_T**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例 4: SS_R < SS_T**'
- en: (0< R² <1) Model is okay.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: (0< R² <1) 模型还可以。
- en: Summary
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: So, in a nutshell, you should know your data set and problem very well, and
    then you can always create a confusion matrix and check for its accuracy, precision,
    recall, and plot the ROC curve and find out AUC as per your needs. But if your
    data set is imbalanced, never use accuracy as a measure. If you want to evaluate
    your model even more deeply so that your probability scores are also given weight,
    then go for Log Loss.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，你应该非常了解你的数据集和问题，然后你可以始终创建一个混淆矩阵，检查其准确性、精确性、召回率，并绘制 ROC 曲线，找出 AUC 以满足你的需求。但如果你的数据集不平衡，绝不要使用准确率作为衡量标准。如果你想更深入地评估你的模型，以便概率评分也被考虑，可以使用对数损失。
- en: Remember, always evaluate your training!
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，始终评估你的训练！
- en: '[Original](https://medium.com/swlh/how-to-evaluate-the-performance-of-your-machine-learning-model-40769784d654).
    Reposted with permission.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/swlh/how-to-evaluate-the-performance-of-your-machine-learning-model-40769784d654).
    经许可转载。'
- en: '**Related:**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Idiot’s Guide to Precision, Recall, and Confusion Matrix](https://www.kdnuggets.com/2020/01/guide-precision-recall-confusion-matrix.html)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[精确度、召回率和混淆矩阵的傻瓜指南](https://www.kdnuggets.com/2020/01/guide-precision-recall-confusion-matrix.html)'
- en: '[Using Confusion Matrices to Quantify the Cost of Being Wrong](https://www.kdnuggets.com/2018/10/confusion-matrices-quantify-cost-being-wrong.html)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用混淆矩阵量化错误成本](https://www.kdnuggets.com/2018/10/confusion-matrices-quantify-cost-being-wrong.html)'
- en: '[Achieving Accuracy with your Training Dataset](https://www.kdnuggets.com/2020/03/supahands-accuracy-training-dataset.html)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过训练数据集实现准确性](https://www.kdnuggets.com/2020/03/supahands-accuracy-training-dataset.html)'
- en: More On This Topic
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并通过找到目标来……](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一个 90 亿美元的 AI 失败，进行分析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功数据科学家的 5 个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么使 Python 成为初创公司的理想编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家都应该知道的三个 R 库（即使你使用 Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
