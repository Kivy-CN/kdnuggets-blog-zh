["```py\npip install transformers torch\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n```", "```py\nmodel_name = \"gpt2\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```", "```py\nprompt = \"Once upon a time in Detroit, \"\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n```", "```py\ngen_tokens = model.generate(input_ids, do_sample=True, max_length=100, pad_token_id=tokenizer.eos_token_id)\ngen_text = tokenizer.batch_decode(gen_tokens)[0]\nprint(gen_text)\n```", "```py\ngen_tokens = model.generate(input_ids, \n                            do_sample=True, \n                            max_length=100, \n                            temperature=0.7, \n                            pad_token_id=tokenizer.eos_token_id)\ngen_text = tokenizer.batch_decode(gen_tokens)[0]\nprint(gen_text)\n```", "```py\ngen_tokens = model.generate(input_ids, \n                            do_sample=True, \n                            max_length=100, \n                            top_k=50, \n                            top_p=0.95, \n                            pad_token_id=tokenizer.eos_token_id)\ngen_text = tokenizer.batch_decode(gen_tokens)[0]\nprint(gen_text)\n```", "```py\n# Example: Generating story beginnings\nstory_prompt = \"In a world where AI contgrols everything, \"\ninput_ids = tokenizer(story_prompt, return_tensors=\"pt\").input_ids\ngen_tokens = model.generate(input_ids, \n                            do_sample=True, \n                            max_length=150, \n                            temperature=0.4, \n                            top_k=50, \n                            top_p=0.95, \n                            pad_token_id=tokenizer.eos_token_id)\nstory_text = tokenizer.batch_decode(gen_tokens)[0]\nprint(story_text)\n\n# Example: Creating poetry lines\npoetry_prompt = \"Glimmers of hope rise from the ashes of forgotten tales, \"\ninput_ids = tokenizer(poetry_prompt, return_tensors=\"pt\").input_ids\ngen_tokens = model.generate(input_ids, \n                            do_sample=True, \n                            max_length=50, \n                            temperature=0.7, \n                            pad_token_id=tokenizer.eos_token_id)\npoetry_text = tokenizer.batch_decode(gen_tokens)[0]\nprint(poetry_text)\n```"]