- en: The Essential Guide to Transformers, the Key to Modern SOTA AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/06/essential-guide-transformers-key-modern-sota-ai.html](https://www.kdnuggets.com/2021/06/essential-guide-transformers-key-modern-sota-ai.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: Are you overwhelmed by the vast array of X-formers?
  prefs: []
  type: TYPE_NORMAL
- en: No, they aren't another new team of mutants from Marvel in the tradition of
    the X-Men, X-Factor, and X-Force. X-formers are the name being given to the wide
    array of Transformer variants that have been implemented or proposed. You likely
    know Transformers from their recent spate of success stories in natural language
    processing, computer vision, and other areas of artificial intelligence, but are
    familiar with all of the X-formers? More importantly, do you know the differences,
    and why you might use one over another?
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '**[A Survey of Transformers](https://arxiv.org/abs/2106.04554)**, by Tianyang
    Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu, has been written to help interested
    readers in this regard.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the abstract:'
  prefs: []
  type: TYPE_NORMAL
- en: '[A] great variety of Transformer variants (a.k.a. X-formers) have been proposed,
    however, a systematic and comprehensive literature review on these Transformer
    variants is still missing. In this survey, we provide a comprehensive review of
    various X-formers. We first briefly introduce the vanilla Transformer and then
    propose a new taxonomy of X-formers. Next, we introduce the various X-formers
    from three perspectives: architectural modification, pre-training, and applications.
    Finally, we outline some potential directions for future research.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: That's the 30,000 foot view of what this survey papers covers; let's have a
    closer look.
  prefs: []
  type: TYPE_NORMAL
- en: '![Vanilla Transformer architecture](../Images/6f98b90273d2f7d0533a2c4a294c024e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1\. Categorization of Transformer variants (source: [A Survey of Transformers](https://arxiv.org/abs/2106.04554))'
  prefs: []
  type: TYPE_NORMAL
- en: 'After introducing the Transformer, the paper establishes the idea of numerous
    subsequent Transformer variants, the aforementioned X-formers, and notes that
    these different architectures have all attempted to improve on the original from
    one of these perspectives: model efficiency, model generalization, and model adaptation.
    The paper will go on to note these perspectives of change as they relate to the
    various X-formers subsequently brought up.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, the paper goes into deeper technical detail of the vanilla Transformer
    architecture (see Figure 2), including its usage and performance analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before getting into specific variants, the authors compare and contrast the
    Transformer with other architectures. This discussion includes an analysis of
    both the self-attention mechanism and inductive bias. Herein, Transformers are
    compared to the architectures of: fully-connected deep neural networks, convolutional
    networks, and recurrent networks, with regard to per-layer complexity, minimum
    number of sequential operations and maximum path lengths.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A taxonomy of Transformers is then presented (see Figure 1), noting that a
    variety of proposed models attempt to improve on the vanilla Transformer from
    three perspectives: types of architecture modification, pre-training methods,
    and applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Categorization of Transformer variants](../Images/786f7c58f8c64287607c976d949eef28.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Vanilla Transformer architecture (source:[A Survey of Transformers](https://arxiv.org/abs/2106.04554))
  prefs: []
  type: TYPE_NORMAL
- en: 'The real meat of the survey comes in the following sections, with the first
    of these focusing on proposed improvements to the attention mechanism, discussing
    in detail a number of these above-mentioned specific architectures and laying
    out a taxonomy of these Transformers based on these improvements. Several distinct
    directions of development have been taken in order to facilitate improvements
    and these are all discussed, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Sparse Attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linearized Attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query Prototyping and Memory Compression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low-rank Self-Attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention with Prior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improved Multi-Head Mechanism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors then turn their attention to other X-former module-level modifications,
    in contrast to architecture level modifications (discussed in a subsequent section).
    These modifications include position representations, layer normalization, and
    position-wise feed-forward network layers. Each of these are discussed in detail,
    connecting how and why each are important to Transfomer architecture, and what
    various improvements have proposed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then comes architecture level X-former variants, discussing modifications beyond
    the module level, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Adapting Transformer to Be Lightweight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strengthening Cross-Block Connectivity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adaptive Computation Time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers with Divide-and-Conquer Strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Alternative Architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last modification type, Exploring Alternative Architecture, notes that "several
    studies have explored alternative architectures for Transformer" and discusses
    where some of this research might be heading.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next section discusses pre-trained Transformers, while noting a potential
    problem with Transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: Transformer does not make any assumption about how the data is structured. On
    the one hand, this effectively makes Transformer a very universal architecture
    that has the potential of capturing dependencies of different ranges. On the other
    hand, this makes Transformer prone to overfitting when the data is limited. One
    way to alleviate this issue is to introduce inductive bias into the model.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A brief overview of encoder-only (including the BERT family), decoder-only (including
    the GPT family), and encoder-decoder (including BART, an extended approach to
    BERT) architectures as relates to inductive bias is presented.
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper follows with a summary of the applications of Transformers — which,
    if you have made it this far, you are probably already aware of — including how
    it is used in NLP, CV, audio, and multimodal applications. Finally, the authors
    finish off with conclusions and future directions for X-former related research,
    noting potential further developments in these directions:'
  prefs: []
  type: TYPE_NORMAL
- en: Theoretical Analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better Global Interaction Mechanism beyond Attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unified Framework for Multimodal Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors have a done a great job in what they set out to do in this paper.
    If you are interested in sorting out the specific relationship between the various
    X-formers out there, I suggest that you, too, give [A Survey of Transformers](https://arxiv.org/abs/2106.04554)
    by Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu a read.
  prefs: []
  type: TYPE_NORMAL
- en: '**Related**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Learn Neural Networks for Natural Language Processing Now](/2021/04/learn-neural-networks-natural-language-processing-now.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Great New Resource for Natural Language Processing Research and Applications](/2021/05/great-new-resource-natural-language-processing-research-applications.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting Started with 5 Essential Natural Language Processing Libraries](/2021/02/getting-started-5-essential-nlp-libraries.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Introducing MetaGPT''s Data Interpreter: SOTA Open Source LLM-based…](https://www.kdnuggets.com/metagpt-data-interpreter-open-source-llm-based-data-solutions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Essential Machine Learning Algorithms: A Beginner''s Guide](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Data Scientist’s Essential Guide to Exploratory Data Analysis](https://www.kdnuggets.com/2023/06/data-scientist-essential-guide-exploratory-data-analysis.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Essential Guide to SQL’s Execution Order](https://www.kdnuggets.com/the-essential-guide-to-sql-execution-order)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Comprehensive Guide to Essential Tools for Data Analysts](https://www.kdnuggets.com/a-comprehensive-guide-to-essential-tools-for-data-analysts)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10 Modern Data Engineering Tools](https://www.kdnuggets.com/2022/07/10-modern-data-engineering-tools.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
