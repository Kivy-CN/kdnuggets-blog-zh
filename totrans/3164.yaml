- en: The 8 Neural Network Architectures Machine Learning Researchers Need to Learn
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习研究人员需要学习的8种神经网络架构
- en: 原文：[https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html](https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html](https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html)
- en: '[comments](/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html?page=2#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html?page=2#comments)'
- en: '![Header image](../Images/ce396eda700c6c69f296bdb37596e40f.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![标题图像](../Images/ce396eda700c6c69f296bdb37596e40f.png)'
- en: '**Why do we need Machine Learning?**'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么我们需要机器学习？**'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Machine learning is needed for tasks that are too complex for humans to code
    directly. Some tasks are so complex that it is impractical, if not impossible,
    for humans to work out all of the nuances and code for them explicitly. So instead,
    we provide a large amount of data to a machine learning algorithm and let the
    algorithm work it out by exploring that data and searching for a model that will
    achieve what the programmers have set it out to achieve.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习对于那些过于复杂而无法直接由人类编码的任务是必要的。有些任务如此复杂，以至于对人类来说不切实际，甚至不可能逐一解决所有细节并明确编写代码。因此，我们向机器学习算法提供大量数据，让算法通过探索这些数据并寻找一个能够实现程序员设定目标的模型来解决问题。
- en: 'Let’s look at these 2 examples:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看这两个例子：
- en: It is very hard to write programs that solve problems like recognizing a 3-dimensional
    object from a novel viewpoint in new lighting conditions in a cluttered scene.
    We don’t know what program to write because we don’t know how it’s done in our
    brain. Even if we had a good idea about how to do it, the program might be horrendously
    complicated.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写能够解决如在混乱场景中从新光照下的视角识别三维物体的问题的程序非常困难。我们不知道该写什么程序，因为我们不了解大脑是如何处理这些问题的。即使我们对如何做到这一点有一个很好的想法，这个程序也可能会极其复杂。
- en: It is hard to write a program to compute the probability that a credit card
    transaction is fraudulent. There may not be any rules that are both simple and
    reliable. We need to combine a very large number of weak rules. Fraud is a moving
    target but the program needs to keep changing.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写一个程序来计算信用卡交易是否欺诈是很困难的。可能没有既简单又可靠的规则。我们需要结合大量的弱规则。欺诈是一个不断变化的目标，但程序需要不断变化。
- en: 'Then comes the **Machine Learning Approach**: Instead of writing a program
    by hand for each specific task, we collect lots of examples that specify the correct
    output for a given input. A machine learning algorithm then takes these examples
    and produces a program that does the job. The program produced by the learning
    algorithm may look very different from a typical hand-written program. It may
    contain millions of numbers. If we do it right, the program works for new cases
    as well as the ones we trained it on. If the data changes the program can change
    too by training on the new data. You should note that massive amounts of computation
    are now cheaper than paying someone to write a task-specific program.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后出现了**机器学习方法**：我们不再手动为每个特定任务编写程序，而是收集大量示例，指定给定输入的正确输出。机器学习算法然后利用这些示例生成一个完成工作的程序。学习算法生成的程序可能与典型的手写程序大相径庭。它可能包含数百万个数字。如果我们做对了，程序不仅能处理我们训练时的案例，也能处理新案例。如果数据发生变化，程序也可以通过对新数据进行训练来改变。你应该注意到，大量的计算现在比支付某人编写特定任务的程序要便宜。
- en: 'Given that, some examples of tasks best solved by machine learning include:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，一些由机器学习最好解决的任务包括：
- en: 'Recognizing patterns: Objects in real scenes, Facial identities or facial expressions,
    Spoken words'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别模式：真实场景中的物体，面部身份或面部表情，口语词汇
- en: 'Recognizing anomalies: Unusual sequences of credit card transactions, Unusual
    patterns of sensor readings in a nuclear power plant'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别异常：不寻常的信用卡交易序列，核电站传感器读数的不寻常模式
- en: 'Prediction: Future stock prices or currency exchange rates, Which movies will
    a person like'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测：未来的股票价格或货币汇率，个人喜欢哪些电影
- en: '**What are Neural Networks?**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是神经网络？**'
- en: Neural networks are a class of models within the general machine learning literature.
    So for example, if you took a Coursera course on machine learning, neural networks
    will likely be covered. Neural networks are a specific set of algorithms that
    has revolutionized the field of machine learning. They are inspired by biological
    neural networks and the current so called deep neural networks have proven to
    work quite very well. Neural Networks are themselves general function approximations,
    that is why they can be applied to literally almost any machine learning problem
    where the problem is about learning a complex mapping from the input to the output
    space.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一般机器学习文献中的一类模型。例如，如果你参加了Coursera上的机器学习课程，神经网络可能会被覆盖。神经网络是一组特定的算法，已经彻底改变了机器学习领域。它们受到生物神经网络的启发，当前所谓的深度神经网络已经证明了其效果非常好。神经网络本身是通用函数逼近器，这就是为什么它们可以应用于几乎任何涉及从输入到输出空间的复杂映射的机器学习问题。
- en: 'Here are the 3 reasons to convince you to study neural computation:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有3个理由说服你学习神经计算：
- en: 'To understand how the brain actually works: It’s very big and very complicated
    and made of stuff that dies when you poke it around. So we need to use computer
    simulations.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解大脑如何实际工作：它非常大且复杂，由在被触碰时会死亡的物质组成。因此，我们需要使用计算机模拟。
- en: 'To understand a style of parallel computation inspired by neurons and their
    adaptive connections: It’s a very different style from a sequential computation.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解一种受神经元及其自适应连接启发的并行计算风格：这与顺序计算风格非常不同。
- en: 'To solve practical problems by using novel learning algorithms inspired by
    the brain: Learning algorithms can be very useful even if they are not how the
    brain actually works.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用受大脑启发的新型学习算法解决实际问题：即使学习算法的工作方式与大脑实际工作方式不同，它们也可以非常有用。
- en: After finishing the famous [Andrew Ng’s Machine Learning Coursera course](https://github.com/khanhnamle1994/machine-learning),
    I started developing interest towards neural networks and deep learning. Thus,
    I started looking at the best online resources to learn about the topics and found [Geoffrey
    Hinton’s Neural Networks for Machine Learning course](https://github.com/khanhnamle1994/neural-nets).
    If you are a deep learning practitioner or someone who want to get into the deep
    learning/machine learning world, you should really take this course. Geoffrey
    Hinton is without a doubt a godfather of the deep learning world. And he actually
    provided something extraordinary in this course. In this blog post, I want to
    share the **8 neural network architectures** from the course that I believe any
    machine learning researchers should be familiar with to advance their work.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成著名的 [Andrew Ng的机器学习Coursera课程](https://github.com/khanhnamle1994/machine-learning)
    后，我开始对神经网络和深度学习产生兴趣。因此，我开始寻找最好的在线资源来学习这些主题，并发现了 [Geoffrey Hinton的机器学习神经网络课程](https://github.com/khanhnamle1994/neural-nets)。如果你是深度学习从业者或希望进入深度学习/机器学习领域的人，你真的应该参加这个课程。Geoffrey
    Hinton 无疑是深度学习领域的教父。他在这个课程中确实提供了非凡的内容。在这篇博客文章中，我想分享我认为任何机器学习研究人员应该熟悉的课程中的**8种神经网络架构**，以推进他们的工作。
- en: '![](../Images/39c93b981f680576756eed859830f9e8.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39c93b981f680576756eed859830f9e8.png)'
- en: 'Generally, these architectures can be put into 3 specific categories:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些架构可以分为3个具体类别：
- en: '**1 — Feed-Forward Neural Networks**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**1 — 前馈神经网络**'
- en: These are the commonest type of neural network in practical applications. The
    first layer is the input and the last layer is the output. If there is more than
    one hidden layer, we call them “deep” neural networks. They compute a series of
    transformations that change the similarities between cases. The activities of
    the neurons in each layer are a non-linear function of the activities in the layer
    below.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是实际应用中最常见的神经网络类型。第一层是输入层，最后一层是输出层。如果有多个隐藏层，我们称之为“深度”神经网络。它们计算一系列变换，改变样本之间的相似性。每一层中神经元的活动是下层活动的非线性函数。
- en: '**2 — Recurrent Networks**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**2 — 递归网络**'
- en: These have directed cycles in their connection graph. That means you can sometimes
    get back to where you started by following the arrows. They can have complicated
    dynamics and this can make them very difficult to train. They are more biologically
    realistic.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络在其连接图中存在有向环路。这意味着有时你可以通过沿箭头的方向回到起点。它们可能具有复杂的动态行为，这使得它们的训练非常困难。它们在生物学上更为真实。
- en: There is a lot of interest at present in finding efficient ways of training
    recurrent nets. Recurrent neural networks are a very natural way to model sequential
    data. They are equivalent to very deep nets with one hidden layer per time slice;
    except that they use the same weights at every time slice and they get input at
    every time slice. They have the ability to remember information in their hidden
    state for a long time but is very hard to train them to use this potential.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当前对寻找高效训练递归网络的方法有很大兴趣。递归神经网络是一种非常自然的建模序列数据的方式。它们等同于每个时间片一个隐藏层的非常深的网络；只不过它们在每个时间片使用相同的权重，并在每个时间片接收输入。它们有能力在其隐藏状态中长时间记住信息，但很难训练它们充分利用这一潜力。
- en: '**3 — Symmetrically Connected Networks**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**3 — 对称连接网络**'
- en: These are like recurrent networks, but the connections between units are symmetrical
    (they have the same weight in both directions). Symmetric networks are much easier
    to analyze than recurrent networks. They are also more restricted in what they
    can do because they obey an energy function. Symmetrically connected nets without
    hidden units are called “Hopfield Nets.” Symmetrically connected network with
    hidden units are called “Boltzmann machines.”
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络类似于递归网络，但单元之间的连接是对称的（它们在两个方向上具有相同的权重）。对称网络比递归网络更容易分析。由于它们遵循能量函数，因此它们在功能上也更加受限。没有隐藏单元的对称连接网络称为“霍普菲尔德网络”。具有隐藏单元的对称连接网络称为“玻尔兹曼机”。
- en: 1 — Perceptrons
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1 — 感知器
- en: Considered the first generation of neural networks, **perceptrons** are simply
    computational models of a single neuron. They were popularized by [Frank Rosenblatt](https://blogs.umass.edu/comphon/2017/06/15/did-frank-rosenblatt-invent-deep-learning-in-1962/) in
    the early 1960s. They appeared to have a very powerful learning algorithm and
    lots of grand claims were made for what they could learn to do. In 1969, Minsky
    and Papers published a book called [“Perceptrons”](https://mitpress.mit.edu/books/perceptrons) that
    analyzed what they could do and showed their limitations. Many people thought
    these limitations applied to all neural network models. However, the perceptron
    learning procedure is still widely used today for tasks with enormous feature
    vectors that contain many millions of features.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 被认为是神经网络的第一代，**感知器**只是单个神经元的计算模型。它们在20世纪60年代初由[弗兰克·罗森布拉特](https://blogs.umass.edu/comphon/2017/06/15/did-frank-rosenblatt-invent-deep-learning-in-1962/)推广。它们似乎具有非常强大的学习算法，并且对其能够学习做的事情提出了许多宏大的主张。1969年，明斯基和佩普斯出版了一本名为[《感知器》](https://mitpress.mit.edu/books/perceptrons)的书，分析了它们的功能并展示了它们的局限性。许多人认为这些局限性适用于所有神经网络模型。然而，感知器学习程序今天仍然广泛用于处理包含数百万特征的庞大特征向量的任务。
- en: '![](../Images/4d94d59e6e4b8eae6bd7be64cc25f945.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d94d59e6e4b8eae6bd7be64cc25f945.png)'
- en: In the standard paradigm for statistical pattern recognition, we first convert
    the raw input vector into a vector of feature activations. We then use hand-written
    programs based on common-sense to define the features. Next, we learn how to weight
    each of the feature activations to get a single scalar quantity. If this quantity
    is above some threshold, we decide that the input vector is a positive example
    of the target class.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计模式识别的标准范式中，我们首先将原始输入向量转换为特征激活向量。然后，我们使用基于常识的手写程序来定义特征。接下来，我们学习如何对每个特征激活进行加权，以得到一个标量量。如果这个量超过某个阈值，我们就认为输入向量是目标类别的正例。
- en: 'The standard Perceptron architecture follows the feed-forward model, meaning
    inputs are sent into the neuron, are processed, and result in an output. In the
    diagram below, this means the network reads bottom-up: input comes in from the
    bottom and output goes out from the top.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 标准感知机架构遵循前馈模型，这意味着输入被送入神经元，经过处理后产生输出。在下面的图示中，这意味着网络是自下而上的：输入从底部进入，输出从顶部出去。
- en: '![](../Images/2b2a45c42c8d6cd62cb8bcdb0870fd89.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b2a45c42c8d6cd62cb8bcdb0870fd89.png)'
- en: 'However, Perceptrons do have limitations: If you are followed to choose the
    features by hand and if you use enough features, you can do almost anything. For
    binary input vectors, we can have a separate feature unit for each of the exponentially
    many binary vectors and so we can make any possible discrimination on binary input
    vectors. But once the hand-coded features have been determined, there are very
    strong limitations on what a perceptron can learn.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，感知机确实存在限制：如果允许手动选择特征，并且使用足够多的特征，你几乎可以做任何事情。对于二进制输入向量，我们可以为每个指数级多的二进制向量设置一个单独的特征单元，因此我们可以对二进制输入向量进行任何可能的区分。但是一旦确定了手工编码的特征，感知机所能学习的东西就有很大的限制。
- en: This result is devastating for Perceptrons because the whole point of pattern
    recognition is to recognize patterns despite transformations like translation.
    Minsky and Papert’s “Group Invariance Theorem” says that the part of a Perceptron
    that learns cannot learn to do this if the transformations form a group. To deal
    with such transformations, a Perceptron needs to use multiple feature units to
    recognize transformations of informative sub-patterns. So the tricky part of pattern
    recognition must be solved by the hand-coded feature detectors, not the learning
    procedure.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这一结果对感知机来说是毁灭性的，因为模式识别的核心在于尽管存在平移等变换，仍能识别模式。明斯基和帕珀特的“群不变性定理”指出，如果变换形成一个群，那么感知机中学习的部分就无法学会这样做。为了处理这些变换，感知机需要使用多个特征单元来识别信息子模式的变换。因此，模式识别中的棘手部分必须由手工编码的特征检测器解决，而不是学习过程。
- en: Networks without hidden units are very limited in the input-output mappings
    they can learn to model. More layers of linear units do not help. It’s still linear.
    Fixed output non-linearities are not enough. Thus, we need multiple layers of
    adaptive, non-linear hidden units. But how we train such nets? We need an efficient
    way of adapting all the weights, not just the last layer. This is hard. Learning
    the weights going into hidden units is equivalent to learning features. This is
    difficult because nobody is telling us directly what the hidden units should do.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 没有隐藏单元的网络在输入-输出映射方面的学习能力非常有限。更多的线性单元层没有帮助。它仍然是线性的。固定输出非线性是不够的。因此，我们需要多个层次的自适应非线性隐藏单元。但我们如何训练这样的网络？我们需要一种有效的方式来调整所有权重，而不仅仅是最后一层。这很困难。学习输入到隐藏单元的权重等同于学习特征。这很困难，因为没有人直接告诉我们隐藏单元应该做什么。
- en: 2 — Convolutional Neural Networks
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2 — 卷积神经网络
- en: 'Machine Learning research has focused extensively on object detection problems
    over the time. There are various things that make it hard to recognize objects:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，机器学习研究已经广泛关注于对象检测问题。识别对象的难点有很多：
- en: 'Segmentation: Real scenes are cluttered with other objects. It’s hard to tell
    which pieces go together as parts of the same object. Parts of an object can be
    hidden behind other objects.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割：真实场景中杂乱无章，难以判断哪些部分属于同一对象的组成部分。物体的部分可能被其他物体遮挡。
- en: 'Lighting: The intensities of the pixels are determined as much by the lighting
    as by the objects.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 照明：像素的强度既由照明条件决定，也由物体决定。
- en: 'Deformation: Objects can deform in a variety of non-affine ways. E.g., a handwritten
    too can have a large loop or just a cusp.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变形：物体可以以各种非仿射方式变形。例如，手写字母“o”可以有一个大圈或只是一个尖点。
- en: 'Affordances: Object classes are often defined by how they are used. E.g., chairs
    are things designed for sitting on so they have a wide variety of physical shapes.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用性：对象类别通常通过它们的使用方式来定义。例如，椅子是为了坐的而设计的，因此它们有各种各样的物理形状。
- en: 'Viewpoint: Changes in viewpoint cause changes in images that standard learning
    methods cannot cope with. Information hops between input dimensions (i.e. pixels)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视角：视角变化导致图像变化，而标准学习方法无法应对。这些信息在输入维度（即像素）之间跳跃。
- en: Imagine a medical database in which the age of a patient sometimes hopes to
    the input dimension that normally codes for weight! To apply machine learning
    we would first want to eliminate this dimension-hopping.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想象一个医疗数据库，其中患者的年龄有时会跳到通常用于编码体重的输入维度！为了应用机器学习，我们首先要消除这种维度跳跃。
- en: '![](../Images/5b43460563dfae211f0ceeee551229b6.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b43460563dfae211f0ceeee551229b6.png)'
- en: The replicated feature approach is currently the dominant approach for neural
    networks to solve object detection problem. It uses many different copies of the
    same feature detector with different positions. It could also replicate across
    scale and orientation, which is tricky and expensive. Replication greatly reduces
    the number of free parameters to be learned. It uses several different feature
    types, each with its own map of replicated detectors. It also allows each patch
    of image to be represented in several ways.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，复制特征的方法是神经网络解决目标检测问题的主要方法。它使用许多位置不同的相同特征检测器的副本。它还可以在尺度和方向上进行复制，这既棘手又昂贵。复制大大减少了需要学习的自由参数的数量。它使用几种不同的特征类型，每种特征都有自己复制的检测器图。它还允许每个图像的补丁以多种方式表示。
- en: So what does replicating the feature detectors achieve?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，复制特征检测器有什么成效呢？
- en: 'Equivalent activities: Replicated features do not make the neural activities
    invariant to translation. The activities of are equivariant.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等效活动：复制的特征并不会使神经活动对平移不变。这些活动是等变的。
- en: 'Invariant knowledge: If a feature is useful in some locations during training,
    detectors for that feature will be available in all locations during testing.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不变知识：如果某个特征在训练中的某些位置有用，那么在测试时该特征的检测器将在所有位置可用。
- en: 'In 1998, Yann LeCun and his collaborators developed a really good recognizer
    for handwritten digits called [LeNet](http://yann.lecun.com/exdb/lenet/). It used
    back propagation in a feedforward net with many hidden layers, many maps of replicated
    units in each layer, pooling of the outputs of nearby replicated units, a wide
    net that can cope with several characters at once even if they overlap, and a
    clever way of training a complete system, not just a recognizer. Later it is formalized
    under the name **convolutional neural networks**. Fun fact: This net was used
    for reading ~10% of the checks in North America.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 1998年，Yann LeCun及其合作者开发了一种非常好的手写数字识别器，叫做[LeNet](http://yann.lecun.com/exdb/lenet/)。它在具有许多隐藏层的前馈网络中使用了反向传播，许多层中都有复制单元的图，每层复制单元的输出进行了池化，一个可以处理多个字符（即使它们重叠）的宽网络，以及一种训练完整系统的巧妙方法，而不仅仅是识别器。后来，它被正式命名为**卷积神经网络**。有趣的是：这个网络用于读取北美约10%的支票。
- en: '![](../Images/5d555581239934e2d31a32647a7003a6.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d555581239934e2d31a32647a7003a6.png)'
- en: Convolutional Neural Networks can be used for all work related to object recognition
    from hand-written digits to 3D objects. However, recognizing real objects in color
    photographs downloaded from the web is much more complicated than recognizing
    hand-written digits. There are hundred times as many classes (1000 vs 10), hundred
    times as many pixels (256 x 256 color vs 28 x 28 gray), two-dimensional images
    of three-dimensional scenes, cluttered scenes requiring segmentation, and multiple
    objects in each image. Will the same type of convolutional neural network work?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络可以用于与目标识别相关的所有工作，从手写数字到三维物体。然而，在从网络下载的彩色照片中识别真实物体比识别手写数字复杂得多。类别数量是前者的百倍（1000对10），像素数量也是前者的百倍（256
    x 256 彩色对 28 x 28 灰度），还有二维图像的三维场景、需要分割的杂乱场景，以及每张图像中的多个物体。相同类型的卷积神经网络会有效吗？
- en: Then came the [ILSVRC-2012 competition](http://www.image-net.org/challenges/LSVRC/2012/) on **ImageNet**,
    a dataset with approximately 1.2 million high-resolution training images. Test
    images will be presented with no initial annotation (no segmentation or labels)
    and algorithms will have to produce labelings specifying what objects are present
    in the images. Some of the best existing computer vision methods were tried on
    this dataset by leading computer vision groups from Oxford, INRIA, XRCE… Typically,
    computer vision systems use complicated multi-stage systems and the early stages
    are typically hand-tuned by optimizing a few parameters.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 随后是[ILSVRC-2012比赛](http://www.image-net.org/challenges/LSVRC/2012/)，该比赛基于**ImageNet**数据集，包含大约120万张高分辨率的训练图像。测试图像在没有初始注释（没有分割或标签）的情况下呈现，算法必须生成标签，指定图像中存在的对象。一些现有的最佳计算机视觉方法已被来自牛津、INRIA、XRCE等领先计算机视觉团队在此数据集上进行尝试。通常，计算机视觉系统使用复杂的多阶段系统，早期阶段通常通过优化少量参数进行手动调整。
- en: '![](../Images/10ef5959d0e91a8e208616591820caa9.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10ef5959d0e91a8e208616591820caa9.png)'
- en: The winner of the competition, [Alex Krizhevsky (NIPS 2012)](http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf),
    developed a very deep convolutional neural net of the type pioneered by Yann LeCun.
    Its architecture includes 7 hidden layers not counting some max-pooling layers.
    The early layers were convolutional, while the last 2 layers were globally connected.
    The activation functions were rectified linear units in every hidden layer. These
    train much faster and are more expressive than logistic units. In addition to
    that, it also uses competitive normalization to suppress hidden activities when
    nearby units have stronger activities. This helps with variations in intensity.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 比赛的获胜者，[Alex Krizhevsky (NIPS 2012)](http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf)，开发了一种非常深的卷积神经网络，这种网络类型是由Yann
    LeCun开创的。其架构包括7个隐藏层，不包括一些最大池化层。早期层是卷积层，而最后两个层是全连接层。每个隐藏层的激活函数是修正线性单元。这些单元的训练速度更快，比逻辑单元更具表达能力。此外，它还使用了竞争性归一化，以在附近单元活动更强时抑制隐藏活动。这有助于处理强度变化。
- en: 'There are a couple of technical tricks that significantly improve generalization
    for the neural net:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个技术技巧显著提高了神经网络的泛化能力：
- en: 'Training on random 224 x 224 patches from the 256 x 256 images to get more
    data and using left-right reflections of the images. At test time, combining the
    opinions from 10 different patches: The four 224 x 224 corner patches plus the
    central 224 x 224 patch plus the reflections of those 5 patches.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在256 x 256的图像中，随机提取224 x 224的图像块以获取更多数据，并使用图像的左右反射。在测试时，结合来自10个不同图像块的意见：四个224
    x 224的角落图像块加上中央的224 x 224图像块，以及这5个图像块的反射。
- en: Using “dropout” to regularize the weights in the globally connected layers (which
    contain most of the parameters). Dropout means that half of the hidden units in
    a layer are randomly removed for each training example. This stops hidden units
    from relying too much on other hidden units.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用“dropout”对全连接层的权重进行正则化（这些层包含大部分参数）。Dropout意味着在每个训练样本中，随机去除一半的隐藏单元。这防止了隐藏单元过度依赖其他隐藏单元。
- en: '![](../Images/adcd19bf34e1806f4d14c72e8c826e70.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adcd19bf34e1806f4d14c72e8c826e70.png)'
- en: In terms of hardware requirement, Alex uses a very efficient implementation
    of convolutional nets on 2 Nvidia GTX 580 GPUs (over 1000 fast little cores).
    The GPUs are very good for matrix-matrix multiplies and also have very high bandwidth
    to memory. This allows him to train the network in a week and makes it quick to
    combine results from 10 patches at test time. We can spread a network over many
    cores if we can communicate the states fast enough. As cores get cheaper and datasets
    get bigger, big neural nets will improve faster than old-fashioned computer vision
    systems.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬件要求方面，Alex使用了非常高效的卷积网络实现，运行在2块Nvidia GTX 580 GPU上（超过1000个快速的小核心）。这些GPU非常适合矩阵乘法，并且具有非常高的内存带宽。这使得他能够在一周内训练网络，并且在测试时快速结合来自10个图像块的结果。如果我们能快速地通信状态，我们可以将网络分布到许多核心上。随着核心变得便宜，数据集变得更大，大型神经网络将比传统计算机视觉系统改进得更快。
- en: 3 — Recurrent Neural Network
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3 — 递归神经网络
- en: '![](../Images/165765dffd0351593bc95483a6eccae1.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/165765dffd0351593bc95483a6eccae1.png)'
- en: To understand RNNs, we need to have a brief overview of sequence modeling. When
    applying machine learning to sequences, we often want to turn an input sequence
    into an output sequence that lives in a different domain; for example, turn a
    sequence of sound pressures into a sequence of word identities. When there is
    no separate target sequence, we can get a teaching signal by trying to predict
    the next term in the input sequence. The target output sequence is the input sequence
    with an advance of 1 step. This seems much more natural than trying to predict
    one pixel in an image from the other pixels, or one patch of an image from the
    rest of the image. Predicting the next term in a sequence blurs the distinction
    between supervised and unsupervised learning. It uses methods designed for supervised
    learning, but it doesn’t require a separate teaching signal.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 RNNs，我们需要对序列建模有一个简要的概述。在将机器学习应用于序列时，我们通常希望将输入序列转换为位于不同领域的输出序列；例如，将一系列声音压力转换为一系列单词身份。当没有单独的目标序列时，我们可以通过尝试预测输入序列中的下一个项来获得教学信号。目标输出序列是输入序列的前进
    1 步。这似乎比尝试从其他像素预测图像中的一个像素，或从图像的其余部分预测图像中的一个补丁自然得多。预测序列中的下一个项模糊了监督学习和无监督学习之间的区别。它使用了为监督学习设计的方法，但不需要单独的教学信号。
- en: '**Memoryless models** are the standard approach to this task. In particular,
    autoregressive models can predict the next term in a sequence from a fixed number
    of previous terms using “delay taps; and feed-forward neural nets are generalized
    autoregressive models that use one or more layers of non-linear hidden units.
    However, if we give our generative model some hidden state, and if we give this
    hidden state its own internal dynamics, we get a much more interesting kind of
    model: It can store information in its hidden state for a long time. If the dynamics
    are noisy and the way they generate outputs from their hidden state is noisy,
    we can never know its exact hidden state. The best we can do is to infer a probability
    distribution over the space of hidden state vectors. This inference is only tractable
    for 2 types of hidden state model.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**无记忆模型** 是处理这一任务的标准方法。特别是，自回归模型可以从固定数量的前一个项中预测序列中的下一个项，使用“延迟抽头”；前馈神经网络是广义的自回归模型，使用一个或多个层的非线性隐藏单元。然而，如果我们给生成模型一些隐藏状态，并且如果我们赋予这个隐藏状态自己的内部动态，我们就会得到一种更有趣的模型：它可以在其隐藏状态中存储信息很长时间。如果这些动态是嘈杂的，并且它们从隐藏状态生成输出的方式也是嘈杂的，我们永远无法知道其确切的隐藏状态。我们能做的最好的是推断隐藏状态向量空间上的概率分布。这种推断仅对两种类型的隐藏状态模型是可行的。'
- en: '**Recurrent Neural Networks **are very powerful, because they combine 2 properties:
    1) distributed hidden state that allows them to store a lot of information about
    the past efficiently, and 2) non-linear dynamics that allow them to update their
    hidden state in complicated ways. With enough neurons and time, RNNs can compute
    anything that can be computed by your computer. So what kinds of behavior can
    RNNs exhibit? They can oscillate, they can settle to point attractors, they can
    behave chaotically. And they could potentially learn to implement lots of small
    programs that each capture a nugget of knowledge and run in parallel, interacting
    to produce very complicated effects.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**递归神经网络** 非常强大，因为它们结合了两个特性：1) 分布式隐藏状态，使它们能够高效地存储大量关于过去的信息，以及 2) 非线性动态，使它们能够以复杂的方式更新隐藏状态。只要有足够的神经元和时间，RNNs
    可以计算出任何计算机能计算的东西。那么 RNNs 可以表现出什么样的行为呢？它们可以发生振荡，可以收敛到点吸引子，可以表现得混乱不堪。它们还有可能学习实现许多小程序，每个程序捕捉一个知识点并并行运行，相互作用产生非常复杂的效果。'
- en: '![](../Images/e6043e3e88d5c61a2b88f6a281218d60.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6043e3e88d5c61a2b88f6a281218d60.png)'
- en: However, the computational power of RNNs makes them very hard to train. It is
    quite difficult to train a RNN because of the exploding or vanishing gradients
    problem. As we backpropagate through many layers, what happens to the magnitude
    of the gradients? If the weights are small, the gradients shrink exponentially.
    If the weights are big, the gradients grow exponentially. Typical feed-forward
    neural nets can cope with these exponential effects because they only have a few
    hidden layers. On the other hand, in a RNN trained on long sequences, the gradients
    can easily explode or vanish. Even with good initial weights, it’s very hard to
    detect that the current target output depends on an input from many time-steps
    ago, so RNNs have difficulty dealing with long-range dependencies.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，RNN的计算能力使得它们非常难以训练。由于梯度爆炸或消失问题，训练RNN相当困难。当我们通过多层进行反向传播时，梯度的幅度会发生什么？如果权重很小，梯度会指数级缩小。如果权重很大，梯度会指数级增长。典型的前馈神经网络可以应对这些指数效应，因为它们只有少量隐藏层。另一方面，在训练长序列的RNN中，梯度很容易爆炸或消失。即使初始权重良好，也很难检测到当前目标输出依赖于许多时间步之前的输入，因此RNN在处理长距离依赖时会遇到困难。
- en: 'There are essentially 4 effective ways to learn a RNN:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 学习RNN的有效方法有4种：
- en: '**Long Short Term Memory**: Make the RNN out of little modules that are designed
    to remember values for a long time.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长短期记忆**：将RNN构建为旨在长期记忆的模块。'
- en: '**Hessian Free Optimization**: Deal with the vanishing gradients problem by
    using a fancy optimizer that can detect directions with a tiny gradient but even
    smaller curvature.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hessian Free Optimization**：通过使用一种可以检测到微小梯度但曲率更小的复杂优化器来处理梯度消失问题。'
- en: '**Echo State Networks**: Initialize the input -> hidden and hidden -> hidden
    and output -> hidden connections very carefully so that the hidden state has a
    huge reservoir of weakly coupled oscillators which can be selectively driven by
    the input.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回声状态网络**：非常仔细地初始化输入->隐藏层和隐藏层->隐藏层以及输出->隐藏层的连接，使得隐藏状态拥有一个巨大的、弱耦合的振荡器储备，这些振荡器可以通过输入选择性地驱动。'
- en: '**Good initialization with momentum**: Initialize like in Echo State Networks,
    but then learn all of the connections using momentum.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**良好的动量初始化**：像在回声状态网络中那样初始化，然后使用动量学习所有的连接。'
- en: 4 — Long/Short Term Memory Network
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4 — 长短期记忆网络
- en: '![](../Images/d6dbe07ea1caf153d300783588d959d4.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6dbe07ea1caf153d300783588d959d4.png)'
- en: '[Hochreiter & Schmidhuber (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf) solved
    the problem of getting a RNN to remember things for a long time (like hundreds
    of time steps) by building what known as **long-short term memory network. **They
    designed a memory cell using logistic and linear units with multiplicative interactions.
    Information gets into the cell whenever its “write” gate is on. The information
    stays in the cell so long as its “keep” gate is on. Information can be read from
    the cell by turning on its “read” gate.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[Hochreiter & Schmidhuber (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf)
    通过建立被称为**长短期记忆网络**的模型解决了使RNN长期记忆的难题（如数百个时间步骤）。他们设计了一个使用逻辑单元和线性单元以及乘法交互的记忆单元。当“写入”门开启时，信息会进入单元。信息会在“保持”门开启时留在单元中。通过打开“读取”门，可以从单元中读取信息。'
- en: Reading cursive handwriting is a natural task for an RNN. The input is a sequence
    of (x, y, p) coordinates of the tip of the pen, where p indicates whether the
    pen is up or down. The output is a sequence of characters. [Graves & Schmidhuber
    (2009)](http://people.idsia.ch/~juergen/nips2009.pdf) showed that RNNs with LSTM
    are currently the best systems for reading cursive writing. In brief, they used
    a sequence of small images as input rather than pen coordinates.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读草书是一项RNN的自然任务。输入是笔尖的（x，y，p）坐标序列，其中p表示笔是上还是下。输出是字符序列。[Graves & Schmidhuber
    (2009)](http://people.idsia.ch/~juergen/nips2009.pdf) 显示带有LSTM的RNN目前是阅读草书的最佳系统。简言之，他们使用了一系列小图像作为输入，而不是笔坐标。
- en: '![](../Images/0de05190c7dbf55ac81c791d617a527e.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0de05190c7dbf55ac81c791d617a527e.png)'
- en: More On This Topic
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成为优秀数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个初学者数据科学家应该掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并找到目标以…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学学习统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
