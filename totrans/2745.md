# LightGBM：高效的梯度提升决策树

> 原文：[https://www.kdnuggets.com/2020/06/lightgbm-gradient-boosting-decision-tree.html](https://www.kdnuggets.com/2020/06/lightgbm-gradient-boosting-decision-tree.html)

[评论](#comments)

LightGBM算法的强大不容小觑（双关语）。LightGBM是一个分布式且高效的[梯度提升框架](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)，它使用[基于树的学习](https://www.analyticsvidhya.com/blog/2016/04/tree-based-algorithms-complete-tutorial-scratch-in-python/)。它是[基于直方图的](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html)，并将连续值分入离散的箱中，这使得训练更快、内存使用更高效。在这篇文章中，我们将深入探讨LightGBM。

### LightGBM的优点

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析能力

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织IT

* * *

根据官方文档，LightGBM框架的优点如下：

+   更快的训练速度和更高的效率

+   更低的内存使用

+   更好的准确性

+   支持并行和GPU学习

+   能够处理大规模数据

### 参数调整

该框架使用叶子优先树生长算法，这不同于许多其他基于树的算法，它们使用深度优先生长。叶子优先树生长算法通常比深度优先算法收敛得更快。然而，它们更容易过拟合。

![图示](../Images/56ad54fca4c0ecc23b36a5ca30e21996.png)

[来源](https://lightgbm.readthedocs.io/en/latest/Features.html?highlight=dart#other-features)![图示](../Images/008a899a4e51dafd9da8c073fcc560db.png)

[来源](https://lightgbm.readthedocs.io/en/latest/Features.html?highlight=dart#other-features)

这里是我们需要调整的参数，以获得良好的叶子优先树算法结果：

+   `num_leaves`：将叶子数量设置为`num_leaves = 2^(max_depth)`会得到与深度优先树相同的叶子数量。但这不是一个好的实践。理想情况下，叶子数量应小于`2^(max_depth)`

+   `min_data_in_leaf`防止过拟合。它的设置依赖于`num_leaves`和训练样本的数量。对于大数据集，可以设置为数百或数千。

+   `max_depth`用于限制树的深度。

使用以下方法可以获得更快的算法速度：

+   一个小的`max_bin`

+   `save_binary` 用于加速未来学习中的数据加载

+   并行学习

+   bagging，通过设置 `bagging_freq` 和 `bagging_fraction`

+   `feature_fraction` 用于特征子采样

为了获得更好的准确性，可以使用较大的 `max_bin`、较小的学习率和较大的 `num_iterations`，以及更多的训练数据。还可以使用更多的 `num_leaves`，但这可能导致过拟合。说到过拟合，你可以通过以下方法处理：

+   增加 `path_smooth`

+   使用更大的训练集

+   尝试 `lambda_l1`、`lambda_l2` 和 `min_gain_to_split` 进行正则化

+   避免生成过深的树

> 机器学习正迅速向数据收集的地方——边缘设备靠近。[订阅 Fritz AI 通讯，了解更多有关这一过渡的内容，以及它如何帮助扩展你的业务。](https://www.fritz.ai/newsletter?utm_campaign=fritzai-newsletter-scale6&utm_source=heartbeat)

### 分类特征

处理机器学习中的分类特征的一种常见方法是独热编码。这种方法对树模型并不理想，特别是对于高基数分类特征。基于独热编码特征构建的树模型往往不平衡，需要生长得过深才能获得良好的准确性。

使用 `categorical_feature` 属性，我们可以为模型指定分类特征（无需独热编码）。分类特征应编码为小于 `Int32.MaxValue` 的非负整数，从零开始。

### LightGBM 应用

LightGBM 最适合应用于以下问题：

+   使用 `logloss` 目标函数的二分类

+   使用 `L2` 损失进行回归

+   多分类

+   使用 `logloss` 目标函数的交叉熵

+   使用 `lambdarank` 进行 LambdaRank，并以 NDCG 作为目标函数

### 指标

LightGBM 支持的指标有：

+   L1 损失

+   L2 损失

+   对数损失

+   分类错误率

+   AUC

+   NDCG

+   MAP

+   多分类对数损失

+   多分类错误率

+   Fair

+   Huber

+   泊松

+   Quantile

+   MAPE

+   Kullback-Leibler

+   Gamma

+   Tweedie

### 处理缺失值

默认情况下，LightGBM 能处理缺失值。你可以通过设置 `use_missing=false` 禁用此功能。它使用 NA 表示缺失值，但如果要使用零，你可以设置 `zero_as_missing=true`。

### 核心参数

这里是 LightGBM 的一些核心参数：

+   `task` 默认值为 `train`。其他选项包括 `predict`、`convert_model` 和 `refit`。该参数的别名是 `task_type`。`convert_model` 将模型转换为 if-else 格式。

+   `objective` 默认值为回归。其他选项包括 `regression_l1`、`huber`、`fair`、`poisson`、`quantile`、`mape`、`gamma`、`tweedie`、`binary`、`multiclass`、`multiclassova`、`cross_entropy`、`cross_entropy_lambda`、`lambdarank` 和 `rank_xendcg`。该参数的别名包括 `objective_type`、`app` 和 `application`。

+   `boosting` 默认为 `gbdt` — 传统的梯度提升决策树。其他选项包括 `rf` — 随机森林，`dart` — [Dropouts meet Multiple Additive Regression Trees](https://arxiv.org/abs/1505.01866)，`goss` — 基于梯度的单边采样。此参数的别名为 `boosting_type` 和 `boost`。

+   `num_leaves`: 基学习器的最大树叶数 — 默认为 31。

+   `max_depth`: 基学习器的最大树深度。

+   `learning_rate`: 提升学习率。

+   `n_estimators`: 要拟合的提升树的数量 — 默认为 200000。

+   `importance_type`: 要填充到 `feature_importances_` 中的重要性类型。使用 `split` 意味着结果中将包含特征在模型中使用的次数。

+   `device_type`: 树学习的设备 — CPU 或 GPU。可以与 `device` 作为别名一起使用。

### 学习控制参数

让我们来看几个学习控制参数：

+   `force_col_wise`: 当设置为 true 时，强制按列构建直方图。当列数较多或总的 bin 数量较大时，建议将其设置为 true。你也可以在需要减少内存消耗并且 `num_threads` 较多（例如大于 20）时设置为 true。此参数仅在 CPU 上使用。

+   `force_row_wise`: 当设置为 true 时，强制按行构建直方图。此参数仅在 CPU 上使用。当数据点数量大，总的 bin 数量较小且 `num_threads` 较少（例如小于或等于 16）时，可以启用此参数。当你希望使用小的 `bagging_fraction` 或 `goss` 提升速度时，也可以设置为 true。

+   `neg_bagging_fraction`: 用于处理不平衡的二分类问题。

+   `bagging_freq`: Bagging 的频率。零表示禁用 bagging。

+   `feature_fraction`: 可用于处理过拟合。例如，将其设置为 0.5 意味着 LightGBM 会在每个树节点选择 50% 的特征。

+   `extra_trees`: 当你希望使用极端随机树时设置为 true。

+   `early_stopping_round`: 当设置为 true 时，一旦某个参数未能改进，训练将停止。

+   `max_drop`: 默认为 50。表示每次迭代中要丢弃的树的数量。

+   `cat_l2`: 类别分裂中的 L2 正则化。

+   `cat_smooth`: 减少类别特征中的噪声效应，特别是对于数据有限的类别。

+   `path_smooth`: 有助于防止在样本较少的叶子上过拟合。

### 目标参数

这里有几个需要注意的目标参数：

+   `is_unbalance`: 如果训练数据在分类问题中不平衡，可以设置为 true。

+   `num_class`: 用于指示多分类问题中的类别数量。

+   `scale_pos_weight`: 正类标签的权重。不能与 `is_unbalance` 一起使用。此参数提高了模型的整体性能指标，但可能导致个别类别概率的估计不佳。

### 实践实施

现在我们将快速实现算法。我们将使用scikit-learn的分类器封装。

一如既往，我们从导入模型开始：

[PRE0]

下一步是创建模型实例并设置目标。目标的选项包括`LGBMRegressor`的回归，`LGBMClassifier`的二分类或多分类，以及`LGBMRanker`的LambdaRank。

[PRE1]

在拟合模型时，我们可以设置分类特征：

[PRE2]

一旦你在模型上进行预测，你还可以获得重要特征：

[PRE3]

### 结论

我希望这段介绍能为你提供足够的LightGBM背景，以便你开始自己的实验。我们已经看到它可以用于回归和分类问题。有关框架的更多信息，你可以查看官方文档：

[**欢迎来到LightGBM文档！ - LightGBM 2.3.2文档**](https://lightgbm.readthedocs.io/en/latest/)

LightGBM是一个使用基于树的学习算法的梯度提升框架。它被设计为分布式...

**个人简介：[德里克·姆维蒂](https://derrickmwiti.com/)** 是一位数据分析师、作家和导师。他致力于在每项任务中提供卓越成果，并且是Lapid Leaders Africa的导师。

[原文](https://heartbeat.fritz.ai/lightgbm-a-highly-efficient-gradient-boosting-decision-tree-53f62276de50)。经许可转载。

**相关：**

+   [研究指南：机器学习模型的高级损失函数](/2019/11/research-guide-advanced-loss-functions-machine-learning-models.html)

+   [Python中的自动化机器学习](/2019/01/automated-machine-learning-python.html)

+   [联邦学习：简介](/2020/04/federated-learning-introduction.html)

### 更多相关内容

+   [数据科学家应使用LightGBM的3个理由](https://www.kdnuggets.com/2022/01/data-scientists-reasons-lightgbm.html)

+   [使用Python和Scikit-learn简化决策树的可解释性](https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)

+   [决策树算法解释](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)

+   [通过实现理解：决策树](https://www.kdnuggets.com/2023/02/understanding-implementing-decision-tree.html)

+   [讲述精彩的数据故事：可视化决策树](https://www.kdnuggets.com/2021/02/telling-great-data-story-visualization-decision-tree.html)

+   [随机森林与决策树：关键区别](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)
