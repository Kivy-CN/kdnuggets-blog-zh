- en: 'Time Complexity: How to measure the efficiency of algorithms'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/06/time-complexity-measure-efficiency-algorithms.html](https://www.kdnuggets.com/2020/06/time-complexity-measure-efficiency-algorithms.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Diego Lopez Yse](https://www.linkedin.com/in/lopezyse/), Data Scientist**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/3837e31968fcd1d9e92f2b9e231fcd8a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Icons8 Team](https://unsplash.com/@icons8?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In computer programming, as in other aspects of life, there are different ways
    of solving a problem. These different ways may imply different times, computational
    power, or any other metric you choose, so we need to compare the efficiency of
    different approaches to pick up the right one.
  prefs: []
  type: TYPE_NORMAL
- en: Now, as you may know, computers are able to solve problems based on algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithms are procedures or instructions (set of steps) that tell a computer
    what to do and how to do it.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Nowadays, they evolved so much that they may be considerably different even
    when accomplishing the same task. In the most extreme case (which is quite usual
    by the way), different algorithms programmed in different programming languages
    may tell different computers with different hardware and operating systems to
    perform the same task, in a completely different way. That’s crazy, isn’t it?
  prefs: []
  type: TYPE_NORMAL
- en: The thing is that while one algorithm takes seconds to finish, another will
    take minutes with even small data sets. How can we compare different performances
    and pick the best algorithm to solve a particular problem?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Fortunately, there are ways of doing this, and we don’t need to wait and see
    the algorithm at work to know if it can get the job done quickly or if it’s going
    to collapse under the weight of its input. When we consider the complexity of
    an algorithm, we shouldn’t really care about the exact number of operations that
    are performed; instead, **we should care about how the number of operations relates
    to the problem size**. Think about it: if the problem size doubles, does the number
    of operations stay the same? Do they double? Do they increase in some other way?
    To answer these questions, we need to measure the time complexity of algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Time complexity represents the number of times a statement is executed**.
    The time complexity of an algorithm is NOT the actual time required to execute
    a particular code, since that depends on other factors like programming language,
    operating software, processing power, etc. The idea behind time complexity is
    that it can measure only the execution time of the algorithm in a way that depends
    only on the algorithm itself and its input.'
  prefs: []
  type: TYPE_NORMAL
- en: To express the time complexity of an algorithm, we use something called the *“Big
    O notation”*. **The Big O notation is a language we use to describe the time complexity
    of an algorithm. **It’s how we compare the efficiency of different approaches
    to a problem, and helps us to make decisions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Big O notation expresses the run time of an algorithm in terms of how quickly
    it grows relative to the input** (this input is called “n”). This way, if we say
    for example that the run time of an algorithm grows “on the order of the size
    of the input”, we would state that as “O(n)”. If we say that the run time of an
    algorithm grows “on the order of the square of the size of the input”, we would
    express it as “O(n²)”. But what does that mean exactly?'
  prefs: []
  type: TYPE_NORMAL
- en: The key to understanding time complexity is understanding the rates at which
    things can grow. The rate in question here is time taken per input size. There
    are different types of time complexities, so let’s check the most basic ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Constant Time Complexity: O(1)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When time complexity is constant (notated as “O(1)”), the size of the input
    (n) doesn’t matter. Algorithms with Constant Time Complexity take a constant amount
    of time to run, independently of the size of n. They don’t change their run-time
    in response to the input data, which makes them the fastest algorithms out there.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/0b3a4281a945ab55bb7541cdb5a1387a.png)'
  prefs: []
  type: TYPE_IMG
- en: Constant Time Complexity
  prefs: []
  type: TYPE_NORMAL
- en: For example, you’d use an algorithm with constant time complexity if you wanted
    to know if a number is odd or even. No matter if the number is 1 or 9 billions
    (the input “n”), the algorithm would perform the same operation only once, and
    bring you the result.
  prefs: []
  type: TYPE_NORMAL
- en: Also, if you wanted to print out once a phrase like the classic “Hello World”,
    you’d run that too with constant time complexity, since the amount of operations
    (in this case 1) with this or any other phrase will remain the same, no matter
    which operating system or which machine configurations you are using.
  prefs: []
  type: TYPE_NORMAL
- en: 'To remain constant, these algorithms shouldn’t contain loops, recursions or
    calls to any other non-constant time function. For constant time algorithms, run-time
    doesn’t increase: the order of magnitude is always 1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear Time Complexity: O(n)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When time complexity grows in direct proportion to the size of the input, you
    are facing Linear Time Complexity, or O(n). Algorithms with this time complexity
    will process the input (n) in “n” number of operations. This means that as the
    input grows, the algorithm takes proportionally longer to complete.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/afda462dc2091a9e79cf422b5e4d47c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear Time Complexity
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the type of situations where you have to look at every item in a
    list to accomplish a task (e.g. find the maximum or minimum value). Or you can
    also think about everyday tasks like reading a book or finding a CD (remember
    them?) in a CD stack: if all data has to be examined, the larger the input size,
    the higher the number of operations are.'
  prefs: []
  type: TYPE_NORMAL
- en: Linear running time algorithms are very common, and they relate to the fact
    that the algorithm visits every element from the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Logarithmic Time Complexity: O(log n)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Algorithms with this complexity make computation amazingly fast. An algorithm
    is said to run in logarithmic time if its time execution is proportional to the
    logarithm of the input size. This means that instead of increasing the time it
    takes to perform each subsequent step, the time is decreased at a magnitude that
    is inversely proportional to the input “n”.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/3a264c2fe5abb45f77e567c54d1f387b.png)'
  prefs: []
  type: TYPE_IMG
- en: Logarithmic Time Complexity
  prefs: []
  type: TYPE_NORMAL
- en: 'What’s the secret of it? These type of algorithms never have to go through
    all of the input, since they usually work by discarding large chunks of unexamined
    input with each step. This time complexity is generally associated with algorithms
    that divide problems in half every time, which is a concept known as “Divide and
    Conquer”. **Divide and Conquer algorithms** solve problems using the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: They **divide** the given problem into sub-problems of the same type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They recursively **solve** these sub-problems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They appropriately **combine** the sub-answers to answer the given problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Consider this example: let’s say that you want to look for a word in a dictionary
    that has every word sorted alphabetically. There are at least two algorithms to
    do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithm A:**'
  prefs: []
  type: TYPE_NORMAL
- en: Starts at the beginning of the book and goes in order until it finds the contact
    you are looking for.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithm B:**'
  prefs: []
  type: TYPE_NORMAL
- en: Opens the book in the middle and checks the first word on it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the word that you are looking for is alphabetically bigger, then it looks
    in the right half. Otherwise, it looks in the left half.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which one of both is faster? While algorithm A goes word by word O(n), algorithm
    B splits the problem in half on each iteration O(log n), achieving the same result
    in a much more efficient way.
  prefs: []
  type: TYPE_NORMAL
- en: Logarithmic time algorithms (O(log n)) are the second quickest ones after constant
    time algorithms (O(1)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Quadratic Time Complexity: O(n²)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this type of algorithms, the time it takes to run grows directly proportional
    to the square of the size of the input (like linear, but squared).
  prefs: []
  type: TYPE_NORMAL
- en: In most scenarios and particularly for large data sets, algorithms with quadratic
    time complexities take a lot of time to execute and should be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/44ccedcee919411f77dc822be5fd0d12.png)'
  prefs: []
  type: TYPE_IMG
- en: Quadratic Time Complexity
  prefs: []
  type: TYPE_NORMAL
- en: Nested **For Loops** run on quadratic time, because you’re running a linear
    operation within another linear operation, or *n*n* which equals *n².*
  prefs: []
  type: TYPE_NORMAL
- en: If you face these types of algorithms, you’ll either need a lot of resources
    and time, or you’ll need to come up with a better algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exponential Time Complexity: O(2^n)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In exponential time algorithms, the growth rate doubles with each addition to
    the input (n), often iterating through all subsets of the input elements. Any
    time an input unit increases by 1, it causes you to double the number of operations
    performed. This doesn’t sound good, right?
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms with this time complexity are usually used in situations where you
    don’t know that much about the best solution, and you have to try every possible
    combination or permutation on the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/103250238115d22ec9fd8205ce3f3712.png)'
  prefs: []
  type: TYPE_IMG
- en: Exponential Time Complexity
  prefs: []
  type: TYPE_NORMAL
- en: Exponential time complexity is usually seen in **Brute-Force algorithms**. These
    algorithms blindly iterate an entire domain of possible solutions in search of
    one or more solutions which satisfy a condition. They try to find the correct
    solution by simply trying every possible solution until they happen to find the
    correct one. This is obviously a not optimal way of performing a task, since it
    will affect the time complexity. Brute-Force algorithms are used in *cryptography* as
    attacking methods to defeat password protection by trying random stings until
    they find the correct password that unlocks the system.
  prefs: []
  type: TYPE_NORMAL
- en: As in quadratic time complexity, you should avoid algorithms with exponential
    running times since they don’t scale well.
  prefs: []
  type: TYPE_NORMAL
- en: How to measure time complexity?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generally speaking, we’ve seen that the fewer operations the algorithm has,
    the faster it will be. This looks like a good principle, but how can we apply
    it to reality?
  prefs: []
  type: TYPE_NORMAL
- en: If we have an algorithm (whatever it is), how do we know its time complexity?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In some cases, this may be relatively easy. Let’s say you have an outer **For
    Loop** that iterates through all the items in the input list and then a nested
    inner **For Loop**, which again iterates through all the items in the input list.
    The total number of steps performed is n * n, where n is the number of items in
    the input array.
  prefs: []
  type: TYPE_NORMAL
- en: But how do you find the time complexity of complex functions?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To find the answer, we need to break down the algorithm code into parts and
    try to find the complexity of the individual pieces. Yes, sorry to tell you that,
    but there isn’t a button you can press that tells you the time complexity of an
    algorithm. You have to do it yourself.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/c76d685c6f1c0f5aaf2f285dae631429.png)'
  prefs: []
  type: TYPE_IMG
- en: Main Time Complexities
  prefs: []
  type: TYPE_NORMAL
- en: '**As a rule of thumb, it is best to try and keep your functions running below
    or within the range of linear time-complexity, **but obviously it won’t always
    be possible.'
  prefs: []
  type: TYPE_NORMAL
- en: There are different **Big O notations**, like *“best case”*, “*average case”,* and *“worst
    case”*, but what really matters is the **worst case scenario**; those are the
    ones that can seriously crash everything. They go right to the heart of why time
    complexity matters and point to why some algorithms simply cannot solve a problem
    without taking a few billion years to do it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Worst case analysis **gives the maximum number of basic operations that have
    to be performed during execution of the algorithm. It assumes that the input is
    in the worst possible state and maximum work has to be done to put things right.
    For example, for a sorting algorithm which aims to sort an array in ascending
    order, the worst case occurs when the input array is in descending order. In this
    case, maximum number of basic operations (comparisons and assignments) have to
    be done to set the array in ascending order. Think it this way: if you had to
    search for a name in a directory by reading every name until you found the right
    one, the worst case scenario is that the name you want is the very last entry
    in the directory.'
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, **the better the time complexity of an algorithm is, the faster the
    algorithm will carry out the work in practice.** You should take into account
    this matter when designing or managing algorithms, and consider that it can make
    a big difference as to whether an algorithm is practical or completely useless.
  prefs: []
  type: TYPE_NORMAL
- en: Interested in these topics? Follow me on [Linkedin](https://www.linkedin.com/in/lopezyse/) or [Twitter](https://twitter.com/lopezyse)
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Diego Lopez Yse](https://www.linkedin.com/in/lopezyse/)** is an experienced
    professional with a solid international background acquired in different industries
    (capital markets, biotechnology, software, consultancy, government, agriculture).
    Always a team member. Skilled in Business Management, Analytics, Finance, Risk,
    Project Management and Commercial Operations. MS in Data Science and Corporate
    Finance.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/essential-programming-time-complexity-a95bb2608cac).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Python For Everybody: The Free eBook](/2020/05/python-everybody-free-ebook.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Classify A Rare Event Using 5 Machine Learning Algorithms](/2020/01/classify-rare-event-machine-learning-algorithms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Coding habits for data scientists](/2020/05/coding-habits-data-scientists.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Memory Complexity with Transformers](https://www.kdnuggets.com/2022/12/memory-complexity-transformers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear Regression Model Selection: Balancing Simplicity and Complexity](https://www.kdnuggets.com/2023/02/linear-regression-model-selection-balancing-simplicity-complexity.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learn how to design, measure and implement trustworthy A/B tests…](https://www.kdnuggets.com/2023/01/sphere-design-measure-implement-trustworthy-ab-tests-ronny-kohavi.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How To Calculate Algorithm Efficiency](https://www.kdnuggets.com/2022/09/calculate-algorithm-efficiency.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Elevate Math Efficiency: Navigating Numpy Array Operations](https://www.kdnuggets.com/elevate-math-efficiency-navigating-numpy-array-operations)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Maximizing Efficiency in Data Analysis with ChatGPT](https://www.kdnuggets.com/maximizing-efficiency-in-data-analysis-with-chatgpt)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
