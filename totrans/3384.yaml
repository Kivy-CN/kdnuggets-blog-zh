- en: An Intuitive Explanation of Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2016/11/intuitive-explanation-convolutional-neural-networks.html/2](https://www.kdnuggets.com/2016/11/intuitive-explanation-convolutional-neural-networks.html/2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Another good way to understand the Convolution operation is by looking at the
    animation in **Figure 6** below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![giphy.gif](../Images/03afaa451f7141fd7a9b9a4ecb9d13c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The Convolution Operation. Source [9]'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A filter (with red outline) slides over the input image (convolution operation)
    to produce a feature map. The convolution of another filter (with the green outline),
    over the same image gives a different feature map as shown. It is important to
    note that the Convolution operation captures the local dependancies in the original
    image. Also notice how these two different filters generate different feature maps
    from the same original image. Remember that the image and the two filters above
    are just numeric matrices as we have discussed above.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, a CNN *learns* the values of these filters on its own during the
    training process (although we still need to specify parameters such as number
    of filters, filter size, architecture of the network etc. before the training
    process). The more number of filters we have, the more image features get extracted
    and the better our network becomes at recognizing patterns in unseen images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The size of the Feature Map (Convolved Feature) is controlled by three parameters
    [4] that we need to decide before the convolution step is performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Depth:** Depth corresponds to the number of filters we use for the convolution
    operation. In the network shown in **Figure 7**, we are performing convolution
    of the original boat image using three distinct filters, thus producing three different
    feature maps as shown. You can think of these three feature maps as stacked 2d
    matrices, so, the ‘depth’ of the feature map would be three.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Screen Shot 2016-08-10 at 3.42.35 AM](../Images/04434b802022918265ef4d1b9a70996a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Stride:** Stride isthe number of pixels by which we slide our filter matrix
    over the input matrix. When the stride is 1 then we move the filters one pixel
    at a time. When the stride is 2, then the filters jump 2 pixels at a time as we
    slide them around. Having a larger stride will produce smaller feature maps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zero-padding:** Sometimes, it is convenient to pad the input matrix with
    zeros around the border, so that we can apply the filter to bordering elements
    of our input image matrix. A nice feature of zero padding is that it allows us
    to control the size of the feature maps. Adding zero-padding is also called *wide
    convolution***,** and not using zero-padding would be a *narrow convolution*. This
    has been explained clearly in [14].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Non Linearity (ReLU)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'An additional operation called ReLU has been used after every Convolution operation
    in **Figure 3** above. ReLU stands for Rectified Linear Unit and is a non-linear
    operation. Its output is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Screen Shot 2016-08-10 at 2.23.48 AM.png](../Images/f884f902a6eaab07c2e0655b59f3c524.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: the ReLU operation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ReLU is an element wise operation (applied per pixel) and replaces all negative
    pixel values in the feature map by zero. The purpose of ReLU is to introduce non-linearity
    in our ConvNet, since most of the real-world data we would want our ConvNet to
    learn would be non-linear (Convolution is a linear operation – element wise matrix
    multiplication and addition, so we account for non-linearity by introducing a
    non-linear function like ReLU).
  prefs: []
  type: TYPE_NORMAL
- en: The ReLU operation can be understood clearly from **Figure 9** below. It shows
    the ReLU operation applied to one of the feature maps obtained in **Figure 6** above.
    The output feature map here is also referred to as the ‘Rectified’ feature map.
  prefs: []
  type: TYPE_NORMAL
- en: '![Screen Shot 2016-08-07 at 6.18.19 PM.png](../Images/b7752825316c6b5e7742808da7e2888b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: ReLU operation. Source [10]'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Other non linear functions such as **tanh** or **sigmoid** can also be used
    instead of ReLU, but ReLU has been found to perform better in most situations.
  prefs: []
  type: TYPE_NORMAL
- en: The Pooling Step
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Spatial Pooling (also called subsampling or downsampling) reduces the dimensionality
    of each feature map but retains the most important information. Spatial Pooling
    can be of different types: Max, Average, Sum etc.'
  prefs: []
  type: TYPE_NORMAL
- en: In case of Max Pooling, we define a spatial neighborhood (for example, a 2×2
    window) and take the largest element from the rectified feature map within that
    window. Instead of taking the largest element we could also take the average (Average
    Pooling) or sum of all elements in that window. In practice, Max Pooling has been
    shown to work better.
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure 10** shows an example of Max Pooling operation on a Rectified Feature
    map (obtained after convolution + ReLU operation) by using a 2×2 window.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Screen Shot 2016-08-10 at 3.38.39 AM.png](../Images/4aecf0fe58685bdfc7e82e425977ebb3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Max Pooling. Source [4]'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We slide our 2 x 2 window by 2 cells (also called ‘stride’) and take the maximum
    value in each region. As shown in **Figure 10**, this reduces the dimensionality
    of our feature map.
  prefs: []
  type: TYPE_NORMAL
- en: In the network shown in **Figure 11,** pooling operation is applied separately
    to each feature map (notice that, due to this, we get three output maps from three
    input maps).
  prefs: []
  type: TYPE_NORMAL
- en: '![Screen Shot 2016-08-07 at 6.19.37 PM.png](../Images/0229193e9d8a3fc3c5c0a01f9c57ec3e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Pooling applied to Rectified Feature Maps'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Figure 12** shows the effect of Pooling on the Rectified Feature Map we received
    after the ReLU operation in **Figure 9** above.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Screen Shot 2016-08-07 at 6.11.53 PM.png](../Images/dcb2453efd17a687f7790c3527cc1475.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Pooling. Source [10]'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The function of Pooling is to progressively reduce the spatial size of the input
    representation [4]. In particular, pooling
  prefs: []
  type: TYPE_NORMAL
- en: makes the input representations (feature dimension) smaller and more manageable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: reduces the number of parameters and computations in the network, therefore,
    controlling [overfitting](https://en.wikipedia.org/wiki/Overfitting) [4]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: makes the network invariant to small transformations, distortions and translations
    in the input image (a small distortion in input will not change the output of Pooling
    – since we take the maximum / average value in a local neighborhood).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: helps us arrive at an almost scale invariant representation of our image (the
    exact term is “equivariant”). This is very powerful since we can detect objects
    in an image no matter where they are located (read [18] and [19] for details).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Story so far
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Screen Shot 2016-08-08 at 2.26.09 AM.png](../Images/7f6e70b60b1771cafb71566ed7d80ab1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So far we have seen how Convolution, ReLU and Pooling work. It is important
    to understand that these layers are the basic building blocks of any CNN. As shown
    in **Figure 13**, we have two sets of Convolution, ReLU & Pooling layers – the
    2nd Convolution layer performs convolution on the output of the first Pooling
    Layer using six filters to produce a total of six feature maps. ReLU is then applied
    individually on all of these six feature maps. We then perform Max Pooling operation
    separately on each of the six rectified feature maps.
  prefs: []
  type: TYPE_NORMAL
- en: Together these layers extract the useful features from the images, introduce
    non-linearity in our network and reduce feature dimension while aiming to make
    the features somewhat equivariant to scale and translation [18].
  prefs: []
  type: TYPE_NORMAL
- en: The output of the 2nd Pooling Layer acts as an input to the Fully Connected
    Layer, which we will discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Fully Connected Layer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Fully Connected layer is a traditional Multi Layer Perceptron that uses
    a softmax activation function in the output layer (other classifiers like SVM
    can also be used, but will stick to softmax in this post). The term “Fully Connected”
    implies that every neuron in the previous layer is connected to every neuron on
    the next layer. I recommend [reading this post](http://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/) if
    you are unfamiliar with Multi Layer Perceptrons.
  prefs: []
  type: TYPE_NORMAL
- en: The output from the convolutional and pooling layers represent high-level features
    of the input image. The purpose of the Fully Connected layer is to use these features
    for classifying the input image into various classes based on the training dataset.
    For example, the image classification task we set out to perform has four possible
    outputs as shown in **Figure 14** below (note that Figure 14 does not show connections
    between the nodes in the fully connected layer)
  prefs: []
  type: TYPE_NORMAL
- en: '![Screen Shot 2016-08-06 at 12.34.02 AM.png](../Images/493fc738b0366d0bca616f59fc9f3766.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Fully Connected Layer -each node is connected to every other node
    in the adjacent layer'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Apart from classification, adding a fully-connected layer is also a (usually)
    cheap way of learning non-linear combinations of these features. Most of the features
    from convolutional and pooling layers may be good for the classification task,
    but combinations of those features might be even better [11].
  prefs: []
  type: TYPE_NORMAL
- en: The sum of output probabilities from the Fully Connected Layer is 1\. This is
    ensured by using the [Softmax](http://cs231n.github.io/linear-classify/#softmax)
    as the activation function in the output layer of the Fully Connected Layer. The
    Softmax function takes a vector of arbitrary real-valued scores and squashes it
    to a vector of values between zero and one that sum to one.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[An Intuitive Explanation of Collaborative Filtering](https://www.kdnuggets.com/2022/09/intuitive-explanation-collaborative-filtering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Image Classification with Convolutional Neural Networks (CNNs)](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Comprehensive Guide to Convolutional Neural Networks](https://www.kdnuggets.com/2023/06/comprehensive-guide-convolutional-neural-networks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a Convolutional Neural Network with PyTorch](https://www.kdnuggets.com/building-a-convolutional-neural-network-with-pytorch)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Support Vector Machines: An Intuitive Approach](https://www.kdnuggets.com/2022/08/support-vector-machines-intuitive-approach.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear vs Logistic Regression: A Succinct Explanation](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
