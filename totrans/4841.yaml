- en: 'How to Implement a YOLO (v3) Object Detector from Scratch in PyTorch: Part
    1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/05/implement-yolo-v3-object-detector-pytorch-part-1.html](https://www.kdnuggets.com/2018/05/implement-yolo-v3-object-detector-pytorch-part-1.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Ayoosh Kathuria](https://www.linkedin.com/in/ayoosh-kathuria-44a319132/),
    Research Intern**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Header image](../Images/eb33d52c1b672fb81ea7c6f8adab2264.png)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Object detection is a domain that has benefited immensely from the recent developments
    in deep learning. Recent years have seen people develop many algorithms for object
    detection, some of which include YOLO, SSD, Mask RCNN and RetinaNet.
  prefs: []
  type: TYPE_NORMAL
- en: For the past few months, I've been working on improving object detection at
    a research lab. One of the biggest takeaways from this experience has been realizing
    that the best way to go about learning object detection is to implement the algorithms
    by yourself, from scratch. This is exactly what we'll do in this tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: We will use PyTorch to implement an object detector based on YOLO v3, one of
    the faster object detection algorithms out there.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this tutorial is designed to run on Python 3.5, and PyTorch **0.4**.
    It can be found in it's entirety at this [Github repo](https://github.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch).
  prefs: []
  type: TYPE_NORMAL
- en: 'This tutorial is broken into 5 parts:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 1 (This one): Understanding How YOLO works'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Part 2 : Creating the layers of the network architecture](https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-2/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Part 3 : Implementing the the forward pass of the network](https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-3/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Part 4 : Objectness score thresholding and Non-maximum suppression](https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-4/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Part 5 : Designing the input and the output pipelines](https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-5/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prerequisites**'
  prefs: []
  type: TYPE_NORMAL
- en: You should understand how convolutional neural networks work. This also includes
    knowledge of Residual Blocks, skip connections, and Upsampling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is object detection, bounding box regression, IoU and non-maximum suppression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic PyTorch usage. You should be able to create simple neural networks with
    ease.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I've provided the link at the end of the post in case you fall short on any
    front.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is YOLO?**'
  prefs: []
  type: TYPE_NORMAL
- en: YOLO stands for You Only Look Once. It's an object detector that uses features
    learned by a deep convolutional neural network to detect an object. Before we
    get out hands dirty with code, we must understand how YOLO works.
  prefs: []
  type: TYPE_NORMAL
- en: '**A Fully Convolutional Neural Network**'
  prefs: []
  type: TYPE_NORMAL
- en: YOLO makes use of only convolutional layers, making it a fully convolutional
    network (FCN). It has 75 convolutional layers, with skip connections and upsampling
    layers. No form of pooling is used, and a convolutional layer with stride 2 is
    used to downsample the feature maps. This helps in preventing loss of low-level
    features often attributed to pooling.
  prefs: []
  type: TYPE_NORMAL
- en: Being a FCN, YOLO is invariant to the size of the input image. However, in practice,
    we might want to stick to a constant input size due to various problems that only
    show their heads when we are implementing the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: A big one amongst these problems is that if we want to process our images in
    batches (images in batches can be processed in parallel by the GPU, leading to
    speed boosts), we need to have all images of fixed height and width. This is needed
    to concatenate multiple images into a large batch (concatenating many PyTorch
    tensors into one)
  prefs: []
  type: TYPE_NORMAL
- en: The network downsamples the image by a factor called the **stride** of the network.
    For example, if the stride of the network is 32, then an input image of size 416
    x 416 will yield an output of size 13 x 13\. Generally, ***stride* of any layer
    in the network is equal to the factor by which the output of the layer is smaller
    than the input image to the network.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Interpreting the output**'
  prefs: []
  type: TYPE_NORMAL
- en: Typically, (as is the case for all object detectors) the features learned by
    the convolutional layers are passed onto a classifier/regressor which makes the
    detection prediction (coordinates of the bounding boxes, the class label.. etc).
  prefs: []
  type: TYPE_NORMAL
- en: In YOLO, the prediction is done by using a convolutional layer which uses 1
    x 1 convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the first thing to notice is our **output is a feature map**. Since we
    have used 1 x 1 convolutions, the size of the prediction map is exactly the size
    of the feature map before it. In YOLO v3 (and it's descendants), the way you interpret
    this prediction map is that each cell can predict a fixed number of bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: Though the technically correct term to describe a unit in the feature map would
    be a *neuron*, calling it a cell makes it more intuitive in our context.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Depth-wise, we have (B x (5 + C)) entries in the feature map.** B represents
    the number of bounding boxes each cell can predict. According to the paper, each
    of these B bounding boxes may specialize in detecting a certain kind of object.
    Each of the bounding boxes have *5 + C*attributes, which describe the center coordinates,
    the dimensions, the objectness score and *C*class confidences for each bounding
    box. YOLO v3 predicts 3 bounding boxes for every cell.'
  prefs: []
  type: TYPE_NORMAL
- en: '**You expect each cell of the feature map to predict an object through one
    of it''s bounding boxes if the center of the object falls in the receptive field
    of that cell.** (Receptive field is the region of the input image visible to the
    cell. Refer to the link on convolutional neural networks for further clarification).'
  prefs: []
  type: TYPE_NORMAL
- en: This has to do with how YOLO is trained, where only one bounding box is responsible
    for detecting any given object. First, we must ascertain which of the cells this
    bounding box belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: To do that, we divide the **input** image into a grid of dimensions equal to
    that of the final feature map.
  prefs: []
  type: TYPE_NORMAL
- en: Let us consider an example below, where the input image is 416 x 416, and stride
    of the network is 32\. As pointed earlier, the dimensions of the feature map will
    be 13 x 13\. We then divide the input image into 13 x 13 cells.
  prefs: []
  type: TYPE_NORMAL
- en: '![yolo-5](../Images/8c0057d5335fa9f881d70482170f6afc.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, the cell (*on the input image*) containing the center of the ground truth
    box of an object is chosen to be the one responsible for predicting the object.
    In the image, it is the cell which marked red, which contains the center of the
    ground truth box (marked yellow).
  prefs: []
  type: TYPE_NORMAL
- en: Now, the red cell is the 7th cell in the 7th row on the grid. We now assign
    the 7th cell in the 7th row **on the feature map** (corresponding cell on the
    feature map) as the one responsible for detecting the dog.
  prefs: []
  type: TYPE_NORMAL
- en: Now, this cell can predict three bounding boxes. Which one will be assigned
    to the dog's ground truth label? In order to understand that, we must wrap out
    head around the concept of anchors.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note that the cell we''re talking about here is a cell on the prediction feature
    map. We divide the **input image** into a grid just to determine which cell of
    the **prediction feature map** is responsible for prediction*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Anchor Boxes**'
  prefs: []
  type: TYPE_NORMAL
- en: It might make sense to predict the width and the height of the bounding box,
    but in practice, that leads to unstable gradients during training. Instead, most
    of the modern object detectors predict log-space transforms, or simply offsets
    to pre-defined default bounding boxes called **anchors**.
  prefs: []
  type: TYPE_NORMAL
- en: Then, these transforms are applied to the anchor boxes to obtain the prediction.
    YOLO v3 has three anchors, which result in prediction of three bounding boxes
    per cell.
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to our earlier question, the bounding box responsible for detecting
    the dog will be the one whose anchor has the highest IoU with the ground truth
    box.
  prefs: []
  type: TYPE_NORMAL
- en: '**Making Predictions**'
  prefs: []
  type: TYPE_NORMAL
- en: The following formulae describe how the network output is transformed to obtain
    bounding box predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '![YOLO Equations](../Images/e289a333ec5b258865fdfe26bf2f7f99.png)'
  prefs: []
  type: TYPE_IMG
- en: '*bx, by, bw, bh* are the x,y center co-ordinates, width and height of our prediction. *tx,
    ty, tw, th* is what the network outputs. *cx* and *cy* are the top-left co-ordinates
    of the grid. *pw* and *ph* are anchors dimensions for the box.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Center Coordinates**'
  prefs: []
  type: TYPE_NORMAL
- en: Notice we are running our center coordinates prediction through a sigmoid function.
    This forces the value of the output to be between 0 and 1\. Why should this be
    the case? Bear with me.
  prefs: []
  type: TYPE_NORMAL
- en: 'Normally, YOLO doesn''t predict the absolute coordinates of the bounding box''s
    center. It predicts offsets which are:'
  prefs: []
  type: TYPE_NORMAL
- en: Relative to the top left corner of the grid cell which is predicting the object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalised by the dimensions of the cell from the feature map, which is, 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, consider the case of our dog image. If the prediction for center
    is (0.4, 0.7), then this means that the center lies at (6.4, 6.7) on the 13 x
    13 feature map. (Since the top-left co-ordinates of the red cell are (6,6)).
  prefs: []
  type: TYPE_NORMAL
- en: But wait, what happens if the predicted x,y co-ordinates are greater than one,
    say (1.2, 0.7). This means center lies at (7.2, 6.7). Notice the center now lies
    in cell just right to our red cell, or the 8th cell in the 7th row. **This breaks
    theory behind YOLO** because if we postulate that the red box is responsible for
    predicting the dog, the center of the dog must lie in the red cell, and not in
    the one beside it.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, to remedy this problem, the output is passed through a sigmoid function,
    which squashes the output in a range from 0 to 1, effectively keeping the center
    in the grid which is predicting.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dimensions of the Bounding Box**'
  prefs: []
  type: TYPE_NORMAL
- en: The dimensions of the bounding box are predicted by applying a log-space transform
    to the output and then multiplying with an anchor.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03bf22eecaa530eb1eeca50c8e76f07b.png)'
  prefs: []
  type: TYPE_IMG
- en: '*How the detector output is transformed to give the final prediction. Image
    Credits. [http://christopher5106.github.io/](http://christopher5106.github.io/)*'
  prefs: []
  type: TYPE_NORMAL
- en: The resultant predictions, *bw* and *bh*, are normalised by the height and width
    of the image. (Training labels are chosen this way). So, if the predictions *bx* and *by* for
    the box containing the dog are (0.3, 0.8), then the actual width and height on
    13 x 13 feature map is (13 x 0.3, 13 x 0.8).
  prefs: []
  type: TYPE_NORMAL
- en: '**Objectness Score**'
  prefs: []
  type: TYPE_NORMAL
- en: Object score represents the probability that an object is contained inside a
    bounding box. It should be nearly 1 for the red and the neighboring grids, whereas
    almost 0 for, say, the grid at the corners.
  prefs: []
  type: TYPE_NORMAL
- en: The objectness score is also passed through a sigmoid, as it is to be interpreted
    as a probability.
  prefs: []
  type: TYPE_NORMAL
- en: '**Class Confidences**'
  prefs: []
  type: TYPE_NORMAL
- en: Class confidences represent the probabilities of the detected object belonging
    to a particular class (Dog, cat, banana, car etc). Before v3, YOLO used to softmax
    the class scores.
  prefs: []
  type: TYPE_NORMAL
- en: However, that design choice has been dropped in v3, and authors have opted for
    using sigmoid instead. The reason is that Softmaxing class scores assume that
    the classes are mutually exclusive. In simple words, if an object belongs to one
    class, then it's guaranteed it cannot belong to another class. This is true for
    COCO database on which we will base our detector.
  prefs: []
  type: TYPE_NORMAL
- en: However, this assumptions may not hold when we have classes like *Women* and *Person*.
    This is the reason that authors have steered clear of using a Softmax activation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction across different scales.**'
  prefs: []
  type: TYPE_NORMAL
- en: YOLO v3 makes prediction across 3 different scales. The detection layer is used
    make detection at feature maps of three different sizes, having **strides 32,
    16, 8** respectively. This means, with an input of 416 x 416, we make detections
    on scales 13 x 13, 26 x 26 and 52 x 52.
  prefs: []
  type: TYPE_NORMAL
- en: The network downsamples the input image until the first detection layer, where
    a detection is made using feature maps of a layer with stride 32\. Further, layers
    are upsampled by a factor of 2 and concatenated with feature maps of a previous
    layers having identical feature map sizes. Another detection is now made at layer
    with stride 16\. The same upsampling procedure is repeated, and a final detection
    is made at the layer of stride 8.
  prefs: []
  type: TYPE_NORMAL
- en: At each scale, each cell predicts 3 bounding boxes using 3 anchors, making the
    total number of anchors used 9\. (The anchors are different for different scales)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f2e5b26f9b0b03ff43fd8eebe3fd82f.png)'
  prefs: []
  type: TYPE_IMG
- en: The authors report that this helps YOLO v3 get better at detecting small objects,
    a frequent complaint with the earlier versions of YOLO. Upsampling can help the
    network learn fine-grained features which are instrumental for detecting small
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: '**Output Processing**'
  prefs: []
  type: TYPE_NORMAL
- en: For an image of size 416 x 416, YOLO predicts ((52 x 52) + (26 x 26) + 13 x
    13)) x 3 = **10647 bounding boxes**. However, in case of our image, there's only
    one object, a dog. How do we reduce the detections from 10647 to 1?
  prefs: []
  type: TYPE_NORMAL
- en: '**Thresholding by Object Confidence**'
  prefs: []
  type: TYPE_NORMAL
- en: First, we filter boxes based on their objectness score. Generally, boxes having
    scores below a threshold are ignored.
  prefs: []
  type: TYPE_NORMAL
- en: '**Non-maximum Suppression**'
  prefs: []
  type: TYPE_NORMAL
- en: NMS intends to cure the problem of multiple detections of the same image. For
    example, all the 3 bounding boxes of the red grid cell may detect a box or the
    adjacent cells may detect the same object.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b98bbebed7b6b3886094716c258cfd09.png)'
  prefs: []
  type: TYPE_IMG
- en: If you don't know about NMS, I've provided a link to a website explaining the
    same.
  prefs: []
  type: TYPE_NORMAL
- en: '**Our Implementation**'
  prefs: []
  type: TYPE_NORMAL
- en: YOLO can only detect objects belonging to the classes present in the dataset
    used to train the network. We will be using the official weight file for our detector.
    These weights have been obtained by training the network on COCO dataset, and
    therefore we can detect 80 object categories.
  prefs: []
  type: TYPE_NORMAL
- en: That's it for the first part. This post explains enough about the YOLO algorithm
    to enable you to implement the detector. However, if you want to dig deep into
    how YOLO works, how it's trained and how it performs compared to other detectors,
    you can read the original papers, the links of which I've provided below.
  prefs: []
  type: TYPE_NORMAL
- en: That's it for this part. In the next [part](https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-2/),
    we implement various layers required to put together the detector.
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading**'
  prefs: []
  type: TYPE_NORMAL
- en: '[YOLO V1: You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/pdf/1506.02640.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[YOLO V2: YOLO9000: Better, Faster, Stronger](https://arxiv.org/pdf/1612.08242.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[YOLO V3: An Incremental Improvement](https://pjreddie.com/media/files/papers/YOLOv3.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Convolutional Neural Networks](http://cs231n.github.io/convolutional-networks/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Bounding Box Regression (Appendix C)](https://arxiv.org/pdf/1311.2524.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[IoU](https://www.youtube.com/watch?v=DNEm4fJ-rto)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Non maximum suppresion](https://www.youtube.com/watch?v=A46HZGR5fMw)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PyTorch Official Tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Bio: [Ayoosh Kathuria](https://www.linkedin.com/in/ayoosh-kathuria-44a319132/)**
    currently an intern at the Defense Research and Development Organization, India,
    where he is working on improving object detection in grainy videos. When he''s
    not working, he''s either sleeping or playing pink floyd on his guitar. You can
    connect with him on [LinkedIn](https://www.linkedin.com/in/ayoosh-kathuria-44a319132/) or
    look at more of what he does at [GitHub](https://github.com/ayooshkathuria).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Getting Started with PyTorch Part 1: Understanding How Automatic Differentiation
    Works](/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PyTorch Tensor Basics](/2018/05/pytorch-tensor-basics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Learning Development with Google Colab, TensorFlow, Keras & PyTorch](/2018/02/google-colab-free-gpu-tutorial-tensorflow-keras-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
