- en: Tuning XGBoost Hyperparameters
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整 XGBoost 超参数
- en: 原文：[https://www.kdnuggets.com/2022/08/tuning-xgboost-hyperparameters.html](https://www.kdnuggets.com/2022/08/tuning-xgboost-hyperparameters.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/08/tuning-xgboost-hyperparameters.html](https://www.kdnuggets.com/2022/08/tuning-xgboost-hyperparameters.html)
- en: '![Tuning XGBoost Hyperparameters](../Images/0ed85dd2a0f411bb383af072d8093e46.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![调整 XGBoost 超参数](../Images/0ed85dd2a0f411bb383af072d8093e46.png)'
- en: '[Garett Mizunaka](https://unsplash.com/@garett3) via Unsplash'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Garett Mizunaka](https://unsplash.com/@garett3) via Unsplash'
- en: To recap, XGBoost stands for Extreme Gradient Boosting and is a supervised learning
    algorithm that falls under the gradient-boosted decision tree (GBDT) family of
    machine learning algorithms.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，XGBoost 代表极端梯度提升，是一种监督学习算法，属于梯度提升决策树（GBDT）家族的机器学习算法。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的 IT 需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: They make their predictions based on combining a set of weaker models and evaluate
    other decision trees through if-then-else true/false feature questions. They are
    created in sequential form to assess and estimate the probability of producing
    a correct decision.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 他们通过将一组较弱的模型进行组合并通过 if-then-else 真/假特征问题来评估其他决策树。他们以序列形式创建，以评估和估计产生正确决策的概率。
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Before we get into the tuning of XGBoost hyperparamters, let’s understand why
    tuning is important
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入调整 XGBoost 超参数之前，让我们了解为什么调优是重要的
- en: Why is Hyperparamter Tuning Important?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么超参数调优很重要？
- en: Hyperparameter tuning is a vital part of improving the overall behavior and
    performance of a machine learning model. It is a type of parameter that is set
    before the learning process and happens outside of the model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调优是提高机器学习模型整体行为和性能的重要部分。它是一种在学习过程之前设置的参数，发生在模型之外。
- en: A lack of hyperparameter tuning can lead to inaccurate results if the loss function
    is not minimized. Our aim is that our model produces as few errors as possible.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果损失函数没有被最小化，缺乏超参数调优可能会导致不准确的结果。我们的目标是使模型产生尽可能少的错误。
- en: Hyperparameters are used to calculate the model parameters where the different
    hyperparameter values have the ability to produce different model parameter values.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数用于计算模型参数，其中不同的超参数值能够生成不同的模型参数值。
- en: Hyperparameter tuning is all about finding a set of optimal hyperparameter values
    which maximizes the models performance, minimizes loss and produces better outputs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调优的目的是找到一组最佳的超参数值，这些值能够最大化模型性能、最小化损失并产生更好的输出。
- en: Features of XGBoost
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost 的特征
- en: 1\. Gradient Tree Boosting
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 梯度树提升
- en: There are a fixed number of trees added and with each iteration which should
    show a reduction in loss function value.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 添加了固定数量的树，每次迭代应显示损失函数值的减少。
- en: 2\. Regularized Learning
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 正则化学习
- en: Regularized Learning helps to minimize the loss function and prevent overfitting
    or underfitting from occurring - helping to smooth out the final learnt weight.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化学习有助于最小化损失函数，并防止发生过拟合或欠拟合 - 有助于平滑最终学习到的权重。
- en: 3\. Shrinkage and Feature Subsampling
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 收缩和特征子抽样
- en: 'These two techniques are used to further prevent overfitting: Shrinkage and
    Feature Subsampling.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种技术用于进一步防止过拟合：收缩和特征子抽样。
- en: These features will be further explored in the hyperparameter tuning of XGBoost.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征将在 XGBoost 的超参数调优中进一步探讨。
- en: XGBoost Parameter Tuning
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost 参数调优
- en: 'XGBoost parameters are divided into 4 groups:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 参数分为 4 类：
- en: 1\. General parameters
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 一般参数
- en: This relates to the type of booster we are using to do boosting. The most common
    types are tree or linear model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们用于增强的提升器类型有关。最常见的类型是树模型或线性模型。
- en: 2\. Booster parameters
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. Booster参数
- en: This depend on which booster you have chosen
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这取决于你选择了哪个booster。
- en: 3\. Learning task parameters
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 学习任务参数
- en: This is about deciding on the learning scenario.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这与决定学习场景有关。
- en: 4\. Command line parameters
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 命令行参数
- en: This relates to the behavior of the CLI version of XGBoost
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这与XGBoost的CLI版本的行为有关。
- en: General parameters, Booster parameters and Task parameters are set before running
    the XGBoost model. The Command line parameters are only used in the console version
    of XGBoost.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一般参数、Booster参数和任务参数在运行XGBoost模型之前设置。命令行参数仅在XGBoost的控制台版本中使用。
- en: General Parameters
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一般参数
- en: 'These are the general parameters in XGBoost:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是XGBoost中的一般参数：
- en: '`booster [default=gbtree]`'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`booster [default=gbtree]`'
- en: Choosing which booster to use such as gbtree and dart for tree based models
    and gblinear for linear functions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 选择使用哪个booster，如树基模型的gbtree和dart，以及线性函数的gblinear。
- en: '`verbosity [default=1]`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`verbosity [default=1]`'
- en: This is printing of messages where valid values are 0 (silent), 1 (warning),
    2 (info), 3 (debug).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这是消息的打印，合法值为0（静默）、1（警告）、2（信息）、3（调试）。
- en: '`validate_parameters [default to false, except for Python, R and CLI interface]`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`validate_parameters [default to false, except for Python, R and CLI interface]`'
- en: When this is set to True, XGBoost will perform validation of input parameters
    to check whether a parameter is used or not.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当设置为True时，XGBoost将执行输入参数的验证，以检查是否使用了某个参数。
- en: '`nthread`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`nthread`'
- en: This is the number of parallel threads used to run XGBoost.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于运行XGBoost的并行线程数。
- en: '`disable_default_eval_metric [default=false]`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`disable_default_eval_metric [default=false]`'
- en: This will flag to disable the default metric. You can set it to 1 or true to
    disable.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这将标记以禁用默认度量。你可以将其设置为1或true来禁用。
- en: '`num_feature `'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`num_feature`'
- en: Feature dimension used in boosting, set to maximum dimension of the feature.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在提升中使用的特征维度，设置为特征的最大维度。
- en: Tree-based Learners Most Common Parameters
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于树的学习器最常见的参数
- en: '`max_depth [default=6]`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_depth [default=6]`'
- en: Increasing this value will make the model more complex and more likely to overfit,
    therefore you need to be careful when choosing your value. The value must be an
    integer greater than 0.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 增加此值将使模型更复杂，更容易过拟合，因此在选择值时需要小心。值必须是大于0的整数。
- en: '`eta [default=0.3, alias: learning_rate]`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`eta [default=0.3, alias: learning_rate]`'
- en: This determines the step size at each iteration. The value must be between 0
    and 1 and the default is 0.3\. A low learning rate makes computation slower, and
    will need more rounds to achieve a reduction in error in comparison with a model
    with a higher learning rate.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这决定了每次迭代的步长。值必须在0和1之间，默认值为0.3。较低的学习率使计算变慢，并且需要更多轮次来减少错误，相比之下，学习率较高的模型则更快。
- en: '`n_estimators [default=100]`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_estimators [default=100]`'
- en: This is the number of trees in our ensemble which is the same number of boosting
    rounds.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们集成中的树的数量，即提升轮次的数量。
- en: The value must be an integer greater than 0 and the default is 100.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 值必须是大于0的整数，默认值为100。
- en: '`subsample [default=1]`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`subsample [default=1]`'
- en: This represents the fraction of observations that need to be sampled for each
    tree. A lower value helps to prevent overfitting, but raises the possibility of
    under-fitting. The value must be between 0 and 1, where the default is 1.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示每棵树需要采样的观察值的比例。较低的值有助于防止过拟合，但会增加欠拟合的可能性。值必须在0和1之间，默认值为1。
- en: '`colsample_bytree, colsample_bylevel, colsample_bynode [default=1]`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`colsample_bytree, colsample_bylevel, colsample_bynode [default=1]`'
- en: This is a family of parameters for subsampling of columns. Feature subsampling
    helps to prevent overfitting and also speeds up computations of the parallel algorithm.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于列子采样的一系列参数。特征子采样有助于防止过拟合，并加速并行算法的计算。
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Regularization Parameters
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化参数
- en: '`alpha [default=0, alias: reg_alpha]`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`alpha [default=0, alias: reg_alpha]`'
- en: L1 (Lasso Regression) regularization on weights. When you increase this value
    it will make the model more conservative. When you work with a large number of
    features, there is a possibility of an improvement in speed performance. It can
    be any integer and the default is 0.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对权重进行L1（Lasso回归）正则化。增加此值将使模型更保守。当特征数量较多时，可能会提高速度性能。可以是任何整数，默认值为0。
- en: '`lambda [default=1, alias: reg_lambda]`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`lambda [default=1, alias: reg_lambda]`'
- en: L2 (Ridge Regression) regularization on weights. When you increase this value
    it will make the model more conservative. It can also be used to help to reduce
    overfitting. It can be any integer and the default is 1.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对权重进行 L2（岭回归）正则化。当你增加这个值时，它会使模型更为保守。它还可以帮助减少过拟合。这个值可以是任何整数，默认值是 1。
- en: '`gamma [default=0, alias: min_split_loss]`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`gamma [default=0, alias: min_split_loss]`'
- en: Minimum loss reduction required to make a further partition on a leaf node of
    the tree. The higher the Gamma value is, the higher the regularization. It can
    be any integer and the default is 0.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使树的叶节点进一步分割所需的最小损失减少。Gamma 值越高，正则化越强。它可以是任何整数，默认值是 0。
- en: You can find the [complete list of XGBoost parameters here](https://xgboost.readthedocs.io/en/latest/parameter.html).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到 [XGBoost 参数的完整列表](https://xgboost.readthedocs.io/en/latest/parameter.html)。
- en: Conclusion
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: From this article, you would have understood how the 3 features of XGBoost are
    achieved by the 4 divided parameter tuning groups. Which were then further explored
    through the different XGBoost parameters.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 从这篇文章中，你应该了解 XGBoost 的 3 个特性是如何通过 4 个划分的参数调优组实现的。这些特性随后通过不同的 XGBoost 参数进行了进一步探索。
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** 是一名数据科学家和自由职业技术作家。她特别关注提供数据科学职业建议或教程以及围绕数据科学的理论知识。她还希望探索人工智能如何/能如何促进人类寿命的延续。她是一位热心学习者，寻求拓宽技术知识和写作技能，同时帮助指导他人。'
- en: More On This Topic
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Tuning Random Forest Hyperparameters](https://www.kdnuggets.com/2022/08/tuning-random-forest-hyperparameters.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[调优随机森林超参数](https://www.kdnuggets.com/2022/08/tuning-random-forest-hyperparameters.html)'
- en: '[Tuning Hyperparameters in Neural Networks](https://www.kdnuggets.com/tuning-hyperparameters-in-neural-networks)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[神经网络中的超参数调优](https://www.kdnuggets.com/tuning-hyperparameters-in-neural-networks)'
- en: '[How to Speed Up XGBoost Model Training](https://www.kdnuggets.com/2021/12/speed-xgboost-model-training.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何加快 XGBoost 模型训练](https://www.kdnuggets.com/2021/12/speed-xgboost-model-training.html)'
- en: '[What are the Assumptions of XGBoost?](https://www.kdnuggets.com/2022/08/assumptions-xgboost.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost 的假设是什么？](https://www.kdnuggets.com/2022/08/assumptions-xgboost.html)'
- en: '[Leveraging XGBoost for Time-Series Forecasting](https://www.kdnuggets.com/2023/08/leveraging-xgboost-timeseries-forecasting.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用 XGBoost 进行时间序列预测](https://www.kdnuggets.com/2023/08/leveraging-xgboost-timeseries-forecasting.html)'
- en: '[WTF is the Difference Between GBM and XGBoost?](https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GBM 和 XGBoost 之间到底有什么区别？](https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost)'
