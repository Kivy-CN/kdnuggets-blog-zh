- en: 'Understanding overfitting: an inaccurate meme in Machine Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解过拟合：机器学习中的不准确的迷思
- en: 原文：[https://www.kdnuggets.com/2017/08/understanding-overfitting-meme-supervised-learning.html](https://www.kdnuggets.com/2017/08/understanding-overfitting-meme-supervised-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/08/understanding-overfitting-meme-supervised-learning.html](https://www.kdnuggets.com/2017/08/understanding-overfitting-meme-supervised-learning.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png)[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png)[评论](#comments)'
- en: '**By [Mehmet Suzen](http://msuzen.github.io/), U. of Frankfurt** .'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Mehmet Suzen](http://msuzen.github.io/)，法兰克福大学**。'
- en: 'This post was inspired by a [recent post](http://andrewgelman.com/2017/07/15/what-is-overfitting-exactly/)
    by Andrew Gelman, who defined ‘overfitting’ as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章的灵感来源于 [Andrew Gelman](http://andrewgelman.com/2017/07/15/what-is-overfitting-exactly/)
    的一篇 [最近文章](http://andrewgelman.com/2017/07/15/what-is-overfitting-exactly/)，他将‘过拟合’定义如下：
- en: '*Overfitting is when you have a complicated model that gives worse predictions,
    on average, than a simpler model.*'
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*过拟合是指你的模型过于复杂，导致预测效果通常比简单模型更差。*'
- en: '**Preamble**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**前言**'
- en: 'There is a lot of confusion among practitioners regarding the concept of [overfitting](https://en.wikipedia.org/wiki/Overfitting).
    It seems like, a kind of *an* *urban legend* or a *meme, a folklore *is circulating in
    data science or allied fields with the following statement:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从业者对*过拟合*的概念有很多困惑。似乎在数据科学或相关领域流传着一种*城市传说*或*迷思*，其陈述如下：
- en: '*Applying [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) prevents
    overfitting and a good out-of-sample performance, low generalisation error in
    unseen data, indicates not an overfit.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*应用 [交叉验证](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) 可以防止过拟合，而良好的样本外表现、未见数据中的低泛化误差，表明不是过拟合。*'
- en: 'This statement is of course not true: cross-validation does not prevent your
    model to overfit and good out-of-sample performance does not guarantee not-overfitted
    model. What actually people refer to in one aspect of this statement is called *overtraining*.
    Unfortunately, this meme is not only propagated in industry but in some academic
    papers as well. This might be at best a confusion on *jargon*. But, it will be
    a good practice if we set the *jargon* right and clear on what do we refer to
    when we say *overfitting,* in communicating our results.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这个说法显然是不正确的：交叉验证并不能防止你的模型过拟合，良好的样本外表现并不保证模型没有过拟合。实际上，人们在这一说法的一个方面提到的叫做*过度训练*。不幸的是，这种误解不仅在行业中传播，也存在于一些学术论文中。这可能在最佳情况下只是对*术语*的混淆。然而，如果我们在沟通结果时对*过拟合*的定义明确和清晰，这将是一个良好的实践。
- en: '**Aim**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**目标**'
- en: In this post, we will give an intuition on why [model validation](https://en.wikipedia.org/wiki/Regression_validation) as
    approximating generalization error of a [model fit](https://en.wikipedia.org/wiki/Goodness_of_fit) and
    detection of overfitting can not be resolved simultaneously on a single model.
    We will work on  a concrete example workflow in understanding *overfitting*, *overtraining* and
    a typical final model building stage  after some conceptual introduction. We will
    avoid giving a reference to the Bayesian interpretations and regularisation and
    restrict the post to regression and cross-validation. While regularisation has
    different ramification due to its mathematical properties and prior distributions
    have different implications in Bayesian statistics. We assume an introductory
    background in machine learning, so this is not a beginners tutorial.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将直观地解释为什么 [模型验证](https://en.wikipedia.org/wiki/Regression_validation) 作为近似 [模型拟合](https://en.wikipedia.org/wiki/Goodness_of_fit) 的泛化误差和检测过拟合不能在单个模型上同时解决。我们将在一些概念介绍之后，通过一个具体的示例工作流程来理解*过拟合*、*过度训练*以及一个典型的最终模型构建阶段。我们将避免提及贝叶斯解释和正则化，限制讨论在回归和交叉验证范围内。虽然正则化由于其数学性质和先验分布在贝叶斯统计中有不同的影响，但我们假设读者具备机器学习的基础知识，因此这不是初学者教程。
- en: A recent question from [Andrew Gelman](https://en.wikipedia.org/wiki/Andrew_Gelman),
    a Bayesian guru, regarding *What is overfitting?* was one of the reasons why this
    post is developed along with my frustration to see practitioners being muddy on
    the meaning of *overfitting* and continuing recently published data science related
    technical articles circulating around and even in some academic papers claiming
    the above statement.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，贝叶斯大师[安德鲁·盖尔曼](https://en.wikipedia.org/wiki/Andrew_Gelman)关于*什么是过拟合？*的提问，是本帖开发的原因之一，加上我看到从业者对*过拟合*的含义模糊不清，以及最近发表的关于数据科学的技术文章和一些学术论文中仍然流传的错误观点，我感到沮丧。
- en: '**What do we need to satisfy in supervised learning? **'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们在监督学习中需要满足什么？**'
- en: 'One of the most basic tasks in mathematics is to find a solution to a function:
    If we restrict ourselves to real numbers in *n*-dimensions and our domain of interest
    would be **R**^(*n*). Now imagine set of *p* points living in this domain *x*[i] form
    a dataset, this is actually *a partial*  solution to a function. The main purpose
    of modelling is to find an explanation of the dataset, meaning that we need to
    determine *m*-parameters, a∈**R**^m which are unknown. (Note that a non-parametric
    model does not mean no parameters.) Mathematically speaking this manifests as
    a function as we said before,  f(x,a). This modelling is usually called *regression*, *interpolation* or *supervised
    learning *depending on the literature you are reading. This is a form of [an inverse
    problem](https://en.wikipedia.org/wiki/Inverse_problem), while we don’t know the
    parameters but we have a partial information regarding variables. The main issue
    is that solutions are not [well-posed](https://en.wikipedia.org/wiki/Well-posed_problem).
    Omitting axiomatic technical details, practical problem is that we can find many
    functions f(x,a) or models, explaining the dataset.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数学中最基本的任务之一是找到一个函数的解：如果我们将自己限制在*n*维实数上，并且我们的兴趣领域是**R**^(*n*)。现在想象在这个领域中存在一组*p*点，这些点形成一个数据集，这实际上是函数的*a
    部分* 解。建模的主要目的是寻找数据集的解释，这意味着我们需要确定*m*个参数，a∈**R**^m，这些参数是未知的。（请注意，非参数模型并不意味着没有参数。）从数学上讲，这表现为我们之前所说的函数f(x,a)。这个建模通常称为*回归*、*插值*或*监督学习*，这取决于你所阅读的文献。这是[逆问题](https://en.wikipedia.org/wiki/Inverse_problem)的一种形式，我们不知道参数，但对变量有部分信息。主要问题是解决方案不是[良构的](https://en.wikipedia.org/wiki/Well-posed_problem)。省略公理技术细节，实际问题是我们可以找到许多函数f(x,a)或模型来解释数据集。
- en: So, we seek the following two concepts to be satisfied by our model solution,  f(x,a)=0.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望我们的模型解决方案满足以下两个概念，f(x,a)=0。
- en: '1\. Generalized: A model should not depend on the dataset. This step is called *model
    validation.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 泛化：模型不应依赖于数据集。这个步骤称为*模型验证*。
- en: '2\. Minimally complex: A model should obey [Occam’s razor or principle of parsimony](https://en.wikipedia.org/wiki/Occam%27s_razor).
    This step is called *model selection*.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 最小复杂度：一个模型应遵循[奥卡姆剃刀或简约原则](https://en.wikipedia.org/wiki/Occam%27s_razor)。这个步骤称为*模型选择*。
- en: '![](../Images/9867ff660ef3f68613dfff30792938af.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9867ff660ef3f68613dfff30792938af.png)'
- en: 'Figure 1: A workflow for model validation and selection in supervised learning.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：监督学习中模型验证和选择的工作流程。
- en: Generalization of a model can be measured by [goodness-of-fit](https://en.wikipedia.org/wiki/Goodness_of_fit).
    It essentially tells us how good our model (chosen function) explains the dataset.
    To find a minimally complex modelrequires comparison against another model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的泛化可以通过[拟合优度](https://en.wikipedia.org/wiki/Goodness_of_fit)来衡量。这本质上告诉我们我们的模型（选定的函数）对数据集的解释有多好。要找到一个*最小复杂度*的模型，需要与另一个模型进行比较。
- en: Up to now, we have not named a technique how to check if a model is generalized
    and selected as the best model. Unfortunately, there is no unique way of doing
    both and that’s the task of data scientist or quantitative practitioner that requires
    human judgement.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有命名一种技术来检查一个模型是否经过泛化并被选为最佳模型。不幸的是，目前没有唯一的方式来同时做到这两点，这正是数据科学家或量化从业者的任务，需要人类判断。
- en: '**Model validation: An example **'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型验证：一个例子**'
- en: One way to check if a model is generalized enough is to come up with a metric
    on how good it explains the dataset. Our task in model validation is to estimate
    the model error. For example, [root mean square deviation](https://en.wikipedia.org/wiki/Root-mean-square_deviation) (RMDS)
    is one metric we can use.  If  RMSD is low, we could say that our model fit is
    good, ideally it should be close to zero.  But it is not generalized enough if
    we use the same dataset to measure the goodness-of-fit.  We could use different
    dataset, specially out-of-sample dataset, to validate this as much as we can,
    i.e. so called hold out method.  Out-of-sample is just a fancy way of saying we
    did not use the same dataset to find the value of parameters a. An improved way
    of doing this is cross-validation. We split our dataset into k partitions, and
    we obtain k RMDS values to averaged over. This is summarised in Figure 1\.  Note
    that, different parameterisation of the same model does not constitute a different
    model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 检查模型是否足够通用的一种方法是制定一个度量来评估它解释数据集的好坏。我们在模型验证中的任务是估计模型误差。例如，[均方根偏差](https://en.wikipedia.org/wiki/Root-mean-square_deviation)
    (RMDS) 是一个可以使用的度量。如果 RMSD 较低，我们可以说我们的模型拟合得很好，理想情况下它应该接近零。但如果我们使用相同的数据集来衡量拟合优度，它的通用性就不够。我们可以尽可能使用不同的数据集，特别是样本外数据集来验证这一点，即所谓的持出方法。样本外仅仅是说我们没有使用相同的数据集来寻找参数值a。改进的方法是交叉验证。我们将数据集分成k个部分，并获得k个RMDS值来取平均。这在图1中总结了。请注意，相同模型的不同参数化并不构成不同的模型。
- en: '**Model Selection: Detection of overfitting **'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型选择：过拟合的检测**'
- en: Overfitting comes into play when we try to satisfy ‘minimally complex model’.
    This is a comparison problem and we need more than one model to judge if a given
    model is an overfit. Douglas Hawkins in his classic paper *[The Problem of Overfitting](http://pubs.acs.org/doi/abs/10.1021/ci0342472)*,
    states that
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们尝试满足“最简复杂模型”时，过拟合就会出现。这是一个比较问题，我们需要多个模型来判断一个给定模型是否过拟合。道格拉斯·霍金斯在他的经典论文 *[过拟合问题](http://pubs.acs.org/doi/abs/10.1021/ci0342472)*
    中指出：
- en: '*Overfitting of models is widely recognized as a concern. It is less recognized
    however that overfitting is not an absolute but involves a comparison. A model
    overfits if it is more complex than another model that fits equally well.*'
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*模型的过拟合被广泛认为是一个问题。然而，过拟合并不是绝对的，而是涉及比较的。如果一个模型比另一个拟合效果相同的模型更复杂，那么这个模型就是过拟合的。*'
- en: The important point here what do we mean by complex model, or how can we quantify
    model complexity? Unfortunately, again there is no unique way of doing this. One
    of the most used approaches is that a model having more parameters is getting
    more complex. But this is again a bit of a *meme* and not generally true. One
    could actually resort to different measures of complexity. For example, by this
    definition f1(a,x)=ax and f2(a,x)=ax² have the same complexity by having the same
    number of free parameters, but intuitively f2 is more complex, while it is nonlinear.
    There are a lot of information theory based measures of complexity but discussion
    of those are beyond the scope of our post. For demonstration purposes, we will
    consider more parameters and degree of nonlinearity as more complex a model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的重要点是我们所说的复杂模型是什么意思，或者我们如何量化模型复杂性？不幸的是，这没有唯一的方法。最常用的方法之一是，拥有更多参数的模型变得更复杂。但这仍然是一种*流行观念*，并不完全正确。实际上，可以使用不同的复杂性度量。例如，根据这个定义，f1(a,x)=ax
    和 f2(a,x)=ax² 具有相同的复杂性，因为它们有相同数量的自由参数，但直观上 f2 更复杂，尽管它是非线性的。还有许多基于信息论的复杂性度量，但讨论这些超出了我们帖子的范围。为了演示的目的，我们将更多的参数和非线性的程度视为模型的复杂性。
- en: '![](../Images/9b071de1261d03d7b516f9dbb9c90fa4.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b071de1261d03d7b516f9dbb9c90fa4.png)'
- en: 'Figure 2: Simulated data and the non-stochastic part of the data.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：模拟数据与数据的非随机部分。
- en: '**Hand on example**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**实际示例**'
- en: We have intuitively covered the reasons behind how we can’t resolve model validation
    and judge overfitting simultaneously. Now try to demonstrate this with a simple
    dataset and models, yet essentially capturing the above premise.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们直观地覆盖了为什么我们不能同时解决模型验证和判断过拟合的问题。现在尝试用一个简单的数据集和模型来展示这一点，同时基本捕捉上述前提。
- en: A usual procedure is to generate a synthetic dataset, or simulated dataset,
    from a model, as a gold standard and use this dataset to build other models. Let’s
    use the following functional form, from [classic text of Bishop](http://www.springer.com/de/book/9780387310732?referer=www.springer.de),
    but with an added Gaussian noise
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通常的程序是从模型生成一个合成数据集或模拟数据集，作为标准数据集，并利用这个数据集构建其他模型。我们使用以下的功能形式，来自[经典文本《Bishop》](http://www.springer.com/de/book/9780387310732?referer=www.springer.de)，但加入了高斯噪声。
- en: f(x)=sin(2πx)+N(0,0.1).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: f(x)=sin(2πx)+N(0,0.1)。
- en: We generate large enough set, 100 points to avoid sample size issue discussed
    in Bishop’s book, see Figure 2\. Let’s decide on two models we would like to apply
    to this dataset in supervised learning task. Note that, we won’t be discussing
    Bayesian interpretation here, so equivalency of these model under a strong prior
    assumption is not an issue as we are using this example for ease of demonstrating
    the concept. A polynomial model of degree 3 and degree 6, we call them g(x) and
    h(x) respectively, are used to learn from the simulated data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成一个足够大的数据集，100 个点，以避免《Bishop》书中讨论的样本大小问题，见图2。我们决定应用两个模型到这个数据集上的监督学习任务中。注意，我们在这里不会讨论贝叶斯解释，因此这些模型在强先验假设下的等价性不是问题，因为我们使用这个示例是为了方便演示概念。我们使用了一个三次和六次的多项式模型，分别称之为
    g(x) 和 h(x)，用来从模拟数据中学习。
- en: g(x)=a[0] + a[1]x + a[2]x² + a[3]x³
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: g(x)=a[0] + a[1]x + a[2]x² + a[3]x³
- en: and
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: h(x)=b[0] + b[1]x + b[2]x² + b[3]x³ + b[4]x⁴ + b[5]x⁵ + b[6]x⁶![](../Images/1da9363012227110e0b6705489ec5899.png)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: h(x)=b[0] + b[1]x + b[2]x² + b[3]x³ + b[4]x⁴ + b[5]x⁵ + b[6]x⁶![](../Images/1da9363012227110e0b6705489ec5899.png)
- en: 'Figure 3: Overtraining occurs at around after 40 percent of the data usage
    for g(x).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：g(x) 在数据使用量达到 40% 之后出现过训练现象。
- en: '**Overtraining is not overfitting**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**过训练并非过拟合**'
- en: '*Overtraining* means a model performance degrades in learning model parameters
    against an objective variable that effects how model is build, for example, an
    objective variable can be a training data size or iteration cycle in neural network.
    This is more prevalent in neural networks (see [Dayhoff 2011](http://onlinelibrary.wiley.com/doi/10.1002/1097-0142(20010415)91:8%2B%3C1615::AID-CNCR1175%3E3.0.CO;2-L/full)).
    In our practical example, this will manifest in hold out method to measure RMSD
    in modelling with g(x). In other words finding an optimal number of data points
    to use to train the model to give a better performance on unseen data, See Figure
    3 and 4.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*过训练* 意味着模型在学习模型参数对目标变量的性能下降，而目标变量会影响模型的构建，例如，目标变量可以是训练数据的大小或神经网络中的迭代周期。这在神经网络中更为普遍（见[Dayhoff
    2011](http://onlinelibrary.wiley.com/doi/10.1002/1097-0142(20010415)91:8%2B%3C1615::AID-CNCR1175%3E3.0.CO;2-L/full)）。在我们的实际示例中，这将表现为在
    g(x) 模型中使用保留方法测量 RMSD。换句话说，找到用于训练模型的最佳数据点数量，以在未见数据上提供更好的性能，见图3和4。'
- en: '**Overfitting with low validation error**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**低验证误差的过拟合**'
- en: We can also estimate 10-fold cross-validation error, CV-RMSD. For this sampling,
    g and h have 0.13 and 0.12 CV-RMSD respectively. So as we can see, we have a situation
    that more complex model reaches similar predictive power with cross validation
    and we can not distinguish this overfitting by just looking at CV-RMSD value or
    detecting ‘overtraining’ curve from Figures 4\. We need two models to compare,
    hence both Figure 3 and 4, with both CV-RMSD values. We might argue that in small
    data sets we might be able tell the difference by looking at test and training
    error differences, this is exactly how Bishop explains *overfitting;* where he
    points out *overtraining* in small datasets.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以估计 10 折交叉验证误差，CV-RMSD。对于这次采样，g 和 h 的 CV-RMSD 分别为 0.13 和 0.12。正如我们所见，我们遇到了一种更复杂的模型在交叉验证中达到类似的预测能力的情况，我们不能仅通过查看
    CV-RMSD 值或从图4中检测‘过训练’曲线来区分这种过拟合。我们需要比较两个模型，因此需要图3和4，并查看 CV-RMSD 值。我们可以认为，在小数据集中，我们可能通过查看测试和训练误差的差异来辨别，这正是《Bishop》解释的*过拟合*；他指出了小数据集中的*过训练*。
- en: '**Which trained model to deploy?**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**部署哪个训练模型？**'
- en: Now the question is, we found out best performing model with minimal complexity
    empirically. All well, but which trained model should we use in production?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是，我们已经通过经验发现了性能最佳且复杂度最低的模型。一切都很好，但我们应该在生产中使用哪个训练模型呢？
- en: Actually we have already build the model in model selection. In above case,
    since we got similar
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上我们已经在模型选择中建立了模型。在上述情况下，由于我们得到了相似的结果
- en: predictive power from g and h, we obviously will use g, trained on the splitting
    sweet spot from Figure 3.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从 g 和 h 的预测能力来看，我们显然会使用 g，基于图3中的划分甜点进行训练。
- en: '![](../Images/6ede4111ec5dc636d31fae03616c4321.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Overtraining occurs at around after 30 percent of the data usage
    for h(x)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: The essential message here is good validation performance would not guarantee
    the detection of an *overfitted* model. As we have seen from examples using synthetic
    data in one dimension. *Overtraining* is actually what most practitioners mean
    when they use the term *overfitting*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '**Outlook**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: As more and more people are using techniques from machine learning or inverse
    problems, both in academia and industry, some key technical concepts are deviated
    a bit and take different definitions and meaning for different people, due to
    the fact that people learn some concepts not from reading the literature carefully
    but from their line managers or senior colleagues verbally. This creates *memes*
    which are actually wrong or at least creating lots of confusion in jargon. It
    is very important for all of us as practitioners that we must *question* all technical
    concepts and try to seek origins from the published scientific literature and
    not rely entirely on verbal explanations from our experienced colleagues. Also,
    we should strongly avoid ridiculing question from colleagues even they sound too
    simple, at the end of the day we don’t stop learning and naive looking questions
    might have very important consequences in fundamentals of the field.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '**P.S. **As I mentioned above, the inspiration of writing this post was, a
    recent post from Gelman ([post](http://andrewgelman.com/2017/07/15/what-is-overfitting-exactly/)).
    He defined ‘overfitting’ as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '*Overfitting is when you have a complicated model that gives worse predictions,
    on average, than a simpler model.*'
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Priors and equivalence of two models aside, Gelman’s definition is weaker than
    Hawkins definition, that he accepts a complex model having a similar predictive
    power. So, if we use Gelman’s definitions it is ok to deploy either of *g* or *h*in
    our toy example above. But strictly speaking from Hawkins perspective we need
    to deploy *g*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](http://memosisland.blogspot.de/2017/08/understanding-overfitting-inaccurate.html).
    Reposted with permission.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio:** [**Dr. Mehmet Suzen**](http://msuzen.github.io/) is a Theoretical
    Physicist and Research Scientist. He blogs on data science at [Memos’ Island](https://memosisland.blogspot.de/).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[Making Predictive Models Robust: Holdout vs Cross-Validation](/2017/08/dataiku-predictive-model-holdout-cross-validation.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Truth About Bayesian Priors and Overfitting](/2017/07/truth-about-bayesian-priors-overfitting.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Lie with Data](/2017/04/how-lie-data.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的 IT'
- en: '* * *'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[How to Avoid Overfitting](https://www.kdnuggets.com/2022/08/avoid-overfitting.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何避免过拟合](https://www.kdnuggets.com/2022/08/avoid-overfitting.html)'
- en: '[KDnuggets News, August 24: Implementing DBSCAN in Python • How to…](https://www.kdnuggets.com/2022/n34.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，8月24日：在 Python 中实现 DBSCAN • 如何…](https://www.kdnuggets.com/2022/n34.html)'
- en: '[Closing the Gap Between Human Understanding and Machine Learning:…](https://www.kdnuggets.com/2023/06/closing-gap-human-understanding-machine-learning-explainable-ai-solution.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[缩小人类理解与机器学习之间的差距：…](https://www.kdnuggets.com/2023/06/closing-gap-human-understanding-machine-learning-explainable-ai-solution.html)'
- en: '[Understanding Machine Learning Algorithms: An In-Depth Overview](https://www.kdnuggets.com/understanding-machine-learning-algorithms-an-indepth-overview)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解机器学习算法：深入概述](https://www.kdnuggets.com/understanding-machine-learning-algorithms-an-indepth-overview)'
- en: '[Understanding Supervised Learning: Theory and Overview](https://www.kdnuggets.com/understanding-supervised-learning-theory-and-overview)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解监督学习：理论与概述](https://www.kdnuggets.com/understanding-supervised-learning-theory-and-overview)'
- en: '[Free MIT Courses on Calculus: The Key to Understanding Deep Learning](https://www.kdnuggets.com/2020/07/free-mit-courses-calculus-key-deep-learning.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[免费 MIT 微积分课程：理解深度学习的关键](https://www.kdnuggets.com/2020/07/free-mit-courses-calculus-key-deep-learning.html)'
