# 你应该了解的更多分类问题性能评估指标

> 原文：[https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html](https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html)

评估模型是构建有效机器学习模型的关键部分。我们使用的最常见分类评估指标应该是**准确率**。你可能会认为当准确率达到99%时模型很好！然而，这并不总是准确的，并且在某些情况下可能会误导。我将在本文中解释如下4个方面：

+   2类分类问题的混淆矩阵

+   关键分类指标：*准确率、召回率、精准率和F1分数*

+   *召回率和精准率在特定情况下的区别*

+   决策阈值和接收者操作特征（ROC）曲线

# 机器学习模型的流程

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析水平

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的IT工作

* * *

在任何二分类任务中，我们的模型只能得到两种结果：**正确**或**不正确**。假设我们现在有一个分类任务，要预测一张图像是狗还是猫。在监督学习中，我们首先在训练数据上**拟合/训练**一个模型，然后在**测试数据**上**测试**该模型。一旦我们从**X_test**数据中获得模型的预测结果，我们将其与**真实的 y_values**（正确标签）进行比较。

![](../Images/f4563f2583050a1b1671bbc374b1be28.png)

在模型预测之前，我们将狗的图像输入到我们训练好的模型中。模型预测这是一只狗，然后我们将预测结果与正确标签进行比较。如果我们将预测结果与“狗”的标签进行比较，则预测正确。然而，如果预测结果是这张图像是一只猫，那么与正确标签的比较将是不正确的。

我们对X测试数据中的所有图像重复这一过程。最终，我们将获得正确匹配的数量和错误匹配的数量。关键的认识是，并非所有的错误或正确匹配在现实中都具有**相同的价值**。因此，单一指标不能全面反映情况。

如前所述，准确率是分类问题中的常见评估指标，即正确预测的总数除以数据集上做出的总预测数。当目标类别*平衡良好*时，准确率是有用的，但对于不平衡的类别就不太适用。想象一下我们有99张狗的图片和只有1张猫的图片在训练数据中，我们的模型将只是一个总是预测狗的线，因此得到了99%的准确率。现实中数据总是失衡的，如垃圾邮件、信用卡欺诈和医疗诊断。因此，如果我们想全面了解模型评估，还应考虑其他指标，如召回率和精准度。

# 混淆矩阵

对分类模型性能的评估基于模型正确和错误预测的测试记录计数。混淆矩阵提供了一个更有洞察力的图景，它不仅展示了预测模型的性能，还显示了哪些类别被正确和错误预测，错误类型是什么。为了说明这一点，我们可以看到四个分类指标（TP、FP、FN、TN）是如何计算的，并且在下面的混淆矩阵表中，我们的预测值与实际值的对比被清晰地呈现出来。

![](../Images/3d5f3e3de47423d7565d725b846eb01d.png)

*可能的分类结果：TP、FP、FN、TN。*

> *混淆矩阵对于测量召回率（也称为敏感性）、精准度、特异性、准确率以及最重要的 AUC-ROC 曲线非常有用。*

读表格时你感到困惑吗？这是正常的。我之前也是如此。让我用怀孕类比来解释 TP、FP、FN 和 TN 的术语。这样我们可以理解召回率、精准度、特异性、准确率，更重要的是 AUC-ROC 曲线。

![](../Images/88b6a0ff716a6bf5b4af5e4fb6cc4a24.png)

*[图像来源](https://dzone.com/articles/understanding-the-confusion-matrix)*

# 四个关键分类指标的公式

![](../Images/8fa9dab027b54588244d29ee3b7acc2b.png)

**召回率与精准度**

![](../Images/590c61e5345073ed8a891b794f61e1c7.png)

**精准度** 是模型预测的所有正样本中*真实正样本*的比例。

低精准度：模型预测的假阳性越多，精准度越低。

**召回率（敏感性）** 是数据集中所有正样本中*真实正样本*的比例。

低召回率：模型预测的假阴性越多，召回率越低。

*召回率和精准度的概念似乎很抽象。让我通过三个实际案例来说明差异。*

![](../Images/caffd27813ca620b178fbec91fbeb25d.png)

+   TP 的结果是 COVID-19 居民被诊断为 COVID-19。

+   TN 的结果是健康的居民身体健康。

+   FP 的结果是那些实际上健康的居民被预测为 COVID-19 居民。

+   FN 的结果是那些实际的 COVID-19 居民被预测为健康居民。

在情况 1 中，你认为哪个场景的成本会最高？

想象一下，如果我们将 COVID-19 居民预测为健康患者，而他们不需要隔离，这将导致大量的 COVID-19 感染。假阴性的成本要远高于假阳性的成本。

![](../Images/27c47014ea641c459da7e6a1bdb23040.png)

+   TP 的结果是垃圾邮件被放入垃圾邮件文件夹。

+   TN 的结果是重要的电子邮件被收到。

+   FP 的结果是重要的电子邮件被放入垃圾邮件文件夹。

+   FN 的结果是垃圾邮件被收到。

在情况 2 中，你认为哪个场景的成本会最高？

嗯，由于错过重要的电子邮件显然比收到垃圾邮件更严重，我们可以说，在这种情况下，FP 的成本会高于 FN。

![](../Images/05234286f46dea9a2fff74e5cfc4edd6.png)

+   TP 的结果是坏贷款被正确预测为坏贷款。

+   TN 的结果是好贷款被正确预测为好贷款。

+   FP 的结果是（实际的）好贷款被错误预测为坏贷款。

+   FN 的结果是（实际的）坏贷款被错误预测为好贷款。

在情况 3 中，你认为哪个场景的成本会最高？

如果实际的坏贷款被预测为好贷款，银行将损失大量资金，因为贷款无法偿还。另一方面，如果实际的好贷款被预测为坏贷款，银行将无法获得更多收入。因此，*假阴性*的成本要远高于*假阳性*的成本。想象一下吧。

**总结**

![](../Images/b0c7d23958f29d84cfc490697ab5f3e2.png)

在实际应用中，假阴性的成本与假阳性的成本不同，这取决于具体情况。显然，我们不仅应计算准确率，还应使用其他指标来评估我们的模型，例如 *召回率* 和 *精确率*。

# 结合精确率和召回率

在上述三个情况中，我们希望在牺牲另一个指标的情况下，最大化召回率或精确率。例如，在好贷款或坏贷款分类的情况下，我们希望减少 FN 以提高召回率。然而，在我们希望找到精确率和召回率的最佳结合的情况下，我们可以通过使用 [F1 分数](https://en.wikipedia.org/wiki/F1_score) 将这两个指标结合起来。

![](../Images/6cc7f8f2835ab8e0ce9b2674d62faf94.png)

F-Measure 提供了一个综合精确率和召回率的单一评分。一个好的 F1 分数意味着你有低假阳性和低假阴性，因此你能正确识别实际威胁，并且不会被虚假警报所干扰。当 F1 分数为 1 时，表示模型完美；而当 F1 分数为 0 时，表示模型完全失败。

![](../Images/af4424d44698efd4190dad446b7b782f.png)

# 决策阈值

ROC 是呈现分类模型性能的主要可视化技术。它总结了使用不同概率阈值时预测模型的真正率（tpr）和假正率（fpr）之间的权衡。

![](../Images/4ba9645520e1252a292cc220de108b25.png)

*tpr 和 fpr 的公式。*

真正率（tpr）是召回率，而假正率（FPR）是误报的概率。

ROC 曲线绘制了真正率（tpr）与假正率（fpr）之间的关系，作为模型分类正例的阈值函数。考虑到**c**是称为决策阈值的常数，下图 ROC 曲线表明默认 c=0.5，当 c=0.2 时，tpr 和 fpr 都增加；当 c=0.8 时，tpr 和 fpr 都减少。一般来说，tpr 和 fpr 随着 c 的减少而增加。在极端情况下，当 c=1 时，所有案例被预测为负例；tpr=fpr=0。而当 c=0 时，所有案例被预测为正例；tpr=fpr=1。

![](../Images/19f60956fb7baa2b0c3430dafd9bbf63.png)

*ROC 图。*

最终，我们可以通过 ROC 曲线下的面积（**AUC**）来评估模型的性能。作为经验法则，0.9–1=优秀；0.8-0.9=良好；0.7–0.8=中等；0.6–0.7=差；0.50–0.6=失败。

# 摘要

+   二分类问题的混淆矩阵

+   主要分类指标：*准确率、召回率、精准率和 F1 分数*

+   *召回率与精准率在特定情况下的区别*

+   决策阈值和接收者操作特征（ROC）曲线

**[Clare Liu](https://www.linkedin.com/in/clareliuchungyan/)** 是一位在香港金融科技行业的 数据科学家。热衷于解决关于数据科学和机器学习的难题。加入我，一起踏上自学之旅。

### 更多相关话题

+   [机器学习评估指标：理论与概述](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)

+   [分类指标详细讲解：逻辑回归与……](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)

+   [理解分类指标：评估模型准确性的指南](https://www.kdnuggets.com/understanding-classification-metrics-your-guide-to-assessing-model-accuracy)

+   [想用你的数据技能解决全球问题？这里是……](https://www.kdnuggets.com/2022/04/jhu-want-data-skills-solve-global-problems.html)

+   [KDnuggets 新闻，4 月 13 日：数据科学家应关注的 Python 库](https://www.kdnuggets.com/2022/n15.html)

+   [可以帮助你解决实际问题的数据科学项目](https://www.kdnuggets.com/2022/11/data-science-projects-help-solve-real-world-problems.html)
