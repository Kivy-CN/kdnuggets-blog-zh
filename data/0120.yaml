- en: 'Textbooks Are All You Need: A Revolutionary Approach to AI Training'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**教科书就是你所需的一切**：一种颠覆性的 AI 培训方法'
- en: 原文：[https://www.kdnuggets.com/2023/07/textbooks-all-you-need-revolutionary-approach-ai-training.html](https://www.kdnuggets.com/2023/07/textbooks-all-you-need-revolutionary-approach-ai-training.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/07/textbooks-all-you-need-revolutionary-approach-ai-training.html](https://www.kdnuggets.com/2023/07/textbooks-all-you-need-revolutionary-approach-ai-training.html)
- en: '![Textbooks Are All You Need: A Revolutionary Approach to AI Training](../Images/0d8f5143e382734bcb1df502380dac67.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![教科书就是你所需的一切：一种颠覆性的 AI 培训方法](../Images/0d8f5143e382734bcb1df502380dac67.png)'
- en: Image created by Author with Midjourney
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用 Midjourney 创建
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Researchers are always looking for new and better ways to train artificial intelligence
    models. A [recent paper from Microsoft](https://arxiv.org/abs/2306.11644) proposed
    an interesting approach - using a synthetic textbook to teach the model instead
    of the massive datasets typically used.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员总是寻求更好、更创新的方式来培训人工智能模型。一篇 [微软最近的论文](https://arxiv.org/abs/2306.11644) 提出了一个有趣的方法——使用合成教科书来教导模型，而不是通常使用的大量数据集。
- en: The paper introduces a model called Phi-1 that was trained entirely on a custom-made
    textbook. The researchers found this was just as effective as much larger models
    trained on huge piles of data for certain tasks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 论文介绍了一种称为 Phi-1 的模型，该模型完全基于定制的教科书进行训练。研究人员发现，这与在大量数据上训练的更大模型在某些任务中同样有效。
- en: The title "Textbooks Are All You Need" is a clever reference to the well-known
    concept in AI "Attention is All You Need." But here they flip the idea - rather
    than focusing on the model architecture itself, they show the value of high-quality,
    curated training data like you'd find in a textbook.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 标题“**教科书就是你所需的一切**”巧妙地参考了 AI 中著名的概念“**注意力即一切**”。但这里他们颠覆了这个想法——与其关注模型架构本身，他们展示了像教科书中那样的高质量、精心策划的训练数据的价值。
- en: The key insight is that a thoughtful, well-designed dataset can be just as useful
    as enormous, unfocused piles of data for teaching an AI model. So the researchers
    put together a synthetic textbook to carefully feed the model the knowledge it
    needed.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 关键见解是，一个经过深思熟虑、设计良好的数据集可以与庞大、无焦点的数据堆一样有用。因此，研究人员精心编制了一本合成教科书，以细致地提供模型所需的知识。
- en: This textbook-based approach is an intriguing new direction for efficiently
    training AI models to excel at specific tasks. It highlights the importance of
    training data curation and quality over just brute force data size.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 基于教科书的方法是一种引人入胜的新方向，可以有效地培训 AI 模型，使其在特定任务中表现出色。这突出了训练数据的策划和质量的重要性，而不仅仅是数据量的暴力方式。
- en: Key Points
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键点
- en: The Phi-1 model, despite being significantly smaller than models like GPT-3,
    performs impressively well in Python coding tasks. This demonstrates that size
    isn't everything when it comes to AI models.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管 Phi-1 模型显著小于 GPT-3 等模型，但在 Python 编码任务中表现优异。这表明，模型的大小并非决定 AI 模型表现的唯一因素。
- en: The researchers used a synthetic textbook for training, emphasizing the importance
    of high-quality, well-curated data. This approach could revolutionize how we think
    about training AI models.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究人员使用了合成教科书进行培训，强调了高质量、精心策划的数据的重要性。这种方法可能会颠覆我们对 AI 模型培训的思考方式。
- en: The Phi-1 model's performance improved significantly when fine-tuned with synthetic
    exercises and solutions, indicating that targeted fine-tuning can enhance a model's
    capabilities beyond the tasks it was specifically trained for.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当对 Phi-1 模型进行合成练习和解答的微调时，其性能显著提高，这表明有针对性的微调可以增强模型在特定任务之外的能力。
- en: Discussion
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论
- en: The Phi-1 model, with 1.3 billion parameters, is relatively small compared to
    models like GPT-3, which has 175 billion parameters. Despite this size difference,
    Phi-1 demonstrates impressive performance in Python coding tasks. This achievement
    underscores the idea that the quality of training data can be as important, if
    not more so, than the size of the model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Phi-1 模型拥有 13 亿个参数，相较于拥有 1750 亿个参数的 GPT-3 模型来说相对较小。尽管存在尺寸差异，Phi-1 在 Python 编码任务中表现出色。这一成就突显了训练数据的质量可能与模型的大小一样重要，甚至更为重要。
- en: The researchers used a synthetic textbook to train the Phi-1 model. This textbook
    was generated using GPT-3.5 and was composed of Python text and exercises. The
    use of a synthetic textbook emphasizes the importance of high-quality, well-curated
    data in training AI models. This approach could potentially shift the focus in
    AI training from creating larger models to curating better training data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员使用合成教材来训练 Phi-1 模型。这本教材是利用 GPT-3.5 生成的，包含了 Python 语言文本和练习。使用合成教材强调了高质量、精心策划的数据在
    AI 模型训练中的重要性。这种方法可能会将 AI 训练的重点从创建更大的模型转移到策划更好的训练数据上。
- en: Interestingly, the Phi-1 model's performance improved significantly when it
    was fine-tuned with synthetic exercises and solutions. This improvement was not
    limited to the tasks it was specifically trained for. For example, the model's
    ability to use external libraries like pygame improved, even though these libraries
    were not included in the training data. This suggests that fine-tuning can enhance
    a model's capabilities beyond the tasks it was specifically trained for.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，Phi-1 模型在用合成练习和解决方案进行微调后，其性能显著提高。这种改进不仅限于它专门训练的任务。例如，尽管训练数据中未包含 pygame
    这类外部库，模型在使用这些库的能力上有所提升。这表明微调可以增强模型在特定任务之外的能力。
- en: Research Q&A
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 研究问答
- en: '**Q: How does the Phi-1 model compare to larger models in terms of versatility?**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**问：Phi-1 模型在多样性方面与较大的模型相比如何？**'
- en: 'A: The Phi-1 model is specialized in Python coding, which restricts its versatility
    compared to multi-language models. It also lacks the domain-specific knowledge
    of larger models, such as programming with specific APIs or using less common
    packages.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 答：Phi-1 模型专注于 Python 编程，这限制了它的多样性，相比于多语言模型，其在特定 API 编程或使用不常见的包方面的领域知识也较少。
- en: '**Q: How does the Phi-1 model handle stylistic variations or errors in the
    prompt?**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**问：Phi-1 模型如何处理提示中的风格变化或错误？**'
- en: 'A: Due to the structured nature of the datasets and the lack of diversity in
    terms of language and style, the Phi-1 model is less robust to stylistic variations
    or errors in the prompt. If there''s a grammatical mistake in the prompt, the
    model''s performance decreases.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 答：由于数据集的结构化性质以及语言和风格的缺乏多样性，Phi-1 模型对风格变化或提示中的错误的鲁棒性较差。如果提示中存在语法错误，模型的表现会下降。
- en: '**Q: Could the Phi-1 model''s performance improve with the use of GPT-4 for
    generating synthetic data?**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**问：Phi-1 模型的性能是否可以通过使用 GPT-4 生成合成数据来提高？**'
- en: 'A: Yes, the researchers believe that significant gains could be achieved by
    using GPT-4 to generate synthetic data instead of GPT-3.5\. However, GPT-4 is
    currently slower and more expensive to use.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 答：是的，研究人员认为通过使用 GPT-4 生成合成数据而非 GPT-3.5，可以取得显著的成果。然而，GPT-4 目前使用起来较慢且费用更高。
- en: '**Q: How does the Phi-1 model''s approach to training differ from traditional
    methods?**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**问：Phi-1 模型的训练方法与传统方法有何不同？**'
- en: 'A: Traditional methods often focus on increasing the size of the model and
    the amount of data. In contrast, the Phi-1 model emphasizes the quality of the
    data and uses a synthetic textbook for training. This approach could potentially
    shift the focus in AI training from creating larger models to curating better
    training data.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 答：传统方法通常关注于增加模型的规模和数据量。与此不同，Phi-1 模型强调数据的质量，并使用合成教材进行训练。这种方法可能会将 AI 训练的重点从创建更大的模型转移到策划更好的训练数据上。
- en: Research Takeaways
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 研究总结
- en: Microsoft Research's "Textbooks Are All You Need" has a rather novel idea for
    training AI models. Instead of just throwing massive piles of data at the model
    like usual, they created a synthetic textbook to teach the model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 微软研究的“教材即一切”在训练 AI 模型方面有一个相当新颖的想法。他们没有像通常那样将大量数据抛给模型，而是创建了一本合成教材来教模型。
- en: They trained this smaller model called Phi-1 only using this custom textbook,
    and it worked shockingly well compared to huge models like GPT-3\. It shows that
    you can train a really effective AI with a thoughtfully designed, high-quality
    dataset, even if it's way smaller.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 他们仅使用这本定制教材训练了一个名为 Phi-1 的较小模型，相比于像 GPT-3 这样的庞大模型，它的效果令人震惊。这表明，即使数据集规模较小，只要设计得当、质量高，也能训练出非常有效的
    AI。
- en: The key is taking the time to curate great training data, like you'd find in
    a textbook, instead of just feeding the model terabytes of random, messy data.
    It's all about the quality, not quantity.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于花时间策划优秀的训练数据，就像你在教科书中看到的一样，而不仅仅是将模型喂入数TB的随机杂乱数据。这一切都是关于质量，而不是数量。
- en: This could change how people think about training AI going forward. Rather than
    chasing ever-bigger models that need giant datasets, maybe we should focus more
    on creating the best possible training textbooks, even if they're smaller. It's
    an intriguing idea that the key is in the textbook, not just in scaling up the
    model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会改变人们对未来AI训练的看法。与其追求需要巨大数据集的越来越大模型，不如更多地关注创造最佳的训练教材，即使它们更小。这是一个引人入胜的想法，关键在于教材，而不仅仅是模型的扩展。
- en: '**[Matthew Mayo](https://www.linkedin.com/in/mattmayo13/)** ([**@mattmayo13**](https://twitter.com/mattmayo13))
    is a Data Scientist and the Editor-in-Chief of KDnuggets, the seminal online Data
    Science and Machine Learning resource. His interests lie in natural language processing,
    algorithm design and optimization, unsupervised learning, neural networks, and
    automated approaches to machine learning. Matthew holds a Master''s degree in
    computer science and a graduate diploma in data mining. He can be reached at editor1
    at kdnuggets[dot]com.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**[马修·梅奥](https://www.linkedin.com/in/mattmayo13/)** ([**@mattmayo13**](https://twitter.com/mattmayo13))
    是一名数据科学家，也是KDnuggets的总编辑，KDnuggets是重要的数据科学和机器学习在线资源。他的兴趣包括自然语言处理、算法设计与优化、无监督学习、神经网络以及自动化机器学习方法。马修拥有计算机科学硕士学位和数据挖掘研究生文凭。他可以通过editor1
    at kdnuggets[dot]com联系。'
- en: More On This Topic
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Synapse CoR: ChatGPT with a Revolutionary Twist](https://www.kdnuggets.com/synapse-cor-chatgpt-with-a-revolutionary-twist)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Synapse CoR：具有革命性转折的ChatGPT](https://www.kdnuggets.com/synapse-cor-chatgpt-with-a-revolutionary-twist)'
- en: '[5 Skills All Marketing Analytics and Data Science Pros Need Today](https://www.kdnuggets.com/2023/08/mads-5-skills-marketing-analytics-data-science-pros-need-today.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[今天所有营销分析和数据科学专业人士需要的5项技能](https://www.kdnuggets.com/2023/08/mads-5-skills-marketing-analytics-data-science-pros-need-today.html)'
- en: '[If You Want to Master Generative AI, Ignore All (But Two) Tools](https://www.kdnuggets.com/if-you-want-to-master-generative-ai-ignore-all-but-two-tools)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如果你想掌握生成性AI，忽略所有（但两个）工具](https://www.kdnuggets.com/if-you-want-to-master-generative-ai-ignore-all-but-two-tools)'
- en: '[We Don''t Need Data Scientists, We Need Data Engineers](https://www.kdnuggets.com/2021/02/dont-need-data-scientists-need-data-engineers.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我们不需要数据科学家，我们需要数据工程师](https://www.kdnuggets.com/2021/02/dont-need-data-scientists-need-data-engineers.html)'
- en: '[A Guide On How To Become A Data Scientist (Step By Step Approach)](https://www.kdnuggets.com/2021/05/guide-become-data-scientist.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何一步步成为数据科学家指南](https://www.kdnuggets.com/2021/05/guide-become-data-scientist.html)'
- en: '[Support Vector Machines: An Intuitive Approach](https://www.kdnuggets.com/2022/08/support-vector-machines-intuitive-approach.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[支持向量机：直观方法](https://www.kdnuggets.com/2022/08/support-vector-machines-intuitive-approach.html)'
