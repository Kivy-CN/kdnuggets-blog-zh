# 训练 sklearn 快速 100 倍

> 原文：[https://www.kdnuggets.com/2019/09/train-sklearn-100x-faster.html](https://www.kdnuggets.com/2019/09/train-sklearn-100x-faster.html)

[评论](#comments)

**作者：[Evan Harris](https://www.linkedin.com/in/evan-harris-387375b2/)，Ibotta, Inc. 机器学习与数据科学经理**

在 Ibotta，我们训练了大量的机器学习模型。这些模型为我们的推荐系统、搜索引擎、定价优化引擎、数据质量等提供支持。它们为数百万用户在使用我们的移动应用程序时进行预测。

虽然我们在数据处理方面大多使用[Spark](https://spark.apache.org/)，但我们首选的机器学习框架是[scikit-learn](https://scikit-learn.org/stable/)。随着计算成本的下降和机器学习解决方案市场时间的日益重要，我们探索了加速模型训练的选项。我们的一种解决方案是将 Spark 和 scikit-learn 的元素结合成我们自己的混合解决方案。

### 介绍 sk-dist

我们很高兴地宣布我们开源项目[sk-dist](https://github.com/Ibotta/sk-dist)的发布。该项目的目标是提供一个通用框架，用于将 scikit-learn 元估计器与 Spark 进行分发。元估计器的示例包括决策树集成（[随机森林](https://en.wikipedia.org/wiki/Random_forest) 和 [额外随机树](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)），[超参数调优器](https://scikit-learn.org/stable/modules/grid_search.html)（[网格搜索](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) 和 [随机搜索](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV)）以及多类技术（[一对其余](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier) 和 [一对一](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html#sklearn.multiclass.OneVsOneClassifier)）。

![](../Images/4df608e27f8783fb241b1762eed6f489.png)

我们的主要动机是填补传统机器学习模型分发选项中的空白。在神经网络和深度学习领域之外，我们发现，训练模型的大部分计算时间并非用于在单一数据集上训练单个模型。大量时间用于在多个数据集的多个迭代上训练模型，通常使用像网格搜索或集成方法这样的元估计器。

### 示例

考虑 [手写数字数据集](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html)。我们这里有编码好的手写数字图像需要进行分类。我们可以在这个包含 1797 条记录的数据集上很快地在单台机器上训练一个 [支持向量机](https://en.wikipedia.org/wiki/Support-vector_machine)。这只需不到一秒钟。但超参数调整需要在训练数据的不同子集上进行多个训练作业。

如下所示，我们构建了一个总计需要 1050 个训练作业的参数网格。在拥有百余个核心的 Spark 集群上，使用 sk-dist 只需 3.4 秒。这项工作的总任务时间为 7.2 分钟，这意味着在没有并行化的情况下，在单台机器上训练需要这么长时间。

```py
import timefrom sklearn import datasets, svm
from skdist.distribute.search import DistGridSearchCV
from pyspark.sql import SparkSession # instantiate spark session
spark = (   
    SparkSession    
    .builder    
    .getOrCreate()    
    )
sc = spark.sparkContext # the digits dataset
digits = datasets.load_digits()
X = digits["data"]
y = digits["target"] # create a classifier: a support vector classifier
classifier = svm.SVC()
param_grid = {
    "C": [0.01, 0.01, 0.1, 1.0, 10.0, 20.0, 50.0], 
    "gamma": ["scale", "auto", 0.001, 0.01, 0.1], 
    "kernel": ["rbf", "poly", "sigmoid"]
    }
scoring = "f1_weighted"
cv = 10# hyperparameter optimization
start = time.time()
model = DistGridSearchCV(    
    classifier, param_grid,     
    sc=sc, cv=cv, scoring=scoring,
    verbose=True    
    )
model.fit(X,y)
print("Train time: {0}".format(time.time() - start))
print("Best score: {0}".format(model.best_score_))------------------------------
Spark context found; running with spark
Fitting 10 folds for each of 105 candidates, totalling 1050 fits
Train time: 3.380601406097412
Best score: 0.981450024203508
```

这个例子展示了一个常见的场景，即将数据适配到内存中并训练单个分类器是简单的，但超参数调整所需的拟合次数会迅速增加。下面是使用 sk-dist 运行如上例所示的网格搜索问题的幕后情况：

![图示](../Images/282b2a6db0d1e7459055893272e39ba7.png)

使用 sk-dist 的网格搜索

在 Ibotta 进行传统机器学习的实际应用中，我们常常发现自己处于类似的情况：中小型数据（100k 到 1M 条记录），需要对多个简单分类器进行超参数调整、集成和多类解决方案的多个迭代。

### 现有解决方案

有现有的解决方案来分布式训练传统机器学习元估计器。第一个是最简单的：scikit-learn 内置的使用 [joblib](https://joblib.readthedocs.io/en/latest/) 的元估计器并行化。这与 sk-dist 的操作非常相似，只是有一个主要限制：性能受限于任何一台机器的资源。即使与理论上的单台拥有数百个核心的机器相比，Spark 仍具有如 [执行器的精细内存规格](https://spark.apache.org/docs/latest/tuning.html)、[容错](https://techvidvan.com/tutorials/fault-tolerance-in-spark/) 以及使用 [竞价实例](https://aws.amazon.com/ec2/spot/) 作为工作节点的成本控制选项等优势。

另一种现有的解决方案是 [Spark ML](https://spark.apache.org/docs/latest/ml-guide.html)。这是 Spark 的原生机器学习库，支持许多与 scikit-learn 相同的分类和回归算法。它还具有如树集成和网格搜索等元估计器，并支持多类问题。尽管这听起来可能是分布式 scikit-learn 风格机器学习工作负载的充分解决方案，但它以一种不能解决我们感兴趣的并行性问题的方式进行训练分配。

![图示](../Images/3fcf7a970a8832f2998bd8ca037f6870.png)

不同维度的分布

如上所示，Spark ML 会在分布在多个执行器上的数据上训练一个单一模型。当数据量很大且无法在单台机器上容纳时，这种方法效果很好。然而，当数据量较小时，这种方法可能会*不如*单台机器上的 scikit-learn。进一步地，例如，在训练随机森林时，Spark ML 会按顺序训练每棵决策树。这个任务的实际时间将随着决策树数量的增加而线性增长，无论分配给该任务的资源是多少。

对于[网格搜索，Spark ML](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator)确实实现了一个[并行度](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator.parallelism)参数，该参数将[并行训练单独模型](https://issues.apache.org/jira/browse/SPARK-19357)。然而，每个单独的模型仍然在分布在执行器上的数据上进行训练。该任务的总并行度是[其可能达到的水平的一个小部分](https://github.com/Ibotta/sk-dist/blob/master/examples/search/spark_ml.py)，如果按模型维度而不是数据维度进行分发的话。

最终，我们希望在不同于 Spark ML 的维度上分发我们的训练。当使用小数据或中等数据时，将数据适配到内存中不是问题。以随机森林为例，我们希望将训练数据完整地广播到每个执行器，在每个执行器上拟合一个独立的决策树，然后将这些拟合好的决策树带回驱动程序以组装随机森林。在这个维度上进行分发可以比将数据分发和按顺序训练决策树要[快几个数量级](https://github.com/Ibotta/sk-dist/blob/master/examples/search/spark_ml.py)。这种行为对于网格搜索和多类等其他元估计器技术也类似。

### 特性

鉴于现有解决方案在我们问题领域中的局限性，我们决定内部开发 sk-dist。底线是我们想要**分发模型，而不是数据**。

虽然主要关注于元估计器的分布式训练，但 sk-dist 还包括用于与 Spark 一起使用的 scikit-learn 模型的分布式预测模块、用于不使用 Spark 的几个预处理/后处理 scikit-learn 转换器，以及一个灵活的特征编码器，适用于有或没有 Spark 的情况。

+   **分布式训练** — 使用 Spark 分发元估计器训练。支持以下算法：[超参数优化](https://github.com/Ibotta/sk-dist/blob/master/skdist/distribute/search.py)（包括网格搜索和随机搜索）、[树集成](https://github.com/Ibotta/sk-dist/blob/master/skdist/distribute/ensemble.py)（包括随机森林、额外树和随机树嵌入）以及[多类策略](https://github.com/Ibotta/sk-dist/blob/master/skdist/distribute/multiclass.py)（包括一对多和一对一）。所有这些元估计器在拟合后都能镜像其 scikit-learn 对应物。

+   **分布式预测** — [分布式预测方法](https://github.com/Ibotta/sk-dist/blob/master/skdist/distribute/predict.py) 可以使用[Spark DataFrames](https://spark.apache.org/docs/latest/sql-programming-guide.html)对拟合的 scikit-learn 估计器进行预测。这使得可以使用或不使用 Spark 的便携式 scikit-learn 估计器进行大规模分布式预测。

+   **特征编码** — 使用一种名为[Encoderizer](https://github.com/Ibotta/sk-dist/blob/master/skdist/distribute/encoder.py)的灵活特征转换器进行特征编码分发。它可以在有或没有 Spark 并行化的情况下运行。它将推断数据类型和形状，自动应用默认特征转换器，作为标准特征编码技术的最佳猜测实现。它还可以作为一个完全可定制的类似特征联合的编码器使用，具有通过 Spark 分布式转换器拟合的额外优势。

### 使用案例

这里有一些指南来决定 sk-dist 是否适合你的机器学习问题领域：

1.  **传统机器学习** — 方法如[广义线性模型](https://scikit-learn.org/stable/modules/linear_model.html)、[随机梯度下降](https://scikit-learn.org/stable/modules/sgd.html)、[最近邻](https://scikit-learn.org/stable/modules/neighbors.html)、[决策树](https://scikit-learn.org/stable/modules/tree.html)和[朴素贝叶斯](https://scikit-learn.org/stable/modules/naive_bayes.html)与 sk-dist 兼容良好。这些方法在 scikit-learn 中都有实现，已准备好与 sk-dist 元估计器直接实施。

1.  **小到中型数据** — 大数据不适用于 sk-dist。请记住，训练的分布维度沿着模型的轴，而不是数据的轴。数据不仅需要适合每个执行器的内存，还需要足够小以便进行广播。根据 Spark 配置，最大广播大小可能会成为限制因素。

1.  **Spark 定向和访问** — sk-dist 的核心功能需要运行 Spark。这对于个人或小型数据科学团队来说并不总是可行的。此外，为了以经济高效的方式充分利用 sk-dist，还需要进行一些 Spark 调优和配置，这需要一些 Spark 基础知识的培训。

需要注意的是，虽然神经网络和深度学习技术可以与 sk-dist 一起使用，但这些技术需要大量的训练数据，有时还需要专门的基础设施才能有效。深度学习不是 sk-dist 的预期用途，因为它违背了上述 (1) 和 (2) 点。作为替代方案，在 Ibotta [我们一直在使用](https://medium.com/building-ibotta/heating-up-word2vec-blazingtext-for-real-time-search-c2121bd1396) [Amazon SageMaker](https://aws.amazon.com/sagemaker/) 进行这些技术应用，我们发现它比 Spark 更有效。

### 入门指南

要开始使用 sk-dist，请查看 [安装指南](https://github.com/Ibotta/sk-dist#installation)。代码库还包含一个 [示例](https://github.com/Ibotta/sk-dist/tree/master/examples) 库，展示 sk-dist 的一些用例。 [欢迎大家](https://github.com/Ibotta/sk-dist/blob/master/CODE_OF_CONDUCT.md) 提交问题并参与项目。

如果这些项目和挑战对你有吸引力，Ibotta 正在招聘！查看我们的 [招聘页面](https://ibotta.com/careers) 获取更多信息。

感谢 Charley Frazier、Chad Foley 和 Sam Weiss。

**简介：[Evan Harris](https://www.linkedin.com/in/evan-harris-387375b2/)** 是一位经验丰富的技术专家，专注于数据科学和机器学习。他目前领导一个机器学习团队，致力于为搜索相关性、内容推荐、定价优化和欺诈检测构建服务，为数百万用户提供移动应用支持。

[原文](https://medium.com/building-ibotta/train-sklearn-100x-faster-bec530fc1f45)。转载已获许可。

**相关：**

+   [在Python中理解分类决策树](/2019/08/understanding-decision-trees-classification-python.html)

+   [高级 Keras — 准确恢复训练过程](/2019/03/advanced-keras-accurately-resuming-training-process.html)

+   [分布式人工智能：多智能体系统、基于代理的建模和群体智能入门](/2019/04/distributed-artificial-intelligence-multi-agent-systems-agent-based-modeling-swarm-intelligence.html)

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速通道进入网络安全职业。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的 IT

* * *

### 更多相关话题

+   [停止学习数据科学以寻找目标，并通过寻找目标来…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [一个90亿美元的人工智能失败，经过审视](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)

+   [数据科学学习统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)

+   [成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)

+   [是什么让Python成为初创企业理想的编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)

+   [每个数据科学家都应该知道的三个R库（即使你使用Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)
