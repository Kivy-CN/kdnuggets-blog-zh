- en: Introduction to k-Nearest Neighbors
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-近邻简介
- en: 原文：[https://www.kdnuggets.com/2018/03/introduction-k-nearest-neighbors.html](https://www.kdnuggets.com/2018/03/introduction-k-nearest-neighbors.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/03/introduction-k-nearest-neighbors.html](https://www.kdnuggets.com/2018/03/introduction-k-nearest-neighbors.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Devin Soni](https://www.linkedin.com/in/devinsoni/), Computer Science
    Student**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者 [Devin Soni](https://www.linkedin.com/in/devinsoni/)，计算机科学学生**'
- en: '![KNN](../Images/f5fc732a89c1efe810602d5129c0b000.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![KNN](../Images/f5fc732a89c1efe810602d5129c0b000.png)'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The** k-Nearest-Neighbors (kNN)** method of classification is one of the simplest
    methods in machine learning, and is a great way to introduce yourself to machine
    learning and classification in general.At its most basic level, it is essentially
    classification by finding the most similar data points in the training data, and
    making an educated guess based on their classifications. Although very simple
    to understand and implement, this method has seen wide application in many domains,
    such as in **recommendation systems**,** semantic searching**, and** anomaly detection**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**k-近邻 (kNN)** 分类方法是机器学习中最简单的方法之一，也是入门机器学习和分类的绝佳方式。在其最基本的层面上，它本质上是通过查找训练数据中最相似的数据点进行分类，并根据这些数据点的分类做出有根据的猜测。尽管这种方法非常简单易懂和实现，但它在许多领域中得到了广泛应用，如**推荐系统**、**语义搜索**和**异常检测**。'
- en: As we would need to in any machine learning problem, we must first find a way
    to represent data points as **feature vectors**. A feature vector is our mathematical
    representation of data, and since the desired characteristics of our data may
    not be inherently numerical, preprocessing and feature-engineering may be required
    in order to create these vectors. Given data with **N** unique features, the feature
    vector would be a vector of length** N**, where entry **I** of the vector represents
    that data point’s value for feature** I**. Each feature vector can thus be thought
    of as a point in **R^N**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在任何机器学习问题中需要做的那样，我们必须首先找到一种方法将数据点表示为**特征向量**。特征向量是数据的数学表示，由于数据的期望特征可能不具有内在的数值性质，因此可能需要预处理和特征工程来创建这些向量。对于具有**N**个唯一特征的数据，特征向量将是长度为**N**的向量，其中向量的**I**项表示该数据点在特征**I**上的值。因此，每个特征向量可以被视为**R^N**中的一个点。
- en: Now, unlike most other methods of classification, kNN falls under **lazy learning**,
    which means that there is **no explicit training phase before classification**.
    Instead, any attempts to generalize or abstract the data is made upon classification.
    While this does mean that we can immediately begin classifying once we have our
    data, there are some inherent problems with this type of algorithm. We must be
    able to keep the entire training set in memory unless we apply some type of reduction
    to the data-set, and performing classifications can be computationally expensive
    as the algorithm parse through all data points for each classification. **For
    these reasons, kNN tends to work best on smaller data-sets that do not have many
    features.**
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，与大多数其他分类方法不同，kNN 属于**惰性学习**，这意味着在分类之前**没有明确的训练阶段**。相反，对数据的任何尝试进行概括或抽象都是在分类时进行的。虽然这意味着我们可以在获取数据后立即开始分类，但这种算法存在一些固有问题。我们必须能够将整个训练集保存在内存中，除非我们对数据集进行某种类型的缩减，否则执行分类可能会很耗费计算资源，因为算法会遍历所有数据点进行每次分类。**由于这些原因，kNN
    在特征较少的小型数据集上效果最佳。**
- en: 'Once we have formed our training data-set, which is represented as an **M** x** N**
    matrix where **M** is the number of data points and **N** is the number of features,
    we can now begin classifying. The gist of the kNN method is, for each classification
    query, to:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们形成了训练数据集，该数据集表示为**M** x**N** 矩阵，其中**M** 是数据点的数量，**N** 是特征的数量，我们现在可以开始分类。kNN
    方法的要点是，对于每个分类查询：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: There are two important decisions that must be made before making classifications.
    One is the value of **k** that will be used; this can either be decided arbitrarily,
    or you can try **cross-validation** to find an optimal value. The next, and the
    most complex, is the** distance metric** that will be used.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行分类之前，必须做出两个重要决定。一个是将要使用的**k**值；这可以是任意决定的，或者你可以尝试**交叉验证**以找到最佳值。下一个也是最复杂的，是将使用的**距离度量**。
- en: There are many different ways to compute distance, as it is a fairly ambiguous
    notion, and the proper metric to use is always going to be determined by the data-set
    and the classification task. Two popular ones, however, are **Euclidean distance **and** Cosine
    similarity**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 计算距离有很多不同的方法，因为这是一个相当模糊的概念，使用的适当度量标准总是由数据集和分类任务决定。然而，**欧几里得距离** 和 **余弦相似度**
    是两种流行的方法。
- en: Euclidean distance is probably the one that you are most familiar with; it is
    essentially the magnitude of the vector obtained by subtracting the training data
    point from the point to be classified.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离可能是你最熟悉的一种；它本质上是通过将训练数据点从待分类点中减去得到的向量的大小。
- en: '![](../Images/d05ff37f97ea122929ddf4ed4524f14c.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d05ff37f97ea122929ddf4ed4524f14c.png)'
- en: General formula for Euclidean distance
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离的一般公式
- en: Another common metric is Cosine similarity. Rather than calculating a magnitude,
    Cosine similarity instead uses the difference in direction between two vectors.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的度量标准是余弦相似度。余弦相似度不是计算大小，而是使用两个向量之间的方向差异。
- en: '![](../Images/ebfcde9367ef46848dd87807d63e0a99.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebfcde9367ef46848dd87807d63e0a99.png)'
- en: General formula for Cosine similarity
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度的一般公式
- en: Choosing a metric can often be tricky, and it may be best to just use cross-validation
    to decide, unless you have some prior insight that clearly leads to using one
    over the other. For example, for something like word vectors, you may want to
    use Cosine similarity because the direction of a word is more meaningful than
    the sizes of the component values. Generally, both of these methods will run in
    roughly the same time, and will suffer from highly-dimensional data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个度量标准通常是棘手的，最好是使用交叉验证来决定，除非你有一些先验的见解可以明确指导使用一个而不是另一个。例如，对于像词向量这样的情况，你可能会想使用余弦相似度，因为词的方向比成分值的大小更有意义。一般来说，这两种方法的运行时间大致相同，并且都会受到高维数据的影响。
- en: After doing all of the above and deciding on a metric, the result of the kNN
    algorithm is a decision boundary that partitions **R^N** into sections. Each section
    (colored distinctly below) represents a class in the classification problem. The
    boundaries need not be formed with actual training examples — they are instead
    calculated using the distance metric and the available training points. By taking **R^N** in
    (small) chunks, we can calculate the most likely class for a hypothetical data-point
    in that region, and we thus color that chunk as being in the region for that class.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成上述所有操作并决定度量标准之后，kNN 算法的结果是一个决策边界，将**R^N**划分为若干部分。每个部分（下图中颜色不同）表示分类问题中的一个类别。边界不一定由实际的训练样本形成——它们是通过使用距离度量和可用的训练点来计算的。通过将**R^N**分成（小）块，我们可以计算该区域中一个假设数据点最可能的类别，从而将该块标记为该类别的区域。
- en: '![](../Images/eb0c8d05603c0f84b9aaa1034acdbd3b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb0c8d05603c0f84b9aaa1034acdbd3b.png)'
- en: This information is all that is needed to begin implementing the algorithm,
    and doing so should be relatively simple. There are, of course, many ways to improve
    upon this base algorithm. Common modifications include weighting, and specific
    preprocessing to reduce computation and reduce noise, such as various algorithms
    for feature extraction and dimension reduction. Additionally, the kNN method has
    also been used, although less-commonly, for regression tasks, and operates in
    a manner very similar to that of the classifier through averaging.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息就是开始实现算法所需的全部，进行实现应该相对简单。当然，还有很多方法可以改进这个基础算法。常见的修改包括加权，以及特定的预处理以减少计算和减少噪声，比如各种特征提取和降维算法。此外，kNN
    方法也被用于回归任务，尽管不那么常见，其操作方式与分类器类似，通过平均来进行。
- en: '**Bio: [Devin Soni](https://www.linkedin.com/in/devinsoni/)** is a computer
    science student interested in machine learning and data science. He will be a
    software engineering intern at Airbnb in 2018\. He can be reached [via LinkedIn](https://www.linkedin.com/in/devinsoni/).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介：[Devin Soni](https://www.linkedin.com/in/devinsoni/)** 是一名计算机科学学生，对机器学习和数据科学感兴趣。他将在2018年成为Airbnb的一个软件工程实习生。可以通过[LinkedIn](https://www.linkedin.com/in/devinsoni/)联系他。'
- en: '[Original](https://towardsdatascience.com/introduction-to-k-nearest-neighbors-3b534bb11d26).
    Reposted with permission.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/introduction-to-k-nearest-neighbors-3b534bb11d26)。经许可转载。'
- en: '**Related:**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Introduction to Markov Chains](/2018/03/introduction-markov-chains.html)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[马尔可夫链简介](/2018/03/introduction-markov-chains.html)'
- en: '[A Tour of The Top 10 Algorithms for Machine Learning Newbies](/2018/02/tour-top-10-algorithms-machine-learning-newbies.html)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习新手的前10大算法巡礼](/2018/02/tour-top-10-algorithms-machine-learning-newbies.html)'
- en: '[K-Nearest Neighbors – the Laziest Machine Learning Technique](/2017/09/rapidminer-k-nearest-neighbors-laziest-machine-learning-technique.html)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[K-最近邻——最懒的机器学习技术](/2017/09/rapidminer-k-nearest-neighbors-laziest-machine-learning-technique.html)'
- en: More On This Topic
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[From Theory to Practice: Building a k-Nearest Neighbors Classifier](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从理论到实践：构建一个k-最近邻分类器](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
- en: '[Nearest Neighbors for Classification](https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类的最近邻](https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html)'
- en: '[K-nearest Neighbors in Scikit-learn](https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Scikit-learn中的K-最近邻](https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html)'
- en: '[Introduction to Binary Classification with PyCaret](https://www.kdnuggets.com/2021/12/introduction-binary-classification-pycaret.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用PyCaret进行二分类简介](https://www.kdnuggets.com/2021/12/introduction-binary-classification-pycaret.html)'
- en: '[Introduction to Clustering in Python with PyCaret](https://www.kdnuggets.com/2021/12/introduction-clustering-python-pycaret.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用PyCaret在Python中进行聚类简介](https://www.kdnuggets.com/2021/12/introduction-clustering-python-pycaret.html)'
- en: '[Essential Math for Data Science: Visual Introduction to Singular…](https://www.kdnuggets.com/2022/06/essential-math-data-science-visual-introduction-singular-value-decomposition.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学的基础数学：奇异值分解的视觉介绍](https://www.kdnuggets.com/2022/06/essential-math-data-science-visual-introduction-singular-value-decomposition.html)'
