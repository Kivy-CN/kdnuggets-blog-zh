["```py\nfrom pyspark import SparkContext\nimport numpy as np\nsc=SparkContext(master=\"local[4]\")\nlst=np.random.randint(0,10,20)\nA=sc.parallelize(lst)\n\n```", "```py\ntype(A)\n>> pyspark.rdd.RDD\n```", "```py\nA.collect()\n>> [4, 8, 2, 2, 4, 7, 0, 3, 3, 9, 2, 6, 0, 0, 1, 7, 5, 1, 9, 7]\n```", "```py\nA.glom().collect()\n>> [[4, 8, 2, 2, 4], [7, 0, 3, 3, 9], [2, 6, 0, 0, 1], [7, 5, 1, 9, 7]]\n```", "```py\nsc.stop()\nsc=SparkContext(master=\"local[2]\")\nA = sc.parallelize(lst)\nA.glom().collect()\n>> [[4, 8, 2, 2, 4, 7, 0, 3, 3, 9], [2, 6, 0, 0, 1, 7, 5, 1, 9, 7]]\n```", "```py\n>> 20\n```", "```py\nA.first()\n>> 4\nA.take(3)\n>> [4, 8, 2]\n```", "```py\nA_distinct=A.distinct()\nA_distinct.collect()\n>> [4, 8, 0, 9, 1, 5, 2, 6, 7, 3]\n```", "```py\nA.reduce(lambda x,y:x+y)\n>> 80\n```", "```py\nA.sum()\n>> 80\n```", "```py\nA.reduce(lambda x,y: x if x > y else y)\n>> 9\n```", "```py\nwords = 'These are some of the best Macintosh computers ever'.split(' ')\nwordRDD = sc.parallelize(words)\nwordRDD.reduce(lambda w,v: w if len(w)>len(v) else v)\n>> 'computers'\n```", "```py\n# Return RDD with elements (greater than zero) divisible by 3\nA.filter(lambda x:x%3==0 and x!=0).collect()\n>> [3, 3, 9, 6, 9]\n```", "```py\ndef largerThan(x,y):\n  \"\"\"\n  Returns the last word among the longest words in a list\n  \"\"\"\n  if len(x)> len(y):\n    return x\n  elif len(y) > len(x):\n    return y\n  else:\n    if x < y: return x\n    else: return y\n\nwordRDD.reduce(largerThan)\n>> 'Macintosh'\n```", "```py\nB=A.map(lambda x:x*x)\nB.collect()\n>> [16, 64, 4, 4, 16, 49, 0, 9, 9, 81, 4, 36, 0, 0, 1, 49, 25, 1, 81, 49]\n```", "```py\ndef square_if_odd(x):\n  \"\"\"\n  Squares if odd, otherwise keeps the argument unchanged\n  \"\"\"\n  if x%2==1:\n    return x*x\n  else:\n    return x\n\nA.map(square_if_odd).collect()\n>> [4, 8, 2, 2, 4, 49, 0, 9, 9, 81, 2, 6, 0, 0, 1, 49, 25, 1, 81, 49]\n```", "```py\nresult=A.groupBy(lambda x:x%2).collect()\nsorted([(x, sorted(y)) for (x, y) in result])\n>> [(0, [0, 0, 0, 2, 2, 2, 4, 4, 6, 8]), (1, [1, 1, 3, 3, 5, 7, 7, 7, 9, 9])]\n```", "```py\nB.histogram([x for x in range(0,100,10)])\n>> ([0, 10, 20, 30, 40, 50, 60, 70, 80, 90], [10, 2, 1, 1, 3, 0, 1, 0, 2])\n```", "```py\nsc = SparkContext(master=\"local[2]\")\n```", "```py\n%%time\nrdd1 = sc.parallelize(range(1000000))\n>> CPU times: user 316 µs, sys: 5.13 ms, total: 5.45 ms, Wall time: 24.6 ms\n```", "```py\nfrom math import cos\ndef taketime(x):\n[cos(j) for j in range(100)]\nreturn cos(x)\n```", "```py\n%%time\ntaketime(2)\n>> CPU times: user 21 µs, sys: 7 µs, total: 28 µs, Wall time: 31.5 µs\n>> -0.4161468365471424\n```", "```py\n%%time\ninterim = rdd1.map(lambda x: taketime(x))\n>> CPU times: user 23 µs, sys: 8 µs, total: 31 µs, Wall time: 34.8 µs\n```", "```py\n%%time\nprint('output =',interim.reduce(lambda x,y:x+y))\n>> output = -0.28870546796843666\n>> CPU times: user 11.6 ms, sys: 5.56 ms, total: 17.2 ms, Wall time: 15.6 s\n```", "```py\n%%time\nprint(interim.filter(lambda x:x>0).count())\n>> 500000\n>> CPU times: user 10.6 ms, sys: 8.55 ms, total: 19.2 ms, Wall time: 12.1 s\n```", "```py\n%%time\ninterim = rdd1.map(lambda x: taketime(x)).cache()\n```", "```py\n%%time\nprint('output =',interim.reduce(lambda x,y:x+y))\n>> output = -0.28870546796843666\n>> CPU times: user 16.4 ms, sys: 2.24 ms, total: 18.7 ms, Wall time: 15.3 s\n```", "```py\n%%time\nprint(interim.filter(lambda x:x>0).count())\n>> 500000\n>> CPU times: user 14.2 ms, sys: 3.27 ms, total: 17.4 ms, Wall time: 811 ms\n```"]