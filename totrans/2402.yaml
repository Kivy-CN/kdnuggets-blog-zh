- en: How Does Logistic Regression Work?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归是如何工作的？
- en: 原文：[https://www.kdnuggets.com/2022/07/logistic-regression-work.html](https://www.kdnuggets.com/2022/07/logistic-regression-work.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/07/logistic-regression-work.html](https://www.kdnuggets.com/2022/07/logistic-regression-work.html)
- en: '![How Does Logistic Regression Work?](../Images/35c287fe299baa779f6a63a515e66b47.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归是如何工作的？](../Images/35c287fe299baa779f6a63a515e66b47.png)'
- en: Image by Editor
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑提供的图片
- en: In this article, we will be focusing on the Logistic Regression Algorithm. This
    is a machine learning technique used for classification or prediction in data
    sets which have many features, but where most of them have little value and should
    be ignored.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将重点介绍逻辑回归算法。这是一种用于分类或预测的机器学习技术，适用于具有许多特征的数据集，但其中大多数特征价值较小，应被忽略。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行 IT 工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'For example, imagine that you have several algorithms for analyzing your photos.
    Some are good at converting images into text, others are good at detecting objects
    in photos but not so much for recognizing people’s faces, and others still focus
    on finding patterns in colour schemes. Now say that you have an image with two
    people in it: one smiling and one frowning—which one is the happier of the two?
    Assuming this isn’t something to be kept as a secret (which it probably shouldn’t
    be), then you need more than just judging by appearances. You want to learn what
    makes people happy and sad, so instead of having various algorithms that each
    specialize in identifying different emotions—you want to create a single algorithm
    that knows how to recognize both emotions equally well. In such cases, using a
    machine learning algorithm like **Logistic Regression** can help you achieve your
    goal more effectively. [**Learn More**](https://www.analyticsvidhya.com/blog/2021/07/an-introduction-to-logistic-regression/).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你有几个算法用于分析你的照片。有些擅长将图像转换为文本，有些擅长检测照片中的物体，但不太擅长识别人脸，另一些则专注于寻找颜色模式。现在假设你有一张包含两个人的照片：一个在微笑，一个在皱眉——他们之中哪个更快乐？假设这不是一个需要保密的事情（其实应该不会），那么你需要的不仅仅是通过外表来判断。你想了解是什么让人们感到快乐或悲伤，因此，你需要创建一个既能识别两种情感又能做到同样好的单一算法。在这种情况下，使用像**逻辑回归**这样的机器学习算法可以帮助你更有效地实现目标。
    [**了解更多**](https://www.analyticsvidhya.com/blog/2021/07/an-introduction-to-logistic-regression/)。
- en: Classification
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: You can use a multi-class classification algorithm to predict the class of a
    new data point. For example, an email may be labelled as spam or ham (not spam)
    for each instance. To create a logistic regression algorithm that works for multi-class
    and multi-labelling problems, you may add this feature. We will demonstrate how
    logistic regression works in Scikit Learn for real-time problems in this lecture.
    This is an example of a binary classification problem. To create a logistic regression
    algorithm that can work for multi-class and multi-labelling problems, you may
    add this feature.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用多类分类算法来预测新数据点的类别。例如，一封电子邮件可能会被标记为垃圾邮件或正常邮件（非垃圾邮件）。为了创建一个适用于多类和多标签问题的逻辑回归算法，你可以添加这个特性。在本讲座中，我们将演示如何在
    Scikit Learn 中实现逻辑回归，以处理实时问题。这是一个二分类问题的示例。要创建一个适用于多类和多标签问题的逻辑回归算法，你可以添加这个特性。
- en: What is Logistic Regression?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是逻辑回归？
- en: Logistic regression is a Machine Learning classification algorithm that is used
    to predict the probability of certain classes based on some dependent variables.
    In short, the logistic regression model computes a sum of the input features (in
    most cases, there is a bias term), and calculates the logistic of the result.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种机器学习分类算法，用于预测基于某些依赖变量的特定类别的概率。简而言之，逻辑回归模型计算输入特征的总和（在大多数情况下，有一个偏置项），并计算结果的逻辑值。
- en: The output of logistic regression is always between (0, and 1), which is suitable
    for a binary classification task. The higher the value, the higher the probability
    that the current sample is classified as class=1, and vice versa.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的输出始终介于 (0 和 1) 之间，这适用于二分类任务。值越高，当前样本被分类为 class=1 的概率越高，反之亦然。
- en: '![Equation](../Images/935e68dcc690fde9de31da341326e585.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![公式](../Images/935e68dcc690fde9de31da341326e585.png)'
- en: As the formula above shows, ![Equation](../Images/8e667acc74548c20685e6712ece84537.png)
    is the parameter we want to learn or train or optimize and ![Equation](../Images/a6eb9fc879ef578de1425ea87d54ce83.png)
    is the input data. The output is the prediction value when the value is closer
    to 1, which means the instance is more likely to be a positive sample(y=1). If
    the value is closer to 0, this means the instance is more likely to be a negative
    sample(y=0).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如上公式所示，![公式](../Images/8e667acc74548c20685e6712ece84537.png) 是我们要学习、训练或优化的参数，![公式](../Images/a6eb9fc879ef578de1425ea87d54ce83.png)
    是输入数据。输出是当值接近1时的预测值，这意味着该实例更有可能是正样本（y=1）。如果值接近0，则意味着该实例更有可能是负样本（y=0）。
- en: To optimize our task, we need to define a loss function(cost or objective function)
    for this task. In logistic regression, we use the log-likelihood loss function.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化我们的任务，我们需要为此任务定义一个损失函数（成本函数或目标函数）。在逻辑回归中，我们使用对数似然损失函数。
- en: '![Equation](../Images/88b0b921f2b92ed2b96d48d56f3fa2a5.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![公式](../Images/88b0b921f2b92ed2b96d48d56f3fa2a5.png)'
- en: m is the number of samples in the training data. ![Equation](../Images/f7b68e109bf4e972865a51007844c2d4.png)
    is the label of the i-th sample, ![Equation](../Images/d187a6e3c2fb53333edff7a4b3d6dc81.png)
    i is the prediction value of the i-th sample. When the current sample’s label
    is 1, then the second term of the formula is 0\. We hope the larger the first
    term, the better, and vice versa. Finally, we add the loss of all samples, take
    the average, and add a negative sign. We want to minimize the quadratic cost function
    ![Equation](../Images/930cb17979943bee3e1b67950f690dc9.png). When ![Equation](../Images/930cb17979943bee3e1b67950f690dc9.png)
    is smaller, it means that the model fits better on the data set. There is no closed-form
    method to find ![Equation](../Images/8e667acc74548c20685e6712ece84537.png). To
    achieve this goal, we need to use some optimization algorithms, such as gradient
    descent. Since ![Equation](../Images/930cb17979943bee3e1b67950f690dc9.png) is
    a convex function, the gradient descent is guaranteed to find a global minimum.
    [More Details](https://builtin.com/data-science/what-is-logistic-regression).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: m 是训练数据中的样本数量。![公式](../Images/f7b68e109bf4e972865a51007844c2d4.png) 是第 i 个样本的标签，![公式](../Images/d187a6e3c2fb53333edff7a4b3d6dc81.png)
    i 是第 i 个样本的预测值。当当前样本的标签为1时，公式的第二项为0。我们希望第一项越大越好，反之亦然。最后，我们将所有样本的损失相加，取平均，然后添加负号。我们希望最小化二次成本函数![公式](../Images/930cb17979943bee3e1b67950f690dc9.png)。当![公式](../Images/930cb17979943bee3e1b67950f690dc9.png)越小时，意味着模型对数据集的拟合越好。没有闭式解法来找出![公式](../Images/8e667acc74548c20685e6712ece84537.png)。为实现这一目标，我们需要使用一些优化算法，如梯度下降。由于![公式](../Images/930cb17979943bee3e1b67950f690dc9.png)是一个凸函数，梯度下降可以保证找到全局最小值。[更多详情](https://builtin.com/data-science/what-is-logistic-regression)。
- en: How Does Logistic Regression Work?
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归是如何工作的？
- en: Machine learning generally involves predicting a quantitative outcome or a qualitative
    class. The former is commonly referred to as a regression problem. In the scenario
    of linear regression, the input is a continuous variable, and the prediction is
    a numerical value. When predicting a qualitative outcome (class), the task is
    considered a classification problem. Examples of classification problems include
    predicting what products a user will buy or if a target user will click on an
    online advertisement.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习通常涉及预测一个定量结果或一个定性类别。前者通常被称为回归问题。在线性回归的情况下，输入是一个连续变量，预测值是一个数值。当预测一个定性结果（类别）时，任务被视为分类问题。分类问题的示例包括预测用户会购买哪些产品或目标用户是否会点击在线广告。
- en: Not all algorithms fit cleanly into this simple dichotomy, though, and logistic
    regression is a notable example. Logistic regression is part of the regression
    family as it involves predicting outcomes based on quantitative relationships
    between variables. However, unlike linear regression, it accepts both continuous
    and discrete variables as input and its output is qualitative. In addition, it
    predicts a discrete class such as “Yes/No” or “Customer/Non-customer”.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有算法都能简洁地归入这种简单的二分法，逻辑回归就是一个显著的例子。逻辑回归属于回归家族，因为它涉及根据变量之间的定量关系预测结果。然而，与线性回归不同的是，它接受连续和离散变量作为输入，其输出是定性的。此外，它预测的是离散类别，如“是/否”或“客户/非客户”。
- en: In practice, the [**logistic regression algorithm**](https://www.interviewbit.com/data-science-interview-questions/)
    analyzes relationships between variables. It assigns probabilities to discrete
    outcomes using the Sigmoid function, which converts numerical results into an
    expression of probability between 0 and 1.0\. Probability is either 0 or 1, depending
    on whether the event happens or not. For binary predictions, you can divide the
    population into two groups with a cut-off of 0.5\. Everything above 0.5 is considered
    to belong to group A, and everything below is considered to belong to group B.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，[**逻辑回归算法**](https://www.interviewbit.com/data-science-interview-questions/)分析变量之间的关系。它通过使用Sigmoid函数将离散结果的概率分配到0和1.0之间，从而将数值结果转换为概率表达式。概率要么是0，要么是1，取决于事件是否发生。对于二元预测，你可以将总体分为两个组，阈值为0.5。高于0.5的被视为属于A组，低于0.5的被视为属于B组。
- en: '![logistic regression](../Images/7a885e336c659eecb89b8a0f6519f0e0.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](../Images/7a885e336c659eecb89b8a0f6519f0e0.png)'
- en: A hyperplane is used as a decision line to separate two categories (as far as
    possible) after data points have been assigned to a class using the Sigmoid function.
    The class of future data points can then be predicted using the decision boundary.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 超平面用作决策线，以在数据点使用Sigmoid函数被分配到一个类别后，尽可能地将两个类别分开。未来数据点的类别可以通过决策边界进行预测。
- en: '![How Does Logistic Regression Work?](../Images/c1a0890ec41b19fb403a3d52625231cd.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归是如何工作的？](../Images/c1a0890ec41b19fb403a3d52625231cd.png)'
- en: Conclusion
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The logistic regression model is an analysis technique that helps predict the
    probability of an event happening in the future. This article will explain what
    logistic regression is, how it works, and how you can use it for forecasting.
    Logistic regression is a supervised learning method that helps to predict events
    that have a binary outcome, such as whether a person will successfully pass a
    driving test. In order to make predictions in this scenario, you need data from
    past test results. The model takes this data and predicts the likelihood that
    the same person will pass the test in the future. The main idea behind logistic
    regression is to use a model based on the probability of an outcome occurring.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型是一种分析技术，有助于预测未来事件发生的概率。本文将解释什么是逻辑回归，它是如何工作的，以及如何用于预测。逻辑回归是一种监督学习方法，帮助预测具有二元结果的事件，例如一个人是否能成功通过驾驶测试。为了在这种情况下做出预测，你需要过去测试结果的数据。该模型利用这些数据预测同一个人未来通过测试的可能性。逻辑回归的核心思想是基于事件发生的概率使用模型。
- en: '[**Sonia Jessica**](https://soniajm.medium.com/) is a passionate learner and
    an avid tech blogger.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[**索尼娅·杰西卡**](https://soniajm.medium.com/)是一个热情的学习者和狂热的科技博主。'
- en: More On This Topic
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Comparing Linear and Logistic Regression](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性回归与逻辑回归比较](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)'
- en: '[An Overview of Logistic Regression](https://www.kdnuggets.com/2022/02/overview-logistic-regression.html)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[逻辑回归概述](https://www.kdnuggets.com/2022/02/overview-logistic-regression.html)'
- en: '[Linear vs Logistic Regression: A Succinct Explanation](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性回归与逻辑回归: 简明解释](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
- en: '[KDnuggets News 22:n12, March 23: Best Data Science Books for…](https://www.kdnuggets.com/2022/n12.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻 22:n12, 3月23日: 最佳数据科学书籍…](https://www.kdnuggets.com/2022/n12.html)'
- en: '[Logistic Regression for Classification](https://www.kdnuggets.com/2022/04/logistic-regression-classification.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[逻辑回归在分类中的应用](https://www.kdnuggets.com/2022/04/logistic-regression-classification.html)'
- en: '[Classification Metrics Walkthrough: Logistic Regression with…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类指标概述：逻辑回归与…](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)'
