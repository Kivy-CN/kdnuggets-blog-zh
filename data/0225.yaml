- en: Best Practices for Creating Domain-Specific AI Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建领域特定AI模型的最佳实践
- en: 原文：[https://www.kdnuggets.com/2022/07/best-practices-creating-domainspecific-ai-models.html](https://www.kdnuggets.com/2022/07/best-practices-creating-domainspecific-ai-models.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/07/best-practices-creating-domainspecific-ai-models.html](https://www.kdnuggets.com/2022/07/best-practices-creating-domainspecific-ai-models.html)
- en: '![Best Practices for Creating Domain-Specific AI Models](../Images/b5cbea59beacf4fad90f4cbfa7430339.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![创建领域特定AI模型的最佳实践](../Images/b5cbea59beacf4fad90f4cbfa7430339.png)'
- en: Photo by [Andrea De Santis](https://unsplash.com/@santesson89?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/ai-models?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Andrea De Santis](https://unsplash.com/@santesson89?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)提供，[Unsplash](https://unsplash.com/s/photos/ai-models?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Large-scale general AI implementations serve as great building blocks for solving
    certain B2B problems, and most organizations have already or are in the process
    of leveraging them. However, the desire for immediate RoI, creating fail-fast
    prototypes, and delivering decision-centric outcomes are driving the need for
    domain-specific AI initiatives.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模通用AI实施作为解决某些B2B问题的优秀基础构件，大多数组织已经或正在利用这些模型。然而，对即时投资回报的渴望、创建快速失败的原型以及提供以决策为中心的成果，推动了领域特定AI项目的需求。
- en: Use cases and subject matter expertise help, but data scientists and analysts
    need to adapt the AI implementation cycle to resolve problems that require more
    specificity and relevancy. The biggest hurdle anyone would encounter while building
    such AI models is finding quality, domain-specific data. Here are some best practices
    and techniques for domain-specific model adaptation that worked for us time and
    again.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 用例和主题专业知识有帮助，但数据科学家和分析师需要调整AI实施周期，以解决需要更多具体性和相关性的问题。构建这样的AI模型时遇到的最大障碍是找到高质量的领域特定数据。以下是一些领域特定模型适配的最佳实践和技术，这些方法在我们身上一次次取得了成功。
- en: Data Matters, But Does It?
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据很重要，但重要吗？
- en: Begin by mining your organization to discover as many relevant domain-specific
    data assets as you can. If it is a problem relating directly to your business
    and industry, you likely have untapped data assets that your implementation can
    leverage. In the unlikely event of finding yourself with insufficient data assets,
    all hope is not lost. There are multiple strategies and methodologies to help
    create or enhance specific datasets, including active learning, transfer learning,
    self-training to improve pre-training, and data augmentation. Some are detailed
    below.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 首先挖掘你的组织，发现尽可能多的相关领域特定数据资产。如果这是一个直接涉及你的业务和行业的问题，你很可能拥有可以利用的未开发的数据资产。如果发现数据资产不足，不必灰心。有多种策略和方法可以帮助创建或增强特定数据集，包括主动学习、迁移学习、自我训练以改善预训练和数据增强。以下是一些详细信息。
- en: Active Learning
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主动学习
- en: Active learning is a type of semi-supervised learning with a Query Strategy
    for selecting specific instances that it wants to learn from. Using domain experts
    with a human-in-the-loop mechanism for labeling such selected instances helps
    refine the process towards meaningful outcomes in a much faster timeline. In addition,
    active learning requires smaller amounts of labeled data, thus lowering manual
    annotation costs while still achieving higher levels of accuracy.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习是一种半监督学习方法，通过查询策略选择特定实例进行学习。使用领域专家和人工环节机制对这些选择的实例进行标注，有助于在更快的时间内将过程优化为有意义的结果。此外，主动学习需要较少的标注数据，从而降低了人工标注成本，同时仍能实现更高的准确性。
- en: 'Here are some tips to help achieve active learning with limited data:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些建议，帮助在数据有限的情况下实现主动学习：
- en: First, split your dataset into seed and unlabeled data.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，将数据集分为种子数据和未标记数据。
- en: Label the seed and use it to train the learner model.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记种子数据，并用它来训练学习模型。
- en: Based on the query function, select from unannotated data the instance(s) for
    human annotation (the most critical step). The query strategy could be based on
    uncertainty sampling (e.g., least confidence, margin sampling, or entropy), query-by-committee
    (e.g., vote entropy, average Kullback-Leibler divergence), etc.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据查询功能，从未标注的数据中选择实例进行人工标注（最关键的步骤）。查询策略可以基于不确定性采样（如最小置信度、边缘采样或熵）、委员会查询（如投票熵、平均Kullback-Leibler散度）等。
- en: Add newly annotated data into the seed dataset and re-train the learner model.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将新标注的数据添加到种子数据集中，并重新训练学习模型。
- en: Repeat the previous two steps until reaching stopping criteria, e.g., number
    of instances queried, number of iterations, or performance improvement.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复前两步，直到达到停止标准，例如查询实例的数量、迭代次数或性能改进。
- en: Transfer Learning
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习
- en: This method leverages knowledge from the source domain to learn new knowledge
    on the target domain. The concept has been around for some time, but in the past
    few years, when people talk about Transfer Learning, they talk about a neural
    network, possibly because of the successful implementation cases there.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法利用源领域的知识在目标领域学习新知识。这个概念存在已经有一段时间了，但在过去几年中，当人们谈论迁移学习时，往往谈论神经网络，可能是因为那里有成功的实施案例。
- en: 'Using ImageNet as an example, here are some lessons learned:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以ImageNet为例，以下是一些经验教训：
- en: Use a pre-trained network as a feature extractor. Determining which layer to
    export features depends on how similar to or different your data is from the base
    training dataset. The difference will determine your strategy, as outlined in
    the following points.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练网络作为特征提取器。确定导出哪些层的特征取决于你的数据与基础训练数据集的相似程度。差异将决定你的策略，如下文所述。
- en: If the domain is different, only use lower-level features of the neural network.
    The features can be exported and serve as the input for your classifier.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果领域不同，只使用神经网络的低级特征。这些特征可以导出，并作为分类器的输入。
- en: If the domain is similar, remove the last layer of the neural network, and use
    the remaining full network as a feature extractor. Or replace the last layer with
    a new layer that matches the number of target dataset classes.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果领域相似，移除神经网络的最后一层，使用剩余的完整网络作为特征提取器。或者将最后一层替换为一个新的层，该层与目标数据集的类别数匹配。
- en: Try unfreezing the last few layers of the base network and conduct fine-tuning
    at a low learning rate (e.g., 1e-5). The closer the target and source domains
    are, the smaller the number of layers that need fine-tuning.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试解冻基础网络的最后几层，并以较低的学习率（例如1e-5）进行微调。目标领域和源领域越接近，需要微调的层数就越少。
- en: Self-Training to Improve Pre-Training ([Jingfei Du et al., 2020](https://arxiv.org/pdf/2010.02194.pdf))
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自训练以改进预训练（[Jingfei Du等，2020](https://arxiv.org/pdf/2010.02194.pdf)）
- en: This combination offers a potential method to make the most out of limited labeled
    data from downstream tasks. It can also help make the best use of the large volume
    of easily available unlabeled data. Here is how it works.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种组合提供了一种方法，能够最大限度地利用来自下游任务的有限标记数据。同时，它也有助于充分利用大量易获得的未标记数据。其工作原理如下。
- en: Fine-tune a pre-trained model (e.g., RoBERTa-Large) with a targeted downstream
    task’s labeled data and use the fine-tuned model as the teacher.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对预训练模型（例如RoBERTa-Large）进行微调，以适应目标下游任务的标记数据，并将微调后的模型用作教师。
- en: Extract from unlabeled dataset task-specific data using query embeddings and
    select nearest neighbors from the dataset.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用查询嵌入从未标记的数据集中提取任务特定数据，并从数据集中选择最近邻。
- en: Use the teacher model to annotate in-domain data retrieved in bullet 2 and select
    top k samples from each class with the highest score.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用教师模型对在第2点中检索的领域内数据进行标注，并从每个类别中选择得分最高的前k个样本。
- en: Use the pseudo data generated in Step 3 to fine-tune a new RoBERTa-Large and
    provide the student model.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用第3步中生成的伪数据对新的RoBERTa-Large进行微调，并提供学生模型。
- en: Data Augmentation
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强
- en: Data augmentation includes a set of low-cost, effective techniques to generate
    new data points from existing data. Bullet 2 in the previous section on self-training
    also deals with data augmentation, which is crucial for the whole exercise.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强包括一系列低成本、高效的技术，从现有数据中生成新数据点。上一节自训练中的第2点也涉及数据增强，这对整个过程至关重要。
- en: Other techniques for NLP applications include back translation, synonym replacement,
    random insertion/swap/deletion, etc. For computer vision, the main methods include
    cropping, flipping, zooming, rotation, noise injection, changing brightness/contrast/saturation,
    and GAN (generative adversarial network).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其他NLP应用的技术包括回译、同义词替换、随机插入/交换/删除等。对于计算机视觉，主要的方法包括裁剪、翻转、缩放、旋转、噪声注入、调整亮度/对比度/饱和度以及GAN（生成对抗网络）。
- en: Determining which methodology to use depends on your use case, initial dataset
    quality, SME in place, and investment available.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 确定使用哪种方法取决于你的用例、初始数据集质量、现有领域专家和可用的投资。
- en: 5 Additional Tips to Help Domain-Specific AI Refinement
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 个额外的提示来帮助领域特定 AI 的优化
- en: You will find the need to continue refining your domain-specific AI. Here are
    some lessons we learned from our experiences customizing implementations to meet
    specific use-cases.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现需要继续优化你的领域特定 AI。以下是我们从定制实现以满足特定用例中的经验中学到的一些经验教训。
- en: Conduct proper data exploration to check quality and quantity issues with data
    before starting your proof of concept (POC). Understand whether data complies
    with actual application scenarios (e.g., the downstream tasks, like NER, Relation
    Extraction, QA, etc.), its variations and distribution, and its level of accuracy.
    For variations and distribution with the classification problem – the structure
    of the taxonomy, the number of classes at each level, amount of data falling under
    each class, balanced vs. unbalanced, etc. – all of it matters. What you find will
    impact your data handling approach and your methodology selection to improve domain-specific
    model performance.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开始你的概念验证（POC）之前，进行适当的数据探索，以检查数据的质量和数量问题。了解数据是否符合实际应用场景（例如下游任务，如命名实体识别（NER）、关系抽取、问答等），其变化和分布，以及其准确性水平。对于分类问题的变化和分布——分类结构、每一层的类别数量、每个类别的数据量、平衡与不平衡等——所有这些都很重要。你发现的内容将影响你的数据处理方法和选择以改进领域特定模型性能的方法论。
- en: Choose appropriate algorithms/models based on your use case and data characteristics.
    Consider factors like speed, accuracy, and deployment as well. The balance of
    these factors is important, as they decide if your development may stop at the
    POC stage or have the potential for real application and productization.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据你的用例和数据特征选择适当的算法/模型。还要考虑速度、准确性和部署等因素。这些因素的平衡很重要，因为它们决定了你的开发是否可能停留在 POC 阶段或有实际应用和产品化的潜力。
- en: For example, if your model will eventually be deployed at the edge, large models
    – although they may have higher prediction accuracy – should not be chosen. It
    is not realistic for edge devices to run such models, given their computational
    power.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，如果你的模型最终会部署到边缘设备上，虽然大模型可能具有更高的预测准确性，但不应选择它们。鉴于边缘设备的计算能力，运行这样的模型并不现实。
- en: Start every development/domain adaptation with a strong baseline model. Consider
    AutoML to quickly check algorithms’ fits for domain-specific datasets, then optimize
    based on the observations.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从一个强大的基线模型开始每一个开发/领域适应。考虑使用 AutoML 快速检查算法在领域特定数据集上的适用性，然后根据观察结果进行优化。
- en: Pre-processing data is an essential part of any NLP project. The steps taken
    should be determined on a case-by-case basis due to domain-specific requirements,
    and their chosen featurization methods and models. Generic pipelines may not give
    the best results.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据预处理是任何 NLP 项目的关键部分。由于领域特定的要求以及选择的特征化方法和模型，所采取的步骤应根据具体情况确定。通用的流程可能无法提供最佳结果。
- en: For example, some steps such as removing stop words/punctuation and lemmatizing
    may not always be necessary for deep learning models. They might cause a loss
    of context; however, they can be useful if you are using TF-IDF and machine learning
    models. Modularize the pipeline so that some common steps can be reused while
    customizing to meet use case needs.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，某些步骤如去除停用词/标点符号和词形还原，可能并不总是对深度学习模型必要。它们可能会导致上下文的丧失；然而，如果你使用的是 TF-IDF 和机器学习模型，它们可能会很有用。将流程模块化，以便在定制以满足用例需求时可以重用一些常见步骤。
- en: Leverage open-source domain-specific pre-trained language models if they are
    available. Some well-known models are SciBERT, BioBERT, ClinicalBERT, PatentBERT,
    and FinBERT. These domain-specific pre-trained models can help achieve better
    representation and contextualized embeddings for downstream tasks. In case these
    models are not available for your domain, but you do have sufficient computational
    resources, consider training your own pre-trained model using in-domain high-quality
    unannotated data.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有可用的领域特定预训练语言模型，请利用它们。一些知名模型包括 SciBERT、BioBERT、ClinicalBERT、PatentBERT 和 FinBERT。这些领域特定的预训练模型可以帮助实现更好的表示和上下文化的嵌入。如果这些模型在你的领域不可用，但你有足够的计算资源，可以考虑使用领域内高质量的未标注数据训练你自己的预训练模型。
- en: Consider incorporating domain-specific vocabulary and rules. For certain scenarios,
    they provide more efficient and accurate results and avoid model iteration problems.
    Creating such vocabulary and defining the rules may require considerable effort
    and domain expertise, which need to be balanced.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑整合领域特定的词汇和规则。在某些情况下，它们可以提供更高效和更准确的结果，并避免模型迭代问题。创建这些词汇和定义规则可能需要相当大的努力和领域专业知识，这些都需要平衡。
- en: Summary
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Channeling AI to meet domain-specific needs and challenges requires discipline
    throughout, not only in approach and resources. The good news is that companies
    are increasingly interested in solutions that address specific challenges and
    resolve them, which in turn creates best practices for data scientists and AI
    developers looking to provide faster ROI for their applications.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 将 AI 定向满足领域特定需求和挑战需要在整个过程中保持严谨，不仅在方法和资源方面。好消息是，公司对解决特定挑战并解决这些挑战的解决方案越来越感兴趣，这反过来为数据科学家和
    AI 开发人员提供了最佳实践，以期为他们的应用提供更快的投资回报。
- en: '**[Cathy Feng](https://www.linkedin.com/in/cathy-feng-46b4792/)** leads Evalueserve’s
    AI practice, guiding the company into the next era of data analytics. Passionate
    about educating people on the possibilities of AI and its business applications,
    her efforts led to creating domain-specific AI applications for companies like
    McDonald’s, Intel, and Syngenta.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Cathy Feng](https://www.linkedin.com/in/cathy-feng-46b4792/)** 领导着 Evalueserve
    的 AI 实践，指导公司进入数据分析的下一个时代。她对教育人们了解 AI 及其业务应用充满热情，她的努力促成了为麦当劳、英特尔和先正达等公司创建特定领域的
    AI 应用程序。'
- en: More On This Topic
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Creating AI-Driven Solutions: Understanding Large Language Models](https://www.kdnuggets.com/creating-ai-driven-solutions-understanding-large-language-models)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[创建 AI 驱动的解决方案：理解大型语言模型](https://www.kdnuggets.com/creating-ai-driven-solutions-understanding-large-language-models)'
- en: '[MLOps: The Best Practices and How To Apply Them](https://www.kdnuggets.com/2022/04/mlops-best-practices-apply.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MLOps：最佳实践及其应用方法](https://www.kdnuggets.com/2022/04/mlops-best-practices-apply.html)'
- en: '[Integrating ChatGPT Into Data Science Workflows: Tips and Best Practices](https://www.kdnuggets.com/2023/05/integrating-chatgpt-data-science-workflows-tips-best-practices.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[将 ChatGPT 集成到数据科学工作流程中：提示和最佳实践](https://www.kdnuggets.com/2023/05/integrating-chatgpt-data-science-workflows-tips-best-practices.html)'
- en: '[5 Python Best Practices for Data Science](https://www.kdnuggets.com/5-python-best-practices-for-data-science)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中的 5 个 Python 最佳实践](https://www.kdnuggets.com/5-python-best-practices-for-data-science)'
- en: '[Data Warehousing and ETL Best Practices](https://www.kdnuggets.com/2023/02/data-warehousing-etl-best-practices.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据仓库和 ETL 最佳实践](https://www.kdnuggets.com/2023/02/data-warehousing-etl-best-practices.html)'
- en: '[11 Best Practices of Cloud and Data Migration to AWS Cloud](https://www.kdnuggets.com/2023/04/11-best-practices-cloud-data-migration-aws-cloud.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[迁移到 AWS 云的 11 个最佳实践](https://www.kdnuggets.com/2023/04/11-best-practices-cloud-data-migration-aws-cloud.html)'
