- en: AI Papers to Read in 2020
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2020 年的 AI 论文阅读推荐
- en: 原文：[https://www.kdnuggets.com/2020/09/ai-papers-read-2020.html](https://www.kdnuggets.com/2020/09/ai-papers-read-2020.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/09/ai-papers-read-2020.html](https://www.kdnuggets.com/2020/09/ai-papers-read-2020.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Ygor Rebouças Serpa](https://www.linkedin.com/in/ygorreboucas/), developing
    explainable AI tools for the healthcare industry**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Ygor Rebouças Serpa](https://www.linkedin.com/in/ygorreboucas/) 提供，他正在为医疗保健行业开发可解释的人工智能工具**'
- en: '![Figure](../Images/bec677c37d3ca6f467317b670f40b37d.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/bec677c37d3ca6f467317b670f40b37d.png)'
- en: Photo by [Alfons Morales](https://unsplash.com/@alfonsmc10?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Alfons Morales](https://unsplash.com/@alfonsmc10?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Artificial Intelligence is one of the most rapidly growing fields in science
    and is one of the most sought skills of the past few years, commonly labeled as
    Data Science. The area has far-reaching applications, being usually divided by
    input type: text, audio, image, video, or graph; or by problem formulation: supervised,
    unsupervised, and reinforcement learning. Keeping up with everything is a massive
    endeavor and usually ends up being a frustrating attempt. In this spirit, I present
    some reading suggestions to keep you updated on the latest and classic breakthroughs
    in AI and Data Science.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能是科学领域中增长最快的领域之一，并且是过去几年中最受追捧的技能之一，通常被标记为数据科学。这个领域有着广泛的应用，通常按输入类型划分：文本、音频、图像、视频或图表；或按问题表述方式划分：监督学习、无监督学习和强化学习。跟上所有这些进展是一项巨大的工作，通常会变成令人沮丧的尝试。基于这一点，我提供了一些阅读建议，以便让你了解人工智能和数据科学领域最新和经典的突破。
- en: Although most papers I listed deal with image and text, many of their concepts
    are fairly input agnostic and provide insight far beyond vision and language tasks.
    Alongside each suggestion, I listed some of the reasons I believe you should read
    (or re-read) the paper and added some further readings, in case you want to dive
    a bit deeper into a given subject.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我列出的多数论文都涉及图像和文本，但它们的许多概念相当不依赖于输入类型，并且提供了超越视觉和语言任务的深入见解。在每个建议旁边，我列出了一些我认为你应该阅读（或重新阅读）论文的理由，并添加了一些进一步阅读的材料，以便你能更深入地研究相关主题。
- en: Before we begin, I would like to apologize to the Audio and Reinforcement Learning
    communities for not adding these subjects to the list, as I have only limited
    experience with both.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我要向音频和强化学习社区道歉，因为我由于对这两个领域的经验有限，没有将这些主题添加到列表中。
- en: Here we go.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们开始吧。
- en: '#1 AlexNet (2012)'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '#1 AlexNet (2012)'
- en: Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. [“Imagenet classification
    with deep convolutional neural networks.”](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networ) *Advances
    in neural information processing systems*. 2012.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Krizhevsky, Alex, Ilya Sutskever, 和 Geoffrey E. Hinton. [“用深度卷积神经网络进行图像分类。”](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networ)
    *神经信息处理系统的进展*。2012年。
- en: In 2012, the authors proposed the use of GPUs to train a large Convolutional
    Neural Network (CNN) for the ImageNet challenge. This was a bold move, as CNNs
    were considered too heavy to be trained on such a large scale problem. To everyone
    surprise, they won first place, with a ~15% Top-5 error rate, against ~26% of
    the second place, which used state-of-the-art image processing techniques.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在2012年，作者提出了使用 GPU 训练大型卷积神经网络（CNN）来应对 ImageNet 挑战。这是一个大胆的举措，因为 CNN 被认为过于庞大，无法在如此大规模的问题上进行训练。令所有人惊讶的是，他们赢得了第一名，Top-5
    错误率约为 15%，相比之下第二名使用了最先进的图像处理技术，错误率约为 26%。
- en: '**Reason #1:** While most of us know AlexNet’s historical importance, not everyone
    knows which of the techniques we use today were already present before the boom.
    You might be surprised by how familiar many of the concepts introduced in the
    paper are, such as dropout and ReLU.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #1：** 虽然我们大多数人知道AlexNet的历史重要性，但并非所有人都知道我们今天使用的技术中哪些在繁荣之前已经存在。你可能会惊讶于论文中介绍的许多概念，如dropout和ReLU，是多么熟悉。'
- en: '**Reason #2:** The proposed network had 60 million parameters, complete insanity
    for 2012 standards. Nowadays, we get to see models with over a billion parameters.
    Reading the AlexNet paper gives us a great deal of insight on how things developed
    since then.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #2：** 提出的网络有6000万参数，对于2012年的标准来说完全是疯狂的。现在，我们看到的模型参数超过了十亿。阅读AlexNet论文为我们提供了大量关于自那时以来事情如何发展的见解。'
- en: '**Further Reading:** Following the history of ImageNet champions, you can read
    the [ZF Net](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53), [VGG](https://arxiv.org/abs/1409.1556), [Inception-v1, ](https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html)and [ResNet ](http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)papers.
    This last one achieved super-human performance, solving the challenge. After it,
    other competitions took over the researchers’ attention. Nowadays, ImageNet is
    mainly used for Transfer Learning and to validate low-parameter models, such as:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读：** 你可以参考图像Net冠军的历史，阅读[ZF Net](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53)、[VGG](https://arxiv.org/abs/1409.1556)、[Inception-v1](https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html)和[ResNet](http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)论文。最后一篇取得了超越人类的表现，解决了挑战。之后，其他竞赛转移了研究者们的注意力。现在，ImageNet
    主要用于迁移学习和验证低参数模型，例如：'
- en: '![Figure](../Images/0c56de4690eaf23d4c7f29e008cec3e7.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/0c56de4690eaf23d4c7f29e008cec3e7.png)'
- en: The original portrayal of the AlexNet structure. The top and bottom halves are
    processed by GPU 1 and 2, respectively. An earlier form of model parallelism.
    Source: [The Alexnet Paper](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networ)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet结构的原始描述。顶部和底部部分分别由GPU 1和2处理。这是一种早期的模型并行形式。来源：[Alexnet论文](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networ)
- en: '#2 MobileNet (2017)'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '#2 MobileNet (2017)'
- en: 'Howard, Andrew G., et al. [“Mobilenets: Efficient convolutional neural networks
    for mobile vision applications.”](https://arxiv.org/abs/1704.04861) *arXiv preprint
    arXiv:1704.04861* (2017).'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Howard, Andrew G., 等人。[“Mobilenets: Efficient convolutional neural networks
    for mobile vision applications.”](https://arxiv.org/abs/1704.04861) *arXiv预印本arXiv:1704.04861*
    (2017)。'
- en: MobileNet is one of the most famous “low-parameter” networks. Such models are
    ideal for low-resources devices and to speed-up real-time applications, such as
    object recognition on mobile phones. The core idea behind MobileNet and other
    low-parameter models is to decompose expensive operations into a set of smaller
    (and faster) operations. Such compound operations are often orders-of-magnitude
    faster and use substantially fewer parameters.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet是最著名的“低参数”网络之一。这些模型非常适合低资源设备和加速实时应用，如手机上的对象识别。MobileNet和其他低参数模型的核心思想是将昂贵的操作分解为一组较小（且更快）的操作。这种复合操作通常快几个数量级，并使用了显著更少的参数。
- en: '**Reason #1: **Most of us have nowhere near the resources the big tech companies
    have. Understanding the low-parameter networks is crucial to make your own models
    less expensive to train and use. In my experience, using depth-wise convolutions can
    save you hundreds of dollars in cloud inference with almost no loss to accuracy.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #1：** 我们大多数人没有大科技公司拥有的资源。理解低参数网络对于使自己的模型更经济地训练和使用至关重要。在我的经验中，使用depth-wise卷积可以节省你数百美元的云推理费用，几乎不会损失精度。'
- en: '**Reason #2: **Common knowledge is that bigger models are stronger models.
    Papers such as MobileNet show that there is a lot more to it than adding more
    filters. Elegance matters.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #2：** 常识认为更大的模型更强大。像MobileNet这样的论文表明，模型的强大不仅仅是增加更多的滤波器。优雅很重要。'
- en: '**Further Reading:** So far, MobileNet [v2 ](https://arxiv.org/abs/1801.04381)and [v3 ](https://arxiv.org/abs/1905.02244)have
    been released, providing new enhancements to accuracy and size. In parallel, other
    authors have devised many techniques to further reduce the model size, such as
    the [SqueezeNet](https://arxiv.org/abs/1602.07360), and to [downsize regular models
    with minimal accuracy loss.](https://arxiv.org/abs/1608.08710) [This paper](https://ieeexplore.ieee.org/abstract/document/8050276) gives
    a comprehensive summary of several models size vs accuracy.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读:** 目前，MobileNet [v2](https://arxiv.org/abs/1801.04381) 和 [v3](https://arxiv.org/abs/1905.02244)
    已经发布，提供了新的准确性和尺寸增强。同时，其他作者也提出了许多技术来进一步缩小模型尺寸，例如 [SqueezeNet](https://arxiv.org/abs/1602.07360)，以及
    [以最小的准确性损失缩小常规模型](https://arxiv.org/abs/1608.08710)。 [这篇论文](https://ieeexplore.ieee.org/abstract/document/8050276)
    对多个模型的尺寸与准确性进行了全面总结。'
- en: '#3 Attention is All You Need (2017)'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '#3 注意力机制是你所需要的（2017）'
- en: Vaswani, Ashish, et al. [“Attention is all you need.”](http://papers.nips.cc/paper/7181-attention-is-all-you-need) *Advances
    in neural information processing systems*. 2017.
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Vaswani, Ashish, 等. [“注意力机制是你所需要的。”](http://papers.nips.cc/paper/7181-attention-is-all-you-need)
    *神经信息处理系统的进展*。2017年。
- en: The paper that introduced the Transformer Model. Prior to this paper, language
    models relied extensively on Recurrent Neural Networks (RNN) to perform sequence-to-sequence
    tasks. However, RNNs are awfully slow, as they are terrible to parallelize to
    multi-GPUs. In contrast, the Transformer model is based solely on Attention layers,
    which are CNNs that capture the relevance of any sequence element to each other.
    The proposed formulation achieved significantly better state-of-the-art results
    and trains markedly faster than previous RNN models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍变压器模型的论文。在此之前，语言模型主要依赖递归神经网络（RNN）来执行序列到序列的任务。然而，RNN 的速度非常慢，因为它们在多GPU并行化时表现很差。相比之下，变压器模型完全基于注意力层，这些层是捕捉序列中任意元素彼此相关性的卷积神经网络（CNN）。提出的公式达到了显著更好的最先进结果，并且训练速度明显快于之前的
    RNN 模型。
- en: '**Reason #1:** Nowadays, most of the novel architectures in the Natural-Language
    Processing (NLP) literature descend from the Transformer. Models such as [GPT-2](https://www.ceid.upatras.gr/webpages/faculty/zaro/teaching/alg-ds/PRESENTATIONS/PAPERS/2019-Radford-et-al_Language-Models-Are-Unsupervised-Multitask-%20Learners.pdf) and [BERT ](https://arxiv.org/abs/1810.04805)are
    at the forefront of innovation. Understanding the Transformer is key to understanding
    most later models in NLP.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #1:** 目前，大多数自然语言处理（NLP）文献中的新颖架构都源自变压器。像 [GPT-2](https://www.ceid.upatras.gr/webpages/faculty/zaro/teaching/alg-ds/PRESENTATIONS/PAPERS/2019-Radford-et-al_Language-Models-Are-Unsupervised-Multitask-%20Learners.pdf)
    和 [BERT](https://arxiv.org/abs/1810.04805) 这样的模型处于创新的前沿。理解变压器是理解大多数后续 NLP 模型的关键。'
- en: '**Reason #2: **Most transformer models are in the order of billions of parameters.
    While the literature on MobileNets addresses more efficient models, the research
    on NLP addresses more efficient training. In combination, both views provide the
    ultimate set of techniques for efficient training and inference.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #2:** 大多数变压器模型的参数量都在数十亿级别。虽然关于 MobileNets 的文献讨论了更高效的模型，但 NLP 领域的研究则集中在更高效的训练上。两者结合提供了高效训练和推理的终极技术集。'
- en: '**Reason #3: **While the transformer model has mostly been restricted to NLP,
    the proposed Attention mechanism has far-reaching applications. Models such as [Self-Attention
    GAN](https://arxiv.org/abs/1805.08318) demonstrate the usefulness of global-level
    reasoning a variety of tasks. New papers on Attention applications pop-up every
    month.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #3:** 尽管变压器模型主要被限制在 NLP 领域，但提出的注意力机制具有广泛的应用。像 [自注意力 GAN](https://arxiv.org/abs/1805.08318)
    这样的模型展示了全局级别推理在多种任务中的有效性。每个月都有新的关于注意力应用的论文出现。'
- en: '**Further Reading: **I highly recommend reading the [BERT ](https://arxiv.org/abs/1810.04805)and [SAGAN ](https://arxiv.org/abs/1805.08318)paper.
    The former is a continuation of the Transformer model, and the latter is an application
    of the Attention mechanism to images in a GAN setup.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读:** 我强烈推荐阅读 [BERT](https://arxiv.org/abs/1810.04805) 和 [SAGAN](https://arxiv.org/abs/1805.08318)
    论文。前者是变压器模型的延续，后者则是在 GAN 设置中将注意力机制应用于图像的研究。'
- en: '#4 Stop Thinking with Your Head / Reformer (~2020)'
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '#4 不要再用你的脑袋思考 / Reformer (~2020)'
- en: 'Merity, Stephen. “[Single Headed Attention RNN: Stop Thinking With Your Head](https://arxiv.org/abs/1911.11423).” *arXiv
    preprint arXiv:1911.11423* (2019).'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Merity, Stephen. “[单头注意力 RNN: 不要再用你的脑袋思考](https://arxiv.org/abs/1911.11423).”
    *arXiv 预印本 arXiv:1911.11423* (2019)。'
- en: 'Kitaev, Nikita, Łukasz Kaiser, and Anselm Levskaya. “[Reformer: The Efficient
    Transformer.](https://arxiv.org/abs/2001.04451)” *arXiv preprint arXiv:2001.04451* (2020).'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Kitaev, Nikita, Łukasz Kaiser, and Anselm Levskaya. “[Reformer: 高效的 Transformer。](https://arxiv.org/abs/2001.04451)”
    *arXiv 预印本 arXiv:2001.04451*（2020年）。'
- en: Transformer / Attention models have attracted a lot of attention. However, these
    tend to be resource-heavy models, not meant for ordinary consumer hardware. Both
    mentioned papers criticize the architecture, providing computationally efficient
    alternatives to the Attention module. As for the MobileNet discussion, elegance
    matters.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer / Attention 模型吸引了大量关注。然而，这些模型往往资源消耗大，不适合普通消费者硬件。上述论文都批评了这种架构，提供了计算上更高效的替代方案。至于
    MobileNet 的讨论，优雅性很重要。
- en: '**Reason #1:** “[Stop Thinking With Your Head](https://arxiv.org/abs/1911.11423)”
    is a damn funny paper to read. This counts as a reason on its own.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #1：** “[停止用脑袋思考](https://arxiv.org/abs/1911.11423)” 是篇非常有趣的论文。这本身就是一个理由。'
- en: '**Reason #2: **Big companies can quickly scale their research to a hundred
    GPUs. We, normal folks, can’t. Scaling the size of models is not the only avenue
    for improvement. I can’t overstate that. Reading about efficiency is the best
    way to ensure you are efficiently using your current resources.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #2：** 大公司可以快速扩展他们的研究到数百个 GPU。而我们普通人不能。模型规模的扩大并不是唯一的改进途径。我无法过分强调这一点。了解效率是确保你有效利用当前资源的最佳方式。'
- en: '**Further Reading:** Since these are late 2019 and 2020, there isn’t much to
    link. Consider reading the [MobileNet paper ](https://arxiv.org/abs/1704.04861)(if
    you haven’t already) for other takes on efficiency.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读：**由于这些论文是2019年底和2020年的，链接不多。考虑阅读[MobileNet 论文](https://arxiv.org/abs/1704.04861)（如果你还没有读的话），了解其他关于效率的看法。'
- en: '#5 Human Baselines for Pose Estimation (2017)'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '#5 姿态估计的人类基线（2017年）'
- en: Xiao, Bin, Haiping Wu, and Yichen Wei. [“Simple baselines for human pose estimation
    and tracking.” ](http://openaccess.thecvf.com/content_iccv_2017/html/Martinez_A_Simple_yet_ICCV_2017_paper.html)*Proceedings
    of the European conference on computer vision (ECCV)*. 2018.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Xiao, Bin, Haiping Wu, and Yichen Wei. [“人类姿态估计与跟踪的简单基线。”](http://openaccess.thecvf.com/content_iccv_2017/html/Martinez_A_Simple_yet_ICCV_2017_paper.html)*欧洲计算机视觉会议论文集（ECCV）*。2018年。
- en: So far, most papers have proposed new techniques to improve the state-of-the-art.
    This paper, on the opposite, argues that a simple model, using current best practices,
    can be surprisingly effective. In sum, they proposed a human pose estimation network
    based solely on a backbone network followed by three de-convolution operations.
    At the time, their approach was the most effective at handling the COCO benchmark,
    despite its simplicity.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，大多数论文提出了新技术以改善最先进技术。相反，本文主张使用当前最佳实践的简单模型可能会出奇地有效。总之，他们提出了一种仅基于骨干网络并经过三次反卷积操作的人体姿态估计网络。当时，他们的方法在处理
    COCO 基准测试方面是最有效的，尽管其简单性。
- en: '**Reason #1:** Being simple is sometimes the most effective approach. While
    we all want to try the shiny and complicated novel architectures, a baseline model
    might be way faster to code and, yet, achieve similar results. This paper reminds
    us that not all good models need to be complicated.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #1：** 有时候简单是最有效的方法。虽然我们都想尝试炫酷和复杂的创新架构，但基线模型可能编码速度更快，同时取得类似的结果。本文提醒我们，并不是所有好的模型都需要复杂。'
- en: '**Reason #2:** Science moves in baby steps. Each new paper pushes the state-of-the-art
    a bit further. Yet, it does not need to be a one-way road. Sometimes it is worthwhile
    to backtrack a bit and take a different turn. “Stop Thinking with Your Head,”
    and “Reformer” are two other good examples of this.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #2：** 科学是循序渐进的。每篇新论文都将最先进技术推向前沿。然而，这不必是单向道路。有时值得稍微倒退一下，转个不同的方向。“停止用脑袋思考”和“Reformer”就是这方面的两个好例子。'
- en: '**Reason #3:** Proper data augmentation, training schedules, and a good problem
    formulation matter more than most people would acknowledge.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #3：** 适当的数据增强、训练计划和良好的问题表述比大多数人意识到的要重要。'
- en: '**Further Reading: **If interested in the Pose Estimation topic, you might
    consider reading [this comprehensive state-of-the-art review.](https://nanonets.com/blog/human-pose-estimation-2d-guide/)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读：**如果你对姿态估计话题感兴趣，可以考虑阅读[这篇全面的最先进综述。](https://nanonets.com/blog/human-pose-estimation-2d-guide/)'
- en: '#6 Bag of Tricks for Image Classification (2019)'
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '#6 图像分类的技巧包（2019年）'
- en: He, Tong, et al. [“Bag of tricks for image classification with convolutional
    neural networks.”](https://arxiv.org/abs/1812.01187) *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*. 2019.
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: He, Tong 等. [“用于卷积神经网络的图像分类技巧包。”](https://arxiv.org/abs/1812.01187) *IEEE 计算机视觉与模式识别会议论文集*.
    2019.
- en: Many times, what you need is not a fancy new model, just a couple of new tricks.
    In most papers, one or two new tricks are introduced to achieve a one or two percentage
    points improvement. However, these are often forgotten amid the major contributions.
    This paper collects a set of tips used throughout the literature and summarizes
    them for our reading pleasure.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 很多时候，你需要的不是一个华丽的新模型，而是几个新的技巧。在大多数论文中，引入了一两种新技巧以获得一到两个百分点的改进。然而，这些往往在重大贡献中被遗忘。本文汇集了一组在文献中使用的技巧，并为我们的阅读乐趣进行了总结。
- en: '**Reason #1: **Most tips are easily applicable'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #1:** 大多数技巧易于应用'
- en: '**Reason #2: **High are the odds you are unaware of most approaches. These
    are not the typical “use [ELU](https://www.tensorflow.org/api_docs/python/tf/nn/elu)”
    kind of suggestions.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #2:** 你可能对大多数方法并不了解。这些建议不是典型的“使用 [ELU](https://www.tensorflow.org/api_docs/python/tf/nn/elu)”类型的。'
- en: '**Further Readings:** Many other tricks exist, some are problem-specific, some
    are not. A topic I believe deserves more attention is class and sample weights.
    Consider reading [this paper on class weights for unbalanced datasets](http://openaccess.thecvf.com/content_CVPR_2019/papers/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.pdf).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读:** 还有许多其他技巧，有些是特定问题的，有些则不是。我认为值得更多关注的主题是类别和样本权重。考虑阅读 [这篇关于不平衡数据集的类别权重的论文](http://openaccess.thecvf.com/content_CVPR_2019/papers/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.pdf)。'
- en: '#7 The SELU Activation (2017)'
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '#7 SELU 激活函数（2017）'
- en: Klambauer, Günter, et al. [“Self-normalizing neural networks.”](https://arxiv.org/abs/1706.02515) *Advances
    in neural information processing systems*. 2017.
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Klambauer, Günter 等. [“自归一化神经网络。”](https://arxiv.org/abs/1706.02515) *神经信息处理系统进展*.
    2017.
- en: 'Most of us use Batch Normalization layers and the ReLU or ELU activation functions.
    In the SELU paper, the authors propose a unifying approach: an activation that
    self-normalizes its outputs. In practice, this renders batch normalization layers
    obsolete. Therefore, models using SELU activations are simpler and need fewer
    operations.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们中的大多数人使用批归一化层和 ReLU 或 ELU 激活函数。在 SELU 论文中，作者提出了一种统一的方法：一种自归一化其输出的激活函数。在实践中，这使得批归一化层变得多余。因此，使用
    SELU 激活函数的模型更简单，所需操作更少。
- en: '**Reason #1:** In the paper, the authors mostly deal with standard machine
    learning problems (tabular data). Most data scientists deal primarily with images.
    Reading a paper on purely dense networks is a bit of a refreshment.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #1:** 论文中，作者主要处理标准机器学习问题（表格数据）。大多数数据科学家主要处理图像。阅读一篇纯粹关于密集网络的论文，确实是一种清新的体验。'
- en: '**Reason #2: **If you have to deal with tabular data, this is one of the most
    up-to-date approaches to the topic within the Neural Networks literature.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #2:** 如果你需要处理表格数据，这是神经网络文献中最先进的方法之一。'
- en: '**Reason #3:** The paper is math-heavy and uses a computationally derived proof.
    This, in itself, is a rare but beautiful thing to be seen.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #3:** 这篇论文数学内容繁重，并使用了计算推导证明。这本身就是一种稀有但美妙的现象。'
- en: '**Further Reading:** If you want to dive into the history and usage of the
    most popular activation functions, I wrote a [guide on activation functions](https://towardsdatascience.com/a-comprehensive-guide-on-activation-functions-b45ed37a4fa5) here
    on Medium. Check it out :)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读:** 如果你想深入了解最流行的激活函数的历史和使用，我在 Medium 上写了一份 [激活函数指南](https://towardsdatascience.com/a-comprehensive-guide-on-activation-functions-b45ed37a4fa5)。快去看看
    :)'
- en: '#8 Bag-of-local-Features (2019)'
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '#8 本地特征包（2019）'
- en: Brendel, Wieland, and Matthias Bethge. [“Approximating cnns with bag-of-local-features
    models works surprisingly well on imagenet.”](https://arxiv.org/abs/1904.00760) *arXiv
    preprint arXiv:1904.00760* (2019).
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Brendel, Wieland, 和 Matthias Bethge. [“用本地特征包模型近似 CNN 在 ImageNet 上效果出奇的好。”](https://arxiv.org/abs/1904.00760)
    *arXiv 预印本 arXiv:1904.00760* (2019).
- en: If you break an image into jigsaw-like pieces, scramble them, and show them
    to a kid, it won’t be able to recognize the original object; a CNN might. In this
    paper, the authors found that classifying all 33x33 patches of an image and then
    averaging their class predictions achieves near state-of-the-art results on ImageNet.
    Moreover, they further explore this idea with VGG and ResNet-50 models, showing
    evidence that CNNs rely extensively on local information, with minimal global
    reasoning
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将一张图像分成拼图状的块，打乱它们，然后展示给一个孩子，它将无法识别原始对象；而 CNN 可能会。在这篇论文中，作者发现对图像的所有 33x33
    区块进行分类，然后平均它们的类别预测，可以在 ImageNet 上获得接近最先进的结果。此外，他们进一步探索了这个想法，使用了 VGG 和 ResNet-50
    模型，显示出 CNN 过度依赖局部信息，而全球推理则极其有限。
- en: '**Reason #1:** While many believe that CNNs “see,” this paper shows evidence
    that they might be way dumber than we would dare to bet our money.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**原因 #1：** 尽管许多人认为 CNN “看见”了，但这篇论文提供了证据表明它们可能比我们敢于下注的要愚蠢得多。'
- en: '**Reason #2: **Only once in a while we get to see a paper with a fresh new
    take on the limitations of CNNs and their interpretability.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**原因 #2：** 我们偶尔会看到一篇对 CNN 的局限性及其可解释性提出新见解的论文。'
- en: '**Further Reading: **Related in its findings, the adversarial attacks literature
    also shows other striking limitations of CNNs. Consider reading the following
    article (and its reference section):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读：** 相关的对抗攻击文献也显示了 CNN 的其他明显限制。考虑阅读以下文章（及其参考部分）：'
- en: '[**Breaking neural networks with adversarial attacks**](https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[**通过对抗攻击破解神经网络**](https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa)'
- en: Are the machine learning models we use intrinsically flawed?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的机器学习模型本质上是否存在缺陷？
- en: '#9 The Lottery Ticket Hypothesis (2019)'
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '#9 彩票假设（2019）'
- en: 'Frankle, Jonathan, and Michael Carbin. [“The lottery ticket hypothesis: Finding
    sparse, trainable neural networks.”](https://arxiv.org/abs/1803.03635) *arXiv
    preprint arXiv:1803.03635* (2018).'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Frankle, Jonathan, 和 Michael Carbin. [“彩票假设：寻找稀疏的可训练神经网络。”](https://arxiv.org/abs/1803.03635)
    *arXiv 预印本 arXiv:1803.03635*（2018）。
- en: Continuing on the theoretical papers, Frankle *et al. *found that if you train
    a big network, prune all low-valued weights, rollback the pruned network, and
    train again, you will get a better performing network. The lottery analogy is
    seeing each weight as a “lottery ticket.” With a billion tickets, winning the
    prize is certain. However, most of the tickets won’t win, only a couple will.
    If you could go back in time and buy only the winning tickets, you would maximize
    your profits. “A billion tickets” is a big initial network. “Training” is running
    the lottery and seeing which weights are high-valued. “Going back in time” is
    rolling-back to the initial untrained network and rerunning the lottery. In the
    end, you will get a better performing network.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 继续讨论理论论文，Frankle *等人* 发现如果你训练一个大网络，剪枝所有低价值的权重，回滚剪枝后的网络，再次训练，你将得到一个表现更好的网络。彩票类比是将每个权重视为一个“彩票票”。有十亿张票，赢得奖品是确定的。然而，大多数票不会中奖，只有少数几张。如果你可以回到过去，只买中奖票，你将最大化你的利润。“十亿张票”是一个大的初始网络。“训练”是进行抽奖，看看哪些权重是高价值的。“回到过去”是回滚到初始的未训练网络，并重新进行抽奖。最终，你会得到一个表现更好的网络。
- en: '**Reason #1: **The idea is insanely cool.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**原因 #1：** 这个想法非常酷。'
- en: '**Reason #2:** As for the Bag-of-Features paper, this sheds some light on how
    limited our current understanding of CNNs is. After reading this paper, I realized
    how underutilized our millions of parameters are. An open question is how much.
    The authors managed to reduce networks to a tenth of their original sizes, how
    much more might be possible in the future?'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**原因 #2：** 关于 Bag-of-Features 论文，这揭示了我们目前对 CNN 理解的局限性。阅读这篇论文后，我意识到我们数百万个参数的利用率是多么低。一个未解之谜是有多少。作者设法将网络的规模缩减到原始大小的十分之一，未来还可能减少多少？'
- en: '**Reason #3: **These ideas also give us more perspective on how inefficient
    behemoth networks are. Consider the Reformer paper, mentioned before. It drastically
    reduced the size of the Transformer by improving the algorithm. How much more
    could be reduced by using the lottery technique?'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**原因 #3：** 这些想法还给我们提供了更多关于庞大网络低效性的视角。考虑之前提到的 Reformer 论文。它通过改进算法大幅减少了 Transformer
    的规模。使用彩票技术还可以减少多少？'
- en: '**Further Reading: **Weight initialization is an often overlooked topic. In
    my experience, most people stick to the defaults, [which might not always be the
    best option.](https://towardsdatascience.com/why-default-cnn-are-broken-in-keras-and-how-to-fix-them-ce295e5e5f2) [“All
    You Need is a Good Init”](https://arxiv.org/abs/1511.06422) is a seminal paper
    on the topic. As for the lottery hypothesis, the following is an easy to read
    review:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读：**权重初始化是一个常被忽视的话题。在我的经验中，大多数人坚持使用默认值，[这可能并非总是最佳选择。](https://towardsdatascience.com/why-default-cnn-are-broken-in-keras-and-how-to-fix-them-ce295e5e5f2)
    [“你只需要一个好的初始化”](https://arxiv.org/abs/1511.06422) 是这方面的开创性论文。至于彩票票据假说，以下是一个易读的综述：'
- en: '[**Breaking down the Lottery Ticket Hypothesis**](https://towardsdatascience.com/breaking-down-the-lottery-ticket-hypothesis-ca1c053b3e58)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[**解构彩票票据假说**](https://towardsdatascience.com/breaking-down-the-lottery-ticket-hypothesis-ca1c053b3e58)'
- en: 'Distilling the ideas from MIT CSAIL’s intriguing paper: “The Lottery Ticket
    Hypothesis: Finding Sparse, Trainable...'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从MIT CSAIL的有趣论文中提炼出的观点：“彩票票据假说：寻找稀疏的、可训练的……”
- en: '#10 Pix2Pix and CycleGAN (2017)'
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '#10 Pix2Pix和CycleGAN（2017年）'
- en: Isola, Phillip, et al. “[Image-to-image translation with conditional adversarial
    networks.”](https://phillipi.github.io/pix2pix/) *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 2017.
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Isola, Phillip等人。“[基于条件对抗网络的图像到图像翻译](https://phillipi.github.io/pix2pix/)。”
    *IEEE计算机视觉与模式识别会议论文集*。2017年。
- en: ''
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zhu, Jun-Yan, et al. [“Unpaired image-to-image translation using cycle-consistent
    adversarial networks.”](http://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html) *Proceedings
    of the IEEE international conference on computer vision*. 2017.
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Zhu, Jun-Yan等人。[“使用循环一致对抗网络进行非配对图像到图像翻译。”](http://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html)
    *IEEE国际计算机视觉会议论文集*。2017年。
- en: This list would not be complete without some GAN papers.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表如果没有一些GAN论文就不完整了。
- en: Pix2Pix and CycleGAN are the two seminal works on conditional generative models.
    Both perform the task of converting images from a domain A to a domain B and differ
    by leveraging paired and unpaired datasets. The former perform tasks such as converting
    line drawings to fully rendered images, and the latter excels at replacing entities,
    such as turning horses into zebras or apples into oranges. By being “conditional,”
    these models allow users to have some degree of control over what is being generated
    by tweaking the inputs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Pix2Pix和CycleGAN是条件生成模型的两个开创性工作。两者都执行将图像从领域A转换到领域B的任务，但通过利用配对和非配对数据集有所不同。前者执行诸如将线条图转换为完全渲染的图像等任务，后者则擅长于替换实体，例如将马变成斑马或将苹果变成橙子。通过“条件化”，这些模型使用户可以通过调整输入来对生成的内容进行一定程度的控制。
- en: '**Reason #1: **GAN papers are usually focused on the sheer quality of the generated
    results and place no emphasis on artistic control. Conditional models, such as
    these, provide an avenue for GANs to actually become useful in practice. [For
    instance, at being a virtual assistant to artists.](https://www.sbgames.org/sbgames2019/files/papers/ComputacaoFull/197880.pdf)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #1：**GAN论文通常集中于生成结果的质量，并且不强调艺术控制。这些条件模型为GAN在实际应用中提供了一个实际的途径。[例如，作为艺术家的虚拟助手。](https://www.sbgames.org/sbgames2019/files/papers/ComputacaoFull/197880.pdf)'
- en: '**Reason #2:** Adversarial approaches are the best examples of multi-network
    models. While generation might not be your thing, reading about multi-network
    setups might be inspiring for a number of problems.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #2：**对抗性方法是多网络模型的最佳示例。虽然生成可能不是你的强项，但阅读关于多网络设置的内容可能会对许多问题产生启发。'
- en: '**Reason #3: **The CycleGAN paper, in particular, demonstrates how an effective
    loss function can work wonders at solving some difficult problems. A similar idea
    is given by the Focal loss paper, which considerably improves object detectors
    by just replacing their traditional losses for a better one.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**理由 #3：**《CycleGAN》论文尤其展示了一个有效的损失函数如何在解决一些困难问题时发挥奇效。一个类似的观点在Focal loss论文中也有提出，该论文通过用更好的损失函数替代传统的损失函数，显著改善了目标检测器的表现。'
- en: '**Further Reading: **While AI is growing fast, GANs are growing faster. I highly
    recommend coding a GAN if you never have. [Here are the official Tensorflow 2
    docs on the matter](https://www.tensorflow.org/tutorials/generative/dcgan). One
    application of GANs that is not so well known (and you should check out) is [semi-supervised
    learning](https://arxiv.org/abs/1905.06484).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读：**虽然AI在快速发展，但GAN的增长更为迅猛。如果你从未编码过GAN，我强烈推荐你尝试一下。[这是关于这一问题的官方Tensorflow
    2文档](https://www.tensorflow.org/tutorials/generative/dcgan)。一个不太为人知晓的GAN应用是[半监督学习](https://arxiv.org/abs/1905.06484)，你应该了解一下。'
- en: With these twelve papers and their further readings, I believe you already have
    plenty of reading material to look at. This surely isn’t an exhaustive list of
    great papers. However, I tried my best to select the most insightful and seminal
    works I have seen and read. Please let me know if there are any other papers you
    believe should be on this list.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这十二篇论文及其后续阅读材料，我相信你已经有了丰富的阅读素材。这肯定不是一份详尽的好论文列表。然而，我尽力选择了我所见过和读过的最具洞察力和开创性的作品。如果你认为还有其他应该在此列表中的论文，请告诉我。
- en: Good reading :)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 祝阅读愉快 :)
- en: '**Edit: After writing this list, I compiled a second one with ten more AI papers
    read in 2020 and a third on GANs. If you enjoyed reading this list, you might
    enjoy its continuations:**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**编辑：在编写这份列表后，我又整理了第二份包含2020年阅读的十篇AI论文的列表，以及第三份关于GAN的列表。如果你喜欢阅读这份列表，你可能会喜欢它的续集：**'
- en: '[**Ten More AI Papers to Read in 2020**](https://towardsdatascience.com/ten-more-ai-papers-to-read-in-2020-8c6fb4650a9b)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[**2020年还需阅读的十篇AI论文**](https://towardsdatascience.com/ten-more-ai-papers-to-read-in-2020-8c6fb4650a9b)'
- en: Additional reading suggestions to keep you up-to-date with the latest and classic
    breakthroughs in AI and Data Science
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 额外的阅读建议，帮助你了解最新和经典的AI及数据科学突破
- en: '[**GAN Papers to Read in 2020**](https://towardsdatascience.com/gan-papers-to-read-in-2020-2c708af5c0a4)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[**2020年需阅读的GAN论文**](https://towardsdatascience.com/gan-papers-to-read-in-2020-2c708af5c0a4)'
- en: Reading suggestions on Generative Adversarial Networks.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 关于生成对抗网络的阅读建议。
- en: '**Bio: [Ygor Rebouças Serpa](https://www.linkedin.com/in/ygorreboucas/)** ([**@ReboucasYgor**](https://twitter.com/ReboucasYgor))
    is a Master in Computer Science, by Universidade de Fortaleza, and is currently
    working on R&D, developing explainable AI tools for the healthcare industry. His
    interests range from music and game development to theoretical computer science
    and AI. See his **[Medium](https://medium.com/@ygorrebouasserpa)** profile for
    more writing.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Ygor Rebouças Serpa](https://www.linkedin.com/in/ygorreboucas/)** （[**@ReboucasYgor**](https://twitter.com/ReboucasYgor)）是来自大学城大学计算机科学硕士，目前从事研发，开发医疗行业的可解释AI工具。他的兴趣从音乐和游戏开发到理论计算机科学和AI。请查看他的**[Medium](https://medium.com/@ygorrebouasserpa)**
    个人主页了解更多写作。'
- en: '[Original](https://towardsdatascience.com/ai-papers-to-read-in-2020-ac0e4e91d915).
    Reposted with permission.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/ai-papers-to-read-in-2020-ac0e4e91d915)。经授权转载。'
- en: '**Related:**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[13 must-read papers from AI experts](/2020/05/13-must-read-papers-ai-experts.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13篇来自AI专家的必读论文](/2020/05/13-must-read-papers-ai-experts.html)'
- en: '[Must-read NLP and Deep Learning articles for Data Scientists](/2020/08/must-read-nlp-deep-learning-articles.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学家必读的NLP和深度学习文章](/2020/08/must-read-nlp-deep-learning-articles.html)'
- en: '[5 Essential Papers on Sentiment Analysis](/2020/06/5-essential-papers-sentiment-analysis.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5篇情感分析的核心论文](/2020/06/5-essential-papers-sentiment-analysis.html)'
- en: More On This Topic
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Must Read NLP Papers from the Last 12 Months](https://www.kdnuggets.com/2023/03/must-read-nlp-papers-last-12-months.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[过去12个月必须阅读的NLP论文](https://www.kdnuggets.com/2023/03/must-read-nlp-papers-last-12-months.html)'
- en: '[KDnuggets News, April 27: A Brief Introduction to Papers With Code;…](https://www.kdnuggets.com/2022/n17.html)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets新闻，4月27日：简要介绍带代码的论文；…](https://www.kdnuggets.com/2022/n17.html)'
- en: '[Top Machine Learning Papers to Read in 2023](https://www.kdnuggets.com/2023/03/top-machine-learning-papers-read-2023.html)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2023年必须阅读的顶级机器学习论文](https://www.kdnuggets.com/2023/03/top-machine-learning-papers-read-2023.html)'
- en: '[Generative Agent Research Papers You Should Read](https://www.kdnuggets.com/generative-agent-research-papers-you-should-read)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你应该阅读的生成代理研究论文](https://www.kdnuggets.com/generative-agent-research-papers-you-should-read)'
- en: '[5 Machine Learning Papers to Read in 2024](https://www.kdnuggets.com/5-machine-learning-papers-to-read-in-2024)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2024年需阅读的5篇机器学习论文](https://www.kdnuggets.com/5-machine-learning-papers-to-read-in-2024)'
- en: '[A Brief Introduction to Papers With Code](https://www.kdnuggets.com/2022/04/brief-introduction-papers-code.html)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[简要介绍带代码的论文](https://www.kdnuggets.com/2022/04/brief-introduction-papers-code.html)'
