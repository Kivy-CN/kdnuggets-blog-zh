- en: 'Lit BERT: NLP Transfer Learning In 3 Steps'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lit BERT：NLP迁移学习的3个步骤
- en: 原文：[https://www.kdnuggets.com/2019/11/lit-bert-nlp-transfer-learning-3-steps.html](https://www.kdnuggets.com/2019/11/lit-bert-nlp-transfer-learning-3-steps.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/11/lit-bert-nlp-transfer-learning-3-steps.html](https://www.kdnuggets.com/2019/11/lit-bert-nlp-transfer-learning-3-steps.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[comments](#comments)'
- en: '**By [William Falcon](https://www.linkedin.com/in/wfalcon/), AI Researcher**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[William Falcon](https://www.linkedin.com/in/wfalcon/)，AI研究员**'
- en: '![Figure](../Images/1d950559d97e64975792a40da35f8bc1.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/1d950559d97e64975792a40da35f8bc1.png)'
- en: '[BERT](https://arxiv.org/pdf/1810.04805.pdf) (Devlin, et al, 2018) is perhaps
    the most popular NLP approach to transfer learning. The implementation by [Huggingface](https://github.com/huggingface/transformers) offers
    a lot of nice features and abstracts away details behind a beautiful API.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[BERT](https://arxiv.org/pdf/1810.04805.pdf)（Devlin等，2018）也许是最流行的迁移学习NLP方法。由[Huggingface](https://github.com/huggingface/transformers)提供的实现提供了许多良好的功能，并通过一个美丽的API抽象化了细节。'
- en: '[PyTorch Lightning](https://github.com/williamFalcon/pytorch-lightning) is
    a lightweight framework (really more like refactoring your PyTorch code) which
    allows anyone using [PyTorch](https://pytorch.org/) such as students, researchers
    and production teams, to scale deep learning code easily while making it reproducible.
    It also provides 42+ advanced research features via trainer flags.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[PyTorch Lightning](https://github.com/williamFalcon/pytorch-lightning)是一个轻量级框架（实际上更像是重构你的PyTorch代码），允许使用[PyTorch](https://pytorch.org/)的学生、研究人员和生产团队等，轻松扩展深度学习代码并确保其可重复性。它还通过训练器标志提供42+个高级研究功能。'
- en: Lightning does not add abstractions on to of PyTorch which means it plays nicely
    with other great packages like Huggingface! In this tutorial we’ll use their implementation
    of BERT to do a finetuning task in Lightning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Lightning不会在PyTorch之上添加抽象，这意味着它与其他优秀的软件包，如Huggingface，兼容良好！在本教程中，我们将使用他们的BERT实现来完成Lightning中的微调任务。
- en: 'In this tutorial we’ll do transfer learning for NLP in 3 steps:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将通过3个步骤进行NLP的迁移学习：
- en: We’ll import BERT from the [huggingface](https://github.com/huggingface/transformers) library.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从[huggingface](https://github.com/huggingface/transformers)库中导入BERT。
- en: We’ll create a [LightningModule](https://pytorch-lightning.readthedocs.io/en/latest/LightningModule/RequiredTrainerInterface/) which
    finetunes using features extracted by BERT
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将创建一个[LightningModule](https://pytorch-lightning.readthedocs.io/en/latest/LightningModule/RequiredTrainerInterface/)，该模块使用BERT提取的特征进行微调。
- en: We’ll train the BertMNLIFinetuner using the [Lighting Trainer](https://github.com/williamFalcon/pytorch-lightning).
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用[Lighting Trainer](https://github.com/williamFalcon/pytorch-lightning)训练BertMNLIFinetuner。
- en: Live DEMO
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实时演示
- en: If you’d rather see this in actual code, [copy this colab notebook!](https://colab.research.google.com/drive/1DovlWenVCuXZ-EZT66wc3GVHZqREyblV)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望看到实际代码，[复制此colab笔记本！](https://colab.research.google.com/drive/1DovlWenVCuXZ-EZT66wc3GVHZqREyblV)
- en: Finetuning (aka transfer learning)
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调（即迁移学习）
- en: '![](../Images/b02a8bd976da1d4c6433ae56b977e8ba.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b02a8bd976da1d4c6433ae56b977e8ba.png)'
- en: If you’re a researcher trying to improve on the [NYU GLUE](https://gluebenchmark.com/) benchmark,
    or a data scientist trying to understand product reviews to recommend new content,
    you’re looking for a way to extract a representation of a piece of text so you
    can solve a different task.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一位研究人员试图改进[NYU GLUE](https://gluebenchmark.com/)基准，或是一位数据科学家试图理解产品评论以推荐新内容，你可能在寻找一种方法来提取文本表示，从而解决不同的任务。
- en: For transfer learning you generally have two steps. You use dataset X to pretrain
    your model. Then you use that pretrained model to carry that knowledge into solving
    dataset B. In this case, BERT has been pretrained on BookCorpus and English Wikipedia [[1]](https://arxiv.org/pdf/1810.04805.pdf).
    The *downstream task* is what you care about which is solving a GLUE task or classifying
    product reviews.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于迁移学习，通常有两个步骤。你使用数据集X来预训练模型。然后，你使用该预训练模型将知识应用于解决数据集B。在这种情况下，BERT已经在BookCorpus和英文维基百科上进行了预训练[[1]](https://arxiv.org/pdf/1810.04805.pdf)。*下游任务*是你关心的，即解决GLUE任务或分类产品评论。
- en: The benefit of pretraining is that we don’t need much data in the downstream
    task to get amazing results.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的好处在于我们不需要在下游任务中使用大量数据即可获得出色的结果。
- en: Finetuning with PyTorch Lightning
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PyTorch Lightning进行微调
- en: '![](../Images/9c69c33774c195f6c27f989c23b28fa0.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c69c33774c195f6c27f989c23b28fa0.png)'
- en: 'In general, we can finetune with PyTorch Lightning using the following abstract
    approach:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们可以使用以下抽象方法通过PyTorch Lightning进行微调：
- en: For transfer learning we define two core parts inside the LightningModule.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于迁移学习，我们在LightningModule中定义了两个核心部分。
- en: 'The pretrained model (ie: feature extractor)'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预训练模型（即：特征提取器）
- en: The finetune model.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微调模型。
- en: You can think of the pretrained model as a feature extractor. This can allow
    you to represent objects or inputs in a much better way than say a boolean or
    some tabular mapping.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将预训练模型视为特征提取器。这可以让你以比布尔值或某些表格映射更好的方式表示对象或输入。
- en: For instance if you have a collection of documents, you could run each through
    the pretrained model, and use the output vectors to compare documents to each
    other.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你有一组文档，你可以将每个文档输入到预训练模型中，并使用输出向量将文档彼此比较。
- en: The finetune model can be arbitrarily complex. It could be a deep network, or
    it could be a simple Linear model or SVM.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型可以任意复杂。它可以是一个深度网络，或者是一个简单的线性模型或SVM。
- en: Finetuning with BERT
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用BERT进行微调
- en: '![Figure](../Images/058fe3b161fc7527c47985b6ed22ae7a.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/058fe3b161fc7527c47985b6ed22ae7a.png)'
- en: Huggingface
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Huggingface
- en: 'Here we’ll use a pretrained BERT to finetune on a task called MNLI. This is
    really just trying to classify text into three categories. Here’s the LightningModule:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用一个预训练的BERT对MNLI任务进行微调。这实际上是试图将文本分类为三种类别。以下是LightningModule：
- en: In this case we’re using the pretrained BERT from the huggingface library and
    adding our own simple linear classifier to classify a given text input into one
    of three classes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用来自huggingface库的预训练BERT，并添加了我们自己的简单线性分类器，将给定的文本输入分类为三种类别之一。
- en: However, we still need to define the validation loop which calculates our validation
    accuracy
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们仍然需要定义计算验证准确性的验证循环
- en: And the test loop which calcualates our test accuracy
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以及计算我们测试准确性的测试循环
- en: Finally, we define the optimizer and dataset we’ll operate on. This dataset
    should be the *downstream dataset* which you’re trying to solve.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义了优化器和我们将操作的数据集。这个数据集应该是你试图解决的*下游数据集*。
- en: The full LightningModule Looks like this.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的LightningModule如下所示。
- en: Summary
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: 'Here we learned to use the Huggingface BERT as a feature extractor inside a
    LightningModule. This approach means you can leverage a really strong text representation
    to do things like:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们学会了在LightningModule中使用Huggingface BERT作为特征提取器。这种方法意味着你可以利用非常强大的文本表示来完成以下任务：
- en: Sentiment analysis
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 情感分析
- en: Suggested replies to chatbots
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建议的聊天机器人回复
- en: Build recommendation engines using NLP
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用NLP构建推荐引擎
- en: '[Improve the Google Search algorithm](https://www.blog.google/products/search/search-language-understanding-bert/)'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[改进Google搜索算法](https://www.blog.google/products/search/search-language-understanding-bert/)'
- en: …
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: …
- en: Create embeddings for documents for similarity search
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为文档创建嵌入进行相似性搜索
- en: Anything you can creatively think about!
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任何你能富有创意地想到的事情！
- en: You also saw how well [PyTorch Lightning](https://github.com/williamFalcon/pytorch-lightning/) plays
    with other libraries including [Huggingface](https://github.com/huggingface/transformers)!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你还看到了[PyTorch Lightning](https://github.com/williamFalcon/pytorch-lightning/)与其他库，包括[Huggingface](https://github.com/huggingface/transformers)，的良好配合！
- en: '**Bio: [William Falcon](https://www.linkedin.com/in/wfalcon/)** is an AI Researcher,
    startup founder, CTO, Google Deepmind Fellow, and current PhD AI research intern
    at Facebook AI.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**简历：[William Falcon](https://www.linkedin.com/in/wfalcon/)** 是一位AI研究员、初创公司创始人、首席技术官、Google
    Deepmind研究员，并且目前是Facebook AI的博士AI研究实习生。'
- en: '[Original](https://towardsdatascience.com/lit-bert-nlp-transfer-learning-in-3-steps-272a866570db).
    Reposted with permission.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/lit-bert-nlp-transfer-learning-in-3-steps-272a866570db)。已获得许可转载。'
- en: '**Related:**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Pytorch Lightning vs PyTorch Ignite vs Fast.ai](/2019/08/pytorch-lightning-vs-pytorch-ignite-vs-fast-ai.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pytorch Lightning与PyTorch Ignite与Fast.ai](/2019/08/pytorch-lightning-vs-pytorch-ignite-vs-fast-ai.html)'
- en: '[Attention Craving RNNS: Building Up To Transformer Networks](/2019/04/attention-craving-rnn-building-transformer-networks.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[渴望注意的RNNS：逐步构建Transformer网络](/2019/04/attention-craving-rnn-building-transformer-networks.html)'
- en: '[9 Tips For Training Lightning-Fast Neural Networks In Pytorch](/2019/08/9-tips-training-lightning-fast-neural-networks-pytorch.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在Pytorch中训练快速神经网络的9个技巧](/2019/08/9-tips-training-lightning-fast-neural-networks-pytorch.html)'
- en: '* * *'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织IT工作'
- en: '* * *'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[8 Innovative BERT Knowledge Distillation Papers That Have Changed…](https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8篇创新的BERT知识蒸馏论文改变了…](https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html)'
- en: '[Fine-Tuning BERT for Tweets Classification with HuggingFace](https://www.kdnuggets.com/2022/01/finetuning-bert-tweets-classification-ft-hugging-face.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用HuggingFace微调BERT以进行推文分类](https://www.kdnuggets.com/2022/01/finetuning-bert-tweets-classification-ft-hugging-face.html)'
- en: '[Classifying Long Text Documents Using BERT](https://www.kdnuggets.com/2022/02/classifying-long-text-documents-bert.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用BERT对长文本进行分类](https://www.kdnuggets.com/2022/02/classifying-long-text-documents-bert.html)'
- en: '[How Fast Can BERT Go With Sparsity?](https://www.kdnuggets.com/2022/04/fast-bert-go-sparsity.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BERT在稀疏性下的处理速度有多快？](https://www.kdnuggets.com/2022/04/fast-bert-go-sparsity.html)'
- en: '[Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT](https://www.kdnuggets.com/comparing-natural-language-processing-techniques-rnns-transformers-bert)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[比较自然语言处理技术：RNNs、Transformers、BERT](https://www.kdnuggets.com/comparing-natural-language-processing-techniques-rnns-transformers-bert)'
- en: '[Extractive Summarization with LLM using BERT](https://www.kdnuggets.com/extractive-summarization-with-llm-using-bert)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用BERT的抽取式总结](https://www.kdnuggets.com/extractive-summarization-with-llm-using-bert)'
