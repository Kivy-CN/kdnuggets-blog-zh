- en: Unraveling the Power of Chain-of-Thought Prompting in Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭示链式思维提示在大型语言模型中的力量
- en: 原文：[https://www.kdnuggets.com/2023/07/power-chain-thought-prompting-large-language-models.html](https://www.kdnuggets.com/2023/07/power-chain-thought-prompting-large-language-models.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/07/power-chain-thought-prompting-large-language-models.html](https://www.kdnuggets.com/2023/07/power-chain-thought-prompting-large-language-models.html)
- en: '![Unraveling the Power of Chain-of-Thought Prompting in Large Language Models](../Images/786c67d6492286a8cfa0ded2b2775d7e.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![揭示链式思维提示在大型语言模型中的力量](../Images/786c67d6492286a8cfa0ded2b2775d7e.png)'
- en: Image created by author with Midjourney
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作者通过 Midjourney 创建的图像
- en: Introducing Chain-of-Thought Prompting
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍链式思维提示
- en: Large Language Models (LLMs) have revolutionized the field of artificial intelligence,
    offering unprecedented capabilities in natural language understanding and generation.
    However, their ability to perform complex reasoning tasks has been a subject of
    intense research. One technique that has shown promise in this regard is Chain-of-Thought
    (CoT) prompting. This article explores the intricacies of CoT prompting and its
    implications for the future of LLMs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经彻底改变了人工智能领域，提供了前所未有的自然语言理解和生成能力。然而，它们执行复杂推理任务的能力一直是密切研究的主题。一种在这方面表现出希望的技术是链式思维（CoT）提示。本文探讨了
    CoT 提示的复杂性及其对 LLMs 未来的影响。
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前 3 个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您组织的 IT 工作'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: CoT prompting, as introduced in a [recent paper](https://arxiv.org/pdf/2201.11903.pdf),
    is a method that encourages LLMs to explain their reasoning process. This is achieved
    by providing the model with a few-shot exemplars where the reasoning process is
    explicitly outlined. The LLM is then expected to follow a similar reasoning process
    when answering the prompt. This technique has been found to significantly improve
    the model's performance on tasks that require complex reasoning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CoT 提示，如在[最近的论文](https://arxiv.org/pdf/2201.11903.pdf)中介绍的，是一种鼓励 LLM 解释其推理过程的方法。这是通过向模型提供几个示例来实现的，其中推理过程被明确地概述。然后，LLM
    预计会在回答提示时遵循类似的推理过程。这种技术已被发现显著提高模型在需要复杂推理任务上的表现。
- en: One of the key advantages of CoT prompting is its ability to enhance the performance
    of LLMs on tasks that require arithmetic, commonsense, and symbolic reasoning.
    The technique has been shown to yield performance gains, particularly with models
    of approximately 100 billion parameters. Smaller models, however, have been found
    to produce illogical chains of thought, leading to lower accuracy than standard
    prompting.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: CoT 提示的一个主要优势是其提升 LLM 在需要算术、常识和符号推理任务中的表现的能力。该技术已被证明可以提高性能，特别是对于大约 1000 亿参数的模型。然而，较小的模型被发现会产生不合逻辑的思维链，导致比标准提示方法更低的准确性。
- en: Understanding Chain-of-Thought Prompting
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解链式思维提示
- en: At its core, CoT prompting is about guiding the LLM to think step by step. This
    is achieved by providing the model with a few-shot exemplar that outlines the
    reasoning process. The model is then expected to follow a similar chain of thought
    when answering the prompt. This approach is particularly effective for complex
    tasks that require a series of reasoning steps before a response can be generated.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，CoT 提示是关于引导 LLM 按步骤思考。这是通过向模型提供几个示例来实现的，这些示例概述了推理过程。然后，模型在回答提示时预计会遵循类似的思维链。这种方法对于需要一系列推理步骤才能生成响应的复杂任务特别有效。
- en: 'Here is a sample CoT prompt (taken from [this article](https://machinelearningmastery.com/prompt-engineering-for-effective-interaction-with-chatgpt/)
    on Machine Learning Mastery), using a few-shot strategy:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个 CoT 提示示例（取自 [这篇文章](https://machinelearningmastery.com/prompt-engineering-for-effective-interaction-with-chatgpt/)
    机器学习大师上的文章），使用了少量示例策略：
- en: 'Q: Joe has 20 eggs. He buys 2 more cartons of eggs. Each carton contains 12
    eggs. How many eggs does Joe have now?'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问：乔有 20 个鸡蛋。他又买了 2 箱鸡蛋。每箱包含 12 个鸡蛋。乔现在有多少个鸡蛋？
- en: 'A: Joe started with 20 eggs. 2 cartons of 12 eggs is 24 eggs. 20 + 24 = 44\.
    Therefore, Joe has 44 eggs, and the answer is 44.'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 答：乔开始时有 20 个鸡蛋。2 箱 12 个鸡蛋就是 24 个鸡蛋。20 + 24 = 44。因此，乔现在有 44 个鸡蛋，答案是 44。
- en: ''
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Q: Farmer John had 93 chickens. If he sold 20 to Farmer Bill and bought twice
    that number more, how many chickens does Farmer John have now?'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问：农民约翰有 93 只鸡。如果他卖给农民比尔 20 只鸡，并买回两倍于卖出的数量，他现在有多少只鸡？
- en: 'ChatGPT’s response:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 的回应：
- en: 'A: Farmer John sold 20 chickens to Farmer Bill, so he was left with 93 – 20
    = 73 chickens. He then bought twice the number of chickens he sold, so he bought
    2 * 20 = 40 chickens. Adding these newly bought chickens to his remaining stick,
    Farmer John now has 73 + 40 = 113 chickens.'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 答：农民约翰卖给农民比尔 20 只鸡，因此他剩下了 93 – 20 = 73 只鸡。他然后购买了卖出的两倍数量的鸡，因此他买了 2 * 20 = 40
    只鸡。将这些新买的鸡加到剩余的鸡上，农民约翰现在有 73 + 40 = 113 只鸡。
- en: CoT prompting can also be used in a zero-shot setting. This involves adding
    a phrase like "Let's think step by step" to the original prompt, which can also
    be made use of alongside few-shot prompting. This simple addition has been found
    to be effective at improving the model's performance on tasks where there are
    not many examples to use in the prompt.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: CoT 提示还可以在零-shot 设置中使用。这涉及在原始提示中添加类似“我们一步一步来思考”的短语，这也可以与少量示例提示一起使用。这种简单的添加已被发现能有效提高模型在没有很多示例的任务上的性能。
- en: Automating Chain-of-Thought Prompting
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化思维链提示
- en: While CoT prompting can be effective, it often involves hand-crafting examples,
    which can be time-consuming and may lead to suboptimal solutions. To address this,
    [researchers have proposed](https://arxiv.org/abs/2210.03493) an approach known
    as Automatic Chain-of-Thought (Auto-CoT). This method leverages LLMs to generate
    reasoning chains for demonstrations automatically, thereby eliminating the need
    for manual effort.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 CoT 提示可能有效，但它通常需要手动制作示例，这可能会耗费时间并导致次优解。为了解决这个问题，[研究人员提出了](https://arxiv.org/abs/2210.03493)
    一种被称为自动化思维链（Auto-CoT）的方法。这种方法利用 LLM 自动生成演示的推理链，从而消除了手动工作的需要。
- en: 'Auto-CoT consists of two main stages: question clustering and demonstration
    sampling. In the question clustering stage, questions from a given dataset are
    partitioned into a few clusters. In the demonstration sampling stage, a representative
    question is selected from each cluster, and its reasoning chain is generated using
    Zero-Shot-CoT with simple heuristics. This process encourages the model to use
    simple and accurate demonstrations.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Auto-CoT 包含两个主要阶段：问题聚类和演示抽样。在问题聚类阶段，将给定数据集中的问题划分为几个簇。在演示抽样阶段，从每个簇中选择一个代表性问题，并使用简单启发式的
    Zero-Shot-CoT 生成其推理链。这个过程鼓励模型使用简单且准确的演示。
- en: 'The Auto-CoT process:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Auto-CoT 过程：
- en: 'Question clustering: Partition questions of a given dataset into a few clusters'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题聚类：将给定数据集中的问题划分为几个簇
- en: 'Demonstration sampling: Select a representative question from each cluster
    and generate its reasoning chain using Zero-Shot-CoT with simple heuristics'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 演示抽样：从每个簇中选择一个代表性问题，并使用简单启发式的 Zero-Shot-CoT 生成其推理链
- en: Limitations and Future Directions
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制与未来方向
- en: While CoT prompting has shown promise, it is not without its limitations. For
    one, it only yields performance gains when used with models of approximately 100
    billion parameters. Smaller models tend to produce illogical chains of thought,
    leading to lower accuracy than standard prompting. Furthermore, the performance
    boosts from CoT prompting are generally proportional to the size of the model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 CoT 提示显示出前景，但它并非没有局限性。首先，它仅在使用约 1000 亿参数的模型时才能获得性能提升。较小的模型往往会产生不合理的思维链，导致比标准提示更低的准确性。此外，CoT
    提示的性能提升通常与模型的大小成正比。
- en: Despite these limitations, CoT prompting represents a significant step forward
    in the quest to enhance the reasoning capabilities of LLMs. Future research will
    likely focus on refining this technique and exploring ways to make it more effective
    across a wider range of tasks and model sizes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些限制，CoT 提示在提升 LLM 推理能力的探索中代表了一个重要的进步。未来的研究可能会集中在改进这一技术，并探索使其在更广泛的任务和模型规模中更为有效的方法。
- en: Conclusion
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Chain-of-Thought prompting represents a significant advancement in the field
    of artificial intelligence, particularly in enhancing the reasoning capabilities
    of Large Language Models. By encouraging these models to explain their reasoning
    process, CoT prompting has shown promise in improving performance on complex tasks
    that require arithmetic, commonsense, and symbolic reasoning. While the technique
    has its limitations, it opens up exciting possibilities for the future of LLMs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 思维链提示代表了人工智能领域的重要进展，特别是在提升大语言模型的推理能力方面。通过鼓励这些模型解释其推理过程，CoT 提示在改进需要算术、常识和符号推理的复杂任务的表现上显示出了潜力。尽管该技术有其局限性，但它为
    LLM 的未来打开了令人兴奋的可能性。
- en: As we continue to push the boundaries of what LLMs can achieve, techniques like
    CoT prompting will play a crucial role. By enabling these models to think step
    by step and explain their reasoning, we can not only improve their performance
    on complex tasks but also gain valuable insights into their inner workings. The
    journey towards fully reasoning LLMs is still long, but with techniques like CoT
    prompting, we are certainly on the right path.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们不断突破 LLM 的能力界限，像 CoT 提示这样的技术将发挥关键作用。通过使这些模型逐步思考并解释其推理过程，我们不仅可以提高它们在复杂任务中的表现，还可以获得对其内部机制的宝贵洞察。虽然完全推理的
    LLM 之路仍然漫长，但有了像 CoT 提示这样的技术，我们无疑走在了正确的道路上。
- en: '**[Matthew Mayo](https://www.linkedin.com/in/mattmayo13/)** ([**@mattmayo13**](https://twitter.com/mattmayo13))
    is a Data Scientist and the Editor-in-Chief of KDnuggets, the seminal online Data
    Science and Machine Learning resource. His interests lie in natural language processing,
    algorithm design and optimization, unsupervised learning, neural networks, and
    automated approaches to machine learning. Matthew holds a Master''s degree in
    computer science and a graduate diploma in data mining. He can be reached at editor1
    at kdnuggets[dot]com.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Matthew Mayo](https://www.linkedin.com/in/mattmayo13/)** ([**@mattmayo13**](https://twitter.com/mattmayo13))
    是数据科学家和 KDnuggets 的总编辑，KDnuggets 是著名的在线数据科学和机器学习资源。他的兴趣领域包括自然语言处理、算法设计与优化、无监督学习、神经网络以及机器学习的自动化方法。Matthew
    拥有计算机科学硕士学位和数据挖掘研究生文凭。他可以通过 editor1 at kdnuggets[dot]com 联系到。'
- en: More On This Topic
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Thought Propagation: An Analogical Approach to Complex Reasoning…](https://www.kdnuggets.com/thought-propagation-an-analogical-approach-to-complex-reasoning-with-large-language-models)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[思维传播：一种复杂推理的类比方法…](https://www.kdnuggets.com/thought-propagation-an-analogical-approach-to-complex-reasoning-with-large-language-models)'
- en: '[Exploring Tree of Thought Prompting: How AI Can Learn to Reason…](https://www.kdnuggets.com/2023/07/exploring-tree-of-thought-prompting-ai-learn-reason-through-search.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[探索思维树提示：AI 如何学习推理…](https://www.kdnuggets.com/2023/07/exploring-tree-of-thought-prompting-ai-learn-reason-through-search.html)'
- en: '[Enhancing LLM Reasoning: Unveiling Chain of Code Prompting](https://www.kdnuggets.com/enhancing-llm-reasoning-unveiling-chain-of-code-prompting)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[增强 LLM 推理：揭示代码链提示的奥秘](https://www.kdnuggets.com/enhancing-llm-reasoning-unveiling-chain-of-code-prompting)'
- en: '[Unlocking GPT-4 Summarization with Chain of Density Prompting](https://www.kdnuggets.com/unlocking-gpt-4-summarization-with-chain-of-density-prompting)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过密度链提示解锁 GPT-4 总结](https://www.kdnuggets.com/unlocking-gpt-4-summarization-with-chain-of-density-prompting)'
- en: '[Automating the Chain of Thought: How AI Can Prompt Itself to Reason](https://www.kdnuggets.com/2023/07/automating-chain-of-thought-ai-prompt-itself-reason.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动化思维链：AI 如何促使自己进行推理](https://www.kdnuggets.com/2023/07/automating-chain-of-thought-ai-prompt-itself-reason.html)'
- en: '[Diving into the Pool: Unraveling the Magic of CNN Pooling Layers](https://www.kdnuggets.com/diving-into-the-pool-unraveling-the-magic-of-cnn-pooling-layers)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深入探索：解开 CNN 池化层的魔力](https://www.kdnuggets.com/diving-into-the-pool-unraveling-the-magic-of-cnn-pooling-layers)'
