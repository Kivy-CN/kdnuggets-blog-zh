- en: Multimodal Models Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/03/multimodal-models-explained.html](https://www.kdnuggets.com/2023/03/multimodal-models-explained.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/f61a619c4285c475d4798e805c871a5f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is a type of intelligence developed to mimic the systems and neurons
    in the human brain, which play an essential role in the human thinking process.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This technology utilizes deep neural networks, which are composed of layers
    of artificial neurons that can analyze and process vast amounts of data, enabling
    them to learn and improve over time.
  prefs: []
  type: TYPE_NORMAL
- en: We, humans, rely on our five senses to interpret the world around us. We use
    our senses of sight, hearing, touch, taste, and smell to gather information about
    our environment and make sense of it.
  prefs: []
  type: TYPE_NORMAL
- en: In a similar vein, multimodal learning is an exciting new field of AI that seeks
    to replicate this ability by combining information from multiple models. By integrating
    information from diverse sources such as text, image, audio, and video, multimodal
    models can build a richer and more complete understanding of the underlying data,
    unlock new insights, and enable a wide range of applications.
  prefs: []
  type: TYPE_NORMAL
- en: The techniques used in multimodal learning include fusion-based approaches,
    alignments-based approaches, and late fusion.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will explore the fundamentals of multimodal learning, including
    the different techniques used to fuse information from diverse sources, as well
    as its many exciting applications, from speech recognition to autonomous cars
    and beyond.
  prefs: []
  type: TYPE_NORMAL
- en: What is Multimodal Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/dac0d519e923f158fca3021294a95ad9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal learning is a subfield of artificial intelligence that seeks to effectively
    process and analyze data from multiple modalities.
  prefs: []
  type: TYPE_NORMAL
- en: In simple terms, this means combining information from different sources such
    as text, image, audio, and video to build a more complete and accurate understanding
    of the underlying data.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of multimodal learning has found applications in a wide range of
    subjects, including speech recognition, autonomous car, and emotion recognition.
    We’ll talk about them in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: The multimodal learning techniques enable models to process and analyze data
    from multiple modalities effectively, providing a more complete and accurate understanding
    of the underlying data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will mention these techniques, but before doing that,
    let’s talk about combining models.
  prefs: []
  type: TYPE_NORMAL
- en: These two concepts might look alike, but you’ll soon discover they aren’t.
  prefs: []
  type: TYPE_NORMAL
- en: Combining Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Combining models is a technique in machine learning that involves using multiple
    models to improve the performance of a single model.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind combining models is that one model's strengths can compensate
    for another's weakness, resulting in a more accurate and robust prediction. Ensemble
    models, stacking, and bagging are techniques used in combining models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/005efd5895d339d294e7f3594d2c552b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/1d74bdadb6f025c9cce32e2ad0ef09e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble models involve combining the outputs of multiple base models to produce
    a better overall model.
  prefs: []
  type: TYPE_NORMAL
- en: One example of an ensemble model is random forests. Random forests are a decision
    tree algorithm that combines multiple decision trees to improve the model's accuracy.
    Each decision tree is trained on a different subset of the data, and the final
    prediction is made by averaging the predictions of all the trees.
  prefs: []
  type: TYPE_NORMAL
- en: You can see how to use the random forests algorithm in the scikit-learn library
    [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).
  prefs: []
  type: TYPE_NORMAL
- en: Stacking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/fea9196c2057a9d8e2e7d4b1f458cdcf.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Stacking involves using the outputs of multiple models as inputs to another
    model.
  prefs: []
  type: TYPE_NORMAL
- en: One real-life example of stacking is in natural language processing, where it
    can be applied to sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the [Stanford Sentiment Treebank dataset](https://www.kaggle.com/datasets/atulanandjha/stanford-sentiment-treebank-v2-sst2)
    contains movie reviews with sentiment labels ranging from very negative to very
    positive. In this case, multiple models such as [random forest](https://www.stratascratch.com/blog/decision-tree-and-random-forest-algorithm-explained/?utm_source=blog&utm_medium=click&utm_campaign=kdn+multimodal+models),
    [Support Vector Machines (SVM)](https://www.stratascratch.com/blog/machine-learning-algorithms-explained-support-vector-machine/?utm_source=blog&utm_medium=click&utm_campaign=kdn+multimodal+models),
    and Naive Bayes can be trained to predict the sentiment of the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: The predictions of these models can then be combined using a meta-model such
    as logistic regression or neural network, which is trained on the outputs of the
    base models to make the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Stacking can improve the accuracy of sentiment prediction and make sentiment
    analysis more robust.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/4e5bf44d5da242bace04cbb436b418ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Bagging is another way of creating ensembles, where several base models are
    trained on different subsets of data, and their predictions are averaged to make
    a final decision.
  prefs: []
  type: TYPE_NORMAL
- en: One example of bagging is the bootstrap aggregating method, where multiple models
    are trained on different subsets of the training data. The final prediction is
    made by averaging the predictions of all the models.
  prefs: []
  type: TYPE_NORMAL
- en: An example of bagging in real life is in finance. The [S&P 500 dataset](https://www.kaggle.com/datasets/camnugent/sandp500)
    contains historical data on the stock prices of the 500 largest publicly traded
    companies in the United States between 2012 and 2018.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging can be used in this dataset by training multiple models in scikit-learn,
    such as [random forests](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
    and [gradient boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html),
    to predict the companies' stock prices.
  prefs: []
  type: TYPE_NORMAL
- en: Each model is trained on a different subset of the training data, and their
    predictions are averaged to make the final prediction. The use of bagging can
    improve the accuracy of the stock price prediction and make the financial analysis
    more robust.
  prefs: []
  type: TYPE_NORMAL
- en: Difference Between Combining Models & Multimodal Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In combining models, the models are trained independently, and the final prediction
    is made by combining the outputs of these models using techniques such as ensemble
    models, stacking, or bagging.
  prefs: []
  type: TYPE_NORMAL
- en: Combining models is particularly useful when the individual models have complementary
    strengths and weaknesses, as the combination can lead to a more accurate and robust
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: In multimodal learning, the goal is to combine information from different modalities
    to perform a prediction task. This can involve using techniques such as the fusion-based
    approach, alignment-based approach, or late fusion to create a high-dimensional
    representation that captures the semantic information of the data from each modality.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal learning is particularly useful when different modalities provide
    complementary information that can improve the accuracy of the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The main difference between combining models and multimodal learning is that
    combining models involve using multiple models to improve the performance of a
    single model. In contrast, multimodal learning involves learning from and combining
    information from multiple modalities such as image, text, and audio to perform
    a prediction test.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at multimodal learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal Learning Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/9053201e86bae0e9a62bdc0cc842e625.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Fusion-Based Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The fusion-based approach involves encoding the different modalities into a
    common representation space, where the representations are fused to create a single
    modality-invariant representation that captures the semantic information from
    all modalities.
  prefs: []
  type: TYPE_NORMAL
- en: This approach can be further divided into early fusion and mid-fusion techniques,
    depending on when the fusion occurs.
  prefs: []
  type: TYPE_NORMAL
- en: Text Captioning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A typical example of a fusion-based approach is an image and text captioning.
  prefs: []
  type: TYPE_NORMAL
- en: It’s the fusion-based approach because the image's visual features and the text's
    semantic information are encoded into a common representation space and then fused
    to generate a single modality-invariant representation that captures the semantic
    information from both modalities.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, the visual features of the image are extracted using a convolutional
    neural network (CNN), and the semantic information of the text is captured using
    a recurrent neural network (RNN).
  prefs: []
  type: TYPE_NORMAL
- en: These two modalities are then encoded into a common representation space. The
    visual and textual features are fused using concatenation or element-wise multiplication
    techniques to create a single modality-invariant representation.
  prefs: []
  type: TYPE_NORMAL
- en: This final representation can then be used to generate a caption for the image.
  prefs: []
  type: TYPE_NORMAL
- en: One open-source dataset that can be used to perform image and text captioning
    is the [Flickr30k dataset](http://shannon.cs.illinois.edu/DenotationGraph/), which
    contains 31,000 images along with five captions per image. This dataset contains
    images of various everyday scenes, with each image annotated by multiple people
    to provide diverse captions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/a4a2a1e298391d5c4ab98511d20012bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [https://paperswithcode.com/dataset/flickr30k-cna](https://paperswithcode.com/dataset/flickr30k-cna)'
  prefs: []
  type: TYPE_NORMAL
- en: The Flickr30k dataset can be used to apply the fusion-based approach for image
    and text captioning by extracting visual features from pre-trained CNNs and using
    techniques such as word embeddings or bag-of-words representations for textual
    features. The resulting fused representation can be used to generate more accurate
    and informative captions.
  prefs: []
  type: TYPE_NORMAL
- en: Alignment-Based Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This approach involves aligning the different modalities so that they can be
    compared directly.
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to create modality-invariant representations that can be compared
    across modalities. This approach is advantageous when the modalities share a direct
    relationship, such as in audio-visual speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Sign Language Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/d91defa682a24eb9c19af0a0bc720d99.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: One example of an alignment-based approach is in the task of sign language recognition.
  prefs: []
  type: TYPE_NORMAL
- en: This use involves the alignment-based approach because it requires the model
    to align the temporal information of both visual (video frames) and audio (audio
    waveform) modalities.
  prefs: []
  type: TYPE_NORMAL
- en: The task is for the model to recognize sign language gestures and translate
    them into text. The gestures are captured using a video camera, and the corresponding
    audio and the two modalities must be aligned to recognize the gestures accurately.
    This involves identifying the temporal alignment between the video frames and
    the audio waveform to recognize the gestures and the corresponding spoken words.
  prefs: []
  type: TYPE_NORMAL
- en: One open-source dataset for sign language recognition is the [RWTH-PHOENIX-Weather
    2014T dataset](https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX/)
    which contains video recordings of German Sign Language (DGS) from various signers.
    The dataset includes both visual and audio modalities, making it suitable for
    multimodal learning tasks that require alignment-based approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Late Fusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This approach involves combining the predictions from models trained on each
    modality separately. The individual predictions are then combined to create a
    final prediction. This approach is particularly useful when the modalities are
    not directly related, or the individual modalities provide complementary information.
  prefs: []
  type: TYPE_NORMAL
- en: Emotion Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/d49bee101c25b6e6ef1b83f047d0a57f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: A real-life example of late fusion is emotion recognition in music. In this
    task, the model must recognize the emotional content of a piece of music using
    both the audio features and the lyrics.
  prefs: []
  type: TYPE_NORMAL
- en: The late fusion approach is applied in this example because it combines the
    predictions from models trained on separate modalities (audio features and lyrics)
    to create a final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The individual models are trained separately on each modality, and the predictions
    are combined at a later stage.Therefore, late fusion.
  prefs: []
  type: TYPE_NORMAL
- en: The audio features can be extracted using techniques such as Mel-frequency cepstral
    coefficients (MFCCs), while the lyrics can be encoded using techniques such as
    bag-of-words or word embeddings. Models can be trained separately on each modality,
    and the predictions can be combined using late fusion to generate a final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The [DEAM dataset](https://cvml.unige.ch/databases/DEAM/) was designed to support
    research on music emotion recognition and analysis, and it includes both audio
    features and lyrics for a collection of over 2,000 songs. The audio features include
    various descriptors such as MFCCs, spectral contrast, and rhythm features, while
    the lyrics are represented using bag-of-words and word embedding techniques.
  prefs: []
  type: TYPE_NORMAL
- en: The late fusion approach can be applied to the DEAM dataset by combining the
    predictions from separate models trained on each modality (audio and lyrics).
  prefs: []
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/6b26f00321e980307d5ea0b278667f29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [DEAM Dataset - Emotional Analysis in Music](https://www.kaggle.com/datasets/imsparsh/deam-mediaeval-dataset-emotional-analysis-in-music)'
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can train a separate machine learning model to predict the
    emotional content of each song using audio features, such as MFCCs and spectral
    contrast.
  prefs: []
  type: TYPE_NORMAL
- en: Another model can be trained to predict the emotional content of each song using
    the lyrics, represented using techniques such as bag-of-words or word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: After training the individual models, the predictions from each model can be
    combined using the late fusion approach to generate a final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal Learning Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/68dc06f828f13da31418301422805d88.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multimodal data can come in different modalities, such as text and audio. Combining
    them in a way that preserves their individual characteristics while still capturing
    their relationships can be challenging.
  prefs: []
  type: TYPE_NORMAL
- en: This can lead to issues such as the model failing to generalize well, being
    biased towards one modality, or not effectively capturing the joint information.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve representation problems in multimodal learning, several strategies
    can be employed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Joint representation**: As mentioned earlier, this approach involves encoding
    both modalities into a shared high-dimensional space. Techniques like deep learning-based
    fusion methods can be used to learn optimal joint representations.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Coordinated representation**: Instead of fusing the modalities directly,
    this approach maintains separate encodings for each modality but ensures that
    their representations are related and convey the same meaning. Alignment or attention
    mechanisms can be used to achieve this coordination.'
  prefs: []
  type: TYPE_NORMAL
- en: Image-Caption Pairs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The [MS COCO dataset](https://cocodataset.org/) is widely used in computer vision
    and natural language processing research, containing many images with objects
    in various contexts, along with multiple textual captions describing the content
    of the images.
  prefs: []
  type: TYPE_NORMAL
- en: When working with the MS COCO dataset, two main strategies for handling representation
    challenges are joint representation and coordinated representation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Joint representation**: By combining the information from both modalities,
    the model can understand their combined meaning. For instance, you can use a deep
    learning model with layers designed to process and merge features from image and
    text data. This results in a joint representation that captures the relationship
    between the image and caption.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Coordinated representation**: In this approach, the image and caption are
    encoded separately, but their representations are related and convey the same
    meaning. Instead of directly fusing the modalities, the model maintains separate
    encodings for each modality while ensuring they are meaningfully associated.'
  prefs: []
  type: TYPE_NORMAL
- en: Both joint and coordinated representation strategies can be employed when working
    with the MS COCO dataset to effectively handle the challenges of multimodal learning
    and create models that can process and understand the relationships between visual
    and textual information.
  prefs: []
  type: TYPE_NORMAL
- en: Fusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fusion is a technique used in multimodal learning to combine information from
    different data modalities, such as text, images, and audio, to create a more comprehensive
    understanding of a particular situation or context. The fusion process helps models
    make better predictions and decisions based on the combined information instead
    of relying on a single modality.
  prefs: []
  type: TYPE_NORMAL
- en: One challenge in multimodal learning is determining the best way to fuse the
    different modalities. Different fusion techniques may be more effective than others
    for specific tasks or situations, and finding the right one can be difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Movie Rating
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/2d7dd04a65b17c61467699ec45528ee1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: A real-life example of fusion in multimodal learning is a movie recommendation
    system. In this case, the system might use text data (movie descriptions, reviews,
    or user profiles), audio data (soundtracks, dialogue), and visual data (movie
    posters, video clips) to generate personalized recommendations for users.
  prefs: []
  type: TYPE_NORMAL
- en: The fusion process combines these different sources of information to create
    a more accurate and meaningful understanding of the user's preferences and interests,
    leading to better movie suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: One real-life dataset suitable for developing a movie recommendation system
    with fusion is the [MovieLens dataset](https://grouplens.org/datasets/movielens/).
    MovieLens is a collection of movie ratings and metadata, including user-generated
    tags, collected by the GroupLens Research project at the University of Minnesota.
    The dataset contains information about movies, such as titles, genres, user ratings,
    and user profiles.
  prefs: []
  type: TYPE_NORMAL
- en: To create a multimodal movie recommendation system using the MovieLens dataset,
    you could combine the textual information (movie titles, genres, and tags) with
    additional visual data (movie posters) and audio data (soundtracks, dialogue).
    You can obtain movie posters and audio data from other sources, such as IMDB or
    TMDB.
  prefs: []
  type: TYPE_NORMAL
- en: How Fusion Might Be a Challenge?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fusion might be challenging when applying multimodal learning to this dataset
    because you need to determine the most effective way to combine the different
    modalities.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you need to find the right balance between the importance of textual
    data (genres, tags), visual data (posters), and audio data (soundtracks, dialogue)
    for the recommendation task.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, some movies may have missing or incomplete data, such as lacking
    posters or audio samples.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the recommendation system should be robust enough to handle missing
    data and still provide accurate recommendations based on the available information.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, using the MovieLens dataset, along with additional visual and audio
    data, you can develop a multimodal movie recommendation system that leverages
    fusion techniques.
  prefs: []
  type: TYPE_NORMAL
- en: However, challenges may arise when determining the most effective fusion method
    and handling missing or incomplete data.
  prefs: []
  type: TYPE_NORMAL
- en: Alignment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alignment is a crucial task in applications such as audio-visual speech recognition.
    In this task, audio and visual modalities must be aligned to recognize speech
    accurately.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers have used alignment methods such as the hidden Markov model and
    dynamic time warping to achieve this synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the hidden Markov model can be used to model the relationship between
    the audio and visual modalities and to estimate the alignment between the audio
    waveform and the video frames. Dynamic time warping can be used to align the data
    sequences by stretching or compressing them in time so that they match more closely.
  prefs: []
  type: TYPE_NORMAL
- en: Audio-Visual Speech Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/53c5cb2279932797b612cbc62616ca88.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: By aligning the audio and visual data in the GRID Corpus dataset, researchers
    can create coordinated representations that capture the relationships between
    the modalities and then use these representations to recognize speech accurately
    using both modalities.
  prefs: []
  type: TYPE_NORMAL
- en: The [GRID Corpus dataset](https://spandh.dcs.shef.ac.uk//gridcorpus/) contains
    audio-visual recordings of speakers producing sentences in English. Each recording
    includes the audio waveform and the video of the speaker's face, which captures
    the movement of the lips and other facial features. The dataset is widely used
    for research in audio-visual speech recognition, where the goal is to recognize
    speech accurately using audio and visual modalities.
  prefs: []
  type: TYPE_NORMAL
- en: Translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The translation is a common multimodal challenge where different modalities
    of data, such as text and images, must be aligned to create a coherent representation.
    One example of such a challenge is the task of image captioning, where an image
    needs to be described in natural language.
  prefs: []
  type: TYPE_NORMAL
- en: In this task, a model needs to be able to recognize not only the objects and
    context in an image but also generate a natural language description that accurately
    conveys the meaning of the image.
  prefs: []
  type: TYPE_NORMAL
- en: This requires aligning the visual and textual modalities to create coordinated
    representations that capture the relationships between them.
  prefs: []
  type: TYPE_NORMAL
- en: Dall-E
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An oil painting of pandas meditating in Tibet
  prefs: []
  type: TYPE_NORMAL
- en: '![An oil painting of pandas meditating in Tibet](../Images/bc8776aab47f0167b68d753d2e30f564.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Reference: Dall-E'
  prefs: []
  type: TYPE_NORMAL
- en: One recent example of a model that can perform multimodal translation is DALL-E2\.
    DALL-E2 is a neural network model developed by OpenAI that can generate high-quality
    images from textual descriptions. It can also generate textual descriptions from
    images, effectively translating between the visual and textual modalities.
  prefs: []
  type: TYPE_NORMAL
- en: DALL-E2 achieves this by learning a joint representation space that captures
    the relationships between the visual and textual modalities.
  prefs: []
  type: TYPE_NORMAL
- en: The model is trained on a large dataset of the image-caption pairs and learns
    to associate images with their corresponding captions. It can then generate images
    from textual descriptions by sampling from the learned representation space and
    generate textual descriptions from images by decoding the learned representation.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, multimodal translation is a significant challenge that requires aligning
    different modalities of data to create coordinated representations. Models like
    DALL-E2 can perform this task by learning joint representation spaces that capture
    the relationships between the visual and textual modalities and can be applied
    to tasks such as image captioning and visual question answering.
  prefs: []
  type: TYPE_NORMAL
- en: Co-learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multimodal co-learning aims to transfer knowledge learned through one or more
    modalities to tasks involving another.
  prefs: []
  type: TYPE_NORMAL
- en: Co-learning is especially important in low-resource target tasks, entirely/partly
    missing or noisy modalities.
  prefs: []
  type: TYPE_NORMAL
- en: However, finding effective methods to transfer knowledge from one modality to
    another while retaining the semantic meaning is a significant challenge in multimodal
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: In medical diagnosis, different medical imaging modalities, such as CT scans
    and MRI scans, provide complementary information for a diagnosis. Multimodal co-learning
    can be used to combine these modalities to improve the accuracy of the diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal Tumor Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/fad94eee408277eec7ae72db7f10211f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [https://www.med.upenn.edu/sbia/brats2018.html](https://www.med.upenn.edu/sbia/brats2018.html)'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in the case of brain tumors, MRI scans provide high-resolution
    images of soft tissues, while CT scans provide detailed images of the bone structure.
    Combining these modalities can provide a complete picture of the patient's condition
    and inform treatment decisions.
  prefs: []
  type: TYPE_NORMAL
- en: A dataset that includes MRI and CT scans of brain tumors for use in multimodal
    co-learning is the multimodal [Brain Tumor Segmentation (BraTS) dataset](https://www.kaggle.com/datasets/sanglequang/brats2018).
    This dataset includes MRI and CT scans of brain tumors and annotations for segmentation
    of the tumor regions.
  prefs: []
  type: TYPE_NORMAL
- en: To implement co-learning with the MRI and CT scans of brain tumors, we need
    to develop an approach that combines the information from both modalities in a
    way that improves the accuracy of the diagnosis. One possible approach is to use
    a multimodal deep learning model trained on MRI and CT scans.
  prefs: []
  type: TYPE_NORMAL
- en: Popular Applications of Multimodal Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will mention several other applications of multimodal learning, like speech
    recognition and autonomous cars.
  prefs: []
  type: TYPE_NORMAL
- en: Speech Recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/b3656f1c5b5fdff71bc8d79d9c6c6f5a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal learning can improve speech recognition accuracy by combining audio
    and visual data.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a multimodal model can analyze both the audio signal of speech
    and corresponding lip movements to improve speech recognition accuracy. By combining
    audio and visual modalities, multimodal models can reduce the effects of noise
    and variability in speech signals, leading to improved speech recognition performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/2dbc1ebed16337fffc221e842f71b485.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [CMU-MOSEI Dataset](http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/)'
  prefs: []
  type: TYPE_NORMAL
- en: One example of a multimodal dataset that can be used for speech recognition
    is the [CMU-MOSEI dataset](http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/).
    This dataset contains 23,500 sentences pronounced by 1,000 Youtube speakers and
    includes both audio and visual data of the speakers.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset can be used to develop multimodal models for emotion recognition,
    sentiment analysis, and speaker identification.
  prefs: []
  type: TYPE_NORMAL
- en: By combining the audio signal of speech with the visual characteristics of the
    speaker, multimodal models can improve the accuracy of speech recognition and
    other related tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous Car
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/4ebc80d86e52ceb02b7f7169f982cb75.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Waymo; Business Insider](https://www.businessinsider.in/transportation/heres-how-waymos-brand-new-self-driving-cars-see-the-world/articleshow/56648715.cms)'
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal learning can be used to enhance the capabilities of robots by integrating
    information from multiple sensors.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, it is essential in developing self-driving cars, which rely on
    information from multiple sensors such as cameras, lidar, and radar to navigate
    and make decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal learning can help to integrate information from these sensors, allowing
    the car to perceive and react to its environment more accurately and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: One example of a dataset for self-driving cars is the [Waymo Open Dataset](https://waymo.com/open/),
    which includes high-resolution sensor data from Waymo's self-driving cars, along
    with labels for objects such as vehicles, pedestrians, and cyclists. Waymo is
    Google’s self-driving car company.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset can be used to develop and evaluate multimodal models for various
    tasks related to self-driving cars, such as object detection, tracking, and predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Voice Recording Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/0004f1ba42c9905680540a9424c7d7ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The [Voice Recordings Analysis project](https://platform.stratascratch.com/data-projects/voice-recordings-analysis?utm_source=blog&utm_medium=click&utm_campaign=kdn+multimodal+models)
    came up during the interviews for the data science positions at Sandvik. It is
    an excellent example of a multimodal learning application, as it seeks to predict
    a person's gender based on vocal features derived from audio data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this scenario, the problem involves analyzing and processing information
    from two distinct modalities: audio signals and textual features. These modalities
    contribute valuable information that can enhance the accuracy and effectiveness
    of the predictive model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Expanding on the multimodal nature of this project:'
  prefs: []
  type: TYPE_NORMAL
- en: Audio Signals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/eed8b5091a98e39f7ebd8d79e398a0fa.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The primary data source in this project is the audio recordings of the English-speaking
    male and female voices. These audio signals contain rich and complex information
    about the speaker's vocal characteristics. By extracting relevant features from
    these audio signals, such as pitch, frequency, and spectral entropy, the model
    can identify patterns and trends that relate to gender-specific vocal properties.
  prefs: []
  type: TYPE_NORMAL
- en: Textual Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Multimodal Models Explained](../Images/e01ddcdf37c2f571a7368e4f8a47bc98.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Accompanying each audio recording is a text file that provides crucial information
    about the sample, such as the speaker's gender, the language spoken, and the phrase
    uttered by the person. This text file not only offers the ground truth (gender
    labels) for training and evaluating the machine learning models but can also be
    used to create additional features in combination with the audio data. By leveraging
    the information in the text file, the model can better understand the context
    and content of each audio sample, potentially improving its overall predictive
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: So, the Voice Recordings Analysis project exemplifies a multimodal learning
    application by leveraging data from multiple modalities, audio signals, and textual
    features to predict a person's gender using extracted vocal characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: This approach highlights the importance of considering different data types
    when developing machine learning models, as it can help uncover hidden patterns
    and relationships that may not be apparent when analyzing each modality in isolation.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In summary, multimodal learning has become a potent tool for integrating diverse
    data to enhance the power of the precision of [machine learning algorithms](https://www.stratascratch.com/blog/machine-learning-algorithms-you-should-know-for-data-science/?utm_source=blog&utm_medium=click&utm_campaign=kdn+multimodal+models).
    Combining different data types, including text, audio, and visual information,
    can yield more robust and accurate predictions. This is particularly true in speech
    recognition, text and image fusion, and the autonomous car industry.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, multimodal learning does come with several challenges, such as those related
    to representation, fusion, alignment, translation, and co-learning. This necessitates
    careful consideration and attention.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, as machine learning techniques and computing power continue to
    evolve, we can anticipate the emergence of even more advanced multimodal in the
    years to come.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nate Rosidi](https://www.stratascratch.com)** is a data scientist and in
    product strategy. He''s also an adjunct professor teaching analytics, and is the
    founder of [StrataScratch](https://www.stratascratch.com/), a platform helping
    data scientists prepare for their interviews with real interview questions from
    top companies. Connect with him on [Twitter: StrataScratch](https://twitter.com/StrataScratch)
    or [LinkedIn](https://www.linkedin.com/in/nathanrosidi/).'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Multimodal Grounded Learning with Vision and Language](https://www.kdnuggets.com/2022/11/multimodal-grounded-learning-vision-language.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](https://www.kdnuggets.com/introduction-to-nextgpt-anytoany-multimodal-large-language-model)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Large Language Models Explained in 3 Levels of Difficulty](https://www.kdnuggets.com/large-language-models-explained-in-3-levels-of-difficulty)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Machine Learning Models Explained in 5 Minutes](https://www.kdnuggets.com/5-machine-learning-models-explained-in-5-minutes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Decision Tree Algorithm, Explained](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Key-Value Databases, Explained](https://www.kdnuggets.com/2021/04/nosql-explained-understanding-key-value-databases.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
