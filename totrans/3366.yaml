- en: 'arXiv Paper Spotlight: Why Does Deep and Cheap Learning Work So Well?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2016/12/arxiv-spotlight-deep-cheap-learning.html](https://www.kdnuggets.com/2016/12/arxiv-spotlight-deep-cheap-learning.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why does deep learning work so well? And... cheap learning?
  prefs: []
  type: TYPE_NORMAL
- en: '![arXiv](../Images/7d2d792a27b6d8c8f5ce5b64714828b1.png)'
  prefs: []
  type: TYPE_IMG
- en: A [recent paper](https://arxiv.org/abs/1608.08225) by Henry W. Lin (Harvard)
    and Max Tegmark (MIT), titled "Why does deep and cheap learning work so well?"
    looks to examine from a different perspective what it is about deep learning that
    makes it work so well. It also introduces (at least, to me) the term "cheap learning."
  prefs: []
  type: TYPE_NORMAL
- en: 'First off, to be clear, "cheap learning" does not refer to using a low end
    GPU; instead, the following explains its relationship to parameter reduction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[A]lthough well-known mathematical theorems guarantee that neural networks
    can approximate arbitrary functions well, the class of functions of practical
    interest can be approximated through "cheap learning" with exponentially fewer
    parameters than generic ones, because they have simplifying properties tracing
    back to the laws of physics.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This central idea of this paper is that neural network success owes as much
    to physics as it does to mathematics (perhaps more), and that simplistic physics
    functions owing to concepts such as symmetry, locality, compositionality, and
    polynomial log-probability can be viewed similarly to deep learning's relationship
    with the reality which it seeks to model. You [may have heard something about
    this](https://www.technologyreview.com/s/602344/the-extraordinary-link-between-deep-neural-networks-and-the-nature-of-the-universe/)
    in September; this is the paper on which said news was based.
  prefs: []
  type: TYPE_NORMAL
- en: '![Pyramid](../Images/e9f37985b69e1cf4bb6f72da199a5fa1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'More from the abstract:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We further argue that when the statistical process generating the data is of
    a certain hierarchical form prevalent in physics and machine-learning, a deep
    neural network can be more efficient than a shallow one. We formalize these claims
    using information theory and discuss the relation to renormalization group procedures.
    We prove various "no-flattening theorems" showing when such efficient deep networks
    cannot be accurately approximated by shallow ones without efficiency loss: flattening
    even linear functions can be costly, and flattening polynomials is exponentially
    expensive; we use group theoretic techniques to show that n variables cannot be
    multiplied using fewer than 2^n neurons in a single hidden layer.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Information loss](../Images/f979328362b981e94aaa5b04e410a42d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With a bit of maths and a few theorems, the paper proceeds as follows: presents
    "shallow" neural network results, with a handful of layers; demonstrates how increasing
    network depth provides polynomial or exponential efficiency gains without adding
    expressivity; summarizes conclusions and discusses a technical point about renormalization
    and deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An interesting excerpt from the paper''s conclusion:'
  prefs: []
  type: TYPE_NORMAL
- en: Whereas previous universality theorems guarantee that there exists a neural
    network that approximates any smooth function to within an error ε, they cannot
    guarantee that the size of the neural network does not grow to infinity with shrinking
    ε or that the activation function σ does not become pathological. We show constructively
    that given a multivariate polynomial and any generic non-linearity, a neural network
    with a fixed size and a generic smooth activation function can indeed approximate
    the polynomial highly efficiently.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The abstract can be found [here](https://arxiv.org/abs/1608.08225), while this
    is a [direct link](https://arxiv.org/pdf/1608.08225v2.pdf) to the paper.
  prefs: []
  type: TYPE_NORMAL
- en: '**Related**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[arXiv Paper Spotlight: Stealing Machine Learning Models via Prediction APIs](/2016/11/arxiv-spotlight-stealing-machine-learning-models-prediction-apis.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[arXiv Paper Spotlight: Automated Inference on Criminality Using Face Images](/2016/12/arxiv-spotlight-automated-inference-criminality-face-images.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 More arXiv Deep Learning Papers, Explained](/2016/01/more-arxiv-deep-learning-papers-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[What Is ChatGPT Doing and Why Does It Work?](https://www.kdnuggets.com/2023/04/chatgpt-work.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[It''s alive! Build your first robots with Python and some cheap,…](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Cheap Books to Master Machine Learning](https://www.kdnuggets.com/5-cheap-books-to-master-machine-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Cheap Books to Master Data Science](https://www.kdnuggets.com/5-cheap-books-to-master-data-science)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What is K-Means Clustering and How Does its Algorithm Work?](https://www.kdnuggets.com/2023/05/kmeans-clustering-algorithm-work.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How Does Logistic Regression Work?](https://www.kdnuggets.com/2022/07/logistic-regression-work.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
