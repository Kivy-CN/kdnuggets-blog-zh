- en: Practical Hyperparameter Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实用的超参数优化
- en: 原文：[https://www.kdnuggets.com/2020/02/practical-hyperparameter-optimization.html](https://www.kdnuggets.com/2020/02/practical-hyperparameter-optimization.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/02/practical-hyperparameter-optimization.html](https://www.kdnuggets.com/2020/02/practical-hyperparameter-optimization.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Pier Paolo Ippolito](https://www.linkedin.com/in/pierpaolo28/), The University
    of Southampton**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者 [Pier Paolo Ippolito](https://www.linkedin.com/in/pierpaolo28/)，南安普顿大学**'
- en: '![](../Images/e7d267de83d374a4b81eacc48c82c3dc.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7d267de83d374a4b81eacc48c82c3dc.png)'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前 3 个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Machine Learning models are composed of two different types of parameters:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型由两种不同类型的参数组成：
- en: '**Hyperparameters** = are all the parameters which can be arbitrarily set by
    the user before starting training (eg. number of estimators in Random Forest).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数** = 是所有可以由用户在开始训练之前任意设置的参数（例如随机森林中的估计器数量）。'
- en: '**Model parameters =** are instead learned during the model training (eg. weights
    in Neural Networks, Linear Regression).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型参数 =** 在模型训练过程中学习得出（例如神经网络中的权重，线性回归中的系数）。'
- en: The model parameters define how to use input data to get the desired output
    and are learned at training time. Instead, Hyperparameters determine how our model
    is structured in the first place.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 模型参数定义了如何使用输入数据得到期望的输出，并在训练时学习。而超参数则决定了我们模型的基本结构。
- en: Machine Learning models tuning is a type of optimization problem. We have a
    set of hyperparameters and we aim to find the right combination of their values
    which can help us to find either the minimum (eg. loss) or the maximum (eg. accuracy)
    of a function (Figure 1).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型调优是一种优化问题。我们有一组超参数，并且目标是找到它们的最佳组合，从而帮助我们找到函数的最小值（例如损失）或最大值（例如准确率）（见图 1）。
- en: This can be particularly important when comparing how different Machine Learning
    models performs on a dataset. In fact, it would be unfair for example to compare
    an SVM model with the best Hyperparameters against a Random Forest model which
    has not been optimized.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当比较不同的机器学习模型在数据集上的表现时，这一点尤其重要。事实上，将一个具有最佳超参数的 SVM 模型与一个未经过优化的随机森林模型进行比较是不公平的。
- en: 'In this post, the following approaches to Hyperparameter optimization will
    be explained:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，将解释以下超参数优化的方法：
- en: '**Manual Search**'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**手动搜索**'
- en: '**Random Search**'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**随机搜索**'
- en: '**Grid Search**'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**网格搜索**'
- en: '**Automated Hyperparameter Tuning (Bayesian Optimization, Genetic Algorithms)**'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**自动化超参数调优（贝叶斯优化，遗传算法）**'
- en: '**Artificial Neural Networks (ANNs) Tuning**'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**人工神经网络（ANNs）调优**'
- en: '![Figure](../Images/a925c643693adacdb575d44ea48f69bb.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/a925c643693adacdb575d44ea48f69bb.png)'
- en: 'Figure 1: ML Optimization Workflow [1]'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：ML 优化工作流程 [1]
- en: In order to demonstrate how to perform Hyperparameters Optimization in Python,
    I decided to perform a complete Data Analysis of the [Credit Card Fraud Detection
    Kaggle Dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud). Our objective
    in this article will be to correctly classify which credit card transactions should
    be labelled as fraudulent or genuine (binary classification). This Dataset has
    been anonymized before being distributed, therefore, the meaning of most of the
    features has not been disclosed.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示如何在 Python 中进行超参数优化，我决定对 [信用卡欺诈检测 Kaggle 数据集](https://www.kaggle.com/mlg-ulb/creditcardfraud)
    进行完整的数据分析。我们在这篇文章中的目标是正确分类哪些信用卡交易应标记为欺诈或真实（即二分类）。该数据集在分发前已被匿名化，因此，大多数特征的含义未被公开。
- en: In this case, I decided to use just a subset of the dataset, in order to speed
    up training times and make sure to achieve a perfect balance between the two different
    classes. Additionally, just a limited amount of features has been used to make
    the optimization tasks more challenging. The final dataset is shown in the figure
    below (Figure 2).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我决定只使用数据集的一个子集，以加快训练速度，并确保在两个不同类别之间实现完美的平衡。此外，仅使用有限数量的特征以使优化任务更加具有挑战性。最终的数据集如下面的图（图
    2）所示。
- en: '![Figure](../Images/ac4eeb1e5f700fc844f3fead905f2a0d.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/ac4eeb1e5f700fc844f3fead905f2a0d.png)'
- en: 'Figure 2: Credit Card Fraud Detection Dataset'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：信用卡欺诈检测数据集
- en: All the code used in this article (and more!) is available in my [GitHub repository](https://github.com/pierpaolo28/Kaggle-Challenges/blob/master/credit-card-fraud-model-tuning.ipynb) and [Kaggle
    Profile](https://www.kaggle.com/pierpaolo28/credit-card-fraud-model-tuning).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中使用的所有代码（及更多！）可以在我的 [GitHub 仓库](https://github.com/pierpaolo28/Kaggle-Challenges/blob/master/credit-card-fraud-model-tuning.ipynb)
    和 [Kaggle 个人主页](https://www.kaggle.com/pierpaolo28/credit-card-fraud-model-tuning)
    找到。
- en: Machine Learning
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习
- en: First of all, we need to divide our dataset into training and test sets.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将数据集分为训练集和测试集。
- en: Throughout this article, we will use a Random Forest Classifier as our model
    to optimize.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将使用随机森林分类器作为优化模型。
- en: Random Forest models are formed by a large number of uncorrelated decision trees,
    which joint together constitute an ensemble. In Random Forest, each decision tree
    makes its own prediction and the overall model output is selected to be the prediction
    which appeared most frequently.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林模型由大量不相关的决策树构成，这些决策树共同组成一个集成模型。在随机森林中，每棵决策树做出自己的预测，最终模型输出的是出现频率最高的预测。
- en: We can now start by calculating our base model accuracy.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始计算我们的基本模型准确率。
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using the Random Forest Classifier with the default scikit-learn parameters
    lead to 95% overall accuracy. Let’s see now if applying some optimization techniques
    we can achieve better accuracy.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用默认 scikit-learn 参数的随机森林分类器可以达到 95% 的总体准确率。现在让我们看看通过应用一些优化技术是否能实现更好的准确率。
- en: Manual Search
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手动搜索
- en: When using Manual Search, we choose some model hyperparameters based on our
    judgment/experience. We then train the model, evaluate its accuracy and start
    the process again. This loop is repeated until a satisfactory accuracy is scored.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用手动搜索时，我们根据判断/经验选择一些模型超参数。然后训练模型，评估其准确率，并重新开始这一过程。这个循环会重复，直到达到令人满意的准确率。
- en: 'The main parameters used by a Random Forest Classifier are:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林分类器使用的主要参数包括：
- en: '**criterion** = the function used to evaluate the quality of a split.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**criterion** = 用于评估切分质量的函数。'
- en: '**max_depth** = maximum number of levels allowed in each tree.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_depth** = 每棵树允许的最大层数。'
- en: '**max_features** = maximum number of features considered when splitting a node.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_features** = 切分节点时考虑的最大特征数量。'
- en: '**min_samples_leaf** = minimum number of samples which can be stored in a tree
    leaf.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**min_samples_leaf** = 可以存储在树叶中的最小样本数量。'
- en: '**min_samples_split** = minimum number of samples necessary in a node to cause
    node splitting.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**min_samples_split** = 节点中进行节点分裂所需的最小样本数量。'
- en: '**n_estimators** = number of trees in the ensemble.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**n_estimators** = 集成中的树木数量。'
- en: More information about Random Forest parameters can be found on the scikit-learn[ Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 关于随机森林参数的更多信息可以在 scikit-learn 的[文档](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)中找到。
- en: As an example of Manual Search, I tried to specify the number of estimators
    in our model. Unfortunately, this didn’t lead to any improvement in accuracy.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 作为手动搜索的一个例子，我尝试指定模型中的估计器数量。不幸的是，这并没有带来准确率的提升。
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Random Search
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机搜索
- en: In Random Search, we create a grid of hyperparameters and train/test our model
    on just some random combination of these hyperparameters. In this example, I additionally
    decided to perform Cross-Validation on the training set.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机搜索中，我们创建一个超参数网格，并在这些超参数的随机组合上训练/测试模型。在这个例子中，我还决定对训练集进行交叉验证。
- en: When performing Machine Learning tasks, we generally divide our dataset in training
    and test sets. This is done so that to test our model after having trained it
    (in this way we can check it’s performances when working with unseen data). When
    using Cross-Validation, we divide our training set into N other partitions to
    make sure our model is not overfitting our data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行机器学习任务时，我们通常将数据集分为训练集和测试集。这是为了在训练模型后测试它（这样我们可以检查它在处理未见数据时的表现）。使用交叉验证时，我们将训练集划分为N个其他分区，以确保模型没有过拟合数据。
- en: One of the most common used Cross-Validation methods is K-Fold Validation. In
    K-Fold, we divide our training set into N partitions and then iteratively train
    our model using N-1 partitions and test it with the left-over partition (at each
    iteration we change the left-over partition). Once having trained N times the
    model we then average the training results obtained in each iteration to obtain
    our overall training performance results (Figure 3).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的交叉验证方法之一是K折验证。在K折验证中，我们将训练集划分为N个分区，然后迭代地使用N-1个分区训练模型，并用剩余的分区进行测试（在每次迭代中，我们会更换剩余的分区）。训练N次模型后，我们将每次迭代中获得的训练结果进行平均，以获得整体训练性能结果（见图3）。
- en: '![Figure](../Images/ef469d0bf742072bd0a720a564e8c996.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/ef469d0bf742072bd0a720a564e8c996.png)'
- en: 'Figure 3: K-Fold Cross-Validation [2]'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：K折交叉验证 [2]
- en: Using Cross-Validation when implementing Hyperparameters optimization can be
    really important. In this way, we might avoid using some Hyperparameters which
    works really good on the training data but not so good with the test data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现超参数优化时使用交叉验证可能非常重要。这样，我们可能会避免使用在训练数据上效果很好但在测试数据上效果不佳的超参数。
- en: We can now start implementing Random Search by first defying a grid of hyperparameters
    which will be randomly sampled when calling ***RandomizedSearchCV()***. For this
    example, I decided to divide our training set into 4 Folds (***cv = 4***) and
    select 80 as the number of combinations to sample (***n_iter = 80***). Using the
    scikit-learn ***best_estimator_ ***attribute, we can then retrieve the set of
    hyperparameters which performed best during training to test our model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过首先定义一个超参数网格来开始实现随机搜索，这些超参数将在调用***RandomizedSearchCV()***时随机采样。以这个例子为例，我决定将训练集分为4个折（***cv
    = 4***）并选择80作为采样组合的数量（***n_iter = 80***）。使用scikit-learn的***best_estimator_***属性，我们可以检索在训练过程中表现最佳的超参数集合来测试我们的模型。
- en: Once trained our model, we can then visualize how changing some of its Hyperparameters
    can affect the overall model accuracy (Figure 4). In this case, I decided to observe
    how changing the number of estimators and the criterion can affect our Random
    Forest accuracy.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练了我们的模型，我们就可以可视化一些超参数的变化如何影响整体模型准确性（见图4）。在这种情况下，我决定观察估算器数量和准则的变化如何影响我们的随机森林准确性。
- en: '![Figure](../Images/f655111ff344502011d6b50d2e15d0ba.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/f655111ff344502011d6b50d2e15d0ba.png)'
- en: 'Figure 4: Criterion vs N Estimators Accuracy Heatmap'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：准则与N个估算器准确性热图
- en: We can then take this a step further by making our visualization more interactive.
    In the chart below, we can examine (using the slider) how varying the number of
    estimators in our model can affect the overall accuracy of our model considered
    the selected min_split and min_leaf parameters.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步通过使可视化更具互动性来提升效果。在下图中，我们可以通过滑块检查模型中估算器数量的变化如何影响整体准确性，考虑选定的min_split和min_leaf参数。
- en: Feel free to play with the graph below by changing the n_estimators parameters,
    zooming in and out of the graph, changing it’s orientation and hovering over the
    single data points to get additional information about them!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 随意调整下面的图表，改变n_estimators参数，放大和缩小图表，改变其方向，并悬停在单个数据点上以获取额外信息！
- en: If you are interested in finding out more about how to create these animations
    using [Plotly](https://towardsdatascience.com/interactive-data-visualization-167ae26016e8),
    my code is available [here](https://www.kaggle.com/kernels/scriptcontent/20590929/download).
    Additionally, this has also been covered in an article written by [Xoel López
    Barata](https://towardsdatascience.com/using-3d-visualizations-to-tune-hyperparameters-of-ml-models-with-python-ba2885eab2e9).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣了解更多关于如何使用[Plotly](https://towardsdatascience.com/interactive-data-visualization-167ae26016e8)创建这些动画的信息，我的代码可以在[这里](https://www.kaggle.com/kernels/scriptcontent/20590929/download)找到。此外，这也被[Xoel
    López Barata](https://towardsdatascience.com/using-3d-visualizations-to-tune-hyperparameters-of-ml-models-with-python-ba2885eab2e9)在一篇文章中介绍过。
- en: We can now evaluate how our model performed using Random Search. In this case,
    using Random Search leads to a consistent increase in accuracy compared to our
    base model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以评估我们的模型使用随机搜索的表现。在这种情况下，使用随机搜索会导致比我们的基线模型更稳定的准确率提升。
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Grid Seach
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网格搜索
- en: In Grid Search, we set up a grid of hyperparameters and train/test our model
    on each of the possible combinations.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在网格搜索中，我们设置超参数的网格，并在每个可能的组合上训练/测试我们的模型。
- en: In order to choose the parameters to use in Grid Search, we can now look at
    which parameters worked best with Random Search and form a grid based on them
    to see if we can find a better combination.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择在网格搜索中使用的参数，我们可以查看哪些参数在随机搜索中表现最好，并基于这些参数形成网格，以便寻找更好的组合。
- en: Grid Search can be implemented in Python using scikit-learn ***GridSearchCV() ***function.
    Also on this occasion, I decided to divide our training set into 4 Folds (***cv
    = 4***).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索可以使用 Python 的 scikit-learn 中的 ***GridSearchCV()*** 函数实现。此次，我决定将我们的训练集分为
    4 折（***cv = 4***）。
- en: When using Grid Search, all the possible combinations of the parameters in the
    grid are tried. In this case, 128000 combinations (2 × 10 × 4 × 4 × 4 × 10) will
    be used during training. Instead, in the Grid Search example before, just 80 combinations
    have been used.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用网格搜索时，尝试所有网格中的参数组合。在这种情况下，训练期间将使用 128000 种组合（2 × 10 × 4 × 4 × 4 × 10）。而在之前的网格搜索示例中，仅使用了
    80 种组合。
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Grid Search is slower compared to Random Search but it can be overall more effective
    because it can go through the whole search space. Instead, Random Search can be
    faster fast but might miss some important points in the search space.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索（Grid Search）相比随机搜索（Random Search）速度较慢，但它可以更有效，因为它能够遍历整个搜索空间。相对而言，随机搜索可能更快，但可能会错过搜索空间中的一些重要点。
- en: Automated Hyperparameter Tuning
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动化超参数调整
- en: 'When using Automated Hyperparameter Tuning, the model hyperparameters to use
    are identified using techniques such as: Bayesian Optimization, Gradient Descent
    and Evolutionary Algorithms.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自动化超参数调整时，通过贝叶斯优化、梯度下降和进化算法等技术来确定要使用的模型超参数。
- en: Bayesian Optimization
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贝叶斯优化
- en: Bayesian Optimization can be performed in Python using the Hyperopt library.
    Bayesian optimization uses probability to find the minimum of a function. The
    final aim is to find the input value to a function which can give us the lowest
    possible output value.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化（Bayesian Optimization）可以使用 Hyperopt 库在 Python 中执行。贝叶斯优化利用概率来寻找函数的最小值。最终目标是找到一个输入值，使函数输出最小值。
- en: Bayesian optimization has been proved to be more efficient than random, grid
    or manual search. Bayesian Optimization can, therefore, lead to better performance
    in the testing phase and reduced optimization time.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化已被证明比随机、网格或手动搜索更高效。因此，贝叶斯优化可以在测试阶段带来更好的性能并减少优化时间。
- en: In Hyperopt, Bayesian Optimization can be implemented giving 3 three main parameters
    to the function **fmin()**.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Hyperopt 中，贝叶斯优化可以通过向 **fmin()** 函数提供 3 个主要参数来实现。
- en: '**Objective Function** = defines the loss function to minimize.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标函数** = 定义要最小化的损失函数。'
- en: '**Domain Space** = defines the range of input values to test (in Bayesian Optimization
    this space creates a probability distribution for each of the used Hyperparameters).'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域空间** = 定义要测试的输入值范围（在贝叶斯优化中，这个空间为每个使用的超参数创建概率分布）。'
- en: '**Optimization Algorithm** = defines the search algorithm to use to select
    the best input values to use in each new iteration.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化算法** = 定义用于选择每次新迭代中最佳输入值的搜索算法。'
- en: Additionally, can also be defined in ***fmin()*** the maximum number of evaluations
    to perform.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还可以在 ***fmin()*** 中定义要执行的最大评估次数。
- en: Bayesian Optimization can reduce the number of search iterations by choosing
    the input values bearing in mind the past outcomes. In this way, we can concentrate
    our search from the beginning on values which are closer to our desired output.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化可以通过考虑过去的结果来减少搜索迭代次数。这样，我们可以从一开始就将搜索集中在更接近我们期望输出的值上。
- en: We can now run our Bayesian Optimizer using the **fmin()** function. A **Trials()** object
    is first created to make possible to visualize later what was going on while the **fmin()** function
    was running (eg. how the loss function was changing and how to used Hyperparameters
    were changing).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 **fmin()** 函数运行贝叶斯优化器。首先创建一个 **Trials()** 对象，以便后来可视化 **fmin()** 函数运行时发生了什么（例如，损失函数的变化情况以及使用的超参数的变化情况）。
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can now retrieve the set of best parameters identified and test our model
    using the ***best*** dictionary created during training. Some of the parameters
    have been stored in the ***best*** dictionary numerically using indices, therefore,
    we need first to convert them back as strings before input them in our Random
    Forest.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以检索到识别出的最佳参数集合，并使用在训练过程中创建的***最佳***字典测试我们的模型。一些参数已经通过索引以数字形式存储在***最佳***字典中，因此我们需要首先将它们转换回字符串形式，然后再输入到我们的随机森林中。
- en: The classification report using Bayesian Optimization is shown below.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯优化的分类报告如下所示。
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Genetic Algorithms
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 遗传算法
- en: Genetic Algorithms tries to apply natural selection mechanisms to Machine Learning
    contexts. They are inspired by the Darwinian process of Natural Selection and
    they are therefore also usually called as Evolutionary Algorithms.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 遗传算法尝试将自然选择机制应用于机器学习背景。它们受到达尔文自然选择过程的启发，因此通常也称为进化算法。
- en: Let’s imagine we create a population of N Machine Learning models with some
    predefined Hyperparameters. We can then calculate the accuracy of each model and
    decide to keep just half of the models (the ones that perform best). We can now
    generate some offsprings having similar Hyperparameters to the ones of the best
    models so that to get again a population of N models. At this point, we can again
    calculate the accuracy of each model and repeat the cycle for a defined number
    of generations. In this way, just the best models will survive at the end of the
    process.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们创建一个包含N个机器学习模型的种群，并设定一些预定义的超参数。然后，我们可以计算每个模型的准确性，并决定只保留表现最好的模型的一半。接下来，我们可以生成一些与最佳模型超参数相似的后代，从而再次获得N个模型的种群。此时，我们可以再次计算每个模型的准确性，并重复这个过程若干代。这样，最终只有表现最好的模型会存活下来。
- en: In order to implement Genetic Algorithms in Python, we can use the [TPOT Auto
    Machine Learning library](https://epistasislab.github.io/tpot/). TPOT is built
    on the scikit-learn library and it can be used for either regression or classification
    tasks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在Python中实现遗传算法，我们可以使用[TPOT自动机器学习库](https://epistasislab.github.io/tpot/)。TPOT基于scikit-learn库构建，可以用于回归或分类任务。
- en: The training report and the best parameters identified using Genetic Algorithms
    are shown in the following snippet.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 训练报告和使用遗传算法识别出的最佳参数在下面的代码片段中显示。
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The overall accuracy of our Random Forest Genetic Algorithm optimized model
    is shown below.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的随机森林遗传算法优化模型的整体准确性如下所示。
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Artificial Neural Networks (ANNs) Tuning
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）调优
- en: 'Using KerasClassifier wrapper, it is possible to apply Grid Search and Random
    Search for Deep Learning models in the same way it was done when using scikit-learn
    Machine Learning models. In the following example, we will try to optimize some
    of our ANN parameters such as: how many neurons to use in each layer and which
    activation function and optimizer to use. More examples of Deep Learning Hyperparameters
    optimization are available [here](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用KerasClassifier封装器，可以像使用scikit-learn机器学习模型时一样，应用网格搜索和随机搜索来优化深度学习模型。在以下示例中，我们将尝试优化一些ANN参数，例如：每层使用多少个神经元以及使用哪种激活函数和优化器。更多深度学习超参数优化的示例请见[这里](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)。
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The overall accuracy scored using our Artificial Neural Network (ANN) can be
    observed below.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的人工神经网络（ANN）得到的整体准确性如下所示。
- en: '[PRE9]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Evaluation
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估
- en: We can now compare how all the different optimization techniques performed on
    this given exercise. Overall, Random Search and Evolutionary Algorithms performed
    best.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以比较所有不同优化技术在这个给定练习中的表现。总体而言，随机搜索和进化算法表现最好。
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The results obtained, are highly dependent on the chosen grid space and dataset
    used. Therefore, in different situations, different optimization techniques will
    perform better than others.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 获得的结果高度依赖于所选择的网格空间和数据集。因此，在不同的情况下，不同的优化技术表现可能会有所不同。
- en: '*I hope you enjoyed this article, thank you for reading!*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*希望你喜欢这篇文章，谢谢阅读！*'
- en: Contacts
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 联系方式
- en: 'If you want to keep updated with my latest articles and projects [follow me
    on Medium](https://medium.com/@pierpaoloippolito28?source=post_page---------------------------) and
    subscribe to my [mailing list](http://eepurl.com/gwO-Dr?source=post_page---------------------------).
    These are some of my contacts details:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想跟进我的最新文章和项目，[在Medium关注我](https://medium.com/@pierpaoloippolito28?source=post_page---------------------------)并订阅我的[邮件列表](http://eepurl.com/gwO-Dr?source=post_page---------------------------)。以下是我的一些联系方式：
- en: '[Linkedin](https://uk.linkedin.com/in/pier-paolo-ippolito-202917146?source=post_page---------------------------)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LinkedIn](https://uk.linkedin.com/in/pier-paolo-ippolito-202917146?source=post_page---------------------------)'
- en: '[Personal Blog](https://pierpaolo28.github.io/blog/?source=post_page---------------------------)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[个人博客](https://pierpaolo28.github.io/blog/?source=post_page---------------------------)'
- en: '[Personal Website](https://pierpaolo28.github.io/?source=post_page---------------------------)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[个人网站](https://pierpaolo28.github.io/?source=post_page---------------------------)'
- en: '[Medium Profile](https://towardsdatascience.com/@pierpaoloippolito28?source=post_page---------------------------)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Medium 个人主页](https://towardsdatascience.com/@pierpaoloippolito28?source=post_page---------------------------)'
- en: '[GitHub](https://github.com/pierpaolo28?source=post_page---------------------------)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub](https://github.com/pierpaolo28?source=post_page---------------------------)'
- en: '[Kaggle](https://www.kaggle.com/pierpaolo28?source=post_page---------------------------)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kaggle](https://www.kaggle.com/pierpaolo28?source=post_page---------------------------)'
- en: Bibliography
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Hyperparameter optimization: Explanation of automatized algorithms, Dawid
    Kopczyk. Accessed at: [https://dkopczyk.quantee.co.uk/hyperparameter-optimization/](https://dkopczyk.quantee.co.uk/hyperparameter-optimization/)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[超参数优化：自动化算法的解释](https://dkopczyk.quantee.co.uk/hyperparameter-optimization/)'
- en: '[2] Model Selection, ethen8181\. Accessed at: [http://ethen8181.github.io/machine-learning/model_selection/model_selection.html](http://ethen8181.github.io/machine-learning/model_selection/model_selection.html)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[模型选择](http://ethen8181.github.io/machine-learning/model_selection/model_selection.html)'
- en: '**Bio: [Pier Paolo Ippolito](https://www.linkedin.com/in/pierpaolo28/)** is
    a final year MSc Artificial Intelligence student at The University of Southampton.
    He is an AI Enthusiast, Data Scientist and RPA Developer.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介： [Pier Paolo Ippolito](https://www.linkedin.com/in/pierpaolo28/)** 是南安普顿大学的最后一年硕士人工智能学生。他是一个AI爱好者、数据科学家和RPA开发者。'
- en: '[Original](https://towardsdatascience.com/hyperparameters-optimization-526348bb8e2d).
    Reposted with permission.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/hyperparameters-optimization-526348bb8e2d)。经授权转载。'
- en: '**Related:**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Automated Machine Learning Project Implementation Complexities](/2019/11/automl-implementation-complexities.html)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动化机器学习项目实施复杂性](/2019/11/automl-implementation-complexities.html)'
- en: '[Automated Machine Learning: How do teams work together on an AutoML project?](/2020/01/teams-work-together-automl-project.html)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动化机器学习：团队如何在AutoML项目中协作？](/2020/01/teams-work-together-automl-project.html)'
- en: '[How to Automate Hyperparameter Optimization](/2019/06/automate-hyperparameter-optimization.html)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何自动化超参数优化](/2019/06/automate-hyperparameter-optimization.html)'
- en: More On This Topic
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Hyperparameter Optimization: 10 Top Python Libraries](https://www.kdnuggets.com/2023/01/hyperparameter-optimization-10-top-python-libraries.html)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[超参数优化：10大Python库](https://www.kdnuggets.com/2023/01/hyperparameter-optimization-10-top-python-libraries.html)'
- en: '[Hyperparameter Tuning Using Grid Search and Random Search in Python](https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用网格搜索和随机搜索进行超参数调整](https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html)'
- en: '[Hyperparameter Tuning: GridSearchCV and RandomizedSearchCV, Explained](https://www.kdnuggets.com/hyperparameter-tuning-gridsearchcv-and-randomizedsearchcv-explained)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[超参数调整：GridSearchCV和RandomizedSearchCV的解释](https://www.kdnuggets.com/hyperparameter-tuning-gridsearchcv-and-randomizedsearchcv-explained)'
- en: '[Machine Learning Pipeline Optimization with TPOT](https://www.kdnuggets.com/2021/05/machine-learning-pipeline-optimization-tpot.html)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用TPOT进行机器学习管道优化](https://www.kdnuggets.com/2021/05/machine-learning-pipeline-optimization-tpot.html)'
- en: '[SQL Query Optimization Techniques](https://www.kdnuggets.com/2023/03/sql-query-optimization-techniques.html)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQL查询优化技巧](https://www.kdnuggets.com/2023/03/sql-query-optimization-techniques.html)'
- en: '[Database Optimization: Exploring Indexes in SQL](https://www.kdnuggets.com/2023/07/database-optimization-exploring-indexes-sql.html)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据库优化：探索SQL中的索引](https://www.kdnuggets.com/2023/07/database-optimization-exploring-indexes-sql.html)'
