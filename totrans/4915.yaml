- en: 'TensorFlow: What Parameters to Optimize?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow：优化哪些参数？
- en: 原文：[https://www.kdnuggets.com/2017/11/tensorflow-parameters-optimize.html](https://www.kdnuggets.com/2017/11/tensorflow-parameters-optimize.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/11/tensorflow-parameters-optimize.html](https://www.kdnuggets.com/2017/11/tensorflow-parameters-optimize.html)
- en: '![](../Images/f161d5c08245164a3a4d69b47351e0e8.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f161d5c08245164a3a4d69b47351e0e8.png)'
- en: This article targets whom have a basic understanding for TensorFlow Core API.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本文针对的是对 TensorFlow Core API 有基本了解的读者。
- en: Learning TensorFlow Core API, which is the lowest level API in TensorFlow, is
    a very good step for starting learning TensorFlow because it let you understand
    the kernel of the library. Here is a very simple example of TensorFlow Core API
    in which we create and train a linear regression model.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 学习 TensorFlow Core API，即 TensorFlow 中的最低级 API，是开始学习 TensorFlow 的一个很好的步骤，因为它可以让你理解库的核心。以下是一个非常简单的
    TensorFlow Core API 示例，其中我们创建并训练一个线性回归模型。
- en: 'The steps are as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤如下：
- en: Read the trainable parameters of the model (Just a weight and a bias in this
    example).
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取模型的可训练参数（在此示例中只有一个权重和一个偏置）。
- en: Read the training data into placeholders.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练数据读取到占位符中。
- en: Create the linear regression model function.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建线性回归模型函数。
- en: Create a loss function to assess the prediction errors of the model.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个损失函数来评估模型的预测误差。
- en: Create a TensorFlow session.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 TensorFlow 会话。
- en: Initialize the trainable parameters.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化可训练参数。
- en: Run the session to train the regression model.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行会话以训练回归模型。
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The loss returned is 53.76\. Existence of error, specially for large error,
    means that the parameters used must be updated. These parameters are expected
    to be updated automatically but we can start updating it manually until reaching
    zero error.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的损失为 53.76。存在误差，特别是对于较大的误差，意味着必须更新所用的参数。这些参数预计会自动更新，但我们可以开始手动更新，直到误差为零。
- en: For W=0.8 and b=0.4, the loss is 3.44
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 W=0.8 和 b=0.4，损失为 3.44。
- en: For W=1.0 and b=0.8, the loss is 12.96
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 W=1.0 和 b=0.8，损失为 12.96。
- en: For W=1.0 and b=-0.5, the loss is 1.0
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 W=1.0 和 b=-0.5，损失为 1.0。
- en: For W=1.0 and b=-1.0, the loss is 0.0
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 W=1.0 和 b=-1.0，损失为 0.0。
- en: Thus when W=1.0 and b=-1.0 the desired results is identical to the predicted
    results and thus we can`t enhance the model more than this. We reached the optimal
    values for the parameters but not using the optimal way. The optimal way of calculating
    the parameters is automatic.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当 W=1.0 和 b=-1.0 时，期望结果与预测结果一致，因此我们不能进一步提高模型性能。我们已达到参数的最佳值，但不是使用最佳方式。计算参数的最佳方式是自动的。
- en: 'There are a number of optimizers already exist in TensorFlow for making things
    simpler. These optimizers exist in APIs in TensorFlow such as:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 中已经存在多种优化器，用于简化操作。这些优化器存在于 TensorFlow 的 API 中，例如：
- en: tensorflow.train
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tensorflow.train
- en: tensorflow.estimator
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tensorflow.estimator
- en: Here is how we can use tensorflow.train for updating the parameters automatically.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何使用 tensorflow.train 自动更新参数的方法。
- en: '**tensorflow.train API**'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**tensorflow.train API**'
- en: There are a number of optimizers that TensorFlow provides that makes the previous
    manual work of calculating the best values for the model parameters automatically.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 提供了多种优化器，这些优化器能够自动完成之前手动计算模型参数最佳值的工作。
- en: The simplest optimizer is the gradient descent that changes the values of each
    parameter slowly until reaching the value that minimizes the loss. Gradient descent
    modifies each variable according to the magnitude of the derivative of loss with
    respect to the variable.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的优化器是梯度下降法，它通过缓慢改变每个参数的值，直到达到最小化损失的值。梯度下降法根据损失对变量的导数的大小来调整每个变量。
- en: Because doing such operations of calculating the derivatives is complex and
    error prone, TensorFlow can calculate the gradients automatically. After calculating
    the gradients, you need to optimize the parameters yourself.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算导数的操作复杂且容易出错，TensorFlow 可以自动计算梯度。在计算梯度后，你需要自行优化参数。
- en: But TensorFlow makes things easier and easier by providing optimizers that will
    calculate the derivatives in addition to optimizing the parameters.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 但 TensorFlow 通过提供优化器，使得事情变得越来越简单，这些优化器除了优化参数外，还会计算导数。
- en: 'tensorflow.train API contains a class called GradientDescentOptimizer that
    can both calculate the derivatives and optimizing the parameters. For example,
    the following code shows how to minimize the loss using the GradientDescentOptimizer:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: tensorflow.train API 包含一个名为 GradientDescentOptimizer 的类，它既可以计算导数，又可以优化参数。例如，以下代码展示了如何使用
    GradientDescentOptimizer 最小化损失：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here is the result returned by the optimizer. It seems that it deduced the right
    values of the parameters automatically in order to get the least loss.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是优化器返回的结果。它似乎自动推导出了正确的参数值，以获得最小损失。
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: There is some advantages of using such built-in optimizers rather than building
    it manually. This is because the code for creating such simple linear regression
    model using TensorFlow Core API is not complex. But it won`t be like that when
    working with much more complex models. Thus it is preferred to use the frequently
    used tasks from high-level APIs in TensorFlow.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这样的内置优化器有一些优势，而不是手动构建它。这是因为使用TensorFlow Core API创建这样简单的线性回归模型的代码并不复杂。但当处理更复杂的模型时情况并非如此。因此，建议使用TensorFlow中的高级API来完成经常使用的任务。
- en: Here is the dataflow graph of the previous program when visualized using TensorBoard
    (TB).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用TensorBoard (TB)可视化的前一个程序的数据流图。
- en: '![](../Images/fa500fc8c03adc571355f6d468cc6e68.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa500fc8c03adc571355f6d468cc6e68.png)'
- en: But there is a very important question. **How the optimizer deduced the parameters
    that it should change? How it deduced that we are to optimize the weight (W) and
    bias (b)? **We have not explicitly told the optimizer that these are the parameters
    that it will change in order to reduce the loss but it deduced it itself. **So,
    how?**
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 但有一个非常重要的问题。**优化器是如何推导出应当改变哪些参数的？它是如何推导出我们需要优化权重（W）和偏置（b）的？**我们没有明确告诉优化器这些是它将改变的参数以减少损失，但它自己推导了出来。**那它是如何做到的？**
- en: In line 35, we run the session and asked to evaluate the train Tensor. TensorFlow
    will follow the chain of graph nodes to evaluate that Tensor. In line 23, TensorFlow
    found that to evaluate the train Tensor it should evaluate the optimizer.minimize
    operation. This operation will try to minimize its input arguments as much as
    possible.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在第35行，我们运行会话并要求评估训练Tensor。TensorFlow会按照图节点链评估该Tensor。在第23行，TensorFlow发现要评估训练Tensor，它需要评估optimizer.minimize操作。该操作将尽可能地最小化其输入参数。
- en: Following back, to evaluate the minimize operation it will accept one Tensor
    which is the loss. So, the goal now is to minimize the loss Tensor. But how to
    minimize the loss? It will still follow the graph back and it will find it is
    evaluated using the tensorflow.reduce_sum() operation. So, our goal now is to
    minimize the result of the tensorflow.reduce_sum() operation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 追溯回来，为了评估最小化操作，它将接受一个Tensor，即损失。因此，现在的目标是最小化损失Tensor。但如何最小化损失？它仍然会沿着图回溯，并发现它是通过tensorflow.reduce_sum()操作进行评估的。因此，我们现在的目标是最小化tensorflow.reduce_sum()操作的结果。
- en: Following back, this operation is evaluated using one Tensor as input which
    is squared_deltas. So, rather than having our goal to minimize the tensorflow.reduce_sum()
    operation, our goal now is to minimize the squared_deltas Tensor.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 追溯回来，这个操作是使用一个Tensor作为输入进行评估的，即squared_deltas。因此，我们现在的目标是最小化squared_deltas Tensor，而不是最小化tensorflow.reduce_sum()操作。
- en: Following the chain back, we find that the squared_deltas Tensor depends on
    the tensorflow.square() operation. So, we should minimize tensorflow.square()
    the operation. Minimizing that operation will ask us to minimize its input Tensors
    which are linear_model and y_train. Looking for thse two tensors, which one can
    be modified? The Tensors of type Variables can be modified. Because y_train is
    not a Variable but placeholder, then we can`t modify it and thus we can modify
    the linear_model to minimize the result.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 沿着链回溯，我们发现squared_deltas Tensor依赖于tensorflow.square()操作。因此，我们应该最小化tensorflow.square()操作。最小化该操作将要求我们最小化其输入Tensors，即linear_model和y_train。查看这两个Tensor，哪个可以被修改？可以修改的Tensor是Variables类型的。由于y_train不是Variable而是placeholder，因此我们不能修改它，因此我们可以修改linear_model来最小化结果。
- en: In line 15, the linear_model Tensor is calculated based on three inputs which
    are W, x, and b. Looking for these Tensors, only W and x can be changed because
    they are variables. So, our goal is to minimize these two Tensors W and x.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在第15行，linear_model Tensor是基于三个输入计算的，即W、x和b。查看这些Tensors，只有W和x可以被改变，因为它们是变量。因此，我们的目标是最小化这两个Tensors
    W和x。
- en: This is how TensorFlow deduced that to minimize the loss it should minimize
    the weight and bias parameters.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是TensorFlow推导出要最小化损失应当最小化权重和偏置参数的方法。
- en: '**Bio: [Ahmed Gad](https://www.linkedin.com/in/ahmedfgad/)** received his B.Sc.
    degree with excellent with honors in information technology from the Faculty of
    Computers and Information (FCI), Menoufia University, Egypt, in July 2015\. For
    being ranked first in his faculty, he was recommended to work as a teaching assistant
    in one of the Egyptian institutes in 2015 and then in 2016 to work as a teaching
    assistant and a researcher in his faculty. His current research interests include
    deep learning, machine learning, artificial intelligence, digital signal processing,
    and computer vision.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**简历：[Ahmed Gad](https://www.linkedin.com/in/ahmedfgad/)** 于 2015 年 7 月获得埃及门努菲亚大学计算机与信息学院信息技术学士学位，并以优异成绩获得荣誉。由于在学院排名第一，他被推荐于
    2015 年在埃及的一所学院担任助教，随后在 2016 年继续在学院担任助教及研究员。他当前的研究兴趣包括深度学习、机器学习、人工智能、数字信号处理和计算机视觉。'
- en: '[Original](https://www.linkedin.com/pulse/tensorflow-what-hyperparameters-optimize-ahmed-gad/).
    Reposted with permission.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.linkedin.com/pulse/tensorflow-what-hyperparameters-optimize-ahmed-gad/)。经许可转载。'
- en: '**Related:**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[TensorFlow: Building Feed-Forward Neural Networks Step-by-Step](/2017/10/tensorflow-building-feed-forward-neural-networks-step-by-step.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow：逐步构建前馈神经网络](/2017/10/tensorflow-building-feed-forward-neural-networks-step-by-step.html)'
- en: '[5 Free Resources for Furthering Your Understanding of Deep Learning](/2017/10/5-free-resources-furthering-understanding-deep-learning.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5 个免费资源，帮助你深入理解深度学习](/2017/10/5-free-resources-furthering-understanding-deep-learning.html)'
- en: '[7 Types of Artificial Neural Networks for Natural Language Processing](/2017/10/7-types-artificial-neural-networks-natural-language-processing.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7种用于自然语言处理的人工神经网络类型](/2017/10/7-types-artificial-neural-networks-natural-language-processing.html)'
- en: '* * *'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前 3 个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速迈向网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的 IT 需求'
- en: '* * *'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Tuning Adam Optimizer Parameters in PyTorch](https://www.kdnuggets.com/2022/12/tuning-adam-optimizer-parameters-pytorch.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 PyTorch 中调整 Adam 优化器参数](https://www.kdnuggets.com/2022/12/tuning-adam-optimizer-parameters-pytorch.html)'
- en: '[Top 10 MLOps Tools to Optimize & Manage Machine Learning Lifecycle](https://www.kdnuggets.com/2022/10/top-10-mlops-tools-optimize-manage-machine-learning-lifecycle.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化和管理机器学习生命周期的十大 MLOps 工具](https://www.kdnuggets.com/2022/10/top-10-mlops-tools-optimize-manage-machine-learning-lifecycle.html)'
- en: '[How to Optimize SQL Queries for Faster Data Retrieval](https://www.kdnuggets.com/2023/06/optimize-sql-queries-faster-data-retrieval.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何优化 SQL 查询以加快数据检索速度](https://www.kdnuggets.com/2023/06/optimize-sql-queries-faster-data-retrieval.html)'
- en: '[How To Optimize Dockerfile Instructions for Faster Build Times](https://www.kdnuggets.com/how-to-optimize-dockerfile-instructions-for-faster-build-times)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何优化 Dockerfile 指令以加快构建时间](https://www.kdnuggets.com/how-to-optimize-dockerfile-instructions-for-faster-build-times)'
- en: '[TensorFlow for Computer Vision - Transfer Learning Made Easy](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow 在计算机视觉中的应用 - 转移学习简化](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
- en: '[The "Hello World" of Tensorflow](https://www.kdnuggets.com/2022/05/hello-world-tensorflow.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tensorflow 的“Hello World”](https://www.kdnuggets.com/2022/05/hello-world-tensorflow.html)'
