- en: Nearest Neighbors for Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html](https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: K-Nearest Neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**K-nearest neighbors** (KNN) is a type of supervised learning machine learning
    algorithm and is used for both regression and classification tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: KNN is used to make predictions on the test data set based on the characteristics
    of the current training data points. This is done by calculating the distance
    between the test data and training data, assuming that similar things exist within
    close proximity.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm will have stored learned data, making it more effective at predicting
    and categorising new data points. When a new data point is inputted, the KNN algorithm
    will learn its characteristics/features. It will then place the new data point
    at closer proximity to the current training data points that share the same characteristics
    or features.
  prefs: []
  type: TYPE_NORMAL
- en: What Is The ‘k’ in KNN?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ‘K’ in KNN is a parameter that refers to the number of nearest neighbors.
    K is a positive integer and is typically small in value and is recommended to
    be an odd number.
  prefs: []
  type: TYPE_NORMAL
- en: In Layman's terms, the K-value creates an environment for the data points. This
    makes it easier to assign which data point belongs to which category.
  prefs: []
  type: TYPE_NORMAL
- en: The example below shows 3 graphs. The first, the ‘Initial Data’ is a graph where
    data points are plotted and clustered into classes, and a new example to classify
    is present. In the ‘Calculate Distance’ graph, the distance from the new example
    data point to the closest trained data points is calculated. However, this still
    does not categorise the new example data point. Therefore, using k-value, essentially
    created a neighborhood where we can classify the new example data point.
  prefs: []
  type: TYPE_NORMAL
- en: We would say that k=3 and the new data point will belong to Class B as there
    are more trained Class B data points with similar characteristics to the new data
    point in comparison to Class A.
  prefs: []
  type: TYPE_NORMAL
- en: '![k-nearest-neighbor](../Images/42005d40ecbc49545df65119eba77d02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [datacamp.com](https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn)'
  prefs: []
  type: TYPE_NORMAL
- en: If we increase the k-value to 7, we will see that the new data point will belong
    to Class A as there are more trained Class A data points with similar characteristics
    to the new data point in comparison to Class B.
  prefs: []
  type: TYPE_NORMAL
- en: '![k-nearest-neighbor](../Images/7804c5e5e9412162aad3060cbce401b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [datacamp.com](https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The k-value is typically a small number because, as we increase the k-value,
    the error rate also increases. The below graph shows this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![training-error](../Images/7e7fd5c0894c6e2a155c91de8e5811f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [analyticsvidhya](https://www.analyticsvidhya.com/wp-content/uploads/2014/10/training-error.png)'
  prefs: []
  type: TYPE_NORMAL
- en: However, If the k-value is small then it causes a low bias but a high variance,
    leading to overfitting of the model.
  prefs: []
  type: TYPE_NORMAL
- en: It is also recommended that the k-value is an odd number. This is because if
    we are trying to classify a new data point and we only have an even number of
    categories/classes (e.g. Class A and Class B) it can produce inaccurate outputs.
    Therefore it is highly recommended to choose a K-value with an odd number to avoid
    a tie.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating The Distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KNN calculates the distance between data points in order to classify new data
    points. The most common methods used to calculate this distance in KNN are Euclidian,
    Manhattan, and Minkowski.
  prefs: []
  type: TYPE_NORMAL
- en: '**Euclidean Distance** is the distance between two points using the length
    of a line between the two points. The formula for Euclidean Distance is the square
    root of the sum of the squared differences between a new data point (x) and an
    existing trained data point (y).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Manhattan Distance** is the distance between two points is the sum of the
    absolute difference of their Cartesian coordinates. The formula for Manhattan
    Distance is the sum of the lengths between a new data point (x) and an existing
    trained data point (y) using a line segment on the coordinate axes.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Minkowski Distance** is the distance between two points in the normed vector
    space and is a generalization of the Euclidean distance and the Manhattan distance.
    In the formula for ​​Minkowski Distance when p=2, we get Euclidian distance, also
    known as L2 Distance. When p=1 we get Manhattan distance, also known as L1 distance,
    city-block distance, and LASSO.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The image below is the formulas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3bc2167ca1bc8ba49564ffa3c8edf704.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The image below explains the difference between the three:'
  prefs: []
  type: TYPE_NORMAL
- en: '![XXXXX](../Images/b7aedd9bfc220a7674cd7252a7b0fdc6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Packt Subscription](https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781785882104/6/ch06lvl1sec40/measuring-distance-or-similarity)'
  prefs: []
  type: TYPE_NORMAL
- en: How Does The KNN Algorithm Work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Below are the steps of how a KNN algorithm works:'
  prefs: []
  type: TYPE_NORMAL
- en: Load in your dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a k-value. An odd number is recommended to avoid a tie.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the distance between the new data point and the neighboring existing
    trained data points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the K nearest neighbor to the new data point
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Below is an image that gives an overview of these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![implementing-your-own-knn](../Images/d45e3a6dc623593dacc476176f823385.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [kdnuggets.com](/2016/01/implementing-your-own-knn-using-python.html)'
  prefs: []
  type: TYPE_NORMAL
- en: KNN Algorithm Classification Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I will go through an example of the KNN algorithm being used on a Classification
    task using the Iris dataset
  prefs: []
  type: TYPE_NORMAL
- en: Import libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Load the Iris Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/28d3e9e7fc4e3581ef92d7257e8a0956.png)'
  prefs: []
  type: TYPE_IMG
- en: Preprocessing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We do this to split our dataset into its attributes and labels. The X variable
    will contain the first four columns of the dataset, which we refer to as the attributes
    and the y variable will contain the last column, which we refer to as the labels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Train Test Split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step, we are going to divide the dataset into training and test splits.
    This gives us an idea of how well the algorithm learned the training data and
    how well it performs on the testing data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Feature Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature scaling is an important element in preprocessing the data before making
    predictions. This method is used to normalise the range of features of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Making predictions with KNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First we need to import the KNeighborsClassifier class from the sklearn.neighbors
    library. We then choose our k-value, in this example, I have chosen 7\. Remember
    that it is highly recommended to choose an odd value to avoid ties.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We then move onto making predictions on our test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Accuracy of the algorithm**'
  prefs: []
  type: TYPE_NORMAL
- en: With sklearn.metrics, we can classification_report to evaluate the accuracy
    of the algorithm, looking at precision, recall and f1-score.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what the output looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![XXXXX](../Images/a6fb0d4f8939a1f9d53d75cd86ceecd5.png)'
  prefs: []
  type: TYPE_IMG
- en: From this, we can see that the KNN algorithm classified 30 data points with
    an average total of 95% on precision, 93% on recall, and 94% on f1-score.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the right k-value
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, I chose a k-value of 7\. However, if we wanted to check what
    the best k-value is, we can produce a graph that shows the different k-values
    and the error rate it produces.
  prefs: []
  type: TYPE_NORMAL
- en: I will be looking at k-values between 1 and 30\. A loop will be executed from
    1 to 30, where during each iteration the mean error is calculated and added to
    the error list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Plotting the k-value against error rate graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Output of graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Output of graph](../Images/0ff8c40f751d4aa9f7c253ab3c5dba74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Author Image'
  prefs: []
  type: TYPE_NORMAL
- en: From this graph, we can see that the k-values that give us a Mean Error of 0
    are predominantly between k-value 13 - 23.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KNN is a simple and easy algorithm to implement and can be used for both regression
    and classification tasks. The K-value is a parameter that refers to the number
    of nearest neighbors. It is recommended that the k-value is an odd number. There
    are different distance metrics you can use, but the most common ones are Euclidian,
    Manhattan, and Minkowski.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[From Theory to Practice: Building a k-Nearest Neighbors Classifier](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[K-nearest Neighbors in Scikit-learn](https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[More Performance Evaluation Metrics for Classification Problems You…](https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to Binary Classification with PyCaret](https://www.kdnuggets.com/2021/12/introduction-binary-classification-pycaret.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fine-Tuning BERT for Tweets Classification with HuggingFace](https://www.kdnuggets.com/2022/01/finetuning-bert-tweets-classification-ft-hugging-face.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Algorithms for Classification](https://www.kdnuggets.com/2022/03/machine-learning-algorithms-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
