- en: 'Essential Math for Data Science: Information Theory'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/01/essential-math-data-science-information-theory.html](https://www.kdnuggets.com/2021/01/essential-math-data-science-information-theory.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)[![Image](../Images/e6b504358bea8f63e781d7dbce4b4afc.png)](https://www.essentialmathfordatascience.com/?utm_source=kdnuggets&utm_medium=blog&utm_campaign=kdnuggets_scalars_vectors)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The field of *information theory* studies the quantification of information
    in signals. In the context of machine learning, some of these concepts are used
    to characterize or compare probability distributions. The ability to quantify
    information is also used in the decision tree algorithm, to select the variables
    associated with the maximum information gain. The concepts of entropy and cross-entropy
    are also important in machine learning because they lead to a widely used loss
    function in classification tasks: the cross-entropy loss or log loss.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Shannon Information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Intuition**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step to understanding information theory is to consider the concept
    of quantity of information associated with a random variable. In information theory,
    this quantity of information is denoted as II and is called the *Shannon information*, *information
    content*, *self-information*, or *surprisal*. The main idea is that likely events
    convey less information than unlikely events (which are thus more *surprising*).
    For instance, if a friend from Los Angeles, California tells you: “It is sunny
    today”, this is less informative than if she tells you: “It is raining today”.
    For this reason, is can be helpful to think of the Shannon information as the
    amount of surprise associated with an outcome. You’ll also see in this section
    why it is also a quantity of information, and why likely events are associated
    with less information.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Units of Information**'
  prefs: []
  type: TYPE_NORMAL
- en: Common units to quantity information are the *nat* and the *bit*. These quantities
    are based on logarithm functions. The word *nat*, short for *natural unit of information* is
    based on the natural logarithm, while the bit, short for “binary digit”, is based
    on base-two logarithms. The bit is thus a rescaled version of the nat. The following
    sections will mainly use the bit and base-two logarithms in formulas, but replacing
    it with the natural logarithm would just change the unit from bits to nats.
  prefs: []
  type: TYPE_NORMAL
- en: Bits represent variables that can take two different states (0 or 1). For instance,
    1 bit is needed to encode the outcome of a coin flip. If you flip two coins, you’ll
    need at least two bits to encode the result. For instance, 00 for HH, 01 for HT,
    10 for TH and 11 for TT. You could use other codes, such as 0 for HH, 100 for
    HT, 101 for TH and 111 for TT. However, this code uses a larger number of bits
    in average (considering that the probability distribution of the four events is
    uniform, as you’ll see)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take an example to see what a bit describes. Erica sends you a message
    containing the result of three coin flips, encoding ‘heads’ as 0 and ‘tails’ as
    1\. There are 8 possible sequences, such as 001, 101, etc. When you receive a
    message of one bit, it divides your uncertainty by a factor of 2\. For instance,
    if the first bit tells you that the first roll was ‘heads’, the remaining possible
    sequences are 000, 001, 010, and 011\. There are only 4 possible sequences instead
    of 8\. Similarly, receiving a message of two bits will divide your uncertainty
    by a factor of 2222; a message of three bits, by a factor of 2323, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we talk about “useful information”, but it is possible that the message
    is redundant and convey less information with the same number of bits.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say that we want to transmit the result of a sequence of eight tosses.
    You’ll allocate one bit per toss. You thus need eight bits to encode the sequence.
    The sequence might be for instance “00110110”, corresponding to HHTTHTTH(four
    “heads” and four “tails”).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, let’s say that the coin is biased: the chance to get “tails” is only
    1 over 8\. You can find a better way to encode the sequence. One option is to
    encode the index of the outcomes “tails”: it will take more than one bit, but
    ‘tails’ occurs only for a small proportion of the trials. With this strategy,
    you allocate more bits to rare outcomes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This example illustrates that more predictable information can be compressed:
    a biased coin sequence can be encoded with a smaller amount of information than
    a fair coin. This means that Shannon information depends on the probability of
    the event.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mathematical Description**'
  prefs: []
  type: TYPE_NORMAL
- en: Shannon information encodes this idea and converts the probability that an event
    will occur into the associated quantity of information. Its characteristics are
    that, as you saw, likely events are less informative than unlikely events and
    also that information from different events is additive (if the events are independent).
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the function *I(x)* is the information of the event *X=x* that
    takes the outcome as input and returns the quantity of information. It is a monotonically
    decreasing function of the probability (that is, a function that never increases
    when the probability increases). Shannon information is described as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/fd55e7d2ac8f63e589fdb3a404b72031.png)'
  prefs: []
  type: TYPE_IMG
- en: The result is a lower bound on the number of bits, that is, the minimum amount
    of bits needed to encode a sequence with an optimal encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'The logarithm of a product is equal to the sum of the elements: ![Equation](../Images/5377a4150756f11be3e984841fce238b.png).
    This property is useful to encode the additive property of the Shannon information.
    The probability of occurrence of two events is their individual probabilities
    multiplied together (because they are independent, as you saw in [Essential Math
    for Data Science](https://bit.ly/3njFklS)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/5ea29c31029510d5ecdebfd8225fb711.png)'
  prefs: []
  type: TYPE_IMG
- en: This means that the information corresponding to the probability of occurrence
    of two events *P(x,y)* equals the information corresponding to *P(x)* added to
    the information corresponding to *P(y)*. The information of independent events
    add together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot this function for a range of probability between 0 and 1 to see
    the shape of the curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure](../Images/2ae17b840d7be726e113f81d3473127c.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 1: The quantity of information is given by the negative logarithm of
    the probability.*'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in Figure 1, the negative logarithm function encodes the idea
    that a very unlikely event (probability around 0) is associated with a large quantity
    of information and a likely event (probability around 1) is associated with a
    quantity of information around 0.
  prefs: []
  type: TYPE_NORMAL
- en: Since you used a base-two logarithm `np.log2()`, the information *I(x)* is measured
    in *bits*.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You saw that Shannon information gives the amount of information associated
    with a single probability. You can also calculate the amount of information of
    a discrete distribution with the *Shannon entropy*, also called *information entropy*,
    or simply *entropy*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**'
  prefs: []
  type: TYPE_NORMAL
- en: Consider for instance a biased coin, where you have a probability of 0.8 of
    getting ‘heads’.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is your distribution: you have a probability of 0.8 of getting ‘heads’
    and a probability of *1 − 0.8 = 0.2* of getting ‘tails’.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These probabilities are respectively associated with a Shannon information
    of:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Equation](../Images/73a405551777ec222441120448d774c4.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/c08c196af95ea74c7b9ce92f2acb466e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Landing ‘heads’ is associated with an information around 0.32 and landing ‘tails’
    to 2.32\. However, you don’t have the same number of ‘heads’ and ‘tails’ in average,
    so you must weight the Shannon information of each probability with the probability
    itself. For instance, if you want to transmit a message with the results of, say,
    100 trials, you’ll need around 20 times the amount of information corresponding
    to ‘tails’ and 80 times the amount of information corresponding to ‘heads’. You
    get:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Equation](../Images/cc39e9187d2c55ba6e924101a1997506.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/50d392d9bee729268e62964131585acf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The sum of these expressions gives you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Equation](../Images/db046008bb43208d8c491754cd730c6a.png)'
  prefs: []
  type: TYPE_IMG
- en: The average number of bits required to describe a series of events from this
    distribution is 0.72 bits.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, you can consider the entropy as a summary of the information
    associated with the probabilities of the discrete distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: You calculate the Shannon information of each probability of your distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You weight the Shannon information with the corresponding probability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You sum the weighted results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Mathematical Formulation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The entropy is the expectation of the information with respect to the probability
    distribution. Remember from [Essential Math for Data Science](https://bit.ly/3njFklS) that
    the expectation is the mean value you’ll get if you draw a large number of samples
    from the distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/f1c1585d04229c82239e2e4538c0f3bc.png)'
  prefs: []
  type: TYPE_IMG
- en: with the random variable *X* having *n* possible outcomes, *x[i]* being the *i*th
    possible outcome corresponding to a probability of *P(x[i])*. The expected value
    of the information of a distribution corresponds to the average of the information
    you’ll get.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the formula of the expectation and the Shannon information, the entropy
    of the random variable *X* is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/175b64d27a5282aee38051690df3cb20.png)'
  prefs: []
  type: TYPE_IMG
- en: The entropy gives you the average quantity of information that you need to encode
    the states of the random variable *X*.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the input of the function *H(X)* is the random variable *X* while *I(x)* denotes
    the Shannon information of the event *X=x*. You can also refer to the entropy
    of the random variable *X* which is distributed with respect to *P(x)* as *H(P)*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Illustration**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take an example: as illustrated in Figure 2 in the bottom panel, you
    have a discrete distribution with four possible outcomes, associated with probabilities
    0.4, 0.4, 0.1, and 0.1, respectively. As you saw previously, the information is
    obtained by log transforming the probabilities (top panel). This is the last part
    of the entropy formula: *log2P(x)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/ecdaafee5cb8e80c770cf60f20c8bdd8.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2: Illustration of the entropy as the weighted sum of the Shannon information.*'
  prefs: []
  type: TYPE_NORMAL
- en: Each of these transformed probabilities is weighted by the corresponding raw
    probability. If an outcome occurs frequently, it will give more weight into the
    entropy of the distribution. This means that a low probability (like 0.1 in Figure
    2) gives a large amount of information (3.32 bits) but has less influence on the
    final result. A larger probability (like 0.4 in Figure 2) is associated with less
    information (1.32 bits as shown in Figure 2) but has more weight.
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary Entropy Function**'
  prefs: []
  type: TYPE_NORMAL
- en: In the example of a biased coin, you calculated the entropy of a Bernoulli process
    (more details about the Bernoulli distribution in [Essential Math for Data Science](https://bit.ly/3njFklS)).
    In this special case, the entropy is called the *binary entropy function*.
  prefs: []
  type: TYPE_NORMAL
- en: To characterize the binary entropy function, you’ll calculate the entropy of
    a biased coin described by various probability distributions (from heavily biased
    in favor of “tails” to heavily biased in favor of “heads”).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by creating a function to calculate the entropy of a distribution
    that takes an array with the probabilities as input and returns the corresponding
    entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can also use `entropy()` from `scipy.stats`, where you can specify the base
    of the logarithm used to calculate the entropy. Here, I have used the base-two
    logarithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take the example of a fair coin, with a probability of 0.5 of landing
    ‘heads’. The distribution is thus 0.5 and 1 − 0.5 = 0.5\. Let’s use the function
    we just defined to calculate the corresponding entropy is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The function calculates the sum of `P * np.log2(P)` over each element of the
    array you use as input. Using an array as input As you saw in the previous section,
    you can expect a lower entropy for biased coin. Let’s plot the entropy for various
    coin biases, from a coin landing only as ‘tails’ to a coin landing only as ‘heads’:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure](../Images/aae2e343d1abfe59210c0290a25e6b00.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3: Entropy as a function of the probability to land “heads”.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3 shows that the entropy increases until you reach the more uncertain
    condition: that is, when the probability of landing ‘heads’ equals the probability
    of landing ‘tails’.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Differential Entropy**'
  prefs: []
  type: TYPE_NORMAL
- en: The entropy of a continuous distribution is called *differential entropy*. It
    is an extension of the entropy for discrete distribution, but it doesn’t satisfy
    the same requirements. The issue is that values have probability tending to zero
    with continuous distributions, and encoding this would require a number of bits
    tending to infinity.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/b083a67b3e20d937ac2bad40e81ff2de.png)'
  prefs: []
  type: TYPE_IMG
- en: Differential entropy can be negative. The reason is that, as you saw in [Essential
    Math for Data Science](https://bit.ly/3njFklS), continuous distributions are not
    probabilities but probability densities, meaning that they don’t satisfy the requirements
    of probabilities. For instance, they are not constrained to be lower than 1\.
    This has the consequence that *p(x)* can take positive values larger than 1 and *log2p(x)* can
    take positive values (leading to negative values because of the negative sign).
  prefs: []
  type: TYPE_NORMAL
- en: Cross Entropy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The concept of entropy can be used to compare two probability distributions:
    this is called the *cross entropy* between two distributions, which measures how
    much they differ.'
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to calculate the information associated with the probabilities of
    a distribution *Q(x)*, but instead of weighting according to *Q(x)* as with the
    entropy, you weight according to the other distribution *P(x)*. Note that you
    compare two distributions concerning the same random variable *X*.
  prefs: []
  type: TYPE_NORMAL
- en: You can also consider cross-entropy as the expected quantity of information
    of events drawn from *P(x)* when you use *Q(x)* to encode them.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is mathematically expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/6c038f4a404d3dc7c1d1390a0f2c42fd.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/8d3bb7ddd0e728ab031ab27ec6c29bb8.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4: Illustration of the cross entropy as the Shannon information of *Q(x)* weighted
    according to the distribution of *P(x)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4 shows two different situations to illustrate the cross entropy. In
    the left, you have two identical distributions *P(x)* (in blue) and *Q(x)* (in
    red). Their cross entropy is equal to the entropy because the information of *Q(x)* is
    weighted according to the distribution of *P(x)*, which is similar to *Q(x)*.
  prefs: []
  type: TYPE_NORMAL
- en: However, in the right panel, *P(x)* and *Q(x)* are different. This results in
    a larger cross entropy, because probabilities associated with a large quantity
    of information have a small weight, while probabilities associated with a small
    quantity of information have large weights.
  prefs: []
  type: TYPE_NORMAL
- en: The cross entropy can’t be smaller than the entropy. Still in the right panel,
    you can see that, when the probability *Q(x)* is larger than *P(x)* (and thus
    associated with a lower amount of information), it is counterbalanced by the low
    weights (resulting in low weights and low information). These low weights will
    be compensated with larger weights in other probabilities from the distribution
    (resulting in large weights and large information).
  prefs: []
  type: TYPE_NORMAL
- en: Note also that both distributions *P(x)* and *Q(x)* must have the same *support* (that
    is the same set of values that the random variable can take associated with positive
    probabilities).
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the cross entropy is minimum when the distributions are identical.
    As you’ll see in [0.1.4](https://hadrienj.github.io/posts/Essential-Math-information_theory/#sec:ch12_section_kullback_leibler_divergence),
    this property makes the cross entropy a useful metric. Note also that the result
    is different according to the distribution you choose as a reference: *H(P,Q)
    ≠ H(Q,P)*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cross Entropy as a Loss Function**'
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, cross entropy is used as a loss function called the *cross
    entropy loss* (also called the *log loss*, or the *logistic loss*, because it
    is used in logistic regression).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/ec9bac288843cb9f238f77e33842fbf7.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5: Cross entropy can be used to compare the true distribution (probability
    of 1 for the correct class and 0 otherwise) and the distribution estimated by
    the model.*'
  prefs: []
  type: TYPE_NORMAL
- en: Say you want to build a model that classifies three different bird species from
    audio samples. As illustrated in Figure 5, the audio samples are converted in
    features (here spectrograms) and the possible classes (the three different birds)
    are *one-hot encoded*, that is, encoded as 1 for the correct class and 0 otherwise.
    Furthermore, the machine learning model outputs probabilities for each class.
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to classify the birds, the model needs to compare the estimated
    distribution *Q(x)* (given by the model) and the true distribution *P(x)*. The
    cross entropy loss is computed as the cross entropy between *P(x)* and *Q(x)*.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5 shows that the true class corresponding the sample you consider in
    this example is “European Green Woodpecker”. The model outputs a probability distribution
    and you’ll compute the cross entropy loss associated with this estimation. Figure
    6 shows both distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/930fad886c9a9c232fbeec17a0e06f0c.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6: Comparison of the true distribution *P(x)* and the estimated distribution *Q(x)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s manually calculate the cross entropy between these two distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/82b182d56293446bad0e1e9ce19444cd.png)'
  prefs: []
  type: TYPE_IMG
- en: The natural logarithm is used in the cross-entropy loss instead of the base-two
    logarithm, but the principle is the same. In addition, note the use of *H(P,Q)* instead
    of *H(Q,P)* because the reference is the distribution *P*.
  prefs: []
  type: TYPE_NORMAL
- en: Since you one-hot encoded the classes (1 for the true class and 0 otherwise),
    the cross entropy is simply the negative logarithm of the estimated probability
    for the true class.
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary Classification: Log Loss**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In machine learning, the cross entropy is widely used as a loss for binary
    classification: the log loss.'
  prefs: []
  type: TYPE_NORMAL
- en: Since the classification is binary, the only possible outcomes are *y* (the
    true label corresponds to the first class) and *1−y* (the true label corresponds
    to the second class). Similarly, you have the estimated probability of the first
    class *y^* and the estimated probability of the second class *1−y^*.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the formula of the cross entropy, *∑x* corresponds here to the sum over
    the two possible outcomes (*y* and *1−y*). You have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/96f46583f71303d20eed4e2126b5c458.png)'
  prefs: []
  type: TYPE_IMG
- en: which is the formula of the log loss.
  prefs: []
  type: TYPE_NORMAL
- en: Kullback-Leibler Divergence (KL Divergence)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You saw that the cross entropy is a value which depends on the similarity of
    two distributions, with the smaller cross entropy value corresponding to identical
    distributions. You can use this property to calculate the *divergence* between
    two distributions: you compare their cross entropy with the situation where the
    distributions are identical. This divergence is called the *Kullback-Leibler divergence* (or
    simply the *KL divergence*), or the *relative entropy*.'
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, the KL divergence is the supplemental amount of information associated
    with the encoding of the distribution *Q(x)* compared to the true distribution *P(x)*.
    It tells you how different the two distributions are.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the KL divergence between two distributions *P(x)* and *Q(x)*,
    denoted as *DKL(P||Q)*, is expressed as the difference between the cross entropy
    of *P(x)* and *Q(x)* and the entropy of *P(x)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/dfc5ed69b7928089bd2e65221771093b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Replacing with the expressions of the cross entropy and the entropy, you get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/d3c5d4d17b8f3f50b5db64937c389c65.png)'
  prefs: []
  type: TYPE_IMG
- en: The KL divergence is always non-negative. Since the entropy *H(P)* is identical
    to the cross entropy *H(P,P)*, and because the smallest cross entropy is between
    identical distributions *(H(P,P)H(P,P))*, *H(P,Q)* is necessarily larger than *H(P)*.
    In addition, the KL divergence is equal to zero when the two distributions are
    identical.
  prefs: []
  type: TYPE_NORMAL
- en: However, the cross entropy is not symmetrical. Comparing a distribution *P(x)* to
    a distribution *Q(x)* can be different than comparing a distribution *Q(x)* to *P(x)* –
    which implies that you can’t consider the KL divergence to be a distance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Hadrien Jean](https://hadrienj.github.io/)** is a machine learning
    scientist. He owns a Ph.D in cognitive science from the Ecole Normale Superieure,
    Paris, where he did research on auditory perception using behavioral and electrophysiological
    data. He previously worked in industry where he built deep learning pipelines
    for speech processing. At the corner of data science and environment, he works
    on projects about biodiversity assessement using deep learning applied to audio
    recordings. He also periodically creates content and teaches at Le Wagon (data
    science Bootcamp), and writes articles in his blog ([hadrienj.github.io](http://hadrienj.github.io)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://hadrienj.github.io/posts/Essential-Math-information_theory/).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Essential Math for Data Science: The Poisson Distribution](/2020/12/introduction-poisson-distribution-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Essential Math for Data Science: Probability Density and Probability Mass
    Functions](/2020/12/essential-math-data-science-probability-density-probability-mass-functions.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Essential Math for Data Science: Integrals And Area Under The Curve](/2020/11/essential-math-data-science-integrals-area-under-curve.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
