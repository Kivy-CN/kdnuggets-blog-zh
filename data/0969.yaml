- en: Dimensionality Reduction Techniques in Data Science
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学中的降维技术
- en: 原文：[https://www.kdnuggets.com/2022/09/dimensionality-reduction-techniques-data-science.html](https://www.kdnuggets.com/2022/09/dimensionality-reduction-techniques-data-science.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/09/dimensionality-reduction-techniques-data-science.html](https://www.kdnuggets.com/2022/09/dimensionality-reduction-techniques-data-science.html)
- en: '![Dimensionality Reduction Techniques in Data Science](../Images/800a46a763162a8519b021b1f506541a.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![数据科学中的降维技术](../Images/800a46a763162a8519b021b1f506541a.png)'
- en: Image by Editor
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：编辑
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全领域的职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Analyzing data with a list of variables in machine learning requires a lot of
    resources and computations, not to mention the manual labor that goes with it.
    This is precisely where the **dimensionality reduction techniques** come into
    the picture. The dimensionality reduction technique is a process that transforms
    a high-dimensional dataset into a lower-dimensional dataset without losing the
    valuable properties of the original data. These **dimensionality reduction techniques**
    are basically a part of the data pre-processing step, performed before training
    the model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，分析包含多个变量的数据需要大量的资源和计算，更不用说所需的人工劳动了。这正是**降维技术**发挥作用的地方。降维技术是一种将高维数据集转化为低维数据集的过程，同时不丢失原始数据的有价值属性。这些**降维技术**基本上是数据预处理步骤的一部分，在训练模型之前进行。
- en: What is Dimensionality Reduction in Data Science?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是数据科学中的降维？
- en: Imagine you are training a model that could predict the next day's weather based
    on the various climatic conditions of the present day. The present-day conditions
    could be based on sunlight, humidity, cold, temperature, and many millions of
    such environmental features, which are too complex to analyze. Hence, we can lessen
    the number of features by observing which of them are strongly correlated with
    each other and clubbing them into one.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在训练一个模型，该模型可以基于当日的各种气候条件预测第二天的天气。当前的气候条件可能包括阳光、湿度、寒冷、温度等数百万个环境特征，这些特征过于复杂而难以分析。因此，我们可以通过观察哪些特征彼此强相关并将它们合并成一个来减少特征数量。
- en: '![Dimensionality Reduction Techniques in Data Science](../Images/8b21115b2119e4c5418d96147df87680.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![数据科学中的降维技术](../Images/8b21115b2119e4c5418d96147df87680.png)'
- en: image by Auhtor
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Here, we can club humidity and rainfall into a single dependent feature since
    we know they are strongly correlated. That's it! This is how the dimensionality
    reduction technique is used to compress complex data into a simpler form without
    losing the essence of the data. Moreover, data science and AI experts are now
    also using **data science solutions** to leverage business ROI. Data visualization,
    data mining, predictive analytics, and other **data analytics services** by Datatobiz
    are changing the business game.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以将湿度和降雨量合并为一个依赖特征，因为我们知道它们之间有很强的相关性。就这样！这就是降维技术用于将复杂数据压缩成更简单形式而不丧失数据本质的方法。此外，数据科学和
    AI 专家现在还使用**数据科学解决方案**来提高业务 ROI。数据可视化、数据挖掘、预测分析以及 Datatobiz 提供的其他**数据分析服务**正在改变商业游戏规则。
- en: Why is Dimensionality Reduction Necessary?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么降维是必要的？
- en: machine learning and Deep Learning techniques are performed by inputting a vast
    amount of data to learn about fluctuations, trends, and patterns. Unfortunately,
    such huge data consists of many features, which often leads to a curse of dimensionality.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和深度学习技术通过输入大量数据来学习波动、趋势和模式。不幸的是，这种巨大的数据包含许多特征，通常会导致“维度诅咒”。
- en: Moreover, sparsity is a common occurrence in large datasets. Sparsity refers
    to having negligible or no value features, and if it is inputted in a training
    model, it performs poorly on testing. In addition, such redundant features cause
    problems in clustering the similar features of the data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，稀疏性在大型数据集中很常见。稀疏性指的是具有微不足道或无值的特征，如果将其输入训练模型，模型在测试时表现不佳。此外，这些冗余特征会导致数据相似特征的聚类问题。
- en: 'Hence, to counter the curse of dimensionality, dimensionality reduction techniques
    come to the rescue. The answers to the question of why dimensionality reduction
    is useful are:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了应对维度诅咒，降维技术发挥了作用。为什么降维有用的问题的答案是：
- en: The model performs more accurately since redundant data will be removed, which
    will lead to less room for assumption.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型表现得更准确，因为冗余数据将被移除，这将减少假设的空间。
- en: Less usage of computational resources, which will save time and financial budget
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更少的计算资源使用，将节省时间和财务预算
- en: A few machine learning/Deep Learning techniques do not work on high-dimensional
    data, a problem that will be solved once the dimension reduces.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些机器学习/深度学习技术在高维数据上无法工作，而这个问题会在降维后得到解决。
- en: Clean and non-sparse data will give rise to more statistically significant results
    because clustering of such data is easier and more accurate.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 干净且非稀疏的数据会产生更具统计显著性的结果，因为这种数据的聚类更容易且更准确。
- en: Now let us understand **which algorithms are used for dimensionality reduction
    of data** with examples.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过示例来了解**用于数据降维的算法**。
- en: What are Dimensionality Reduction Techniques
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是降维技术
- en: The dimensionality reduction techniques are broadly divided into two categories,
    namely,
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 降维技术大致分为两类，即：
- en: Linear Methods
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性方法
- en: Non-Linear Methods
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 非线性方法
- en: 1\. Linear Methods
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 线性方法
- en: PCA
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PCA
- en: Principal Component Analysis (PCA) is one of the used DR techniques in data
    science. Consider a set of '*p*' variables that are correlated with each other.
    This technique reduces this set of '*p*' variables into a smaller number of uncorrelated
    variables, usually denoted by '*k*', where (*k<p*). These '*k*' variables are
    called principal components, and their variation is similar to the original dataset.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）是数据科学中使用的降维技术之一。考虑一组互相关联的'*p*' 变量。该技术将这组'*p*' 变量减少为较小数量的不相关变量，通常用'*k*'
    表示，其中 (*k<p*)。这些'*k*' 变量称为主成分，它们的变化类似于原始数据集。
- en: PCA is used to figure out the correlation among features, which it combines
    together. As a result, the resultant dataset has lesser features that are linearly
    correlated with each other. This way, the model performs the reduction of correlated
    features while simultaneously calculating maximum variance in the original dataset.
    After finding the directions of this variance, it directs them into a smaller
    dimensional space which gives rise to new components called principal components.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 用于找出特征之间的相关性，它将这些相关性结合在一起。因此，结果数据集具有较少的彼此线性相关的特征。这样，模型在同时计算原始数据集的最大方差时，会减少相关特征。找到这些方差的方向后，它将这些方向引导到一个较小的维度空间中，产生称为主成分的新成分。
- en: These components are pretty sufficient in representing the original features.
    Therefore, it reduces the reconstruction error while finding out the optimum components.
    This way, data is reduced, making the machine learning algorithms perform better
    and faster.  PrepAI is one of the perfect examples of AI that has made use of
    the PCA technique in the backend to generate questions from a given raw text intelligently.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些成分在表示原始特征方面非常充分。因此，它在寻找最佳成分的同时减少了重构误差。这样，数据得到了减少，使得机器学习算法表现得更好、更快。PrepAI 是一个完美的
    AI 示例，它在后台利用 PCA 技术从给定的原始文本中智能生成问题。
- en: Factor Analysis
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 因素分析
- en: This technique is an extension of Principal Component Analysis (PCA). The main
    focus of this technique is not just to reduce the dataset. It focuses more on
    finding out latent variables, which are results of other variables from the dataset.
    They are not measured directly in a single variable.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术是主成分分析（PCA）的扩展。此技术的主要关注点不仅仅是减少数据集。它更多地关注发现潜在变量，这些变量是数据集中其他变量的结果。它们不是在单一变量中直接测量的。
- en: Latent variables are also called factors. Hence, the process of building a model
    which measures these latent variables is known as factor analysis. It not only
    helps in reducing the variables but also helps in distinguishing response clusters.
    For example, you have to build a model which will predict customer satisfaction.
    You will prepare a questionnaire that has questions like,
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在变量也称为因子。因此，建立一个测量这些潜在变量的模型的过程称为因子分析。它不仅有助于减少变量，还帮助区分响应簇。例如，你需要建立一个预测客户满意度的模型。你将准备一个问卷，其中包含类似的问题，
- en: '"Are you happy with our product?"'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: “你对我们的产品满意吗？”
- en: '"Would you share your experience with your acquaintances?"'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: “你愿意与熟人分享你的经验吗？”
- en: If you want to create a variable to rate customer satisfaction, you will either
    average the responses or create a factor-dependent variable. This can be performed
    using PCA and keeping the first factor as a principal component.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想创建一个变量来评估客户满意度，你可以选择对反馈进行平均或创建一个因子依赖的变量。这可以通过PCA完成，并将第一个因子作为主成分。
- en: Linear Discriminant Analysis
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性判别分析
- en: It is a dimensionality reduction technique that is used mainly for supervised
    classification problems. Logistic Regression fails in multi-classification. Hence,
    LDA comes into the picture to counter that shortcoming. It efficiently discriminates
    between training variables in their respective classes. Moreover, it is different
    from PCA as it calculates a linear combination between the input features to optimize
    the process of distinguishing different classes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种主要用于监督分类问题的降维技术。逻辑回归在多分类问题中效果不佳。因此，LDA应运而生，以弥补这一不足。它有效地区分了各个类别的训练变量。此外，它与PCA不同，因为它通过计算输入特征之间的线性组合来优化区分不同类别的过程。
- en: 'Here is an example to help you understand LDA:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子帮助你理解LDA：
- en: 'Consider a set of balls belonging to two classes: Red Balls and Blue Balls.
    Imagine they are plotted on a 2D plane randomly, such that they cannot be separated
    into two distinct classes using a straight line. In such cases, LDA is used, which
    can convert a 2D graph into a 1D graph, thereby maximizing the distinction between
    the classes of balls. The balls are projected to a new axis which separates them
    into their classes in the best possible way. The new axis is formed using two
    steps:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一组属于两类的球：红球和蓝球。假设它们随机地绘制在二维平面上，以至于不能用直线将它们分成两个不同的类别。在这种情况下，使用LDA可以将二维图转换为一维图，从而最大限度地区分球的类别。球被投影到一个新的轴上，这个轴以最佳方式将它们分隔到各自的类别中。新的轴是通过两个步骤形成的：
- en: By maximizing the distances between the means of two classes
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过最大化两个类别均值之间的距离
- en: By minimizing the variation within each individual class
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过最小化每个类别内部的变异
- en: SVD
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SVD
- en: Consider data with '*m*' columns. Truncated Singular Value Decomposition method
    (TSVD) is a projection method where these '*m*' columns (features) are projected
    into a subspace with '*m*' or lesser columns without losing the characteristics
    of the data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具有'*m*'列的数据。截断奇异值分解方法（TSVD）是一种投影方法，其中这些'*m*'列（特征）被投影到一个具有'*m*'列或更少列的子空间中，而不会丢失数据的特征。
- en: An example where TSVD can be used is a dataset containing reviews about e-commerce
    products. The review column is mostly left blank, which gives rise to null values
    in the data, and TSVD tackles it efficiently. This method can be implemented easily
    using the TruncatedSVD() function.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一个TSVD可以使用的例子是包含电子商务产品评论的数据集。评论列通常是空白的，这会导致数据中出现空值，而TSVD可以有效处理这种情况。这个方法可以通过TruncatedSVD()函数轻松实现。
- en: While the PCA uses dense data, SVD uses sparse data. Moreover, the covariance
    matrix is used for PCA factorization, whereas TSVD is done on a data matrix.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: PCA使用的是密集数据，而SVD使用的是稀疏数据。此外，PCA因式分解使用协方差矩阵，而TSVD是在数据矩阵上进行的。
- en: 2\. Non-Linear Methods
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 非线性方法
- en: Kernel PCA
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核PCA
- en: PCA is quite efficient for datasets that are linearly separable. However, if
    we apply it to datasets that are non-linear, the reduced dimension of the dataset
    may not be accurate. Hence, this is where Kernel PCA becomes efficient.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: PCA对于线性可分的数据集非常有效。然而，如果我们将它应用于非线性数据集，数据集的降维可能不准确。因此，这时核PCA变得高效。
- en: The dataset undergoes a kernel function and is temporarily projected into a
    higher dimensional feature space. Here, the classes are transformed and can be
    separated linearly and distinguished with the help of a straight line. Further,
    a general PCA is applied, and the data is projected back onto a reduced dimensional
    space. Conducting this linear dimensionality reduction method in that space will
    be as good as conducting non-linear dimensionality reduction in the actual space.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集经过一个核函数并暂时投影到更高维的特征空间。在这里，类别被转换，可以用直线分开并加以区分。然后，应用一般的 PCA，将数据投影回降维空间。在该空间中进行线性降维方法将与在实际空间中进行非线性降维一样有效。
- en: 'Kernel PCA operates on 3 important hyperparameters: the number of components
    we wish to retain, the type of kernel we want to use, and the kernel coefficient.
    There are different types of the kernel, namely, ''linear'', ''poly'', ''rbf'',
    ''sigmoid'', ''cosine''. Radial Basis Function kernel (RBF) is widely used among
    them.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 核主成分分析（Kernel PCA）操作有 3 个重要的超参数：我们希望保留的组件数量、我们要使用的核类型和核系数。有不同类型的核，即“线性”（'linear'）、“多项式”（'poly'）、“径向基函数”（'rbf'）、“
    sigmoid”和“余弦”（'cosine'）。其中，径向基函数核（RBF）被广泛使用。
- en: T-Distributed Stochastic Neighbor Embedding
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: T-分布随机邻域嵌入（T-Distributed Stochastic Neighbor Embedding）
- en: It is a non-linear dimensionality reduction method primarily applied for data
    visualization, image processing, and NLP. T-SNE has a flexible parameter, namely,
    'perplexity'. It showcases how to maintain attendance between global and local
    aspects of the dataset. It gives an estimate of the number of close neighbors
    of each data point. Also, it transforms the similarities between different data
    points into joint probabilities, and the Kullback-Leibler divergence is minimized
    between the joint probabilities of low-dimensional embedding and high-dimensional
    datasets. Moreover, T-SNE also comes up with a cost function that is not convex
    in nature, and with different initializations, one can get different results.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种非线性降维方法，主要用于数据可视化、图像处理和自然语言处理（NLP）。T-SNE 具有一个灵活的参数，即“困惑度”（'perplexity'）。它展示了如何在数据集的全局和局部方面之间保持平衡。它提供了每个数据点的近邻数量的估计。此外，它将不同数据点之间的相似性转换为联合概率，并在低维嵌入和高维数据集的联合概率之间最小化
    Kullback-Leibler 散度。此外，T-SNE 还提供了一个非凸的成本函数，使用不同的初始化方法可能会得到不同的结果。
- en: T-SNE preserves only minimum pairwise distances or local similarities, while
    PCA preserves maximum pairwise distances to maximize variance. Also, PCA or TSVD
    is highly recommended to reduce the dimensions of the features in the dataset
    to exceed more than 50 because T-SNE fails in this case.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: T-SNE 仅保留最小的成对距离或局部相似性，而 PCA 保留最大对距离以最大化方差。此外，推荐使用 PCA 或 TSVD 将数据集中特征的维度减少到超过
    50，因为在这种情况下 T-SNE 会失效。
- en: Multi-Dimensional Scaling
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多维缩放（Multi-Dimensional Scaling）
- en: Scaling refers to making the data simpler by reducing it to a lower dimension.
    It is a non-linear dimensionality reduction technique that showcases the distances
    or dissimilarities between the sets of features in a visualized manner. Features
    with shorter distances are considered similar, whereas those with larger distances
    are dissimilar.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放指的是通过将数据减少到较低的维度来简化数据。这是一种非线性降维技术，通过可视化的方式展示特征集之间的距离或差异。特征之间距离较短被认为是相似的，而距离较大的则被认为是不相似的。
- en: MDS reduces the data dimension and interprets the dissimilarities in the data.
    Also, data doesn't lose its essence after scaling down; two data points will always
    be at the same distance irrespective of their dimension. This technique can only
    be applied to matrices having relational data, such as correlations, distances,
    etc. Let's understand this with the help of an example.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: MDS 减少数据维度并解释数据中的差异。此外，数据在缩小后不会失去本质；两个数据点的距离总是相同的，无论它们的维度如何。此技术仅适用于具有关系数据的矩阵，例如相关性、距离等。让我们通过一个示例来理解这一点。
- en: Consider you have to make a map, where you are provided with a list of city
    locations. The map should also showcase the distances between the two cities.
    The only possible method to do this would be to measure the distances between
    the cities with the help of a meter tape. But what if you are only provided with
    the distances between the cities and their similarities instead of the city locations?
    You could still draw a map using logical assumptions and a wide knowledge of geometry.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你需要制作一张地图，其中提供了城市位置的列表。地图还应该展示两个城市之间的距离。唯一可能的方法是通过卷尺测量城市之间的距离。但如果你只得到城市之间的距离和它们的相似性，而不是城市的位置呢？你仍然可以通过逻辑假设和广泛的几何知识来绘制地图。
- en: Here, you are basically applying MDS to create a map. MDS observes the differences
    in the dataset and creates a map that calculates the original distances and tells
    you where they are located.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你基本上是在应用MDS来创建一个地图。MDS观察数据集中的差异，并创建一个地图，以计算原始距离并告诉你它们的位置。
- en: Isometric Mapping (Isomap)
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 等距映射（Isomap）
- en: It is a non-linear dimensionality reduction technique that is basically an extension
    of MDS or Kernel PCA. It reduces dimensionality by connecting every feature on
    the basis of their curved or geodesic distances between their nearest neighbors.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种非线性降维技术，基本上是MDS或Kernel PCA的扩展。它通过连接每个特征的曲线或测地距离来减少维度。
- en: Isomap is initiated by building a neighborhood network. Then, it uses graph
    distance to estimate the geodesic distance between every pair of points. Finally,
    the dataset is embedded in a lower dimension by decomposing the eigenvalues of
    the geodesic matrix. It can be specified how many neighbors to consider for each
    data point using the *n_neighbours* hyperparameter of the Isomap() class. This
    class implements the Isomap algorithm.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Isomap通过建立邻域网络来启动。然后，它使用图距离来估计每对点之间的测地距离。最后，通过分解测地矩阵的特征值，将数据集嵌入到较低维度中。可以使用*Isomap()*类的*n_neighbours*超参数指定每个数据点要考虑的邻居数量。该类实现了Isomap算法。
- en: What is the need for Dimensionality Reduction in Data Mining?
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据挖掘中为何需要降维？
- en: Data mining is the process of observing hidden patterns, relations, and anomalies
    within vast datasets in order to estimate outcomes. Vast datasets have many variables
    increasing at an exponential rate. Therefore, finding and analyzing patterns in
    them during the data mining process takes lots of resources and computational
    time. Hence, the dimensionality reduction technique can be applied while data
    mining to limit those data features by clubbing them and still be sufficient enough
    to represent the original dataset.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 数据挖掘是观察大规模数据集中的隐藏模式、关系和异常的过程，以估计结果。大规模数据集包含许多以指数速率增长的变量。因此，在数据挖掘过程中找到并分析这些模式需要大量的资源和计算时间。因此，可以在数据挖掘时应用降维技术，通过将数据特征合并来限制这些特征，同时仍然足够代表原始数据集。
- en: Advantages and Disadvantages of Dimensionality Reduction
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维的优缺点
- en: Advantages
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优势
- en: Storage space and the processing time are less
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储空间和处理时间减少
- en: Multi-collinearity of the dependent variables is removed
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 被解释变量的多重共线性已被消除
- en: Reduced chances of overfitting the model
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少模型过拟合的机会
- en: Data Visualization becomes easier
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化变得更容易
- en: Disadvantages
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 劣势
- en: Some amount of data is lost.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 会丢失一定量的数据。
- en: PCA cannot be applied where data cannot be defined through mean and covariance.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA无法应用于那些不能通过均值和协方差定义的数据。
- en: Not every variable needs to be linearly correlated, which PCA tends to find.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不是每个变量都需要线性相关，而PCA倾向于发现这种相关性。
- en: Labeled data is required for LDA to function, which is not available in a few
    cases.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDA需要标记数据才能运行，这在某些情况下不可用。
- en: Conclusion
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: A vast amount of data is generated every second. So, analyzing them with optimal
    use of resources and with accuracy is equally important. Dimensionality Reduction
    techniques help in data pre-processing in a precise and efficient manner—no wonder
    why it is considered a boon for data scientists.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 每秒生成大量数据。因此，利用资源进行准确分析同样重要。降维技术帮助以精确高效的方式进行数据预处理—这也是为什么它被认为是数据科学家的福音。
- en: '**[Kavika Roy](https://www.datatobiz.com/)** is Head of Information Management
    at [DataToBiz](https://www.datatobiz.com/). She is responsible for the identification,
    acquisition, distribution, and organization of technical oversight. Her strong
    attention to detail lets her deliver precise information regarding functional
    aspects to the right audience.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Kavika Roy](https://www.datatobiz.com/)** 是 [DataToBiz](https://www.datatobiz.com/)
    的信息管理部门负责人。她负责技术监督的识别、获取、分配和组织。她对细节的高度关注使她能够向正确的受众提供有关功能方面的精确信息。'
- en: More On This Topic
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[KDnuggets News, August 31: The Complete Data Science Study Roadmap…](https://www.kdnuggets.com/2022/n35.html)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，8月31日：完整的数据科学学习路线图…](https://www.kdnuggets.com/2022/n35.html)'
- en: '[Top AI and Data Science Tools and Techniques for 2022 and Beyond](https://www.kdnuggets.com/2022/03/nvidia-0317-top-ai-data-science-tools-techniques-2022-beyond.html)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2022年及以后顶级人工智能和数据科学工具与技术](https://www.kdnuggets.com/2022/03/nvidia-0317-top-ai-data-science-tools-techniques-2022-beyond.html)'
- en: '[A Beginner''s Guide to Anomaly Detection Techniques in Data Science](https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中的异常检测技术初学者指南](https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html)'
- en: '[The Role of Resampling Techniques in Data Science](https://www.kdnuggets.com/2023/02/role-resampling-techniques-data-science.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中重采样技术的作用](https://www.kdnuggets.com/2023/02/role-resampling-techniques-data-science.html)'
- en: '[Exploratory Data Analysis Techniques for Unstructured Data](https://www.kdnuggets.com/2023/05/exploratory-data-analysis-techniques-unstructured-data.html)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于非结构化数据的探索性数据分析技术](https://www.kdnuggets.com/2023/05/exploratory-data-analysis-techniques-unstructured-data.html)'
- en: '[7 Techniques to Handle Imbalanced Data](https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[处理不平衡数据的7种技术](https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html)'
