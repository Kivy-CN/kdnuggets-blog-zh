- en: 'Comparing Machine Learning Models: Statistical vs. Practical Significance'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/01/comparing-machine-learning-models-statistical-vs-practical-significance.html](https://www.kdnuggets.com/2019/01/comparing-machine-learning-models-statistical-vs-practical-significance.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Dina Jankovic](https://www.linkedin.com/in/dinajankovich/), Hubdoc**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/2171799f4f40ba65fd1868bfff774514.png)'
  prefs: []
  type: TYPE_IMG
- en: Left or right?A lot of work has been done on building and tuning ML models,
    but a natural question that eventually comes up after all that hard work is — how
    do we actually compare the models we’ve built? If we’re facing a choice between
    models A and B, which one is the winner and why? Could the models be combined
    together so that optimal performance is achieved?
  prefs: []
  type: TYPE_NORMAL
- en: A very shallow approach would be to compare the overall accuracy on the test
    set, say, model A’s accuracy is 94% vs. model B’s accuracy is 95%, and blindly
    conclude that B won the race. **In fact, there is so much more than the overall
    accuracy to investigate and more facts to consider.**
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, I’d love to share my recent findings on model comparison.
    I like using simple language when explaining statistics, so this post is a good
    read for those who are not so strong in statistics, but would love to learn a
    little more.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. “Understand” the Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If possible, it’s a really good idea to come up with some plots that can tell
    you right away what’s actually going on. It seems odd to do any plotting at this
    point, but plots can provide you with some insights that numbers just can’t.
  prefs: []
  type: TYPE_NORMAL
- en: In one of my projects, my goal was to compare the accuracy of 2 ML models on
    the same test set when predicting user’s tax on their documents, so I thought
    it’d be a good idea to aggregate the data by user’s id and compute the proportion
    of correctly predicted taxes for each model.
  prefs: []
  type: TYPE_NORMAL
- en: The data set I had was big (100K+ instances), so I broke down the analysis by
    region and focused on smaller subsets of data — the accuracy may differ from subset
    to subset. This is generally a good idea when dealing with ridiculously large
    data sets, simply because it is impossible to digest a huge amount of data at
    once, let alone come up with reliable conclusions (more about the *sample size
    issue* later). **A huge advantage of a Big Data set is that not only you have
    an insane amount of information available, but you can also zoom in the data and
    explore what’s going on on a certain subset of pixels.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01736de785863756acb538f363091260.png)'
  prefs: []
  type: TYPE_IMG
- en: 'subset 1: model A vs. model B scores![](../Images/b32ecfb8cdd351ad5875825b2811c276.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'subset 2: model A vs. model B scores![](../Images/fa791c786b30d8a38b160d371e99f53a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'subset 2: model A is clearly doing better than B… look at all those spikes!![](../Images/62da4f998c18a9cb3b711ce88c2d11d2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'subset 3: model A vs. model B scoresAt this point, I was suspicious that one
    of the models is doing better on some subsets, while they’re doing pretty much
    the same job on other subsets of data. **This is a huge step forward from just
    comparing the overall accuracy. **But this suspicion could be further investigated
    with **hypothesis testing**. Hypothesis tests can spot differences better than
    human eye — we have a limited amount of data in the test set, and we may be wondering
    how is the accuracy going to change if we compare the models on a different test
    set. Sadly, it’s not always possible to come up with a different test set, so
    knowing some statistics may be helpful to investigate the nature of model accuracies.'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Hypothesis Testing: Let’s do it right!'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It seems trivial at the first sight, and you’ve probably seen this before:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up H0 and H1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Come up with a test-statistic, and assume Normal distribution out of the blue
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Somehow calculate the p-value
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If p < alpha = 0.05 reject H0, and ta-dam you’re all done!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In practice, hypothesis testing is a little more complicated and sensitive.
    Sadly, people use it without much caution and misinterpret the results. Let’s
    do it together step by step!
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1. **We set up H0: **the** **null hypothesis** = no *statistically significant*
    difference between the 2 models and H1: **the** **alternative hypothesis = **there
    is a *statistically significant* difference between the accuracy of the 2 models — up
    to you: model A != B (two-tailed) or model A < or > model B (one-tailed)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2.** We come up with a **test-statistic** in such a way as to quantify,
    within the observed data, behaviours that would distinguish the null from the
    alternative hypothesis. There are many options, and even the best statisticians
    could be clueless about an X number of statistical tests — and that’s totally
    fine! There are way too many assumptions and facts to consider, so once you know
    your data, you can pick the right one. The point is to understand how hypothesis
    testing works, and the actual test-statistic is just a tool that is easy to calculate
    with a software.'
  prefs: []
  type: TYPE_NORMAL
- en: Beware that there is a bunch of assumptions that need to be met before applying
    any statistical test. For each test, you can look up the required assumptions;
    however, the vast majority of real life data is not going to strictly meet all
    conditions, so feel free to relax them a little bit! But what if your data, for
    example, seriously deviates from Normal distribution?
  prefs: []
  type: TYPE_NORMAL
- en: There are 2 big families of statistical tests: **parametric** and **non-parametric**tests,
    and I highly recommend reading a little more about them [here](https://keydifferences.com/difference-between-parametric-and-nonparametric-test.html).
    I’ll keep it short: **the major difference between the two is the fact that parametric
    tests require certain assumptions about the population distribution, while non-parametric
    tests are a bit more robust (*no parameters, please!*).**
  prefs: []
  type: TYPE_NORMAL
- en: In my analysis, I initially wanted to use the [paired samples t-test](https://www.statisticssolutions.com/manova-analysis-paired-sample-t-test/),
    but my data was clearly not normally distributed, so I went for the [Wilcoxon
    signed rank test](https://www.statisticssolutions.com/how-to-conduct-the-wilcox-sign-test/) (non-parametric
    equivalent of the paired samples t-test). It’s up to you to decide which test-statistic
    you’re going to use in your analysis, but always **make sure the assumptions are
    met**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4dcb62ee025aa9d5319183a4648f3e63.png)'
  prefs: []
  type: TYPE_IMG
- en: 'My data wasn’t normally distributed :(**Step 3.** Now the p-value. The concept
    of p-value is sort of abstract, and I bet many of you have used p-values before,
    but let’s clarify what a p-value actually is: a p-value is just a number that
    measures the evidence against H0: the stronger the evidence against H0, the smaller
    the p-value is. If your p-value is small enough, you have enough credit to reject
    H0.'
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, the p-value can be easily found in R/Python so you don’t need to torture
    yourself and do it manually, and although I’ve been mostly using Python, I prefer
    doing hypothesis testing in R since there are more options available. Below is
    a code snippet. We see that on subset 2, we indeed obtained a small p-value, but
    the confidence interval is useless.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4. **Very straightforward: if p-value < pre-specified alpha (0.05, traditionally),
    you can reject H0 in favour of H1\. Otherwise, there is not enough evidence to
    reject H0, **which does not mean that H0 is true!** In fact, it may still be false,
    but there was simply not enough evidence to reject it, based on the data. If alpha
    is 0.05 = 5%, that means there is only a 5% risk of concluding a difference exists
    when it actually doesn’t (aka **type 1 error**). You may be asking yourself: so
    why can’t we go for alpha = 1% instead of 5%? It’s because the analysis is going
    to be more conservative, so it is going to be harder to reject H0 (and we’re aiming
    to reject it).'
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly used alphas are 5%, 10% and 1%, but you can pick any alpha
    you’d like! It really depends on how much risk you’re willing to take.
  prefs: []
  type: TYPE_NORMAL
- en: Can alpha be 0% (i.e. no chance of type 1 error)? Nope :) In reality, there’s
    always a chance you’ll commit an error, so it doesn’t really make sense to pick
    0%. It’s always good to leave some room for errors.
  prefs: []
  type: TYPE_NORMAL
- en: If you wanna play around and *p-hack*, you may increase your alpha and reject
    H0, but then you have to settle for a lower level of confidence (as alpha increases,
    confidence level goes down — you can’t have it all :) ).
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Post-hoc Analysis: Statistical vs. Practical Significance'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you get a ridiculously small p-value, that certainly means that there is
    a*statistically significant* difference between the accuracy of the 2 models.
    Previously, I indeed got a small p-value, so mathematically speaking, the models
    differ for sure, but being “significant” does not imply being *important*. Does
    that difference actually mean anything? Is that small difference relevant to the
    business problem?
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical significance **refers to the unlikelihood that mean differences
    observed in the sample have occurred due to sampling error. Given a large enough
    sample, despite seemingly insignificant population differences, one might still
    find statistical significance. On the other side, **practical significance **looks
    at whether the difference is large enough to be of value in a practical sense.
    While statistical significance is strictly defined, practical significance is
    more intuitive and subjective.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d71a9ca7ca53abcaf1f4ea8ff31601f.png)'
  prefs: []
  type: TYPE_IMG
- en: At this point, you may have realized that p-values are not super powerful as
    you may think. There’s more to investigate. It’d be great to consider the **effect
    size** as well. The effect size measures the magnitude of the difference — if
    there is a statistically significant difference, we may be interested in its *magnitude*. **Effect
    size** emphasizes the size of the difference rather than confounding it with sample
    size.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: What is considered a small, medium, large effect size? The traditional cut-offs
    are 0.1, 0.3, 0.5 respectively, but again, this really depends on your business
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: '*And what’s the issue with the sample size? *Well, if your sample is too small,
    then your results are not going to be reliable, but that’s trivial. What if your
    sample size is too large? This seems awesome — but in that case even the ridiculously
    small differences could be detected with a hypothesis test. There’s so much data
    that even the tiny deviations could be perceived as significant. That’s why the
    effect size becomes useful.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more to do — we could try to find the power or the test and the optimal
    sample size. But we’re good for now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hypothesis testing could be really useful in model comparison if it’s done
    right. Setting up H0 & H1, calculating the test-statistic and finding the p-value
    is routine work, but interpreting the results require some intuition, creativity
    and deeper understanding of the business problem. Remember that if the testing
    is based on a very large test set, relationships found to be **statistically significant** may
    not have much **practical significance**. Don’t just blindly trust those magical
    p-values: zooming in the data and conducting a post-hoc analysis is always a good
    idea! :)'
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to reach out to me via [email](https://dinajankovic93@gmail.com/) or [LinkedIn](https://www.linkedin.com/in/dinajankovich/),
    I’m always up for a chat about Data Science!
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Dina Jankovic](https://www.linkedin.com/in/dinajankovich/)** is a Data
    Analyst in the Data Science team at Hubdoc in Toronto, Canada. She earned her
    BSc in Math from University of Belgrade and MSc in Statistics from University
    of Ottawa. She utilizes computational statistics and machine learning techniques
    to make smart business decisions and better understand the world.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/comparing-machine-learning-models-statistical-vs-practical-significance-de345c38b42a).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Understanding the Bias-Variance Tradeoff: An Overview](/2016/08/bias-variance-tradeoff-overview.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Compute the Statistical Significance of Two Classifiers Performance
    Difference](/2016/03/statistical-significance-two-classifiers-performance-difference.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Statistical Modeling: A Primer](/2017/03/statistical-modeling-primer.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[The Significance of Data Quality in Making a Successful Machine…](https://www.kdnuggets.com/2022/03/significance-data-quality-making-successful-machine-learning-model.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PyTorch or TensorFlow? Comparing popular Machine Learning frameworks](https://www.kdnuggets.com/2022/02/packt-pytorch-tensorflow-comparing-popular-machine-learning-frameworks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Feature Store Summit 2023: Practical Strategies for Deploying ML…](https://www.kdnuggets.com/2023/09/hopsworks-feature-store-summit-2023-practical-strategies-deploying-ml-models-production-environments)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Comparing Linear and Logistic Regression](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Comparing Natural Language Processing Techniques: RNNs, Transformers, BERT](https://www.kdnuggets.com/comparing-natural-language-processing-techniques-rnns-transformers-bert)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Practical Approach To Feature Engineering In Machine Learning](https://www.kdnuggets.com/2023/07/practical-approach-feature-engineering-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
