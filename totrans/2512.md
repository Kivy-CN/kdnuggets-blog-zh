# 在生产环境中服务ML模型：常见模式

> 原文：[https://www.kdnuggets.com/2021/10/serving-ml-models-production-common-patterns.html](https://www.kdnuggets.com/2021/10/serving-ml-models-production-common-patterns.html)

[评论](#comments)

**作者：[Simon Mo](https://www.anyscale.com/blog?author=simon-mo)、[Edward Oakes](https://www.anyscale.com/blog?author=edward-oakes) 和 [Michael Galarnyk](https://www.anyscale.com/blog?author=michael-galarnyk)**

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织在IT方面

* * *

本文基于Simon Mo在Ray Summit 2021上的“生产中的机器学习模式”[演讲](https://www.youtube.com/watch?v=mM4hJLelzSw)。

在过去的几年里，我们听取了来自不同领域的ML从业者的意见，以改进ML生产使用案例的工具。通过这次经历，我们观察到生产中的机器学习有四种常见模式：管道、集成、业务逻辑和在线学习。在ML服务领域，实施这些模式通常涉及开发简易性和生产就绪性的权衡。[Ray Serve](https://docs.ray.io/en/latest/serve/index.html) 旨在支持这些模式，通过易于开发和生产就绪的特性。它是一个可扩展和可编程的服务框架，建立在[Ray](https://www.ray.io/)之上，帮助你在生产环境中扩展微服务和ML模型。

本文讨论了：

+   什么是Ray Serve

+   Ray Serve在ML服务领域中的位置

+   生产中一些常见的ML模式

+   如何使用Ray Serve实现这些模式

## 什么是Ray Serve？

![Ray生态系统服务ML模型：常见模式](../Images/4bf023c751a82dcc888c87fd498e4ab9.png)

Ray Serve建立在Ray分布式计算平台之上，能够轻松扩展到许多机器，无论是在你的数据中心还是在云端。

Ray Serve是一个易于使用的可扩展模型服务库，建立在Ray之上。该库的一些优点包括：

+   可扩展性：横向扩展到数百个进程或机器，同时保持开销在个位数毫秒内

+   多模型组合：轻松组合多个模型，将模型服务与业务逻辑混合，并独立扩展组件，无需复杂的微服务。

+   [批处理](https://docs.ray.io/en/latest/serve/tutorials/batch.html)：原生支持请求批处理，以更好地利用硬件并提高吞吐量。

+   [FastAPI 集成](https://docs.ray.io/en/latest/serve/http-servehandle.html#serve-fastapi-http): 通过其简单、优雅的 API，轻松扩展现有的 FastAPI 服务器或为你的模型定义 HTTP 接口。

+   框架无关：使用单一工具包来服务从 [PyTorch](https://docs.ray.io/en/master/serve/tutorials/pytorch.html)、[Tensorflow 和 Keras](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html) 构建的深度学习模型，到 [Scikit-Learn 模型](https://docs.ray.io/en/latest/serve/tutorials/sklearn.html)，再到 [任意 Python 业务逻辑](https://www.anyscale.com/blog/how-ikigai-labs-serves-interactive-ai-workflows-at-scale-using-ray-serve)。

你可以通过查看 [Ray Serve 快速入门](https://docs.ray.io/en/latest/serve/index.html) 来开始使用 Ray Serve。

## Ray Serve 在 ML 服务领域中的定位

![Ray Serve 的定位](../Images/b4eed5d580a6125b7245a30168ed04ba.png)

上图显示，在 ML 服务领域，开发的便利性与生产准备性的平衡通常是一个权衡。

**Web 框架**

要部署 ML 服务，人们通常从像 Flask 或 FastAPI 这样的开箱即用的简单系统开始。然而，即便它们可以很好地提供单次预测并在概念验证中表现良好，它们通常无法实现高性能，并且扩展往往成本高昂。

**自定义工具**

如果 Web 框架失败，团队通常会过渡到某种自定义工具，通过将几个工具粘合在一起来使系统准备好投入生产。然而，这些自定义工具通常难以开发、部署和管理。

**专用系统**

有一类专门用于在生产环境中部署和管理 ML 模型的系统。虽然这些系统在管理和服务 ML 模型方面表现出色，但它们通常不如 Web 框架灵活，并且学习曲线较高。

**Ray Serve**

Ray Serve 是一个专门用于 ML 模型服务的 Web 框架。它致力于易于使用、易于部署，并且具备生产就绪性。

### Ray Serve 有何不同？

![许多工具运行 1 个模型很好](../Images/4b40ac8b5d230e62b6a366aa3861b83a.png)

训练和服务一个模型的工具有很多。这些工具能够很好地运行和部署单个模型。问题在于，现实中的机器学习通常不那么简单。在生产环境中，你可能会遇到以下问题：

+   处理基础设施以超越一个模型的副本进行扩展。

+   需要处理复杂的 YAML 配置文件，学习自定义工具，并发展 MLOps 专业知识。

+   遇到可扩展性或性能问题，无法满足业务 SLA 目标。

+   许多工具成本高昂，且经常导致资源利用不充分。

扩展单一模型本身就足够困难。对于许多生产环境中的机器学习案例，我们观察到复杂的工作负载需要将许多不同的模型组合在一起。Ray Serve 本身就是为了这种涉及多个节点的多个模型的用例而构建的。你可以查看 [this part of the talk](https://youtu.be/mM4hJLelzSw?t=651)，其中我们深入探讨了Ray Serve的架构组件。

## 生产环境中的机器学习模型模式

![13PatternsMLProduction](../Images/5373302cc6c845b5e4c850313e950697.png)

生产环境中许多机器学习应用遵循四种模型模式：

+   管道

+   集成

+   业务逻辑

+   在线学习

本节将描述这些模式中的每一种，展示它们的使用方式，讲解现有工具如何实现它们，并展示Ray Serve如何解决这些挑战。

### 管道模式

![图](../Images/54548db95bd65dc4919d3abfdede19df.png)

一个典型的计算机视觉管道

上图展示了一个典型的计算机视觉管道，该管道使用多个深度学习模型对图片中的物体进行标注。这个管道包括以下步骤：

1) 原始图像经过常见的预处理，如图像解码、增强和裁剪。

2) 使用检测分类模型来识别边界框和类别。它是一只猫。

3) 图像被传递到一个关键点检测模型，以识别物体的姿势。对于猫的图像，模型可以识别出像爪子、脖子和头部这样的关键点。

4) 最后，一个NLP合成模型生成图片显示的类别。在这种情况下，是一只站立的猫。

一个典型的管道很少仅由一个模型组成。为了应对现实问题，机器学习应用通常使用许多不同的模型来执行即使是简单的任务。通常，管道将特定任务分解为多个步骤，每个步骤由机器学习算法或某些过程完成。现在，让我们回顾几个你可能已经熟悉的管道。

**Scikit-Learn 管道**

`Pipeline([(‘scaler’, StandardScaler()), (‘svc’, SVC())])`

[scikit-learn 的管道](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) 可以用来将多个“模型”和“处理对象”组合在一起。

**推荐系统**

`[EmbeddingLookup(), FeatureInteraction(), NearestNeighbors(), Ranking()]`

推荐系统中存在常见的管道模式。像你在 [Amazon](https://www.amazon.science/publications/two-decades-of-recommender-systems-at-amazon-com) 和 [YouTube](https://research.google/pubs/pub45530/) 上看到的物品和视频推荐，通常会经过多个阶段，如嵌入查找、特征交互、最近邻模型和排名模型。

**常见的预处理**

`[HeavyWeightMLMegaModel(), DecisionTree()/BoostingModel()]`

有一些非常常见的用例，其中一些大型 ML 模型用于处理文本或图像的常见处理任务。例如，在 Facebook，[FAIR](https://ai.facebook.com/) 的一组 ML 研究人员创建了用于视觉和文本的最先进的重型模型。然后不同的产品组创建下游模型以解决他们的业务用例（例如，[自杀预防](https://ai.facebook.com/blog/under-the-hood-suicide-prevention-tools-powered-by-ai/)），通过使用随机森林实现较小的模型。共享的常见预处理步骤通常会被实现为 [特征存储管道](https://www.tecton.ai/blog/what-is-a-feature-store/)。

### 一般管道实现选项

![17PipelineImplementation](../Images/79bbb459ea0883039df1a9aef463ae8a.png)

在 Ray Serve 之前，实现管道通常意味着你必须在将模型封装在 web 服务器中或使用许多专用微服务之间做出选择。

通常，实现管道有两种方法：将模型封装在 web 服务器中或使用许多专用微服务。

**将模型封装在 web 服务器中**

上图的左侧展示了在 web 处理路径中以 for 循环方式运行的模型。每当有请求进来时，模型会被加载（它们也可以被缓存）并通过管道运行。虽然这种方法简单易实现，但一个主要缺陷是它难以扩展，并且由于每个请求都是按顺序处理的，因此性能不佳。

**许多专用微服务**

上图的右侧展示了许多专用微服务，其中你基本上为每个模型构建和部署一个微服务。这些微服务可以是本地的 ML 平台、[Kubeflow](https://www.kubeflow.org/)，甚至是像 AWS [SageMaker](https://aws.amazon.com/sagemaker/) 这样的托管服务。然而，随着模型数量的增加，复杂性和运营成本也会急剧上升。

### 在 Ray Serve 中实现管道

```py
@serve.deployment
class Featurizer: …

@serve.deployment
class Predictor: …

@serve.deployment
class Orchestrator
   def __init__(self):
      self.featurizer = Featurizer.get_handle()
      self.predictor = Predictor.get_handle()

   async def __call__(self, inp):
      feat = await self.featurizer.remote(inp)
      predicted = await self.predictor.remote(feat)
      return predicted

if __name__ == “__main__”:
    Featurizer.deploy()
    Predictor.deploy()
    Orchestrator.deploy()
```

伪代码显示了 Ray Serve 如何允许部署调用其他部署

在 Ray Serve 中，你可以在你的部署中直接调用其他部署。在上面的代码中，有三个部署。`Featurizer` 和 `Predictor` 只是包含模型的常规部署。`Orchestrator` 接收 web 输入，通过特征提取器句柄将其传递给特征提取器进程，然后将计算出的特征传递给预测器进程。接口只是 Python，你不需要学习任何新的框架或领域特定语言。

Ray Serve 通过一种叫做 ServeHandle 的机制实现这一点，它使你能够将所有内容嵌入 web 服务器，而不牺牲性能或可扩展性。它允许你直接调用其他在不同节点上运行的进程中的部署。这允许你单独扩展每个部署，并在副本之间进行负载均衡。

如果你想深入了解其工作原理， [查看Simon Mo的演讲部分](https://youtu.be/mM4hJLelzSw?t=650) 了解Ray Serve的架构。如果你想了解生产中的计算机视觉管道示例，[查看Robovision如何使用5个机器学习模型进行车辆检测](https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures#:~:text=A%20Tool%20for%20High%20Performance%20ML%20Systems)。

## 集成模式

![集成模式](../Images/16434d4cd2a985d8583f19809d92c256.png)

在许多生产使用案例中，管道是合适的。然而，管道的一个限制是，对于给定的下游模型，通常会有很多上游模型。这就是集成模型有用的地方。

![图示](../Images/3322debd6651907f43438df2050ec8ac.png)

集成使用案例

集成模式涉及将一个或多个模型的输出混合。在某些情况下，它们也被称为模型堆叠。以下是集成模式的三种使用案例。

**模型更新**

新模型会随着时间的推移进行开发和训练。这意味着在生产环境中总会有模型的新版本。问题是，如何确保新模型在实时在线流量场景中有效且性能优越？一种方法是将部分流量通过新模型。你仍然从已知良好的模型中选择输出，但你也在收集新版本模型的实时输出以进行验证。

**聚合**

最广为人知的使用案例是聚合。对于回归模型，多个模型的输出会被平均。对于分类模型，输出将是多个模型输出的投票版本。例如，如果两个模型投票给猫，一个模型投票给狗，则聚合输出将是猫。聚合有助于减少个别模型的错误，并通常使输出更准确和“安全”。

**动态选择**

集成模型的另一个使用案例是根据输入属性动态执行模型选择。例如，如果输入包含猫，模型A将被使用，因为它专门用于猫。如果输入包含狗，则会使用模型B，因为它专门用于狗。请注意，这种动态选择不一定意味着管道本身必须是静态的。它也可以根据用户反馈选择模型。

### 一般集成实现选项

![一般集成实现](../Images/1749b4e53b9e59fc2b20a77e30ca3589.png)

在Ray Serve之前，实现集成通常意味着你必须在将模型包装在网络服务器中或使用许多专门的微服务之间做出选择。

集成方法面临与管道相同的问题。将模型包装在网络服务器中很简单，但性能不佳。当使用专门的微服务时，随着微服务数量与模型数量的增长，你会遇到大量的操作开销。

![图](../Images/cef1f239a6ce6e2c866380f7578463f6.png)

来自[2020 Anyscale 演示](https://youtu.be/8GTd8Y_JGTQ)的集成示例

使用 Ray Serve，这种模式非常简单。你可以查看[2020 Anyscale 演示](https://youtu.be/8GTd8Y_JGTQ)，了解如何利用 Ray Serve 的处理机制执行动态模型选择。

使用 Ray Serve 进行集成的另一个例子是 Wildlife Studios 将多个分类器的输出合并为一个预测。你可以查看他们如何通过[Ray Serve 以 3 倍的速度提供游戏内优惠](https://www.anyscale.com/blog/wildlife-studios-serves-in-game-offers-3x-faster-at-1-10th-the-cost-with-ray)。

## 业务逻辑模式

将机器学习投入生产总是涉及业务逻辑。没有模型能够独立运行并自行处理请求。业务逻辑模式涉及在常见的机器学习任务中，除了机器学习模型推理之外的一切内容。这包括：

+   用于关系记录的数据库查找

+   Web API 调用外部服务

+   用于预计算特征向量的特征存储查找

+   特征转换，如数据验证、编码和解码。

### 一般业务逻辑实现选项

![一般业务逻辑实现选项](../Images/0563870ba1f93676e64c2abf3153cea3.png)

上述 Web 处理器的伪代码完成了以下任务：

1.  加载模型（比如从 S3）

1.  验证来自数据库的输入

1.  从特征存储中查找一些预计算的特征。

只有在 Web 处理器完成这些业务逻辑步骤后，输入才会传递到机器学习模型中。问题在于模型推理和业务逻辑的要求导致服务器*既受网络限制又受计算限制*。这是因为模型加载步骤、数据库查找和特征存储查找受网络限制且 I/O 负荷重，而模型推理受计算限制且内存需求大。这些因素的组合导致资源利用效率低下。扩展将非常昂贵。

![图](../Images/0f770ce2d44b93326cd3be5d2f05d86d.png)

Web 处理器方法（左）和微服务方法（右）

增加利用率的一个常见方法是将模型拆分到模型服务器或微服务中。

Web 应用完全是网络限制，而模型服务器则是计算限制。然而，一个常见的问题是两者之间的接口。如果你在模型服务器中放入过多的业务逻辑，那么模型服务器就会变成网络限制和计算限制调用的混合体。

如果你让模型服务器仅作为纯模型服务器，那么你会遇到“**张量输入，张量输出**”接口问题。模型服务器的输入类型通常仅限于张量或某种替代形式。这使得将预处理、后处理和业务逻辑与模型本身保持同步变得困难。

在训练过程中，由于处理逻辑和模型紧密耦合，因此很难推理处理逻辑与模型之间的交互，但在服务时，它们分布在两个服务器和两个实现中。

无论是 Web 处理程序方法还是微服务方法，都不令人满意。

### 在 Ray Serve 中实现业务逻辑

![图示](../Images/d89c6e5d4fea7fa9ed2e67228e990351.png)

Ray Serve 中的业务逻辑

使用 Ray Serve，你只需对旧的 Web 服务器做一些简单的更改，就能缓解上述问题。你可以检索一个包装了模型的 ServeHandle，而不是直接加载模型，并将计算任务转移到另一个部署中。所有数据类型都得以保留，不需要编写“tensor-in, tensor-out” API 调用——你可以直接传入常规 Python 类型。此外，模型部署类可以保留在同一个文件中，并与预测处理程序一起部署。这使得理解和调试代码变得更加容易。`model.remote` 看起来就像一个函数，你可以轻松追踪到模型部署类。

通过这种方式，Ray Serve 帮助你将业务逻辑和推断分离成两个独立的组件，一个 I/O 密集型，另一个计算密集型。这使得你可以单独扩展每个部分，而不会丧失部署的便利性。此外，由于 `model.remote` 只是一个函数调用，它比独立的外部服务更容易测试和调试。

### Ray Serve FastAPI 集成

![图示](../Images/8595ad1b26ebf6e48945fe49e0c55bab.png)

Ray Serve: 使用 FastAPI 进行 Ingress

实现业务逻辑和其他模式的重要部分是身份验证和输入验证。[Ray Serve 本地集成 FastAPI](https://docs.ray.io/en/latest/serve/http-servehandle.html#serve-fastapi-http)，这是一种类型安全且符合人体工程学的 Web 框架。[FastAPI](https://fastapi.tiangolo.com/) 具有自动依赖注入、类型检查和验证以及 OpenAPI 文档生成等功能。

使用 Ray Serve，你可以直接将 FastAPI 应用对象传递给 `@serve.ingress`。这个装饰器确保所有现有的 FastAPI 路由仍然有效，并且你可以通过部署类附加新的路由，这样加载的模型和网络数据库连接等状态就能轻松管理。从架构上讲，我们只是确保你的 FastAPI 应用正确嵌入到副本演员中，并且 FastAPI 应用可以在多个 Ray 节点上扩展。

## 在线学习

在线学习是一种新兴的模式，已变得越来越广泛使用。它指的是一个在生产环境中运行的模型，该模型不断更新、训练、验证和部署。以下是在线学习模式的三个使用案例。

**动态学习模型权重**

有一些用例涉及在线动态学习模型权重。当用户与您的服务交互时，这些更新的模型权重可以有助于为每个用户或群体提供个性化的模型。

![图](../Images/f7f04df0e61fb914656ea0ffd641fc7f.png)

[蚂蚁集团在线学习示例](https://www.anyscale.com/blog/online-resource-allocation-with-ray-at-ant-group)（图片由蚂蚁集团提供）

在线学习的一个案例研究涉及蚂蚁集团的在线资源分配业务解决方案。该模型从离线数据中训练，然后与实时流数据源结合，并且服务于实时流量。需要注意的是，在线学习系统比其静态服务对手复杂得多。在这种情况下，将模型放在网络服务器上，甚至拆分成多个微服务，都不会有助于实现。

**动态学习参数以协调模型**

还有一些用例涉及学习参数以协调或组合模型，例如，[学习用户偏好的模型](https://dl.acm.org/doi/fullHtml/10.1145/3383313.3411550)。这通常体现在模型选择场景或上下文赌博算法中。

**强化学习**

强化学习是机器学习的一个分支，训练代理与环境进行互动。环境可以是物理世界或模拟环境。您可以在[这里](https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google)了解强化学习，并在[这里](https://docs.ray.io/en/latest/serve/tutorials/rllib.html#serve-rllib-tutorial)查看如何使用 Ray Serve 部署 RL 模型。

## 结论

![图](../Images/2f8011f22aa2376f8e75a951ce194e89.png)

Ray Serve 易于开发且准备好投入生产。

本文介绍了生产环境中机器学习的4种主要模式，Ray Serve 如何原生扩展并与复杂架构协同工作，以及生产中的机器学习通常意味着多个模型的生产。Ray Serve 在分布式运行时 Ray 之上构建，考虑了所有这些因素。如果您有兴趣了解更多关于 Ray 的信息，您可以查看[文档](https://ray.io/)、加入我们的[讨论](https://discuss.ray.io/)以及查看[白皮书](https://tinyurl.com/ray-white-paper)! 如果您有兴趣与我们合作以简化 Ray 的使用，我们正在[招聘](https://jobs.lever.co/anyscale)!

[原文](https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns)。转载已获许可。

**相关：**

+   [使用 PyTorch 和 Ray 开始分布式机器学习](/2021/03/getting-started-distributed-machine-learning-pytorch-ray.html)

+   [如何加速 Scikit-Learn 模型训练](/2021/02/speed-up-scikit-learn-model-training.html)

+   [如何构建数据科学投资组合](/2018/07/build-data-science-portfolio.html)

### 更多相关主题

+   [顶级 7 大模型部署和服务工具](https://www.kdnuggets.com/top-7-model-deployment-and-serving-tools)

+   [优先考虑数据科学模型的生产](https://www.kdnuggets.com/2022/04/prioritizing-data-science-models-production.html)

+   [2023 年特性存储峰会：机器学习模型生产环境部署的实用策略](https://www.kdnuggets.com/2023/09/hopsworks-feature-store-summit-2023-practical-strategies-deploying-ml-models-production-environments)

+   [机器学习中的设计模式与 MLOps](https://www.kdnuggets.com/2022/02/design-patterns-machine-learning-mlops.html)

+   [揭示隐藏模式：层次聚类简介](https://www.kdnuggets.com/unveiling-hidden-patterns-an-introduction-to-hierarchical-clustering)

+   [将机器学习算法完整端到端部署到生产环境中](https://www.kdnuggets.com/2021/12/deployment-machine-learning-algorithm-live-production-environment.html)
