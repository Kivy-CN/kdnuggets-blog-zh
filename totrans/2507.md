# 开始使用 PyTorch Lightning

> 原文：[https://www.kdnuggets.com/2021/10/getting-started-pytorch-lightning.html](https://www.kdnuggets.com/2021/10/getting-started-pytorch-lightning.html)

[评论](#comments)

![开始使用 PyTorch Lightning](../Images/d90c82e8ba6cac010727d13ce27b16a8.png)

### **开始使用 PyTorch Lightning：一个高性能研究的高级库**

像 TensorFlow 和 PyTorch 这样的库处理了大部分构建深度学习模型的复杂细节，这些模型训练和推理速度快。可以预见，这使得机器学习工程师将大部分时间花在更高抽象层级的工作上，如运行超参数搜索、验证性能，以及对模型和实验进行版本控制以跟踪一切。

深度学习不仅仅是将一些层粘在一起。

如果 PyTorch 和 TensorFlow（现在还有 JAX）是深度学习的“蛋糕”，那么高级库就是“糖霜”。多年来，TensorFlow 一直拥有其“糖霜”——高级 Keras API，这在 2019 年 TF 2.0 发布时成为 TensorFlow 本身的一部分。类似地，PyTorch 用户也受益于高级的 fastai 库，这对于高效性和迁移学习非常合适，使 fastai 成为 Kaggle 比赛平台上成功数据科学家的最爱。最近，另一个针对 PyTorch 的简化封装在名为 [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) 的库中迅速获得了关注。

PyTorch Lightning 实际上自 2019 年以来就存在了，至少在某种程度上。它起初是威廉·法尔孔（William Falcon）在纽约大学博士研究期间的一个副项目。到 2020 年（我们指的是从三月开始的 2020 年）时，PyTorch Lightning 不再只是一个个人项目，因为法尔孔[宣布了风险投资](https://medium.com/pytorch/pytorch-lightning-0-7-1-release-and-venture-funding-dd12b2e75fb3)。与此同时，开源（根据 Apache 2.0 许可）代码库也从法尔孔的个人 GitHub 账户迁移到了自己的专用账户。截至目前，PyTorch Lightning 已经成长为超过 15,000 个星标和近 2,000 个分支，几乎与[fastai](https://github.com/fastai/fastai)（拥有超过 21,000 个星标）一样受欢迎，明显比 PyTorch 的内部高级库[Ignite](https://github.com/pytorch/ignite)（拥有约 4,000 个星标）更受欢迎！

fastai 旨在方便首个 fastai 课程，[Practical Deep Learning for Coders](https://course.fast.ai/)，而 PyTorch Lightning 则旨在简化生产研究。Fastai 专注于迁移学习和效率，其易用性使其成为 Kaggle 数据科学竞赛平台上流行的高级库，已有超过[4,500 个笔记本](https://www.kaggle.com/search?q=fastai)引用该库。相比之下，PyTorch Ignite 只有[100 多个](https://www.kaggle.com/search?q=ignite)笔记本结果，而 PyTorch Lightning 约有[500 个](https://www.kaggle.com/search?q=PyTorch+Lightning)。PyTorch Lightning 是一个相对较新的库，但它也面向不同的受众。PyTorch Lightning 简化了开发新模型的工程方面，例如日志记录、验证和钩子，它的目标是机器学习研究人员。

研究就是回答和证伪问题，在本教程中，我们将探讨 PyTorch Lightning 能为我们做些什么以简化这一过程。我们将设置一个简单的模拟研究问题：使用一种“华丽”的激活函数（例如所谓的 [swish function](https://en.wikipedia.org/wiki/Swish_function)）是否比使用更标准的修正线性单元（[ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))）具有优势。我们将使用 SciKit-Learn 的极小（无论是样本数量还是图像大小）数字数据集来设置我们的实验。从数字开始应该使这个项目对在高效笔记本上运行代码的人来说较为可及，但鼓励读者使用像 CIFAR10 这样的更实际的图像数据集以获得额外积分。

作为一个旨在生产研究的库，PyTorch Lightning 也简化了硬件支持和分布式训练，我们将在最后展示将训练转移到 GPU 的简便性。

### ****开始使用：安装 PyTorch Lightning****

像许多现代 Python 项目一样，PyTorch Lightning 可以通过 pip 轻松安装，我们建议使用你喜欢的虚拟环境管理工具来管理安装和依赖项，而不会使你的基础 Python 安装变得杂乱。我们将提供三个示例，第一个是使用 `virtualenv` 和 pip，我们假设你正在使用 Linux 或 Mac 上的 Unix 风格命令行，或者你足够熟练，可以使用类似 Git Bash 或 Anaconda Prompt 的工具来适应 Windows。导航到本教程的项目文件夹后：

```py
virtualenv ptl_env --python=python3
source ptl_env/bin/activate
pip install pytorch-lightning
pip install torchvision
pip install scikit-learn
```

你也可以使用 [Anaconda](https://www.anaconda.com/) 来管理你的虚拟环境：

```py
conda create -n ptl_env
conda activate ptl_env
conda install -n ptl_env pytorch-lighnting -c conda-forge
conda install -n ptl_env torchvision 
conda install -n ptl_env scikit-learn
```

或者甚至将两者结合起来，创建一个新的 Anaconda 环境，然后使用 pip 安装包。对于更一般的使用，有一些[警告](https://www.anaconda.com/blog/using-pip-in-a-conda-environment)涉及到 pip 和 Anaconda 的结合使用，但对于本教程的目的应该没问题：

```py
conda create -n ptl_env
conda activate ptl_env
conda install -n ptl_env pip
pip install pytorch-lightning
pip install torchvision
pip install scikit-learn
```

### 使用 PyTorch Lightning

PyTorch Lightning 所采用的设计策略围绕 LightningModule 类展开。该类本身继承自 `pytorch.nn.Module` 类，提供了一个方便的入口，并尝试将尽可能多的训练和验证过程组织在一个地方。

该策略的一个关键特性是，典型的训练和验证循环的内容被定义在模型本身中，可以通过类似于 keras、fastai 或 SciKit-Learn 的 `fit` API 访问。与其他示例中通过模型本身访问 `fit` 不同，在 PyTorch Lightning 中，`fit` 是通过 Trainer 对象访问的。但这还要再往后说，首先让我们通过导入我们需要的所有东西来为实验做准备。

```py
import os
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms
from torch.utils.data import DataLoader, random_split
from torchvision.datasets import MNIST

# for rapid prototyping with a small dataset
import sklearn
import sklearn.metrics
import sklearn.datasets
# for building intuition with a few tens of thousands of samples
from torchvision.datasets import MNIST

import pytorch_lightning as pl
from pytorch_lightning.metrics import functional as FM
```

然后我们可以继续定义我们的模型：

```py
class MyClassifier(pl.LightningModule):

    def __init__(self, dim=28, activation=nn.ReLU()):

        super(MyClassifier, self).__init__()

        self.image_dim = dim
        self.hid_dim = 128
        self.num_classes = 10
        self.act = activation

        self.feature_extractor = nn.Sequential(\
                nn.Conv2d(1, 4, 3, padding=1), \
                self.act, \
                nn.Conv2d(4, 4, 3, padding=1), \
                self.act, \
                nn.Conv2d(4, 1, 3, padding=1), \
                self.act, \
                nn.Flatten())

        self.head = nn.Sequential(\
                nn.Linear(self.image_dim**2, self.hid_dim), \
                self.act, \
                nn.Linear(self.hid_dim, self.hid_dim), \
                self.act, \
                nn.Linear(self.hid_dim, self.num_classes))

    def forward(self, x):

        x = self.feature_extractor(x)
        output = self.head(x)

        return output

    def training_step(self, batch, batch_index):

        x, y = batch

        output = self.forward(x)

        loss = F.nll_loss(F.log_softmax(output, dim = -1), y)

        y_pred = output.argmax(-1).cpu().numpy()
        y_tgt = y.cpu().numpy()
        accuracy = sklearn.metrics.accuracy_score(y_tgt, y_pred)
        self.log("train loss", loss)
        self.log("train accuracy", accuracy)
        return loss

    def validation_step(self, batch, batch_idx):

        x, y = batch

        output = self.forward(x)

        loss = F.cross_entropy(output, y)

        pred = output.argmax(-1)

        return output, pred, y

    def validation_epoch_end(self, validation_step_outputs):

        losses = 0
        outputs = None
        preds = None
        tgts = None
        for output, pred, tgt in validation_step_outputs:
        preds = torch.cat([preds, pred]) if preds is not None else pred
        outputs = torch.cat([outputs, output], dim = 0) \
        if outputs is not None else output
        tgts = torch.cat([tgts, tgt]) if tgts is not None else tgt

        loss = F.nll_loss(F.log_softmax(outputs, dim = -1), tgts)

        y_preds = preds.cpu().numpy()
        y_tgts = tgts.cpu().numpy()

        fm_accuracy = FM.accuracy(outputs, tgts)

        # pytorch lightning prints a deprecation warning for FM.accuracy,
        # so we'll include sklearn.metrics.accuracy_score as an alternative
        accuracy = sklearn.metrics.accuracy_score(y_tgts, y_preds)

        self.log("val_accuracy", accuracy)
        self.log("val_loss", loss)

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=3e-4)
```

值得注意的是，训练功能被委托给 `training_step` 函数中的模块本身。大多数有一些 PyTorch 实践经验的机器学习从业者已经对重载 `forward` 函数相当熟悉，而 LightningModule 对象还有许多其他方法可以重载，以实现对内置的相对轻松的日志记录和评估功能的精细控制。

定义我们 `MyClassifier` 模型类的代码可能看起来比较冗长，但这种策略在实际开始训练时大大简化了操作，我们稍后会看到。`LightningModule` 类中包含了许多其他的回调和函数，所有这些都可以被重载以实现更精细的控制。完整的回调列表可以在[PyTorch Lightning 文档中找到](https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html#hooks)。

在本教程中，我们还将定义一个`torch.utils.data.Dataset`对象，以封装来自 SciKit-Learn 的数字数据集。这应该能让我们在切换到更大且信息量更丰富的数据集（如 MNIST 或 CIFAR10）之前，快速使一切正常运行。

```py
class SKDigitsDataset(torch.utils.data.Dataset):

    def __init__(self, mode="train"):
        super(SKDigitsDataset, self).__init__()
        x, y = sklearn.datasets.load_digits(return_X_y = True)

        num_samples = int(x.shape[0] * 0.8)
        np.random.seed(42)
        np.random.shuffle(x)
        np.random.seed(42)
        np.random.shuffle(y)

        if mode == "train":
        self.x = x[:num_samples]
        self.y = y[:num_samples]
        elif mode == "val":
        self.x = x[num_samples:]
        self.y = y[num_samples:]
        else:
        self.x = x
        self.y = y

        self.transform = lambda my_dict: \
        (torch.tensor(my_dict["x"]).float(), \
        torch.tensor(my_dict["y"]).long())

    def __len__(self):
        return self.x.shape[0]

    def __getitem__(self, index):

        got_x = self.x[index].reshape(-1, 8, 8)
        got_y = self.y[index]

        sample = {"x": got_x, "y": got_y}

        sample = self.transform(sample)

        return sample
```

处理完这些之后，实际启动训练过程变得极其简单。我们只需创建一个数据集并将其输入到`DataLoader`中，实例化我们的模型，创建一个 PyTorch Lightning `Trainer` 对象，然后调用训练器的 fit 方法。以下是一个简化版本：

```py
dataset = SKDigitsDataset()
dataloader = DataLoader(dataset)
model = MyClassifier(dim=8)
trainer = pl.Trainer()
trainer.fit(model, dataloader)
```

当然，我们还希望在整个训练过程中持续记录验证指标，利用我们在模型中重载的 `validation_step` 和 `validation_epoch_end` 方法。以下是我用来启动训练过程的实际代码，使用了 `if __name__ == "__main__":` 模式，为以模块形式运行 Python 文件提供了简单的入口点。

```py
if __name__ == "__main__":
    # if using digits from sklearn

    train_dataset = SKDigitsDataset(mode = "train")
    val_dataset = SKDigitsDataset(mode = "val")

    dim = 8
    validation_interval = 1.0

    train_dataloader = DataLoader(train_dataset)
    val_dataloader = DataLoader(val_dataset)

    model = MyClassifier(dim=dim, activation=nn.ReLU())
    trainer = pl.Trainer(max_epochs = 100, \
    val_check_interval = validation_interval)

    trainer.fit(model, train_dataloader, val_dataloader)

    print("Training finished, all ok")
```

当你运行上述代码时，你应该会在终端中看到一个进度条，类似于下面的样子。

![pl-terminal-1.png](../Images/28210cdaae631256933e335d905b6fb6.png)

在允许训练运行一段时间后，查看你的工作目录，你会注意到一个名为**lightning_logs**的新文件夹。这是 PyTorch Lightning 记录训练会话的地方，你可以快速启动 Tensorboard 会话来查看进展。在使用下面的命令启动 tensorboard 后，使用浏览器导航到 localhost:6006（默认）以打开仪表板。

```py
tensorboard --logdir=lightning_logs
```

如果你花了一些时间才使训练顺利进行，你会注意到左侧边栏中显示了一个训练运行的列表，其中有 version_0、version_1、version_2 等等。PyTorch Lightning 自动对你的训练运行进行版本控制，因此比较几种不同的实验条件或随机种子应该是非常容易的。

例如，如果我们想要运行一个小实验来比较使用 Swish 和 ReLU 激活函数的效果，我们可以使用下面的代码。

```py
if __name__ == "__main__":

    if(1):
        # if using digits from sklearn

        train_dataset = SKDigitsDataset(mode = "train")
        val_dataset = SKDigitsDataset(mode = "val")

        dim = 8
        validation_interval = 1.0

    else:
        # if using MNIST
        train_dataset = MNIST(os.getcwd(), download=True, \
                train=True, transform=transforms.ToTensor())
        val_dataset = MNIST(os.getcwd(), download=True, \
                train=False, transform=transforms.ToTensor())

        dim = 28
        validation_interval = 0.1

    train_dataloader = DataLoader(train_dataset)
    val_dataloader = DataLoader(val_dataset)
class Swish(nn.Module):

    def __init__(self):
        super(Swish, self).__init__()

    def forward(self, x):
        return x * torch.sigmoid(x)

for replicate in range(3):
    for activation in [Swish(), nn.ReLU()]:

        model = MyClassifier(dim=dim, activation=activation)

        trainer = pl.Trainer(max_epochs = 100, \
        val_check_interval = validation_interval)
        trainer.fit(model, train_dataloader, val_dataloader)

        print(f" round {replicate} finished.")
```

在运行我们的实验后，我们会发现结果被很好地记录在 Tensorboard 中，供我们查阅。

![pl-tensorboard-1.png](../Images/60cae6ec1ddb4e9612d2f76104311e20.png)

你可能会注意到我们有选项在更大的 MNIST 数据集上进行训练。MNIST 数据集包含 60,000 个 28 x 28 像素的图像训练样本，比起提供不到 2,000 个 8 x 8 图像样本的微型 sklearn digits 数据集，它更接近于一个有用的现实世界数据集。然而，你可能不想在性能不足的笔记本电脑 CPU 上运行 6 次重复训练，因此我们首先要将所有内容转移到 GPU 上。

如果你已经习惯从零开始在标准 PyTorch 中构建实验和训练管道，你可能知道被遗忘的张量滞留在 CPU 设备上的挫败感，以及它们产生的致命错误。这通常很容易解决，但依然令人沮丧。

### **使用 GPU 进行训练**

如果你正在使用一台可用 GPU 的机器，你可以轻松地使用它进行训练。为了在 GPU 上启动训练而不是 CPU，我们需要修改一些代码：

```py
trainer = pl.Trainer(max_epochs = 100, \
                val_check_interval = validation_interval, \ 
                gpus=[0])
```

没错，通过修改定义训练器对象的一行代码，我们可以在 GPU 上运行训练。不用担心被遗忘的张量，并且拥有我们在原始模型中构建的日志记录和验证的所有便利。

![pl-tensorboard-2.png](../Images/040ed16ce6762f52159139f7c5ae3211.png)

### ****下一步****

使用 PyTorch Lightning 的一个显著特点是，随着你不断深入，它似乎变得越来越容易。定义我们的`MyClassifer`模型比从`torch.nn.Module`子类化的相似复杂度模型稍微复杂一些，但一旦训练、验证和日志记录都由`LightningModule`模型处理，之后的每一步都比正常情况要简单。

PyTorch Lightning 还简化了硬件管理，我们在将 MNIST 训练切换到 GPU 时，简单性一目了然。PyTorch Lightning 还方便地支持更为冷门的硬件，如谷歌的张量处理单元（TPU），以及多个 GPU，并且它与 [Grid](https://www.grid.ai/)、一个用于扩展使用 PyTorch Lightning 的实验的云平台，和 [Lightning Bolts](https://www.pytorchlightning.ai/bolts)，一个由 PyTorch Lightning 社区驱动的深度学习示例模块工具箱，正在同步开发。

这涵盖了我们对 PyTorch Lightning 的“Hello, World”介绍，但我们仅仅触及了 Lightning 打算为你的深度学习工作流提供的表面。

**在我们的下一个 PyTorch Lightning 教程中，我们将深入探讨两个互补的 PyTorch Lightning 库：Lightning Flash 和 TorchMetrics。** TorchMetrics 提供了一个模块化的方法来定义和跟踪批次和设备上的有用指标，而 Lightning Flash 提供了一整套功能，促进了更高效的迁移学习和数据处理，并且包含了应对典型深度学习问题的最新方法的食谱。

**接下来，我们将进入下一节 PyTorch Lightning 教程：**

[PyTorch Lightning 教程 #2：使用 TorchMetrics 和 Lightning Flash](https://www.exxactcorp.com/blog/Deep-Learning/advanced-pytorch-lightning-using-torchmetrics-and-lightning-flash)

**简介：[Kevin Vu](https://www.kdnuggets.com/author/kevin-vu)** 负责管理 Exxact Corp 博客，并与许多才华横溢的作者合作，这些作者撰写关于深度学习不同方面的文章。

[原文](https://www.exxactcorp.com/blog/Deep-Learning/getting-started-with-pytorch-lightning)。转载已获许可。

**相关：**

+   [PyTorch Lightning 介绍](/2021/10/introduction-pytorch-lightning.html)

+   [如何将 PyTorch Lightning 模型部署到生产环境](/2020/11/deploy-pytorch-lightning-models-production.html)

+   [PyTorch 多 GPU 指标库及更多内容的新 PyTorch Lightning 版本](/2020/07/pytorch-multi-gpu-metrics-library-pytorch-lightning.html)

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析能力

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持组织的 IT

* * *

### 更多相关话题

+   [开始使用 PyTorch Lightning](https://www.kdnuggets.com/2022/12/getting-started-pytorch-lightning.html)

+   [深度学习库简介：PyTorch和Lightning AI](https://www.kdnuggets.com/introduction-to-deep-learning-libraries-pytorch-and-lightning-ai)

+   [5步开始使用PyTorch](https://www.kdnuggets.com/5-steps-getting-started-pytorch)

+   [免费使用Lightning AI Studio](https://www.kdnuggets.com/using-lightning-ai-studio-for-free)

+   [SQL备忘单入门](https://www.kdnuggets.com/2022/08/getting-started-sql-cheatsheet.html)

+   [开始使用PyCaret](https://www.kdnuggets.com/2022/11/getting-started-pycaret.html)
