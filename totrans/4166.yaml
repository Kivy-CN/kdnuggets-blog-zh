- en: Hands-On Reinforcement Learning Course, Part 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践强化学习课程，第 1 部分
- en: 原文：[https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-course-part-1.html](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-course-part-1.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-course-part-1.html](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-course-part-1.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Pau Labarta Bajo](https://www.linkedin.com/in/pau-labarta-bajo-4432074b/),
    mathematician and data scientist**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Pau Labarta Bajo](https://www.linkedin.com/in/pau-labarta-bajo-4432074b/)，数学家和数据科学家**。'
- en: '![](../Images/cf56880d1f13250ab6cd5d4f84b891dd.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf56880d1f13250ab6cd5d4f84b891dd.png)'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行 IT 工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This first part covers the bare minimum concept and theory you need to embark
    on this journey from the fundamentals to cutting edge reinforcement learning (RL),
    step-by-step, with coding examples and tutorials in Python. In each following
    chapter, we will solve a different problem with increasing difficulty.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分涵盖了你从基础到前沿强化学习（RL）所需的最低概念和理论，逐步进行，包含 Python 代码示例和教程。在接下来的每一章中，我们将解决一个不同难度的问题。
- en: Ultimately, the most complex RL problems involve a mixture of reinforcement
    learning algorithms, optimization, and Deep Learning. You do not need to know
    deep learning (DL) to follow along with this course. I will give you enough context
    to get you familiar with DL philosophy and understand how it becomes a crucial
    ingredient in modern reinforcement learning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，最复杂的 RL 问题涉及到强化学习算法、优化和深度学习的混合。你不需要了解深度学习（DL）才能跟上本课程。我会给你足够的背景知识，让你熟悉 DL
    的理念，并理解它如何成为现代强化学习的关键组成部分。
- en: In this first lesson, we will cover the fundamentals of reinforcement learning
    with examples, 0 maths, and a bit of Python.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节课中，我们将通过示例、零数学和一些 Python 代码来讲解强化学习的基本原理。
- en: 1\. What is a Reinforcement Learning problem?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 什么是强化学习问题？
- en: Reinforcement Learning (RL) is an area of Machine Learning (ML) concerned with
    learning problems where
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是机器学习（ML）的一个领域，涉及那些
- en: An intelligent **agent** needs to learn, through trial and error, how to take
    **actions** inside and **environment** in order to maximize a **cumulative reward**.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一个智能的**代理**需要通过反复试验来学习如何在**环境**内部采取**行动**，以最大化**累积奖励**。
- en: Reinforcement Learning is the kind of machine learning closest to how humans
    and animals learn.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是最接近人类和动物学习方式的机器学习类型。
- en: What is an agent? And an environment? What are exactly these actions the agent
    can take? And the reward? Why do you say cumulative reward?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是代理？什么是环境？代理可以采取哪些行动？奖励是什么？为什么说累积奖励？
- en: If you are asking yourself these questions, you are on the right track.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在问自己这些问题，你就走在正确的道路上。
- en: The definition I just gave introduces a bunch of terms that you might not be
    familiar with. In fact, they are ambiguous on purpose. This generality is what
    makes RL applicable to a wide range of seemingly different learning problems.
    This is the philosophy behind mathematical modeling, which stays at the roots
    of RL.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我刚才给出的定义引入了一些你可能不熟悉的术语。实际上，它们故意模糊不清。这种泛化使得强化学习（RL）能够应用于看似不同的各种学习问题。这就是数学建模的哲学，它是RL的根本。
- en: Let’s take a look at a few learning problems, and see how they use the RL lens.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看几个学习问题，并看看它们如何使用 RL 视角。
- en: 'Example 1: Learning to walk'
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 1：学习走路
- en: As a father of a baby who recently started walking, I cannot stop asking myself,
    *how did he learn that?*
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一位最近开始学步的婴儿的父亲，我忍不住问自己，*他是怎么学会的？*
- en: '![](../Images/51cf73933a411b4e9a38ce484f8debad.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51cf73933a411b4e9a38ce484f8debad.png)'
- en: '*Kai and Pau.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kai 和 Pau。*'
- en: As a Machine Learning engineer, I fantasize about understanding and replicating
    that incredible learning curve with software and hardware.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名机器学习工程师，我幻想着通过软件和硬件来理解和复制这种令人难以置信的学习曲线。
- en: 'Let’s try to model this learning problem using the RL ingredients:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用强化学习的要素来建模这个学习问题：
- en: 'The **agent** is my son, Kai. And he wants to stand up and walk. His muscles
    are strong enough at this point in time to have a chance at it. The learning problem
    for him is: how to sequentially adjust his body position, including several angles
    on his legs, waist, back, and arms to balance his body and not fall.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理**是我的儿子Kai。他想站起来走路。在这个时间点，他的肌肉足够强壮，有可能做到这一点。他面临的学习问题是：如何逐步调整他的身体位置，包括腿部、腰部、背部和手臂的多个角度，以平衡他的身体并防止摔倒。'
- en: '![](../Images/dd0f67c240d923b637e48b116e93f60f.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd0f67c240d923b637e48b116e93f60f.png)'
- en: '*Sai hi to Kai!*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*Sai hi to Kai!*'
- en: 'The **environment** is the physical world surrounding him, including the laws
    of physics. The most important of which is gravity. Without gravity, the learning-to-walk
    problem would drastically change and even become irrelevant: why would you want
    to walk in a world where you can simply fly? Another important law in this learning
    problem is Newton’s third law, which in plain words tells that if you fall on
    the floor, the floor is going to hit you back with the same strength. Ouch!'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境**是指他周围的物理世界，包括物理定律。其中最重要的定律是重力。没有重力，学习走路的问题会发生剧变，甚至变得无关紧要：在一个你可以简单地飞翔的世界里，你为什么还想走路呢？另一个在这个学习问题中重要的定律是牛顿的第三定律，用简单的话来说就是如果你摔到地板上，地板会以相同的力量反弹到你身上。哎呀！'
- en: The **actions** are all the updates in these body angles that determine his
    body position and speed as he starts chasing things around. Sure he can do other
    things at the same time, like imitating the sound of a cow, but these are probably
    not helping him accomplish his goal. We ignore these actions in our framework.
    Adding unnecessary actions does not change the modeling step, but it makes the
    problem harder to solve later on.An important (and obvious) remark is that Kai
    does not need to learn the physics of Newton to stand up and walk. He will learn
    through observing the **state** of the environment, taking action, and collecting
    feedback from this environment. He does not need to learn a model of the environment
    to achieve his goal.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**是所有更新的身体角度，这些角度决定了他在开始追逐物体时的身体位置和速度。当然，他可以同时做其他事情，比如模仿牛的声音，但这些可能并没有帮助他实现目标。我们在我们的框架中忽略这些动作。增加不必要的动作不会改变建模步骤，但会使后来解决问题变得更困难。一个重要（也是显而易见）的备注是，Kai不需要学习牛顿的物理学就能站起来走路。他将通过观察**环境**的**状态**、采取行动并从环境中收集反馈来学习。他不需要学习环境模型来实现他的目标。'
- en: The **reward** he receives is a stimulus coming from the brain that makes him
    happy or makes him feel pain. There is the negative reward he experiences when
    falling on the floor, which is physical pain, maybe followed by frustration. On
    the other side, there are several things that contribute positively to his happiness,
    like the happiness of getting to places faster or the external stimulus that comes
    from my wife Jagoda and me when we say “good job!” or “bravo!” to each attempt
    and marginal improvement he shows.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他所获得的**奖励**是来自大脑的刺激，使他感到快乐或痛苦。他在摔倒在地板上的时候会体验到负面奖励，即身体的疼痛，可能还伴随着挫败感。另一方面，还有一些事情对他的幸福感产生积极贡献，比如快速到达目的地的喜悦，或者当我们对每一次尝试和微小进步说“干得好！”或“好极了！”时，来自我妻子Jagoda和我外部的刺激。
- en: '***A little bit more about rewards***'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '***关于奖励的更多信息***'
- en: The reward is a signal to Kai that what he has been doing is good or bad for
    his learning. As he takes new actions and experiences pain or happiness, he starts
    to adjust his behavior to collect more positive feedback and less negative feedback.
    In other words, he learns
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励是一个信号，告诉Kai他所做的事情对他的学习是好还是坏。随着他采取新的行动并体验到痛苦或快乐，他开始调整他的行为以获取更多的积极反馈，减少负面反馈。换句话说，他在学习。
- en: Some actions might seem very appealing for the baby at the beginning, like trying
    to run to get a boost of excitement. However, he soon learns that in some (or
    most) cases, he ends up falling on his face and experiencing an extended period
    of pain and tears. This is why intelligent agents maximize **cumulative reward**
    and not marginal reward. They trade short-term rewards with long-term ones. An
    action that would give immediate reward, but put my body in a position about to
    fall, is not an optimal one.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一些行动在开始时可能对婴儿很有吸引力，比如尝试跑步以获得兴奋感。然而，他很快学会了在一些（或大多数）情况下，最终摔倒并经历长时间的痛苦和眼泪。这就是为什么智能代理会最大化**累计奖励**而不是边际奖励。他们用长期奖励来换取短期奖励。一个能够立即获得奖励，但会让我的身体处于快要摔倒的位置的行动，并不是最优的。
- en: Great happiness followed by greater pain is not a recipe for long-term well-being.
    This is something that babies often learn easier than we grown-ups.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 巨大的快乐之后伴随更大的痛苦不是长期幸福的秘诀。这是婴儿比我们成人更容易学到的事情。
- en: The frequency and intensity of the rewards are key for helping the agent learn.
    Very infrequent (sparse) feedback means harder learning. Think about it, if you
    do not know if what you do is good or bad, how can you learn? This is one of the
    main reasons why some RL problems are harder than others.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励的频率和强度对帮助代理学习至关重要。非常不频繁（稀疏）的反馈意味着更困难的学习。想一想，如果你不知道你做的事是好是坏，你怎么能学习？这就是为什么一些
    RL 问题比其他问题更难的主要原因之一。
- en: Reward shaping is a tough modeling decision for many real-world RL problems.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励塑形是许多现实世界 RL 问题中的一个艰难建模决策。
- en: 'Example 2: Learning to play Monopoly like a PRO'
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 2：像专家一样学习玩《垄断》
- en: As a kid, I spent a lot of time playing Monopoly with friends and relatives.
    Well, who hasn’t? It is an exciting game that combines luck (you roll the dices)
    and strategy.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 小时候，我花了很多时间和朋友和亲戚一起玩《垄断》。嗯，谁没有呢？这是一款结合了运气（你掷骰子）和策略的刺激游戏。
- en: Monopoly is a real-estate board game for two to eight players. You roll two
    dices to move around the board, buying and trading properties, and developing
    them with houses and hotels. You collect rent from your opponents, with the goal
    being to drive them into bankruptcy.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 《垄断》是一款适合两到八名玩家的房地产棋盘游戏。你掷两个骰子来在棋盘上移动，购买和交易属性，并用房屋和酒店来发展它们。你从对手那里收取租金，目标是让他们破产。
- en: '![](../Images/d40f5aef98ab5e32e4c6f8789a3d163d.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d40f5aef98ab5e32e4c6f8789a3d163d.png)'
- en: '*Photo by [Suzy Hazelwood](https://www.pexels.com/@suzyhazelwood?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)
    from [Pexels](https://www.pexels.com/photo/miniature-toy-car-on-monopoly-board-game-1422673/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels).*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*照片由 [Suzy Hazelwood](https://www.pexels.com/@suzyhazelwood?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)
    提供，来自 [Pexels](https://www.pexels.com/photo/miniature-toy-car-on-monopoly-board-game-1422673/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)。*'
- en: If you were so into this game that you wanted to find intelligent ways to play
    it, you could use some reinforcement learning.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这个游戏如此着迷，以至于想找到智能的玩法，你可以使用一些强化学习的方法。
- en: What would the 4 RL ingredients be?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 4 个 RL 成分是什么？
- en: The **agent** is you, the one who wants to win at Monopoly.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理**是你，那个想在《垄断》中获胜的人。'
- en: 'Your **actions** are the ones you see on this screenshot below:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的**行动**就是你在下面这个截图中看到的：
- en: '![](../Images/87e5b54f45b2fdbc051d542e1e31ba9a.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87e5b54f45b2fdbc051d542e1e31ba9a.png)'
- en: '*Action space in Monopoly. Credits to [aleph aseffa](https://github.com/aleph-aseffa/monopoly).*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*《垄断》的动作空间。感谢 [aleph aseffa](https://github.com/aleph-aseffa/monopoly)。*'
- en: The **environment** is the current state of the game, including the list of
    properties, positions, and cash amount each player has. There is also the strategy
    of your opponent, which is something you cannot predict and lies outside of your
    control.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境**是游戏的当前状态，包括每个玩家拥有的属性列表、位置和现金金额。还有对手的策略，这些是你无法预测并且超出你控制范围的。'
- en: And the **reward** is 0, except in your last move, where it is +1 if you win
    the game and -1 if you go bankrupt. This reward formulation makes sense but makes
    the problem hard to solve. As we said above, a more sparse reward means a harder
    solution. Because of this, there are [other ways](http://doc.gold.ac.uk/aisb50/AISB50-S02/AISB50-S2-Bailis-paper.pdf)
    to model the reward, making them noisier but less sparse.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励**为 0，除非在你的最后一次移动中，如果你赢得比赛，则为 +1，如果破产，则为 -1。这种奖励设定是合理的，但使问题难以解决。正如我们之前所说，更稀疏的奖励意味着更难解决。因此，还有
    [其他方式](http://doc.gold.ac.uk/aisb50/AISB50-S02/AISB50-S2-Bailis-paper.pdf) 来建模奖励，这些方法噪声更大但稀疏性更低。'
- en: When you play against another person in Monopoly, you do not know how she or
    he will play. What you can do is play against yourself. As you learn to play better,
    your opponent does too (because it is you), forcing you to level up your game
    to keep on winning. You see the positive feedback loop.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在《大富翁》中与另一个人对战时，你不知道他或她会怎么打。你可以做的就是与自己对战。随着你打得越来越好，你的对手也会变强（因为就是你自己），这迫使你提升游戏水平以继续获胜。你可以看到这种正反馈循环。
- en: This trick is called self-play. It gives us a path to bootstrap intelligence
    without using the external advice of an expert player. Self-play is the main difference
    between [AlphaGo](https://deepmind.com/research/case-studies/alphago-the-story-so-far)
    and [AlphaGo Zero](https://deepmind.com/blog/article/alphago-zero-starting-scratch),
    the two models developed by DeepMind that play the game of Go better than any
    human.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技巧被称为自我对战。它为我们提供了一条在不依赖专家玩家外部建议的情况下引导智能的路径。自我对战是[AlphaGo](https://deepmind.com/research/case-studies/alphago-the-story-so-far)和[AlphaGo
    Zero](https://deepmind.com/blog/article/alphago-zero-starting-scratch)这两个DeepMind开发的围棋模型之间的主要区别，这两个模型在围棋游戏中的表现超过了任何人类。
- en: 'Example 3: Learning to drive'
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例3：学习驾驶
- en: In a matter of decades (maybe less), machines will drive our cars, trucks, and
    buses.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 几十年内（可能更短），机器将会驾驶我们的汽车、卡车和公交车。
- en: '![](../Images/81c12d2c3292ee118314a5ac123dc958.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81c12d2c3292ee118314a5ac123dc958.png)'
- en: '*Photo by [Ruiyang Zhang](https://www.pexels.com/@ruiyang-zhang-915467?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)
    from [Pexels](https://www.pexels.com/photo/time-lapse-photo-of-cars-in-asphalt-road-3717291/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels).*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*照片由[Ruiyang Zhang](https://www.pexels.com/@ruiyang-zhang-915467?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)拍摄，来源于[Pexels](https://www.pexels.com/photo/time-lapse-photo-of-cars-in-asphalt-road-3717291/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)。*'
- en: But, how?
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，怎么做呢？
- en: 'Learning to drive a car is not easy. The goal of the driver is clear: to get
    from point A to point B comfortably for her and any passengers on board.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 学习驾驶汽车并不容易。司机的目标很明确：是让她自己和车上任何乘客都能舒适地从A点到达B点。
- en: 'There are many external aspects to the driver that make driving challenging,
    including:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 驾驶中有许多外部因素使得驾驶变得具有挑战性，包括：
- en: other drivers behavior
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他司机的行为
- en: traffic signs
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交通标志
- en: pedestrian behaviors
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行人行为
- en: pavement conditions
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 道路状况
- en: weather conditions
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天气条件
- en: …even fuel optimization (who wants to spend extra on this?)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: …甚至燃料优化（谁想为此多花钱呢？）
- en: How would we approach this problem with reinforcement learning?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何用强化学习来解决这个问题呢？
- en: The **agent** is the driver who wants to get from A to B comfortably.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**智能体**是希望从A点舒适到达B点的司机。'
- en: The **state** of the environment the driver observes has lots of things, including
    the position, speed, and acceleration of the car, all other cars, passengers,
    road conditions, or traffic signs. Transforming such a big vector of inputs into
    an appropriate action is challenging, as you can imagine.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 司机观察到的环境**状态**包括很多方面，包括汽车的位置、速度和加速度，所有其他车辆、乘客、道路条件或交通标志。将这样一个庞大的输入向量转化为适当的动作是具有挑战性的，正如你所能想象的那样。
- en: 'The **actions** are basically three: the direction of the steering wheel, throttle
    intensity, and break intensity.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**基本上有三种：方向盘的转动、油门的强度和刹车的强度。'
- en: The **reward** after each action is a weighted sum of the different aspects
    you need to balance when driving. A decrease in distance to point B brings a positive
    reward, while an increase in a negative one. To ensure no collisions, getting
    too close (or even colliding) with another car or even a pedestrian should have
    a very big negative reward. Also, in order to encourage smooth driving, sharp
    changes in speed or direction contribute to a negative reward.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次动作后的**奖励**是你在驾驶时需要平衡的不同方面的加权总和。距离B点的减少带来正奖励，而增加则带来负奖励。为了确保不发生碰撞，过于接近（甚至碰撞）其他车辆或行人应该有一个非常大的负奖励。此外，为了鼓励平稳驾驶，速度或方向的剧烈变化会导致负奖励。
- en: 'After these 3 examples, I hope the following representation of RL elements
    and how they play together makes sense:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这三个示例后，我希望以下关于强化学习（RL）元素及其相互作用的表示能够让你理解：
- en: '![](../Images/408c751b47786479e65c3f9eeaa7bd41.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/408c751b47786479e65c3f9eeaa7bd41.png)'
- en: '*RL in a nutshell. Credits to [Wikipedia](https://commons.wikimedia.org/wiki/File:Markov_diagram_v2.svg).*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*强化学习概述。感谢[维基百科](https://commons.wikimedia.org/wiki/File:Markov_diagram_v2.svg)。*'
- en: Now that we understand how to formulate an RL problem, we need to solve it.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了如何表述一个强化学习问题，我们需要解决它。
- en: But how?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 但是怎么做呢？
- en: 2\. Policies and value functions
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 策略和价值函数
- en: Policies
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略
- en: The agent picks the action she thinks is the best based on the current state
    of the environment. This is the agent’s strategy, commonly referred to as the
    agent’s **policy**.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 代理根据环境的当前状态选择她认为最好的动作。这是代理的策略，通常称为代理的 **策略**。
- en: A **policy** is a learned mapping from states to actions.
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**策略** 是一个从状态到动作的学习映射。'
- en: ''
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Solving a reinforcement learning problem means finding the best possible policy.*'
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*解决强化学习问题意味着找到最佳可能的策略。*'
- en: Policies are **deterministic** when they map each state to one action,
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 策略是**确定性的**，当它们将每个状态映射到一个动作时，
- en: '![](../Images/b3e2a88738239d078ff1f68eb33c54f5.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3e2a88738239d078ff1f68eb33c54f5.png)'
- en: or **stochastic** when they map each state to a probability distribution over
    all possible actions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 或**随机**的，当它们将每个状态映射到所有可能动作的概率分布时。
- en: '![](../Images/b69a22cb9601f896870bca518585a078.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b69a22cb9601f896870bca518585a078.png)'
- en: '*Stochastic* is a word you often read and hear in Machine Learning, and it
    essentially means *uncertain* or *random*. In environments with high uncertainty,
    like Monopoly where you are rolling dices, stochastic policies are better than
    deterministic ones.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机* 是你在机器学习中经常看到和听到的一个词，它本质上意味着 *不确定* 或 *随机*。在不确定性很高的环境中，例如掷骰子的 Monopoly 游戏，随机策略比确定性策略更好。'
- en: There exist several methods to actually compute this optimal policy. These are
    called **policy optimization methods**.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上存在几种方法来计算这个最优策略。这些被称为 **策略优化方法**。
- en: Value functions
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 价值函数
- en: Sometimes, depending on the problem, instead of directly trying to find the
    optimal policy, one can try to find the **value function** associated with that
    optimal policy.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，根据问题的不同，可以尝试找到与最优策略相关的**价值函数**，而不是直接寻找最优策略。
- en: But, what is a value function? And before that, what does value mean in this
    context?
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 但是，什么是价值函数？在此之前，价值在这个上下文中是什么意思？
- en: ''
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The **value** is a number associated with each state **s** of the environment
    that estimates how good it is for the agent to be in state **s**.
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**价值** 是与环境中每个状态 **s** 相关的一个数字，用于估计代理处于状态 **s** 的好坏。'
- en: ''
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is the cumulative reward the agent collects when starting at state **s**
    and choosing actions according to policy **π**.
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这是代理在状态 **s** 下开始并根据策略 **π** 选择动作时获得的累积奖励。
- en: ''
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A value function is a learned mapping from states to values.
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 价值函数是一个从状态到值的学习映射。
- en: The value function of a policy is commonly denoted as
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 策略的价值函数通常表示为
- en: '![](../Images/e2bf7a482c2cee4f6c7ce5e173a6ce91.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2bf7a482c2cee4f6c7ce5e173a6ce91.png)'
- en: Value functions can also map pairs of (action, state) to values. In this case,
    they are called *q-value* functions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 价值函数也可以将 (动作, 状态) 对映射到值。在这种情况下，它们被称为 *q-值* 函数。
- en: '![](../Images/46a02e26123c6cea5223829cc42ecf5f.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46a02e26123c6cea5223829cc42ecf5f.png)'
- en: The optimal value function (or q-value function) satisfies a mathematical equation,
    called the **Bellman equation**.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最优价值函数（或 q-值函数）满足一个数学方程，称为 **贝尔曼方程**。
- en: '![](../Images/af9b227173562522eba577013e197e8b.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af9b227173562522eba577013e197e8b.png)'
- en: This equation is useful because it can be transformed into an iterative procedure
    to find the optimal value function.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程式很有用，因为它可以转化为一个迭代过程来寻找最优价值函数。
- en: '*But, why are value functions useful?* Because you can infer an optimal policy
    from an optimal q-value function.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*但是，为什么价值函数有用？* 因为你可以从最优 q-值函数推断出最优策略。'
- en: '*How?* The optimal policy is the one where at each state **s** the agent chooses
    the action **a** that maximizes the q-value function.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*如何？* 最优策略是在每个状态 **s** 下，代理选择最大化 q-值函数的动作 **a**。'
- en: So, you can jump from optimal policies to optimal q-functions and vice versa.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以在最优策略和最优 q-函数之间跳转，反之亦然。
- en: There are several RL algorithms that focus on finding optimal q-value functions.
    These are called **Q-learning methods**.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种 RL 算法专注于寻找最优 q-值函数。这些被称为 **Q-学习方法**。
- en: The zoology of reinforcement learning algorithms
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强化学习算法的分类
- en: There are lots of different RL algorithms. Some try to directly find optimal
    policies, and others q-value functions, and others both at the same time.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的 RL 算法。有些尝试直接寻找最优策略，有些寻找 q 值函数，还有些同时寻找两者。
- en: The zoology of RL algorithms is diverse and a bit intimidating.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习算法的分类是多样的，有些让人有些畏惧。
- en: There is no *one-size-fits-all* when it comes to RL algorithms. You need to
    experiment with a few of them each time you solve an RL problem and see what works
    for your case.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 关于RL算法，没有*一刀切*的方法。你需要每次解决RL问题时实验几种算法，看看哪种适合你的情况。
- en: As you follow along this course, you will implement several of these algorithms
    and gain an insight into what works best in each situation.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当你跟随本课程时，你将实现这些算法中的几个，并获得每种情况中什么效果最佳的洞察。
- en: 3\. How to generate training data?
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 如何生成训练数据？
- en: Reinforcement learning agents are VERY data-hungry.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习代理非常需要数据。
- en: '![](../Images/f517210521bf3ccb89c9b4435d074b6d.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f517210521bf3ccb89c9b4435d074b6d.png)'
- en: '*[Photo by Karsten Winegeart](https://unsplash.com/photos/pQVecS8pBNY)*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*[Karsten Winegeart拍摄](https://unsplash.com/photos/pQVecS8pBNY)*'
- en: To solve RL problems, you need a lot of data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决RL问题，你需要大量数据。
- en: A way to overcome this hurdle is by using **simulated environments**. Writing
    the engine that simulates the environment usually requires more work than solving
    the RL problem. Also, changes between different engine implementations can render
    comparisons between algorithms meaningless.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 克服这一障碍的一种方法是使用**模拟环境**。编写模拟环境的引擎通常比解决RL问题需要更多的工作。此外，不同引擎实现之间的差异可能会使算法之间的比较变得毫无意义。
- en: This is why guys at OpenAI released the [Gym toolkit](https://gym.openai.com/)
    back in 2016\. OpenAIs’s gym offers a standardized API for a collection of environments
    for different problems, including
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是OpenAI团队在2016年发布[Gym工具包](https://gym.openai.com/)的原因。OpenAI的gym为不同问题的环境集合提供了一个标准化的API，包括
- en: the classic Atari games,
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经典的Atari游戏，
- en: robotic arms
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器人手臂
- en: or landing on the Moon (well, a simplified one)
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者着陆月球（嗯，简化版）
- en: There are proprietary environments, too, like [MuJoCo](https://www.endtoend.ai/envs/gym/mujoco/)
    ([recently bought by DeepMind](https://venturebeat.com/2021/10/18/deepmind-acquires-and-open-sources-robotics-simulator-mujoco/)).
    MuJoCo is an environment where you can solve continuous control tasks in 3D, like
    learning to walk.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些专有环境，如[MuJoCo](https://www.endtoend.ai/envs/gym/mujoco/)（[最近被DeepMind收购](https://venturebeat.com/2021/10/18/deepmind-acquires-and-open-sources-robotics-simulator-mujoco/)）。MuJoCo是一个可以在3D环境中解决连续控制任务的环境，例如学习行走。
- en: OpenAI Gym also defines a standard API to build environments, allowing third
    parties (like you) to create and make your environments available to others.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym还定义了一个标准API来构建环境，允许第三方（比如你）创建并将你的环境提供给其他人。
- en: If you are interested in self-driving cars, then you should check out CARLA,
    the most popular open urban driving simulator.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对自动驾驶汽车感兴趣，那么你应该看看CARLA，最受欢迎的开放城市驾驶模拟器。
- en: 4\. Python boilerplate code
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. Python样板代码
- en: 'You might be thinking:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在想：
- en: '*What we covered so far is interesting, but how do I actually write all this
    in Python?*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们到目前为止所涵盖的内容很有趣，但我怎么在Python中实际编写这些呢？*'
- en: And I completely agree with you
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我完全同意你的观点
- en: Let’s see how all this looks like in Python.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些在Python中是什么样的。
- en: Did you find something unclear in this code?
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这段代码中发现了什么不清楚的地方吗？
- en: What about line 23? What is this epsilon?
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第23行怎么样？这个epsilon是什么？
- en: Don’t panic. I didn’t mention this before, but I won’t leave you without an
    explanation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 不要惊慌。我之前没有提到这一点，但我不会让你没有解释。
- en: Epsilon is a key parameter to ensure our agent explores the environment enough
    before drawing definite conclusions on what is the best action to take in each
    state.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Epsilon是一个关键参数，用于确保我们的代理在得出每个状态中最佳行动的明确结论之前，足够地探索环境。
- en: It is a value between 0 and 1, and it represents the probability the agent chooses
    a random action instead of what she thinks is the best one.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个介于0和1之间的值，它表示代理选择随机行动而不是她认为最佳行动的概率。
- en: This tradeoff between exploring new strategies vs. sticking to already known
    ones is called the **exploration-exploitation problem**. This is a key ingredient
    in RL problems and something that distinguishes RL problems from supervised machine
    learning.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 探索新策略与坚持已知策略之间的权衡被称为**探索-利用问题**。这是RL问题的关键成分，也是RL问题与监督机器学习问题的区别所在。
- en: Technically speaking, we want the agent to find the global optimum, not a local
    one.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，我们希望代理找到全局最优解，而不是局部最优解。
- en: It is good practice to start your training with a large value (e.g., 50%) and
    progressively decrease after each episode. This way, the agent explores a lot
    at the beginning and less as it perfects its strategy.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践是从一个较大的值开始训练（例如50%），并在每个回合后逐渐减少。这样，代理在开始时会进行大量探索，并在逐步完善策略时减少探索。
- en: 5\. Recap and homework
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 总结与作业
- en: 'The key takeaways for this 1st part are:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分的关键要点是：
- en: Every RL problem has an agent (or agents), environment, actions, states, and
    rewards.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个强化学习问题都有一个（或多个）代理、环境、动作、状态和奖励。
- en: The agent sequentially takes actions with the goal of maximizing total rewards.
    For that, it needs to find the optimal policy.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理顺序地采取行动，目标是最大化总奖励。为此，它需要找到最优策略。
- en: Value functions are useful as they give us an alternative path to find the optimal
    policy.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 价值函数很有用，因为它们为我们提供了一种寻找最优策略的替代路径。
- en: In practice, you need to try different RL algorithms for your problem and see
    what works best.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实践中，你需要尝试不同的强化学习算法来解决你的问题，看看哪种方法最有效。
- en: RL agents need a lot of training data to learn. OpenAI gym is a great tool to
    re-use and create your environments.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习代理需要大量的训练数据来进行学习。OpenAI gym 是一个很好的工具，用于重用和创建你的环境。
- en: Exploration vs. exploitation is necessary when training RL agents to ensure
    the agent does not get stuck in local optimums.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索与利用的平衡在训练强化学习代理时是必要的，以确保代理不会陷入局部最优解。
- en: A course without a bit of homework would not be a course.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一个没有一点作业的课程不会算是课程。
- en: I want you to pick a real-world problem that interests you and that you could
    model and solve using reinforcement learning.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你选择一个你感兴趣的实际问题，并且你可以使用强化学习进行建模和解决。
- en: Define what are the agent(s), actions, states, and rewards.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 定义代理（agent）、动作、状态和奖励是什么。
- en: Feel free to send me an e-mail at [plabartabajo@gmail.com](mailto:plabartabajo@gmail.com)
    with your problem, and I can give you feedback.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 随时可以通过电子邮件联系我，[plabartabajo@gmail.com](mailto:plabartabajo@gmail.com)，告知你的问题，我可以给你反馈。
- en: '[Original](http://datamachines.xyz/2021/11/17/hands-on-reinforcement-learning-course-part-1/).
    Reposted with permission.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](http://datamachines.xyz/2021/11/17/hands-on-reinforcement-learning-course-part-1/)。经授权转载。'
- en: More On This Topic
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解更多相关话题
- en: '[Hands-on Reinforcement Learning Course Part 3: SARSA](https://www.kdnuggets.com/2022/01/handson-reinforcement-learning-course-part-3-sarsa.html)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[动手强化学习课程第3部分：SARSA](https://www.kdnuggets.com/2022/01/handson-reinforcement-learning-course-part-3-sarsa.html)'
- en: '[Hands-On Reinforcement Learning Course, Part 2](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-part-2.html)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[动手强化学习课程，第2部分](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-part-2.html)'
- en: '[HuggingFace Has Launched a Free Deep Reinforcement Learning Course](https://www.kdnuggets.com/2022/05/huggingface-launched-free-deep-reinforcement-learning-course.html)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HuggingFace推出了免费的深度强化学习课程](https://www.kdnuggets.com/2022/05/huggingface-launched-free-deep-reinforcement-learning-course.html)'
- en: '[Hands-On with Supervised Learning: Linear Regression](https://www.kdnuggets.com/handson-with-supervised-learning-linear-regression)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[动手监督学习：线性回归](https://www.kdnuggets.com/handson-with-supervised-learning-linear-regression)'
- en: '[Hands-On with Unsupervised Learning: K-Means Clustering](https://www.kdnuggets.com/handson-with-unsupervised-learning-kmeans-clustering)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[动手无监督学习：K均值聚类](https://www.kdnuggets.com/handson-with-unsupervised-learning-kmeans-clustering)'
- en: '[Generative AI with Large Language Models: Hands-On Training](https://www.kdnuggets.com/2023/07/generative-ai-large-language-models-handson-training.html)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[生成式AI与大型语言模型：动手训练](https://www.kdnuggets.com/2023/07/generative-ai-large-language-models-handson-training.html)'
