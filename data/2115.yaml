- en: Distribute and Run LLMs with llamafile in 5 Simple Steps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 llamafile 分发和运行 LLMs 的 5 个简单步骤
- en: 原文：[https://www.kdnuggets.com/distribute-and-run-llms-with-llamafile-in-5-simple-steps](https://www.kdnuggets.com/distribute-and-run-llms-with-llamafile-in-5-simple-steps)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/distribute-and-run-llms-with-llamafile-in-5-simple-steps](https://www.kdnuggets.com/distribute-and-run-llms-with-llamafile-in-5-simple-steps)
- en: '![Distribute and Run LLMs with llamafile in 5 Simple Steps](../Images/f0ab7be9ca7fc383ce9e47655b07be97.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![使用 llamafile 分发和运行 LLMs 的 5 个简单步骤](../Images/f0ab7be9ca7fc383ce9e47655b07be97.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: For many of us, exploring the possibilities of LLMs has felt out of reach. Whether
    it's downloading complicated software, figuring out coding, or needing powerful
    machines - getting started with LLMs can seem daunting. But just imagine, if we
    could interact with these powerful language models as easily as starting any other
    program on our computers. No installation, no coding, just click and talk. This
    accessibility is key for both developers and end-users. llamaFile emerges as a
    novel solution, merging the [llama.cpp](https://github.com/ggerganov/llama.cpp)
    with [Cosmopolitan Libc](https://github.com/jart/cosmopolitan) into a single framework.
    This framework reduces the complexity of LLMs by offering a **one-file executable
    called “llama file”**, which runs on local machines without the need for installation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多人来说，探索 LLMs 的可能性感觉遥不可及。无论是下载复杂的软件，弄懂编码，还是需要强大的机器——开始使用 LLMs 似乎都令人望而却步。但试想一下，如果我们能够像启动其他计算机程序一样轻松地与这些强大的语言模型互动。无需安装，无需编码，只需点击和对话。这种可达性对于开发者和最终用户都至关重要。llamaFile
    作为一种新颖的解决方案，将 [llama.cpp](https://github.com/ggerganov/llama.cpp) 和 [Cosmopolitan
    Libc](https://github.com/jart/cosmopolitan) 合并成一个单一的框架。这个框架通过提供一个**称为“llama 文件”的单文件可执行文件**来简化
    LLMs 的复杂性，可以在本地机器上运行而无需安装。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'So, how does it work? llamaFile offers **two convenient methods** for running
    LLMs:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，它是如何工作的呢？llamaFile 提供了**两种便捷的方法**来运行 LLMs：
- en: The first method involves downloading the latest release of llamafile along
    with the corresponding model weights from Hugging Face. Once you have those files,
    you're good to go!
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一种方法涉及从 Hugging Face 下载 llamafile 的最新版本及相应的模型权重。一旦获得这些文件，你就可以开始使用了！
- en: The second method is even simpler - you can access [pre-existing example llamafiles](https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#other-example-llamafiles)
    that have weights built-in.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二种方法更简单——你可以访问 [预先存在的示例 llamafiles](https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#other-example-llamafiles)，这些文件已内置权重。
- en: In this tutorial, you will work with the llamafile of the **LLaVa model** using
    the second method. It's a 7 Billion Parameter model that is quantized to 4 bits
    that you can interact with via chat, upload images, and ask questions. The example
    llamafiles of other models are also available, but we will be working with the
    LLaVa model as its llamafile size is 3.97 GB, while Windows has a maximum executable
    file size of 4 GB. The process is simple enough, and you can run LLMs by following
    the steps mentioned below.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你将使用第二种方法操作**LLaVa 模型**的 llamafile。它是一个 70 亿参数的模型，被量化为 4 位，你可以通过聊天、上传图片和提问与之互动。其他模型的示例
    llamafiles 也可用，但我们将使用 LLaVa 模型，因为其 llamafile 大小为 3.97 GB，而 Windows 的最大可执行文件大小为
    4 GB。这个过程非常简单，你可以按照下面提到的步骤运行 LLMs。
- en: 'Step 1: Download the llamafile'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 1：下载 llamafile
- en: First, you need to download the llava-v1.5-7b-q4.llamafile (3.97 GB) executable
    from the source provided [here](https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-q4.llamafile?download=true).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要从提供的 [这里](https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-q4.llamafile?download=true)
    下载 llava-v1.5-7b-q4.llamafile (3.97 GB) 可执行文件。
- en: 'Step 2: Grant Execution Permission (For macOS, Linux, or BSD Users)'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 2 步：授予执行权限（适用于 macOS、Linux 或 BSD 用户）
- en: Open your computer’s terminal and navigate to the directory where the file is
    located. Then run the following command to grant permission for your computer
    to execute this file.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 打开你计算机的终端并导航到文件所在的目录。然后运行以下命令以授予你的计算机执行此文件的权限。
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Step 3: Rename the File (For Windows Users)'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 3 步：重命名文件（适用于 Windows 用户）
- en: If you are on Windows, add “.exe” to the llamafile’s name on the end. You can
    run the following command on the terminal for this purpose.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 Windows，请在 llamafile 的名称末尾添加 “.exe”。你可以在终端中运行以下命令来完成这个操作。
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Step 4: Run the llamafile'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 4 步：运行 llamafile
- en: Execute the llama file by the following command.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令执行 llama 文件。
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '⚠️ Since MacOS uses zsh as its default shell and if you run across `zsh: exec
    format error: ./llava-v1.5-7b-q4.llamafile` error then you need to execute this:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '⚠️ 由于 MacOS 使用 zsh 作为默认的 shell，如果你遇到 `zsh: exec format error: ./llava-v1.5-7b-q4.llamafile`
    错误，则需要执行以下操作：'
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For Windows, your command may look like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Windows，你的命令可能如下所示：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Step 5: Interact with the User Interface'
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 5 步：与用户界面互动
- en: After running the llamafile, it should automatically open your default browser
    and display the user interface as shown below. If it doesn’t, open the browser
    and navigate to [http://localhost:8080](http://localhost:8080) manually.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 llamafile 后，它应该会自动打开你的默认浏览器并显示如下所示的用户界面。如果没有，打开浏览器并手动导航到 [http://localhost:8080](http://localhost:8080)。
- en: '![Distribute and Run LLMs with llamafile in 5 Simple Steps](../Images/42d390371b6887443db5df99300b6657.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![分发和运行 LLMs 的 llamafile 的 5 个简单步骤](../Images/42d390371b6887443db5df99300b6657.png)'
- en: Image by Author
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'Let''s start by interacting with the interface with a simple question to provide
    some information related to the LLaVa model. Below is the response generated by
    the model:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的问题来与界面互动，以提供一些与 LLaVa 模型相关的信息。以下是模型生成的回复：
- en: '![Distribute and Run LLMs with llamafile in 5 Simple Steps](../Images/86d758132c338800202b92705f4f5437.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![分发和运行 LLMs 的 llamafile 的 5 个简单步骤](../Images/86d758132c338800202b92705f4f5437.png)'
- en: Image by Author
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: The response highlights the approach to developing the LLaVa model and its applications.
    The response generated was reasonably fast. Let’s try to implement another task.
    We will upload the following sample image of a bank card with details on it and
    extract the required information from it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 该回复强调了 LLaVa 模型的开发方法及其应用。生成的回复相当迅速。让我们尝试实施另一个任务。我们将上传以下带有详细信息的银行卡样本图像，并从中提取所需的信息。
- en: '![Distribute and Run LLMs with llamafile in 5 Simple Steps](../Images/f2154617727a40577296a6d51bdb1cd4.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![分发和运行 LLMs 的 llamafile 的 5 个简单步骤](../Images/f2154617727a40577296a6d51bdb1cd4.png)'
- en: Image by [Ruby Thompson](https://qph.cf2.quoracdn.net/main-qimg-664f0732755fc0ba072da4286668fef8-lq)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Ruby Thompson](https://qph.cf2.quoracdn.net/main-qimg-664f0732755fc0ba072da4286668fef8-lq)
    提供
- en: 'Here’s the response:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是回复：
- en: '![Distribute and Run LLMs with llamafile in 5 Simple Steps](../Images/481ad10a447dea169aa9fba0fe91b01e.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![分发和运行 LLMs 的 llamafile 的 5 个简单步骤](../Images/481ad10a447dea169aa9fba0fe91b01e.png)'
- en: Image by Author
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Again, the response is pretty reasonable. The authors of LLaVa claim that it
    attains top-tier performance across various tasks. Feel free to explore diverse
    tasks, observe their successes and limitations, and experience the outstanding
    performance of LLaVa yourself.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 再次地，回复是相当合理的。LLaVa 的作者声称它在各种任务中表现出色。随意探索不同的任务，观察其成功和局限性，并亲自体验 LLaVa 的卓越表现。
- en: Once your interaction with the LLM is complete, you can shut down the llama
    file by returning to the terminal and pressing "Control - C".
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦与你的 LLM 的互动完成，你可以通过返回终端并按 "Control - C" 来关闭 llama 文件。
- en: Wrapping Up
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Distributing and running LLMs has never been more straightforward. In this tutorial,
    we explained how easily you can run and experiment with different models with
    just a single executable llamafile. This not only saves time and resources but
    also expands the accessibility and real-world utility of LLMs. We hope you found
    this tutorial helpful and would love to hear your thoughts on it.  Additionally,
    if you have any questions or feedback, please don't hesitate to reach out to us.
    We're always happy to help and value your input.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 分发和运行 LLM 从未如此简单。在本教程中，我们解释了如何仅通过一个可执行的 llamafile 轻松运行和试验不同的模型。这不仅节省了时间和资源，还扩展了
    LLM 的可访问性和现实世界的实用性。我们希望您发现本教程有帮助，并非常希望听到您的意见。此外，如果您有任何问题或反馈，请随时与我们联系。我们随时乐于提供帮助并重视您的意见。
- en: Thank you for reading!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您的阅读！
- en: '**[](https://www.linkedin.com/in/kanwal-mehreen1/)**[Kanwal Mehreen](https://www.linkedin.com/in/kanwal-mehreen1/)****
    Kanwal is a machine learning engineer and a technical writer with a profound passion
    for data science and the intersection of AI with medicine. She co-authored the
    ebook "Maximizing Productivity with ChatGPT". As a Google Generation Scholar 2022
    for APAC, she champions diversity and academic excellence. She''s also recognized
    as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and
    Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded
    FEMCodes to empower women in STEM fields.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**[](https://www.linkedin.com/in/kanwal-mehreen1/)**[Kanwal Mehreen](https://www.linkedin.com/in/kanwal-mehreen1/)****
    Kanwal 是一位机器学习工程师和技术作家，对数据科学和 AI 与医学的交叉领域充满了深厚的热情。她合著了电子书《用 ChatGPT 最大化生产力》。作为
    2022 年亚太地区的 Google Generation Scholar，她倡导多样性和学术卓越。她还被认可为 Teradata 多样性技术学者、Mitacs
    Globalink 研究学者和哈佛 WeCode 学者。Kanwal 是变革的积极倡导者，创立了 FEMCodes 以赋能 STEM 领域的女性。'
- en: More On This Topic
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[How to Package and Distribute Machine Learning Models with MLFlow](https://www.kdnuggets.com/2022/08/package-distribute-machine-learning-models-mlflow.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 MLFlow 打包和分发机器学习模型](https://www.kdnuggets.com/2022/08/package-distribute-machine-learning-models-mlflow.html)'
- en: '[Learn How to Run Alpaca-LoRA on Your Device in Just a Few Steps](https://www.kdnuggets.com/2023/05/learn-run-alpacalora-device-steps.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[只需几个步骤即可在您的设备上运行 Alpaca-LoRA](https://www.kdnuggets.com/2023/05/learn-run-alpacalora-device-steps.html)'
- en: '[Schedule & Run ETLs with Jupysql and GitHub Actions](https://www.kdnuggets.com/2023/05/schedule-run-etls-jupysql-github-actions.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Jupysql 和 GitHub Actions 安排和运行 ETL](https://www.kdnuggets.com/2023/05/schedule-run-etls-jupysql-github-actions.html)'
- en: '[Getting Started with PyTest: Effortlessly Write and Run Tests in Python](https://www.kdnuggets.com/getting-started-with-pytest-effortlessly-write-and-run-tests-in-python)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何快速上手 PyTest：轻松编写和运行 Python 测试](https://www.kdnuggets.com/getting-started-with-pytest-effortlessly-write-and-run-tests-in-python)'
- en: '[Run an LLM Locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在本地使用 LM Studio 运行 LLM](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)'
- en: '[How to Make Python Code Run Incredibly Fast](https://www.kdnuggets.com/2021/06/make-python-code-run-incredibly-fast.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何让 Python 代码运行得更快](https://www.kdnuggets.com/2021/06/make-python-code-run-incredibly-fast.html)'
