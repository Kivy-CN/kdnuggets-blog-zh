# 使用 Dask 进行云中的数据科学

> 原文：[https://www.kdnuggets.com/2020/10/data-science-cloud-dask.html](https://www.kdnuggets.com/2020/10/data-science-cloud-dask.html)

[评论](#comments)

**由 [Hugo Bowne-Anderson](https://hugobowne.github.io/) 撰写，数据科学布道部负责人和 [Coiled](https://coiled.io/) 的市场营销副总裁**

扩展大型数据分析的能力在数据科学和机器学习中变得越来越重要，并且速度非常快。幸运的是，像 Dask 和 [Coiled](https://coiled.io/) 这样的工具让人们可以轻松快速地做到这一点。

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速入门网络安全职业。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你组织的 IT

* * *

Dask 是一个流行的解决方案，用于在 PyData 生态系统和 Python 中扩展分析。这是因为 Dask 旨在并行化任何 PyData 库，因此可以无缝地与 PyData 工具集成。

*扩展* 你的分析以利用单台工作站的所有核心是开始处理大型数据集的第一步。

接下来，为了利用云上的集群（如 Azure、Google Cloud Platform、AWS 等），你可能需要*扩展*你的计算。

阅读下文，我们将：

+   使用 pandas 展示数据科学工作流程中的常见模式，

+   利用 Dask 扩展工作流程，利用单台工作站的所有核心，并

+   演示如何将我们的工作流程扩展到云端，使用 [Coiled Cloud](https://cloud.coiled.io/)。

所有代码 [请见 github](https://github.com/coiled/data-science-at-scale/blob/master/01b-data-analysis-at-scale-extended.ipynb)。

注：在开始之前，重要的是要考虑扩展计算是否真的必要。在开始之前，考虑让你的 pandas 代码更高效。对于机器学习，你可以通过绘制学习曲线来测量更多数据是否会导致模型改进。

### PANDAS 和 ETL：数据科学中的常见模式

首先，我们将使用 pandas 对一个内存中的数据集进行操作，以介绍一种常见的数据科学模式。这是 NYC 出租车数据集的 700 MB 子集，总数据集约为 10 GB。

我们希望看到扩展效果明显，因此我们选择了一个相对简单的工作流程。现在我们读取数据，处理数据，创建特征，并将其保存为 Parquet（不可读但比 CSV 高效得多）。

```py
# Import pandas and read in beginning of 1st file
import pandas as pd
df = pd.read_csv("data_taxi/yellow_tripdata_2019-01.csv")

# Alter data types for efficiency
df = df.astype({
    "VendorID": "uint8",
    "passenger_count": "uint8",
    "RatecodeID": "uint8",
    "store_and_fwd_flag": "category",
    "PULocationID": "uint16",
    "DOLocationID": "uint16",    
})

# Create new feature in dataset: tip_ratio
df["tip_ratio"] = df.tip_amount / df.total_amount

# Write to parquet
df.to_parquet("data_taxi/yellow_tripdata_2019-01.parq")
```

在我的笔记本电脑上大约花了 1 分钟，这是我们可以容忍的分析等待时间（也许）。

现在我们想在整个数据集上进行相同的分析。

### DASK：提升你的数据科学

数据集的10GB大小超过了我笔记本上的RAM，因此我们不能将其存储在内存中。

我们可以改写成一个for循环：

```py
for filename in glob("~/data_taxi/yellow_tripdata_2019-*.csv"):
	df = pd.read_csv(filename)
	...
	df.to_parquet(...)
```

然而，这种方法并没有充分利用我笔记本上的多个核心，也不是一个优雅的解决方案。这里就是Dask单机并行的作用。

引入Dask的多个方面后，我们将启动一个本地集群并启动一个Dask客户端：

```py
from dask.distributed import LocalCluster, Client
cluster = LocalCluster(n_workers=4)
client = Client(cluster)
client
```

然后我们导入Dask DataFrame，惰性读取数据，并像之前使用pandas一样执行ETL管道。

```py
import dask.dataframe as dd

df = dd.read_csv(
	"data_taxi/yellow_tripdata_2019-*.csv",
	dtype={'RatecodeID': 'float64',
   	'VendorID': 'float64',
   	'passenger_count': 'float64',
   	'payment_type': 'float64'}
)

# Alter data types for efficiency
df = df.astype({
	"VendorID": "UInt8",
	"passenger_count": "UInt8",
	"RatecodeID": "UInt8",
	"store_and_fwd_flag": "category",
	"PULocationID": "UInt16",
	"DOLocationID": "UInt16",    
})

# Create new feature in dataset: tip_ratio
df["tip_ratio"] = df.tip_amount / df.total_amount
# Write to Parquet
df.to_parquet("data_taxi/yellow_tripdata_2019.parq")
```

在我的笔记本上大约花了5分钟，我们可以称之为可以接受（我猜）。但如果我们想做一些稍微复杂一点的事情（我们通常会做！），这个时间会迅速增加。

如果我有一个云端集群的访问权限，现在正是利用它的时候！

但首先，让我们回顾一下我们刚刚完成的工作：

+   我们使用了Dask DataFrame——一个大的虚拟数据框，沿索引划分为多个Pandas DataFrame

+   我们正在一个本地集群上工作，由以下组成：

    +   一个*调度器*（负责组织并将工作/任务发送给工作者），以及，

    +   *工作者*，即计算这些任务的组件

+   我们已启动一个Dask客户端，这是“集群用户的用户接口”。

简而言之——客户端存在于你编写Python代码的地方，客户端与调度器交谈，将任务传递给它。

![Dask](../Images/ca8caa73a57c3f6319bea6abe0e8f212.png)

### COILED：扩展你的数据科学

现在我们期待的时刻到了——是时候跃升到云端了。如果你有云资源的访问权限（如AWS）并知道如何配置Docker和Kubernetes容器，你可以在云端启动一个Dask集群。然而，这将是耗时的。

输入一个方便的替代方案：Coiled，我们将在这里使用。为此，我已登录[Coiled Cloud](http://beta.coiled.io/)（Beta版本目前提供免费计算！），通过pip安装了coiled，并进行了身份验证。随时可以跟着做一下。

```py
pip install coiled --upgrade
coiled login   # redirects you to authenticate with github or google
```

然后我们进行必要的导入，启动一个集群（大约需要一分钟），并实例化我们的客户端：

```py
import coiled
from dask.distributed import LocalCluster, Client
cluster = coiled.Cluster(n_workers=10)
client = Client(cluster)
```

接下来我们导入数据（这次来自s3），并进行分析：

```py
import dask.dataframe as dd

# Read data into a Dask DataFrame
df = dd.read_csv(
	"s3://nyc-tlc/trip data/yellow_tripdata_2019-*.csv",
	dtype={
    	'RatecodeID': 'float64',
   	'VendorID': 'float64',
   	'passenger_count': 'float64',
   	'payment_type': 'float64'
	},
	storage_options={"anon":True}
)

# Alter data types for efficiency
df = df.astype({
	"VendorID": "UInt8",
	"passenger_count": "UInt8",
	"RatecodeID": "UInt8",
	"store_and_fwd_flag": "category",
	"PULocationID": "UInt16",
	"DOLocationID": "UInt16",    
})

# Create new feature in dataset: tip_ratio
df["tip_ratio"] = df.tip_amount / df.total_amount

# Write to Parquet
df.to_parquet("s3://hugo-coiled-tutorial/nyctaxi-2019.parq")
```

在Coiled Cloud上花了多久？*30秒。* 这比我在笔记本电脑上所花的时间少了一个数量级，即使是这种相对简单的分析也是如此。

很容易看到能够在单一工作流中完成这组分析的强大之处。我们不需要切换上下文或环境。而且，完成后，回到本地工作站上的Dask或pandas也很简单。云计算在需要时很棒，但在不需要时可能成为负担。我们刚刚经历了一次负担较轻的体验。

### 你需要更快的数据科学吗？

现在你可以免费开始使用 Coiled 集群。Coiled 还处理安全、conda/docker 环境和团队管理，让你可以专注于数据科学工作。立即在 [Coiled Cloud](http://beta.coiled.io/) 上开始使用。

**简介：[Hugo Bowne-Anderson](https://hugobowne.github.io/)** Hugo Bowne-Anderson 是 **[Coiled](https://coiled.io/)** 的数据科学推广负责人兼市场副总裁 ([**@CoiledHQ,**](https://twitter.com/CoiledHQ) **[LinkedIn](https://www.linkedin.com/company/coiled-computing)**)。他拥有丰富的数据科学家、教育者、推广者、内容营销人员和数据战略顾问的经验，涵盖了行业和基础研究。他还在耶鲁大学和冷泉港实验室等机构、SciPy、PyCon 和 ODSC 等会议、Data Carpentry 等组织中教授数据科学。他致力于传播数据技能、数据科学工具的使用以及开源软件，服务于个人和企业。

**相关：**

+   [Dask 中的机器学习](/2020/06/machine-learning-dask.html)

+   [为什么以及如何在大数据中使用 Dask](/2020/04/dask-big-data.html)

+   [Dask 中的 K-means 聚类：猫咪图片的图像滤镜](/2019/06/k-means-clustering-dask-image-filters.html)

### 更多相关话题

+   [11 种最佳实践：AWS 云中的云和数据迁移](https://www.kdnuggets.com/2023/04/11-best-practices-cloud-data-migration-aws-cloud.html)

+   [Anaconda 新品！数据科学培训和云托管笔记本](https://www.kdnuggets.com/2022/11/anaconda-new-anaconda-data-science-training-cloud-hosted-notebooks.html)

+   [如何利用云计算高效扩展数据科学项目](https://www.kdnuggets.com/2023/05/efficiently-scale-data-science-projects-cloud-computing.html)

+   [云计算如何提升数据科学工作流](https://www.kdnuggets.com/2023/08/cloud-computing-enhances-data-science-workflows.html)

+   [数据科学中的云计算简介](https://www.kdnuggets.com/introduction-to-cloud-computing-for-data-science)

+   [数据科学的 7 个最佳免费云笔记本](https://www.kdnuggets.com/top-7-free-cloud-notebooks-for-data-science)
