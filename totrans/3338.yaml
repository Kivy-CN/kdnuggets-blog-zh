- en: 7 More Steps to Mastering Machine Learning With Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 掌握 Python 机器学习的 7 个步骤
- en: 原文：[https://www.kdnuggets.com/2017/03/seven-more-steps-machine-learning-python.html/2](https://www.kdnuggets.com/2017/03/seven-more-steps-machine-learning-python.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/03/seven-more-steps-machine-learning-python.html/2](https://www.kdnuggets.com/2017/03/seven-more-steps-machine-learning-python.html/2)
- en: 'Step 4: More Ensemble Methods'
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 4 步：更多集成方法
- en: 'The first post only dealt with a single ensemble method: Random Forests (RF).
    RF has seen a great deal of success over the years as a top performing classifier,
    but it is certainly not the only ensemble classifier available. We will have a
    look at bagging, boosting, and voting.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 第一篇文章仅涉及一种集成方法：随机森林（RF）。多年来，RF 作为一种顶级分类器取得了巨大成功，但它绝不是唯一的集成分类器。我们将探讨装袋、提升和投票。
- en: '![Boosting](../Images/658f99083c93080b92c1f86c7f94ecf7.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![提升](../Images/658f99083c93080b92c1f86c7f94ecf7.png)'
- en: Give me a boost.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 给我一个提升。
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能。'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT。'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'First, read these overviews of ensemble learners, the first being general,
    and the second being as they relate to Scikit-learn:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，阅读这些集成学习者的概述，第一篇是一般性的，第二篇是与 Scikit-learn 相关的：
- en: '[An Introduction to Ensemble Learners](/2016/11/data-science-basics-intro-ensemble-learners.html),
    by Matthew Mayo'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习者简介](/2016/11/data-science-basics-intro-ensemble-learners.html)，作者：Matthew
    Mayo'
- en: '[Ensemble methods in Scikit-learn](http://scikit-learn.org/stable/modules/ensemble.html),
    Scikit-learn documentation'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Scikit-learn 中的集成方法](http://scikit-learn.org/stable/modules/ensemble.html)，Scikit-learn
    文档'
- en: 'Then, before moving on to new ensemble methods, get back up to speed on Random
    Forests with a fresh tutorial here:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在继续了解新的集成方法之前，通过这里的最新教程重新学习随机森林：
- en: '[Random Forests in Python](/2016/12/random-forests-python.html), from Yhat'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 中的随机森林](/2016/12/random-forests-python.html)，作者：Yhat'
- en: Bagging, boosting, and voting are all different forms of ensemble classifiers.
    All involve building multiple models; however, what **algorithms** the models
    are built from, the **data** which the models use, and how the results are ultimately
    **combined** differ between schemes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 装袋、提升和投票都是不同形式的集成分类器。它们都涉及构建多个模型；然而，这些模型是从什么**算法**构建的、模型使用的**数据**以及结果如何最终**结合**在不同方案之间有所不同。
- en: '**Bagging** builds multiple models from the same classification algorithm,
    while using different (independent) samples of data from the training set -- Scikit-learn
    implements BaggingClassifier'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**装袋**从相同的分类算法构建多个模型，同时使用训练集中的不同（独立）数据样本——Scikit-learn 实现了 BaggingClassifier。'
- en: '**Boosting** builds multiple models from the same classification algorithm,
    chaining models one after another in order to boost the learning of each subsequent
    model -- Scikit-learn implements AdaBoost'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升**从相同的分类算法构建多个模型，将模型一个接一个地链式排列，以提升每个后续模型的学习——Scikit-learn 实现了 AdaBoost。'
- en: '**Voting** builds multiple models from different classification algorithms,
    and a criteria is used to determine how the models are best combined -- Scikit-learn
    implements VotingClassifier'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投票**通过不同的分类算法构建多个模型，并使用标准来确定如何最好地结合这些模型——Scikit-learn 实现了 VotingClassifier。'
- en: 'So, why combine models? To approach this question from one specific angle,
    here is an overview of the [bias-variance trade-off](/2016/08/bias-variance-tradeoff-overview.html)
    as related specifically to boosting, from the Scikit-learn documentation:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么要结合模型？从一个具体的角度来看，这里是一个关于 [偏差-方差权衡](/2016/08/bias-variance-tradeoff-overview.html)
    的概述，特别是与提升相关，来自 Scikit-learn 文档：
- en: '[Single estimator versus bagging: bias-variance decomposition](http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html),
    Scikit-learn documentation'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[单一估计器与袋装法：偏差-方差分解](http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html)，Scikit-learn
    文档'
- en: 'Now that you have read some introductory material on ensemble learners in general,
    and have a basic understanding of a few specific ensemble classifiers, follow
    this introduction to implementing the ensemble classifiers in Python using Scikit-learn,
    from Machine Learning Mastery:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经阅读了一些关于集成学习者的一般介绍，并对一些特定的集成分类器有了基本了解，请按照来自 Machine Learning Mastery 的教程介绍，在
    Python 中使用 Scikit-learn 实现集成分类器：
- en: '[Ensemble Machine Learning Algorithms in Python with scikit-learn](http://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/),
    by Jason Brownlee'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 scikit-learn 的 Python 集成机器学习算法](http://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/)，作者
    Jason Brownlee'
- en: 'Step 5: Gradient Boosting'
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 5 步：梯度提升
- en: Our next step keeps us in the realm of ensemble classifiers, focusing on one
    of the most popular contemporary machine learning algorithms. Gradient boosting
    has made a noticeable impact in machine learning in the recent past, becoming
    (of note) one of the most utilized and successful algorithms in Kaggle competitions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一步将继续探讨集成分类器领域，专注于一种最受欢迎的现代机器学习算法。梯度提升在最近的机器学习领域中产生了显著影响，成为（值得注意的是）Kaggle
    竞赛中最常用且成功的算法之一。
- en: '![Gradient boosting](../Images/8a508ad2c2ca40aaed56f1ad7dfa9128.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![梯度提升](../Images/8a508ad2c2ca40aaed56f1ad7dfa9128.png)'
- en: Give me a gradient boost.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 给我一个梯度提升。
- en: 'First, read an overview of what gradient boosting is:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，阅读关于梯度提升的概述：
- en: '[Gradient boosting on Wikipedia](https://en.wikipedia.org/wiki/Gradient_boosting)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[维基百科上的梯度提升](https://en.wikipedia.org/wiki/Gradient_boosting)'
- en: 'Next, read up on why gradient boosting is the "most winning-est" approach to
    Kaggle competitions:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，了解为何梯度提升是 Kaggle 竞赛中“最获胜”的方法：
- en: '[Why does Gradient boosting work so well for so many Kaggle problems? on Quora](https://www.quora.com/Why-does-Gradient-boosting-work-so-well-for-so-many-Kaggle-problems)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么梯度提升对许多 Kaggle 问题效果如此好？在 Quora 上](https://www.quora.com/Why-does-Gradient-boosting-work-so-well-for-so-many-Kaggle-problems)'
- en: '[A Kaggle Master Explains Gradient Boosting](https://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/),
    by Ben Gorman'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kaggle 大师讲解梯度提升](https://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)，作者
    Ben Gorman'
- en: While Scikit-learn has its own gradient boosting implementation, we will diverge
    somewhat and use the [XGBoost library](https://github.com/dmlc/xgboost), which
    has [been noted](http://www.jmlr.org/proceedings/papers/v42/chen14.pdf) to be
    a faster implementation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Scikit-learn 也有自己的梯度提升实现，但我们会稍作偏离，使用 [XGBoost 库](https://github.com/dmlc/xgboost)，它
    [被指出](http://www.jmlr.org/proceedings/papers/v42/chen14.pdf) 是一种更快的实现。
- en: 'The following links provide some additional info on the XGBoost library, as
    well as gradient boosting (out of necessity):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接提供了关于 XGBoost 库及梯度提升的一些额外信息（出于必要性）：
- en: '[XGBoost on Wikipedia](https://en.wikipedia.org/wiki/Xgboost)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost 在维基百科](https://en.wikipedia.org/wiki/Xgboost)'
- en: '[XGBoost library on Github](https://github.com/dmlc/xgboost)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost 库在 GitHub](https://github.com/dmlc/xgboost)'
- en: '[XGBoost documentation](https://xgboost.readthedocs.io/en/latest/model.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost 文档](https://xgboost.readthedocs.io/en/latest/model.html)'
- en: 'Now, follow this tutorial which brings it all together:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，请按照这个将所有内容整合在一起的教程进行操作：
- en: '[A Guide to Gradient Boosted Trees with XGBoost in Python](https://jessesw.com/XG-Boost/),
    by Jesse Steinweg-Woods'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Python 中使用 XGBoost 的梯度提升树指南](https://jessesw.com/XG-Boost/)，作者 Jesse Steinweg-Woods'
- en: 'You can also follow these more concise examples for reinforcement:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以参考这些更简洁的例子来加强理解：
- en: '[XGBoost Example (Python) on Kaggle](https://www.kaggle.com/cbrogan/titanic/xgboost-example-python)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kaggle 上的 XGBoost 示例（Python）](https://www.kaggle.com/cbrogan/titanic/xgboost-example-python)'
- en: '[Iris Dataset and XGBoost Simple Tutorial](http://ieva.rocks/2016/08/25/iris_dataset_and_xgboost_simple_tutorial/),
    by Ieva Zarina'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Iris 数据集与 XGBoost 简单教程](http://ieva.rocks/2016/08/25/iris_dataset_and_xgboost_simple_tutorial/)，作者
    Ieva Zarina'
- en: 'Step 6: More Dimensionality Reduction'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 6 步：更多降维
- en: '[Dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction)
    is the act of reducing the variables being used for model building from its initial
    number to a reduced number, by utilizing processes to obtain a set of **principal
    variables**.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[降维](https://en.wikipedia.org/wiki/Dimensionality_reduction)是指通过利用处理过程将模型构建所用的变量从最初数量减少到较少数量的行为，以获得一组**主要变量**。'
- en: 'There are 2 major forms of dimensionality reduction:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 降维有两种主要形式：
- en: '[Feature selection](https://en.wikipedia.org/wiki/Feature_selection) - selecting
    a subset of relevant features'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[特征选择](https://en.wikipedia.org/wiki/Feature_selection) - 选择相关特征的子集'
- en: '[Feature extraction](https://en.wikipedia.org/wiki/Feature_extraction) - building
    an informative and non-redundant set of derived values features'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[特征提取](https://en.wikipedia.org/wiki/Feature_extraction) - 构建一个信息丰富且不冗余的衍生特征值集'
- en: The following is a look at a pair of common feature extraction methods.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对一对常见特征提取方法的介绍。
- en: '![LDA Iris dataset](../Images/64b868a81463c323186ccc586b238ac0.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![LDA Iris 数据集](../Images/64b868a81463c323186ccc586b238ac0.png)'
- en: LDA of Iris dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: LDA Iris 数据集。
- en: '[**Principal component analysis (PCA)**](https://en.wikipedia.org/wiki/Principal_component_analysis)
    is a statistical procedure that uses an orthogonal transformation to convert a
    set of observations of possibly correlated variables into a set of values of linearly
    uncorrelated variables called principal components. The number of principal components
    is less than or equal to the number of original variables. This transformation
    is defined in such a way that the first principal component has the largest possible
    variance (that is, accounts for as much of the variability in the data as possible)[.]'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**主成分分析 (PCA)**](https://en.wikipedia.org/wiki/Principal_component_analysis)
    是一种统计程序，通过正交变换将可能相关的变量的观测值集转换为一组线性不相关的变量，这些变量称为主成分。主成分的数量小于或等于原始变量的数量。这种变换的定义使得第一个主成分具有最大的可能方差（即尽可能多地解释数据的变异性）[.]'
- en: 'The above definition comes from the [PCA Wikipedia entry](https://en.wikipedia.org/wiki/Principal_component_analysis),
    which you can read further if interested. However, the following overview-slash-tutorial
    is very thorough:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 上述定义来自于 [PCA 维基百科条目](https://en.wikipedia.org/wiki/Principal_component_analysis)，如果有兴趣，可以进一步阅读。然而，以下概述教程非常全面：
- en: '[Principal Component Analysis in 3 Simple Steps](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html),
    by Sebastian Raschka'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[主成分分析的 3 个简单步骤](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html)，作者：Sebastian
    Raschka'
- en: '[**Linear discriminant analysis (LDA)**](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)
    is a generalization of Fisher''s linear discriminant, a method used in statistics,
    pattern recognition and machine learning to find a linear combination of features
    that characterizes or separates two or more classes of objects or events. The
    resulting combination may be used as a linear classifier, or, more commonly, for
    dimensionality reduction before later classification.'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**线性判别分析 (LDA)**](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)
    是 Fisher 线性判别分析的推广，这是一种用于统计学、模式识别和机器学习的方法，用于寻找特征的线性组合，这种组合能够表征或分离两个或更多类的对象或事件。结果的组合可以用作线性分类器，或者更常见的是，在后续分类前进行降维。'
- en: ''
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: LDA is closely related to analysis of variance (ANOVA) and regression analysis,
    which also attempt to express one dependent variable as a linear combination of
    other features or measurements. However, ANOVA uses categorical independent variables
    and a continuous dependent variable, whereas discriminant analysis has continuous
    independent variables and a categorical dependent variable (i.e. the class label)
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: LDA 与方差分析 (ANOVA) 和回归分析密切相关，后者也尝试将一个因变量表达为其他特征或测量的线性组合。然而，ANOVA 使用分类自变量和连续因变量，而判别分析具有连续自变量和分类因变量（即类别标签）
- en: 'The above definition is also from Wikipedia. And again, the following read
    is thorough:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 上述定义也来自维基百科。再一次，以下读物非常详尽：
- en: '[Linear Discriminant Analysis – Bit by Bit](http://sebastianraschka.com/Articles/2014_python_lda.html),
    by Sebastian Raschka'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性判别分析 – 一步步解读](http://sebastianraschka.com/Articles/2014_python_lda.html)，作者：Sebastian
    Raschka'
- en: 'Are you at all confused as to the practical difference between PCA and LDA
    for dimensionality reduction? [Sebastian Raschka clarifies](http://sebastianraschka.com/Articles/2014_python_lda.html):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否对 PCA 和 LDA 在降维上的实际差异感到困惑？[Sebastian Raschka 进行了阐明](http://sebastianraschka.com/Articles/2014_python_lda.html)：
- en: Both Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA)
    are linear transformation techniques that are commonly used for dimensionality
    reduction. PCA can be described as an “unsupervised” algorithm, since it “ignores”
    class labels and its goal is to find the directions (the so-called principal components)
    that maximize the variance in a dataset. In contrast to PCA, LDA is “supervised”
    and computes the directions (“linear discriminants”) that will represent the axes
    that that maximize the separation between multiple classes.
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 线性判别分析（LDA）和主成分分析（PCA）都是常用于降维的线性变换技术。PCA可以被描述为一种“无监督”的算法，因为它“忽略”类标签，其目标是找到最大化数据集方差的方向（即所谓的主成分）。与PCA相比，LDA是“有监督”的，它计算出将表示最大化多个类之间分离的方向（即“线性判别”的方向）。
- en: 'For a concise elaboration on this, read the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个主题的简明阐述，请阅读以下内容：
- en: '[What is the difference between LDA and PCA for dimensionality reduction?](https://sebastianraschka.com/faq/docs/lda-vs-pca.html),
    by Sebastian Raschka'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LDA和PCA在降维上的区别是什么？](https://sebastianraschka.com/faq/docs/lda-vs-pca.html)，作者：Sebastian
    Raschka'
- en: 'Step 7: More Deep Learning'
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第7步：更多深度学习
- en: The original 7 Steps... post provided an entry point to neural networks and
    deep learning. If you have made it this far without much trouble and want to solidify
    your understanding of neural networks, and practice implementing a few common
    neural network models, there is no reason not to continue on.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的7步骤...文章提供了进入神经网络和深度学习的入口。如果你已经顺利到达这里并希望巩固对神经网络的理解，并练习实现一些常见的神经网络模型，那么没有理由不继续下去。
- en: '![Deep learning](../Images/35cca5c97cb4d4033c23fdadcb5820b8.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习](../Images/35cca5c97cb4d4033c23fdadcb5820b8.png)'
- en: The many layers of deep neural networks.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络的多个层次。
- en: 'First, have a look at a few deep learning foundation materials:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，查看一些深度学习基础材料：
- en: '[Deep Learning Key Terms, Explained](/2016/10/deep-learning-key-terms-explained.html),
    by Matthew Mayo'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习关键术语解析](/2016/10/deep-learning-key-terms-explained.html)，作者：Matthew Mayo'
- en: '[7 Steps to Understanding Deep Learning](/2016/01/seven-steps-deep-learning.html),
    by Matthew Mayo'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解深度学习的7个步骤](/2016/01/seven-steps-deep-learning.html)，作者：Matthew Mayo'
- en: 'Next, try a few introductory overview-slash-tutorials on [TensorFlow](https://www.tensorflow.org/),
    Google''s "open-source software library for Machine Intelligence," effectively
    a deep learning framework and nearly *de facto* contemporary go-to neural network
    tool:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，尝试几个关于[TensorFlow](https://www.tensorflow.org/)的入门概述或教程，这是Google的“开源机器智能软件库”，实际上是一个深度学习框架，并且几乎是*事实上的*当代神经网络工具：
- en: '[The Gentlest Introduction to Tensorflow – Part 1](/2016/08/gentlest-introduction-tensorflow-part-1.html),
    by Soon Hin Khor'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[最温和的Tensorflow介绍 – 第1部分](/2016/08/gentlest-introduction-tensorflow-part-1.html)，作者：Soon
    Hin Khor'
- en: '[The Gentlest Introduction to Tensorflow – Part 2](/2016/08/gentlest-introduction-tensorflow-part-2.html),
    by Soon Hin Khor'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[最温和的Tensorflow介绍 – 第2部分](/2016/08/gentlest-introduction-tensorflow-part-2.html)，作者：Soon
    Hin Khor'
- en: '[The Gentlest Introduction to Tensorflow – Part 3](/2017/02/gentlest-introduction-tensorflow-part-3.html),
    by Soon Hin Khor'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[最温和的Tensorflow介绍 – 第3部分](/2017/02/gentlest-introduction-tensorflow-part-3.html)，作者：Soon
    Hin Khor'
- en: '[The Gentlest Introduction to Tensorflow – Part 4](/2017/02/gentlest-introduction-tensorflow-part-4.html),
    by Soon Hin Khor'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[最温和的Tensorflow介绍 – 第4部分](/2017/02/gentlest-introduction-tensorflow-part-4.html)，作者：Soon
    Hin Khor'
- en: 'Finally, try your hand at these tutorials directly from the TensorFlow site,
    which implement a few of the most popular and common neural network models:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，试试这些来自TensorFlow网站的教程，它们实现了一些最流行和常见的神经网络模型：
- en: '[Recurrent Neural Networks](https://www.tensorflow.org/tutorials/recurrent),
    Google TensorFlow tutorial'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[递归神经网络](https://www.tensorflow.org/tutorials/recurrent)，Google TensorFlow教程'
- en: '[Convolutional Neural Networks](https://www.tensorflow.org/tutorials/deep_cnn),
    Google TensorFlow tutorial'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[卷积神经网络](https://www.tensorflow.org/tutorials/deep_cnn)，Google TensorFlow教程'
- en: Also, a 7 Steps... post focusing on deep learning is currently in the works,
    and will focus on the use of a high-level API sitting atop TensorFlow to increase
    the ease and flexibility with which a practitioner can implement models. I will
    also add a link here once complete.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一篇关于深度学习的7步骤...文章目前正在制作中，将着重于使用高层次API，这些API位于TensorFlow之上，以增加从业者实现模型的便捷性和灵活性。完成后我也会在这里添加一个链接。
- en: '**Related**:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容**：'
- en: '[5 EBooks to Read Before Getting into A Machine Learning Career](/2016/10/5-free-ebooks-machine-learning-career.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[进入机器学习职业前阅读的5本电子书](/2016/10/5-free-ebooks-machine-learning-career.html)'
- en: '[7 Steps to Understanding Deep Learning](/2016/01/seven-steps-deep-learning.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解深度学习的7个步骤](/2016/01/seven-steps-deep-learning.html)'
- en: '[Machine Learning Key Terms, Explained](/2016/05/machine-learning-key-terms-explained.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习关键术语解释](/2016/05/machine-learning-key-terms-explained.html)'
- en: More On This Topic
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用管道编写干净的Python代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[建立一个稳固的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么让Python成为初创公司的理想编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家都应了解的三个R库（即使你使用Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并通过找到目标…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学统计的最佳资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
