- en: Multiscale Methods and Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多尺度方法与机器学习
- en: 原文：[https://www.kdnuggets.com/2018/03/multiscale-methods-machine-learning.html](https://www.kdnuggets.com/2018/03/multiscale-methods-machine-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/03/multiscale-methods-machine-learning.html](https://www.kdnuggets.com/2018/03/multiscale-methods-machine-learning.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: Multiscale methods, in which a dataset is viewed and analyzed at different scales,are
    becoming more commonplace in machine learning recently and are proving to be valuable
    tools. At their core, multiscale methods capture the local geometry of neighborhoods
    defined by a series of distances between points or sets of nearest neighbors.
    This is a bit like viewing a part of a slide through a series of microscope resolutions.
    At high resolutions, very small features are captured in a small space within
    the sample. At lower resolutions, more of the slide is visible, and a person can
    investigate bigger features.Main advantages of multiscale methods include improved
    performance relative to state-of-the-art methods and dramatic reductions in necessary
    sample size to achieve these results.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 多尺度方法，在不同尺度下查看和分析数据集，最近在机器学习中变得越来越普遍，并且被证明是有价值的工具。多尺度方法的核心是捕捉由一系列距离或最近邻集合定义的局部几何。这有点像通过一系列显微镜分辨率查看幻灯片的一部分。在高分辨率下，非常小的特征会在样本的一个小区域内被捕捉。在低分辨率下，更多的幻灯片区域可见，可以调查更大的特征。多尺度方法的主要优点包括相对于最新方法的性能提升和实现这些结果所需样本量的显著减少。
- en: 'As a simple example, consider taking the mean of nearest neighbors within a
    number line or two-dimensional space. When k increases, the mean of point sets
    on the number line becomes more spread out, with farther neighbors pulling the
    mean more than nearer neighbors. Nearest neighbors that are farther away from
    an existing set of points tend to drag the mean further than added points that
    are closer to the existing set of points. In the number line example, say the
    points sit at 1, 2, 4, 5, 9, 11, and 14\. Consider the set of the left most point
    (at 1) and its 3 nearest neighbors: 2, 4, and 5\. The mean for this set is 3,
    which has 2 points with higher values and 2 points with lower values. When 9 is
    added to the set as a 4^(th) nearest neighbor of point 1, the mean jumps to 4.2,
    which is skewed towards point 9 rather than the 4 points whose values are close
    to one another. Examining the mean of point 1’s 3 nearest neighbors reveals a
    set of points with fairly similar values and variance among values. However, examining
    the 4 nearest neighbors of point 1 captures the presence of an outlier. The KNN
    mean function with a single value of k fails to capture some of these important
    local properties that emerge at different scales, and important information is
    lost.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个简单的例子，考虑在一个数轴或二维空间中取最近邻的均值。当k增加时，数轴上点集的均值会变得更加分散，较远的邻居对均值的影响大于较近的邻居。距离现有点集较远的最近邻往往会将均值拖得更远，而距离现有点集较近的点对均值的影响则较小。在数轴示例中，假设点位于1、2、4、5、9、11和14。考虑最左边的点（1）及其3个最近邻：2、4和5。这个集合的均值是3，其中有2个点的值较高，2个点的值较低。当9作为第4个最近邻被添加到集合中时，均值跳升到4.2，这个均值倾向于9点，而不是4个值接近的点。检查点1的3个最近邻的均值揭示了一组值和方差相似的点。然而，检查点1的4个最近邻则捕捉到了一个异常值。单一k值的KNN均值函数无法捕捉到不同尺度下出现的一些重要局部特性，从而丢失了重要信息。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的IT需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '![Multiscale methods](../Images/c4b6f227c0a5700e4497ef647a6076cf.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![多尺度方法](../Images/c4b6f227c0a5700e4497ef647a6076cf.png)'
- en: Not all multiscale methods rely on KNN-defined scales, but they do employ similar
    principles that allow the method to learn multiscale features within a dataset.
    Grouping points within a series of distances, providing a series of convolutions
    of different scales, and employing hierarchies of clusters are common choices
    in multiscale methods. A recent deep learning paper by Pelt & Sethian (2017) based
    on multiscale methods was able to learn image segmentation and image classification
    on training sets as small as 6 images with high accuracy (>90%); this algorithm
    significantly reduced tuning requirements of the deep learning framework, as well.
    Another deep learning paper (Xiao et al., 2018) blends global image characteristics
    with a series of local extractions (in convolution layers, shown below) to capture
    many different scales of image features within a single deep learning framework.
    This algorithm consistently outperformed state-of-the-art deep learning algorithms
    on several benchmark datasets, suggesting that multiscale methods can improve
    even state-of-the-art convolution networks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有的多尺度方法都依赖于 KNN 定义的尺度，但它们确实采用了类似的原理，使得方法能够在数据集中学习多尺度特征。在一系列距离内对点进行分组、提供不同尺度的卷积系列以及采用层次化的簇是多尺度方法中的常见选择。Pelt
    和 Sethian（2017）的一篇基于多尺度方法的深度学习论文能够在仅有 6 张图像的小训练集上实现高准确率（>90%）的图像分割和图像分类；这一算法显著减少了深度学习框架的调优要求。另一篇深度学习论文（Xiao
    et al., 2018）将全局图像特征与一系列局部提取（在卷积层中，如下所示）结合在一起，以捕捉单一深度学习框架内的多种图像特征尺度。该算法在多个基准数据集上持续优于最先进的深度学习算法，这表明多尺度方法可以改善即使是最先进的卷积网络。
- en: '![KNN Multiscale methods](../Images/796baf813395aac399997d6b283897e7.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![KNN 多尺度方法](../Images/796baf813395aac399997d6b283897e7.png)'
- en: Recent work on k-nearest neighbor (KNN) regression and classification ensembles
    using varied neighborhood size have shown dramatic improvement over not only the
    KNN algorithms with a single value for k but also other machine learning methods.
    The multiscale tools of topological data analysis (TDA), including persistent
    homology, have aided analysis of small datasets, images, videos, and other types
    of data, providing data scientists and machine learning researchers with a robust,
    general unsupervised learning tool. A recently-added multiscale component on the
    TDA tool Mapper improved theoretical stability of the algorithm and yielded consistent
    results.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近关于 k 最近邻（KNN）回归和分类集成的研究显示，使用不同邻域大小的改进不仅显著优于具有单一 k 值的 KNN 算法，还优于其他机器学习方法。拓扑数据分析（TDA）的多尺度工具，包括持久同调，已帮助分析小数据集、图像、视频以及其他类型的数据，为数据科学家和机器学习研究人员提供了一种强大而通用的无监督学习工具。最近在
    TDA 工具 Mapper 上添加的多尺度组件提高了算法的理论稳定性，并产生了一致的结果。
- en: Alternatives to multiscale methods exist, but they often come with a cost. Rather
    than improve performance and compute times with a small training sample, many
    provide either improved performance or reduction in necessary sample size. Bayesian
    methods, the main competitor of multiscale methods in recent years, have reduced
    the necessary sample size for good performance of deep learning algorithms, but
    they come with a high computational cost that may not be appealing for online
    algorithm use or quick turnaround of analyses in industry. These methods also
    require a high degree of statistical expertise that many practitioners may not
    have. Deep learning has become a ubiquitous, general tool in recent years, but
    its power seems to lie in the asymptotic properties. Rather than level-off in
    performance when a large enough sample size is reached for the algorithm to converge,
    deep learning algorithms continue to improve with further increases in sample
    size. This is desirable when a lot of data is available, but many industry use
    cases and research problems involve small datasets or limited examples for a given
    class.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多种替代多尺度方法的选择，但它们往往伴随着一定的成本。与通过小样本改善性能和计算时间不同，许多方法提供的要么是提高性能，要么是减少所需样本大小。贝叶斯方法，在近年来作为多尺度方法的主要竞争对手，已经减少了深度学习算法良好性能所需的样本大小，但它们具有高的计算成本，这可能不适合在线算法使用或行业分析的快速回转。这些方法还需要高度的统计专业知识，许多从业者可能并不具备。近年来，深度学习已成为一种无处不在的通用工具，但其能力似乎在渐近性质上。当有大量数据可用时，深度学习算法继续随着样本大小的进一步增加而改善。这在许多行业用例和研究问题涉及小数据集或给定类别的有限示例时是理想的。
- en: It seems that local geometry matters. What is relevant and important for one
    neighborhood may not be relevant or important for a larger neighborhood surrounding
    the original one, and examining multiple scales can help capture all the important
    features, rather than the neighborhood yielding the best subset of features. Given
    the success of multiscale methods on a variety of learning tasks and their enhancement
    many types of algorithms, it is likely that machine learning will see more algorithms
    incorporating multiscale subsets in the coming years.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来局部几何很重要。对于一个邻域而言是相关和重要的内容，对于周围较大的邻域可能就不是那么相关或重要了，而检查多个尺度可以帮助捕捉所有重要特征，而不是仅得到最佳特征子集的邻域。鉴于多尺度方法在各种学习任务上的成功及其增强多种类型算法，可以预见在未来几年内机器学习将看到更多融合多尺度子集的算法。
- en: References
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: References
- en: 'Dey, T. K., Mémoli, F., & Wang, Y. (2016). Multiscale mapper: Topological summarization
    via codomain covers. In *Proceedings of the twenty-seventh annual acm-siam symposium
    on discrete algorithms* (pp. 997-1013). Society for Industrial and Applied Mathematics.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Dey, T. K., Mémoli, F., & Wang, Y. (2016). 多尺度映射器：通过值域覆盖进行拓扑总结。在*第二十七届年度ACM-SIAM离散算法研讨会论文集*
    (pp. 997-1013). Society for Industrial and Applied Mathematics.
- en: 'Farrelly, C. M. (2017). KNN Ensembles for Tweedie Regression: The Power of
    Multiscale Neighborhoods. *arXiv preprint arXiv:1708.02122*.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Farrelly, C. M. (2017). KNN集成用于Tweedie回归：多尺度邻域的力量。*arXiv预印本 arXiv:1708.02122*.
- en: Pelt, D. M., & Sethian, J. A. (2017). A mixed-scale dense convolutional neural
    network for image analysis. *Proceedings of the National Academy of Sciences*,
    201715832.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Pelt, D. M., & Sethian, J. A. (2017). 用于图像分析的混合尺度密集卷积神经网络。*国家科学院院刊*, 201715832.
- en: Xia, K., & Wei, G. W. (2014). Persistent homology analysis of protein structure,
    flexibility, and folding. *International journal for numerical methods in biomedical
    engineering*, *30*(8), 814-844.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Xia, K., & Wei, G. W. (2014). 蛋白质结构、柔性和折叠的持久同调分析。*国际生物医学工程数值方法期刊*, *30*(8),
    814-844.
- en: 'Xiao, F., Deng, W., Peng, L., Cao, C., Hu, K., &Gao, X. (2018). MSDNN: Multi-Scale
    Deep Neural Network for Salient Object Detection. *arXiv preprint arXiv:1801.04187*.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Xiao, F., Deng, W., Peng, L., Cao, C., Hu, K., &Gao, X. (2018). MSDNN：用于显著目标检测的多尺度深度神经网络。*arXiv预印本
    arXiv:1801.04187*.
- en: '**Bio: [Colleen M. Farrelly](https://www.linkedin.com/in/colleenmfarrelly)**
    is a data scientist whose industry experience includes positions related to healthcare,
    education, biotech, marketing, and finance. Her areas of research include topology/topological
    data analysis, ensemble learning, nonparametric statistics, manifold learning,
    and explaining mathematics to lay audiences (https://www.quora.com/profile/Colleen-Farrelly-1).
    When she isn’t doing data science, she is a poet and author (https://www.amazon.com/Colleen-Farrelly/e/B07832WQG7).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介： [科琳·M·法雷利](https://www.linkedin.com/in/colleenmfarrelly)** 是一位数据科学家，她的行业经验涵盖了医疗保健、教育、生物技术、市场营销和金融等领域。她的研究领域包括拓扑学/拓扑数据分析、集成学习、非参数统计、流形学习以及向普通读者解释数学（https://www.quora.com/profile/Colleen-Farrelly-1）。当她不从事数据科学工作时，她还是一位诗人和作者（https://www.amazon.com/Colleen-Farrelly/e/B07832WQG7）。'
- en: '**Related:**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[A Tour of The Top 10 Algorithms for Machine Learning Newbies](https://www.kdnuggets.com/2018/02/tour-top-10-algorithms-machine-learning-newbies.html)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习新手的十大算法巡礼](https://www.kdnuggets.com/2018/02/tour-top-10-algorithms-machine-learning-newbies.html)'
- en: '[Deep Misconceptions About Deep Learning](https://www.kdnuggets.com/2018/03/deep-misconceptions-about-deep-learning.html)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于深度学习的深刻误解](https://www.kdnuggets.com/2018/03/deep-misconceptions-about-deep-learning.html)'
- en: '[How to do Machine Learning Efficiently](https://www.kdnuggets.com/2018/03/machine-learning-efficiently.html)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何高效进行机器学习](https://www.kdnuggets.com/2018/03/machine-learning-efficiently.html)'
- en: More On This Topic
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题更多内容
- en: '[Alternative Feature Selection Methods in Machine Learning](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的替代特征选择方法](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
- en: '[Understanding Python''s Iteration and Membership: A Guide to…](https://www.kdnuggets.com/understanding-pythons-iteration-and-membership-a-guide-to-__contains__-and-__iter__-magic-methods)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解Python的迭代和成员关系：__contains__和__iter__魔法方法指南](https://www.kdnuggets.com/understanding-pythons-iteration-and-membership-a-guide-to-__contains__-and-__iter__-magic-methods)'
- en: '[Centroid Initialization Methods for k-means Clustering](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[k-means聚类的质心初始化方法](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
- en: '[Python String Methods](https://www.kdnuggets.com/2022/12/python-string-methods.html)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python字符串方法](https://www.kdnuggets.com/2022/12/python-string-methods.html)'
- en: '[Data Science Methods Drive Business Success](https://www.kdnuggets.com/2023/10/nwu-data-science-methods-drive-business-success)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学方法驱动商业成功](https://www.kdnuggets.com/2023/10/nwu-data-science-methods-drive-business-success)'
- en: '[11 Python Magic Methods Every Programmer Should Know](https://www.kdnuggets.com/11-python-magic-methods-every-programmer-should-know)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个程序员都应该知道的11个Python魔法方法](https://www.kdnuggets.com/11-python-magic-methods-every-programmer-should-know)'
