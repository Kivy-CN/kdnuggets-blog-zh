- en: Building an image search service from scratch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始构建图像搜索服务
- en: 原文：[https://www.kdnuggets.com/2019/01/building-image-search-service-from-scratch.html](https://www.kdnuggets.com/2019/01/building-image-search-service-from-scratch.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/01/building-image-search-service-from-scratch.html](https://www.kdnuggets.com/2019/01/building-image-search-service-from-scratch.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](/2019/01/building-image-search-service-from-scratch.html?page=2#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](/2019/01/building-image-search-service-from-scratch.html?page=2#comments)'
- en: '**By [Emmanuel Ameisen](https://www.linkedin.com/in/ameisen/), Head of AI at
    Insight Data Science**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Emmanuel Ameisen](https://www.linkedin.com/in/ameisen/)，Insight Data Science
    的 AI 负责人**'
- en: '![Figure](../Images/99a2bb1a64166a78df8fbd109586dda2.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/99a2bb1a64166a78df8fbd109586dda2.png)'
- en: Teaching computers to look at pictures the way we do
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 教导计算机以我们看的方式看图像
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速迈向网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '***Want to learn applied Artificial Intelligence from top professionals in
    Silicon Valley or New York?***Learn more about the [Artificial Intelligence](http://insightdata.ai/?utm_source=representations&utm_medium=blog&utm_content=top)
    program at Insight.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '***想要从硅谷或纽约的顶尖专业人士那里学习应用人工智能吗？*** 了解更多关于[人工智能](http://insightdata.ai/?utm_source=representations&utm_medium=blog&utm_content=top)项目的内容。'
- en: '***Are you a company working in AI and would like to get involved in the Insight
    AI Fellows Program?**** Feel free to *[*get in touch*](http://insightdatascience.com/partnerships?utm_source=representations&utm_medium=blog&utm_content=top)*.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '***你是从事 AI 领域的公司并希望参与 Insight AI Fellows 计划吗？*** 请随时*[*联系*](http://insightdatascience.com/partnerships?utm_source=representations&utm_medium=blog&utm_content=top)*。*'
- en: '*For more content like this, follow *[*Insight*](https://twitter.com/InsightDataSci)* and *[*Emmanuel*](https://twitter.com/EmmanuelAmeisen)* on
    Twitter.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*想要获取更多类似内容，请关注*[*Insight*](https://twitter.com/InsightDataSci)*和*[*Emmanuel*](https://twitter.com/EmmanuelAmeisen)* 在
    Twitter 上。*'
- en: Why similarity search?
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么要进行相似性搜索？
- en: '*An image is worth a thousand words, and even more lines of code.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*一张图胜千言，甚至更多的代码行。*'
- en: Many products **fundamentally appeal to our perception**. When browsing through
    outfits on clothing sites, looking for a vacation rental on Airbnb, or choosing
    a pet to adopt, the way something looks is often an important factor in our decision.
    The way we perceive things is a strong predictor of what kind of items we will
    like, and therefore a valuable quality to measure.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 许多产品**从根本上吸引我们的感知**。无论是在浏览服装网站上的服装、寻找 Airbnb 上的度假租赁，还是选择一个要收养的宠物，事物的外观通常是我们决策中的一个重要因素。我们对事物的感知是我们喜欢哪种物品的强预测指标，因此是一个宝贵的衡量标准。
- en: However, making computers understand images the way humans do has been a computer
    science challenge for quite some time. Since 2012, Deep Learning has slowly started
    overtaking classical methods such as [Histograms of Oriented Gradients](https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients) (HOG)
    in perception tasks like image classification or object detection. One of the
    main reasons often credited for this shift is deep learning’s ability to automatically **extract
    meaningful representations** when trained on a large enough dataset.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，让计算机像人类一样理解图像一直是计算机科学的一个挑战。自 2012 年以来，深度学习已经开始逐渐取代经典方法，如[梯度方向直方图](https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients)（HOG），在图像分类或物体检测等感知任务中。这一变化的主要原因之一通常归功于深度学习在大数据集上训练时能自动**提取有意义的表示**。
- en: '![](../Images/3ff1a1e4787003e862ab6ca309ad7c8b.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ff1a1e4787003e862ab6ca309ad7c8b.png)'
- en: Visual search at Pinterest
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Pinterest 的视觉搜索
- en: This is why many teams — like at [Pinterest](https://medium.com/@Pinterest_Engineering/introducing-a-new-way-to-visually-search-on-pinterest-67c8284b3684), [StitchFix](https://multithreaded.stitchfix.com/blog/2015/09/17/deep-style/),
    and [Flickr](http://code.flickr.net/2017/03/07/introducing-similarity-search-at-flickr/) — started
    using Deep Learning to learn representations of their images, and **provide recommendations **based
    on the content users find visually pleasing. Similarly, Fellows at [Insight](http://insightdatascience.com/) have
    used deep learning to build models for applications such as helping people find
    cats to adopt, recommending sunglasses to buy, and searching for art styles.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么许多团队——如[Pinterest](https://medium.com/@Pinterest_Engineering/introducing-a-new-way-to-visually-search-on-pinterest-67c8284b3684)、[StitchFix](https://multithreaded.stitchfix.com/blog/2015/09/17/deep-style/)和[Flickr](http://code.flickr.net/2017/03/07/introducing-similarity-search-at-flickr/)——开始使用深度学习来学习他们图像的表示，并**根据用户认为视觉上令人愉悦的内容提供推荐**。类似地，[Insight](http://insightdatascience.com/)的研究员们使用深度学习构建了各种应用的模型，例如帮助人们找到待领养的猫、推荐购买太阳镜和搜索艺术风格。
- en: 'Many recommendation systems are based on [collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering):
    leveraging user correlations to make recommendations (“users that liked the items
    you have liked have also liked…”). However, these models require a **significant
    amount of data** to be accurate, and [struggle](https://en.wikipedia.org/wiki/Cold_start_%28computing%29) to
    handle **new items**that have not yet been viewed by anyone. Item representation
    can be used in what’s called [content-based](https://en.wikipedia.org/wiki/Recommender_system#Content-based_filtering) recommendation
    systems, which do not suffer from the problem above.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 许多推荐系统基于[协同过滤](https://en.wikipedia.org/wiki/Collaborative_filtering)：利用用户之间的关联来做出推荐（“喜欢你喜欢的物品的用户也喜欢…”）。然而，这些模型需要**大量数据**才能准确，并且[难以](https://en.wikipedia.org/wiki/Cold_start_%28computing%29)处理**新物品**，即那些尚未被任何人查看的物品。项目表示可以用于所谓的[基于内容的](https://en.wikipedia.org/wiki/Recommender_system#Content-based_filtering)推荐系统，这些系统不会遇到上述问题。
- en: In addition, these representations allow consumers to **efficiently search**
    photo libraries for images that are similar to the selfie they just took (querying
    by image), or for photos of particular items such as cars (querying by text).
    Common examples of this include Google Reverse Image Search, as well as Google
    Image Search.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这些表示允许用户**高效地搜索**照片库，找到与他们刚拍的自拍（按图像查询）相似的图像，或找到特定物品如汽车的照片（按文本查询）。常见的例子包括谷歌反向图像搜索和谷歌图像搜索。
- en: Based on our experience providing technical mentorship for many semantic understanding
    projects, we wanted to write a tutorial on how you would go about **building your
    own representations**, both for image and text data, and **efficiently do similarity
    search**. By the end of this post, you should be able to build a quick semantic
    search model from scratch, no matter the size of your dataset.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们在许多语义理解项目中提供技术指导的经验，我们想写一篇教程，讲解如何**构建你自己的表示**，无论是针对图像还是文本数据，以及如何**高效地进行相似性搜索**。阅读完这篇文章后，无论数据集的大小，你都应该能够从零开始构建一个快速的语义搜索模型。
- en: '***This post is accompanied by ***[***an annotated code notebook***](http://insight.streamlit.io/0.13.3-8ErS/index.html?id=QAKzY9mLjr4WbTCgxz3XBX)*** using ***[***streamlit***](http://streamlit.io/)
    ***and a ***[***self-standing codebase***](https://github.com/hundredblocks/semantic-search)*** demonstrating
    and applying all these techniques. Feel free to run the code and follow along!***'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '***这篇文章附有***[***一个带注释的代码笔记本***](http://insight.streamlit.io/0.13.3-8ErS/index.html?id=QAKzY9mLjr4WbTCgxz3XBX)***，使用***[***streamlit***](http://streamlit.io/)
    ***和一个***[***独立的代码库***](https://github.com/hundredblocks/semantic-search)***，演示和应用所有这些技术。欢迎运行代码并跟随学习！***'
- en: What’s our plan?
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们的**计划**是什么？
- en: '**A break to chat about optimization**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**谈谈优化的休息时间**'
- en: In machine learning, just like in software engineering, there are many ways
    to tackle a problem, each with different tradeoffs. If we are doing research or
    local prototyping, we can get away with very inefficient solutions. But if we
    are building an image similarity search engine that needs to be maintainable and
    scalable, we have to consider both how we can **adapt to data evolution**, and **how
    fast our model can run**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，就像在软件工程中一样，有许多方法来解决一个问题，每种方法都有不同的权衡。如果我们在做研究或本地原型制作时，我们可以使用非常低效的解决方案。但如果我们要构建一个需要可维护和可扩展的图像相似性搜索引擎，我们必须考虑如何**适应数据演变**以及**模型运行的速度**。
- en: 'Let’s imagine a few approaches:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象几种方法：
- en: '![](../Images/a427ef58daaf78749bd20d1d063f63e6.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a427ef58daaf78749bd20d1d063f63e6.png)'
- en: Workflow for approach 1
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 方法1的工作流程
- en: 1/ We build an end-to-end model that is trained on all our images to take an
    image as an input, and output a similarity score over all of our images. Predictions
    happen quickly (one forward pass), but we would need to** train a new model** every
    time we add a new image. We would also quickly reach a state with so many classes
    that it would be extremely **hard to optimize** it correctly. This approach is
    fast, but does not scale to large datasets. In addition, we would have to label
    our dataset by hand with image similarities, which could be extremely time consuming.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 1/ 我们构建一个端到端的模型，该模型在所有图像上进行训练，接受一张图像作为输入，并输出在所有图像上的相似度分数。预测发生得很快（一次前向传递），但每次添加新图像时，我们需要**训练一个新模型**。我们还会迅速达到一个类过多的状态，从而使其**优化起来非常困难**。这种方法速度快，但不适用于大型数据集。此外，我们还必须手动标注图像相似性，这可能非常耗时。
- en: '![](../Images/ddbe731e434abeb62b80a84f4113608a.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddbe731e434abeb62b80a84f4113608a.png)'
- en: Workflow for approach 2
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 方法2的工作流程
- en: 2/ Another approach is to build a model that takes in two images, and outputs
    a pairwise similarity score between 0 and 1 ([Siamese Networks](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)[PDF],
    for example). These models are accurate for large datasets, but lead to another
    scalability issue. We usually want to find a similar image by **looking through
    a vast collection of images**, so we have to run our similarity model once for
    each image pair in our dataset. If our model is a CNN, and we have more than a
    dozen images, this becomes too slow to even be considered. In addition, this only
    works for image similarity, not text search. This method scales to large datasets,
    but is slow.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 2/ 另一种方法是构建一个模型，该模型接受两张图像，并输出一个介于0和1之间的配对相似度分数（例如，[Siamese Networks](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)[PDF]）。这些模型在大型数据集上准确，但导致另一个可扩展性问题。我们通常希望通过**浏览大量图像**来找到相似图像，因此我们必须对数据集中每对图像运行一次相似度模型。如果我们的模型是CNN，而我们有十几张图像，这变得太慢而无法考虑。此外，这只适用于图像相似性，不适用于文本搜索。这种方法适用于大型数据集，但速度较慢。
- en: '![](../Images/49246a68255663d7d010e7b1b53a460d.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49246a68255663d7d010e7b1b53a460d.png)'
- en: Workflow for approach 3
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 方法3的工作流程
- en: 3/ There is a simpler method, which is similar to [word embeddings](https://en.wikipedia.org/wiki/Word_embedding).
    If we find an **expressive vector representation, or embedding **for images, we
    can then calculate their similarity by looking at **how close their vectors are
    to each other**. This type of search is a common problem that is well studied,
    and many libraries implement fast solutions (we will use [Annoy](https://github.com/spotify/annoy) here).
    In addition, if we calculate these vectors for all images in our database ahead
    of time, this approach is both fast (one forward pass, and an efficient similarity
    search), and scalable. Finally, if we manage to find **common embeddings** for
    our images and our words, we could use them to do text to image search!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 3/ 有一种更简单的方法，类似于**[词嵌入](https://en.wikipedia.org/wiki/Word_embedding)**。如果我们为图像找到**表达性向量表示或嵌入**，则可以通过查看**向量彼此的接近程度**来计算它们的相似性。这种类型的搜索是一个常见问题，已得到充分研究，许多库实现了快速解决方案（我们将在这里使用[Annoy](https://github.com/spotify/annoy)）。此外，如果我们提前计算数据库中所有图像的这些向量，这种方法既快速（一次前向传递，且高效的相似性搜索），又具有可扩展性。最后，如果我们设法找到图像和词汇的**共同嵌入**，我们可以用它们来进行文本到图像的搜索！
- en: Because of its simplicity and efficiency, the third method will be the focus
    of this post.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其简单性和高效性，第三种方法将是本文的重点。
- en: '**How do we get there?**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们怎么到达那里？**'
- en: So, how do we actually use **deep learning representations** to create a **search
    engine?**
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何实际利用**深度学习表示**来创建一个**搜索引擎？**
- en: 'Our final goal is to have a search engine that can take in images and output
    either similar images or tags, and take in text and output similar words, or images.
    To get there, we will go through three successive steps:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终目标是拥有一个能够接受图像并输出相似图像或标签，接受文本并输出相似词或图像的搜索引擎。为此，我们将经历三个连续的步骤：
- en: Searching for **similar images to an input image** (Image → Image)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索**与输入图像相似的图像**（图像 → 图像）
- en: Searching for **similar words to an input word** (Text → Text)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索**与输入词相似的词**（文本 → 文本）
- en: Generating **tags for images**, and **searching images using tex**t (Image ↔
    Text)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成**图像标签**，以及**使用文本搜索图像**（图像 ↔ 文本）
- en: To do this, we will use **embeddings**, vector representations of images and
    text. Once we have embeddings, searching simply becomes a matter of finding vectors
    close to our input vector.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们将使用**嵌入**，即图像和文本的向量表示。一旦我们有了嵌入，搜索就变成了寻找接近我们输入向量的向量的问题。
- en: The way we find these is by calculating the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between
    our image embedding, and embeddings for other images. Similar images will have
    similar embeddings, meaning a **high cosine similarity between embeddings**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过计算我们的图像嵌入与其他图像的嵌入之间的[余弦相似度](https://en.wikipedia.org/wiki/Cosine_similarity)来找到这些。相似的图像将具有相似的嵌入，这意味着嵌入之间的**高余弦相似度**。
- en: Let’s start with a dataset to experiment with.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个数据集开始实验。
- en: Dataset
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集
- en: '**Images**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**图像**'
- en: Our image dataset consists of a total of a **1000 images**, divided in 20 classes
    with 50 images for each. This dataset can be found [here](http://vision.cs.uiuc.edu/pascal-sentences/).
    Feel free to use the script in the linked code to automatically download all image
    files. *Credit to Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier
    for the dataset.*
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的图像数据集总共有**1000 张图像**，分为 20 个类别，每个类别 50 张图像。这个数据集可以在[这里](http://vision.cs.uiuc.edu/pascal-sentences/)找到。可以使用链接中的脚本自动下载所有图像文件。*感谢
    Cyrus Rashtchian、Peter Young、Micah Hodosh 和 Julia Hockenmaier 提供的数据集。*
- en: 'This dataset contains a category and a set of captions for every image. To
    make this problem harder, and to show how well our approach generalizes, we will **only
    use the categories**, and disregard the captions. We have a total of 20 classes,
    which I’ve listed out below:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含每张图像的类别和一组描述。为了增加问题的难度，并展示我们的方法如何泛化，我们将**仅使用类别**，忽略描述。我们总共有 20 个类别，下面列出了：
- en: '`aeroplane` `bicycle` `bird` `boat` `bottle` `bus` `car` `cat` `chair` `cow`
    `dining_table` `dog` `horse` `motorbike` `person` `potted_plant` `sheep` `sofa`
    `train` `tv_monitor`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`aeroplane` `bicycle` `bird` `boat` `bottle` `bus` `car` `cat` `chair` `cow`
    `dining_table` `dog` `horse` `motorbike` `person` `potted_plant` `sheep` `sofa`
    `train` `tv_monitor`'
- en: '![](../Images/b9d72a5ca3bde69eba594c1d7642d0d0.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9d72a5ca3bde69eba594c1d7642d0d0.png)'
- en: Image examples. As we can see, the labels are quite noisy.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图像示例。如我们所见，标签非常嘈杂。
- en: 'We can see our labels are pretty **noisy**: many photos contain multiple categories,
    and the label is not always from the most prominent one. For example, on the bottom
    right, the image is labeled `chair` and not `person`even though 3 people stand
    in the center of the image, and the chair is barely visible.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的标签非常**嘈杂**：许多照片包含多个类别，标签不总是来自最突出的类别。例如，在右下角，图像被标记为`chair`，而不是`person`，尽管有3个人站在图像中心，椅子几乎不可见。
- en: '**Text**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本**'
- en: In addition, we load word embeddings that have been pre-trained on Wikipedia
    (this tutorial will use the ones from the [GloVe](https://nlp.stanford.edu/projects/glove/) model).
    We will use these vectors to incorporate text to our semantic search. For more
    information on how these word vectors work, see [Step 7](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e) of
    our NLP tutorial.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们加载了在 Wikipedia 上预训练的词嵌入（本教程将使用来自[GloVe](https://nlp.stanford.edu/projects/glove/)模型的词嵌入）。我们将使用这些向量将文本纳入我们的语义搜索。有关这些词向量如何工作的更多信息，请参见我们
    NLP 教程的[第 7 步](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)。
- en: Image -> Image
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像 -> 图像
- en: '*Starting simple*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*从简单开始*'
- en: We are now going to load a model that was **pre-trained** on a large data set
    ([Imagenet](http://www.image-net.org/)), and is freely available online. We use
    VGG16 here, but this approach would work with any recent CNN architecture. We
    use this model to generate **embeddings** for our images.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将加载一个在大数据集上**预训练**的模型（[Imagenet](http://www.image-net.org/)），该模型在网上免费提供。我们在这里使用
    VGG16，但这种方法也适用于任何最近的 CNN 架构。我们使用这个模型为我们的图像生成**嵌入**。
- en: '![](../Images/cf1d7ca19487c4ecfd529a923d805dca.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf1d7ca19487c4ecfd529a923d805dca.png)'
- en: VGG16 (credit to Data Wow Blog)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16（归功于 Data Wow Blog）
- en: What do we mean by generating embeddings? We will use our pre-trained model **up
    to the penultimate layer**, and store the value of the activations. In the image
    below, this is represented by the embedding layer highlighted in green, which
    is before the final classification layer.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所说的生成嵌入（embeddings）是什么意思？我们将使用我们**直到倒数第二层**的预训练模型，并存储激活值。在下图中，这由用绿色突出显示的嵌入层表示，它位于最终分类层之前。
- en: '![](../Images/b48af203c047c628d0d3b0cbe99dafb1.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b48af203c047c628d0d3b0cbe99dafb1.png)'
- en: For our embeddings, we use the layer before the final classification layer.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的嵌入，我们使用的是最后分类层之前的层。
- en: Once we’ve used the model to generate image features, we can then store them
    to disk and re-use them **without needing to do inference again**! This is one
    of the reasons that embeddings are so popular in practical applications, as they
    allow for huge efficiency gains. On top of storing them to disk, we will build
    a **fast index** of the embeddings using [Annoy](https://github.com/spotify/annoy),
    which will allow us to very quickly find the nearest embeddings to any given embedding.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们使用模型生成了图像特征，我们可以将它们存储到磁盘并重新使用**而无需再次推理**！这也是嵌入在实际应用中如此受欢迎的原因之一，因为它们允许巨大的效率提升。在将它们存储到磁盘的基础上，我们将使用[Annoy](https://github.com/spotify/annoy)构建一个**快速索引**，这将使我们能够非常快速地找到任何给定嵌入的最近嵌入。
- en: 'Below are our embeddings. Each image is now represented by a sparse vector
    of size 4096. *Note: the reason the vector is sparse is that we have taken the
    values after the activation function, which zeros out negatives.*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们的嵌入。每张图像现在由一个大小为4096的稀疏向量表示。*注意：向量之所以稀疏，是因为我们在激活函数之后取值，该函数将负值置为零。*
- en: '![](../Images/9e88bd3f556fc3351ff4b2fcc6a7665c.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e88bd3f556fc3351ff4b2fcc6a7665c.png)'
- en: Image Embeddings
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图像嵌入
- en: '****Using our embeddings to search through images****'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '****使用我们的嵌入来搜索图像****'
- en: We can now simply take in an image, get its embedding, and look in our fast
    index to find similar embeddings, and thus similar images.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以简单地接收一张图像，获取它的嵌入，并在我们的快速索引中查找相似的嵌入，从而找到相似的图像。
- en: This is especially useful since image labels are often **noisy**, and there
    is more to an image than its label.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这特别有用，因为图像标签通常是**噪声**的，图像的内容远不止其标签。
- en: For example, in our dataset, we have both a class `cat`, and a class `bottle`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们的数据集中，我们有`cat`类别和`bottle`类别。
- en: Which class do you think this image is labeled as?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为这张图像被标记为哪个类别？
- en: '![](../Images/f4adf6aa30619bed0551ca62dceeeab9.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4adf6aa30619bed0551ca62dceeeab9.png)'
- en: Cat or Bottle? (Image rescaled to 224x224, which is what the neural network sees.)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 猫还是瓶子？（图像缩放至224x224，这是神经网络所见的。）
- en: The correct answer is **bottle** … This is an actual issue that comes up often
    in real datasets. Labeling images as unique categories is quite limiting, which
    is why we hope to use more nuanced representations. Luckily, this is exactly what **deep
    learning is good at**!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案是**瓶子**……这是实际数据集中经常出现的一个问题。将图像标记为唯一类别是相当有限的，这就是我们希望使用更细致表示的原因。幸运的是，这正是**深度学习擅长的**！
- en: Let’s see if our image search using embeddings does better than human labels.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看使用嵌入的图像搜索是否比人工标签更好。
- en: Searching for similar images to`dataset/bottle/2008_000112.jpg`…
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索类似于`dataset/bottle/2008_000112.jpg`的图像…
- en: '![](../Images/1513c1ebe450a4c5323649217f9ec054.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1513c1ebe450a4c5323649217f9ec054.png)'
- en: Great — we mostly get more images of **cats**, which seems very reasonable!
    Our pre-trained network has been trained on a wide variety of images, including
    cats, and so it is able to accurately find similar images, even though it has
    never been trained on this particular dataset before.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 很好——我们大多得到更多**猫**的图像，这似乎很合理！我们的预训练网络已经在各种图像上进行了训练，包括猫，因此即使它之前没有在这个特定的数据集上训练过，它也能准确找到类似的图像。
- en: However, one image in the middle of the bottom row shows a shelf of bottles.
    This approach perform wells to find similar images, in general, but sometimes
    we are only interested in **part of the image**.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，底行中间的一张图像展示了一个瓶子架。这个方法通常对查找类似图像表现良好，但有时我们只对**图像的一部分**感兴趣。
- en: For example, given an image of a cat and a bottle, we might be only interested
    in similar cats, not similar bottles.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，给定一张猫和一瓶子的图像，我们可能只对类似的猫感兴趣，而不是类似的瓶子。
- en: '**Semi-supervised search**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**半监督搜索**'
- en: A common approach to solve this issue is to use an **object detection** model
    first, detect our cat, and do image search on a cropped version of the original
    image.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种常见方法是首先使用**对象检测**模型，检测我们的猫，然后对裁剪后的原始图像进行图像搜索。
- en: This adds a huge computing overhead, which we would like to avoid if possible.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这会增加巨大的计算开销，如果可能的话，我们希望避免这种情况。
- en: There is a simpler “hacky” approach, which consists of **re-weighing** the activations.
    We do this by loading the last layer of weights we had initially discarded, and
    only use the weights tied to the index of the class we are looking for to re-weigh
    our embedding. This cool trick was initially brought to my attention by [Insight](http://insightdatascience.com/) Fellow [Daweon
    Ryu](https://www.linkedin.com/in/daweonryu/). For example, in the image below,
    we use the weights of the `Siamese cat` class to re-weigh the activations on our
    dataset (highlighted in green). Feel free to check out the attached notebook to
    see the implementation details.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种更简单的“黑科技”方法，就是**重新加权**激活值。我们通过加载最初被丢弃的最后一层权重，并仅使用与我们所寻找的类别索引相关的权重来重新加权我们的嵌入。这一酷炫技巧最初是由[Insight](http://insightdatascience.com/)的[Daweon
    Ryu](https://www.linkedin.com/in/daweonryu/)引起我的注意的。例如，在下图中，我们使用`Siamese cat`类别的权重来重新加权我们数据集上的激活（以绿色突出显示）。可以查看附带的笔记本，以了解实现细节。
- en: '![](../Images/a9273d9908a7a583903e6318b8b97a5d.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9273d9908a7a583903e6318b8b97a5d.png)'
- en: The hack to get weighted embeddings. The classification layer is shown for reference
    only.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 获取加权嵌入的技巧。分类层仅供参考。
- en: Let’s examine how this works by weighing our activations according to class `284` in
    Imagenet, `Siamese cat`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过根据Imagenet中的类别`284`（Siamese cat）来加权我们的激活，来研究一下这如何工作。
- en: Searching for similar images to`dataset/bottle/2008_000112.jpg` using weighted
    features…
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用加权特征搜索与`dataset/bottle/2008_000112.jpg`相似的图像…
- en: '![](../Images/e4c0c5e0d27fe2fd2c6b80ea47d6aa2f.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4c0c5e0d27fe2fd2c6b80ea47d6aa2f.png)'
- en: We can see that the search has been biased to look for **Siamese cat-like things**.
    We no longer show any bottles, which is great. You might however notice that our
    last image is of a sheep! This is very interesting, as biasing our model has led
    to a **different kind of error**, which is more appropriate for our current domain.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，搜索已经偏向于寻找**类似Siamese cat的事物**。我们不再显示任何瓶子，这很好。然而，你可能会注意到我们最后的图像是一只羊！这非常有趣，因为对我们模型的偏置导致了一种**不同类型的错误**，这更适合我们当前的领域。
- en: We have seen we can search for similar images in a **broad** way, or by **conditioning
    on a particular class** our model was trained on.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，我们可以以**广泛**的方式或通过**条件化于特定类别**来搜索相似图像，这取决于我们模型的训练方式。
- en: This is a great step forward, but since we are using a model **pre-trained on
    Imagenet**, we are thus limited to the 1000 **Imagenet classes**. These classes
    are far from all-encompassing (they lack a category for people, for example),
    so we would ideally like to find something more **flexible. **In addition, what
    if we simply wanted to search for cats **without providing an input image?**
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很大的进步，但由于我们使用的是**在Imagenet上预训练的模型**，因此我们受限于1000个**Imagenet类别**。这些类别远非包罗万象（例如，它们缺少对人的分类），所以我们理想的情况是找到一些更**灵活**的东西。此外，如果我们只是想搜索猫**而不提供输入图像**呢？
- en: In order to do this, we are going to use more than simple tricks, and leverage
    a model that can understand the semantic power of words.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们将使用比简单技巧更多的方法，并利用一个可以理解单词语义能力的模型。
- en: More On This Topic
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Building a Visual Search Engine - Part 2: The Search Engine](https://www.kdnuggets.com/2022/02/building-visual-search-engine-part-2.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建视觉搜索引擎 - 第2部分：搜索引擎](https://www.kdnuggets.com/2022/02/building-visual-search-engine-part-2.html)'
- en: '[Hyperparameter Tuning Using Grid Search and Random Search in Python](https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用网格搜索和随机搜索进行超参数调整](https://www.kdnuggets.com/2022/10/hyperparameter-tuning-grid-search-random-search-python.html)'
- en: '[Elevate Your Search Engine Skills with Uplimit''s Search with ML Course!](https://www.kdnuggets.com/2023/10/uplimit-elevate-your-search-engine-skills-search-with-ml-course)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过Uplimit的机器学习课程提升您的搜索引擎技能！](https://www.kdnuggets.com/2023/10/uplimit-elevate-your-search-engine-skills-search-with-ml-course)'
- en: '[Building a Visual Search Engine - Part 1: Data Exploration](https://www.kdnuggets.com/2022/02/building-visual-search-engine-part-1.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建视觉搜索引擎 - 第1部分：数据探索](https://www.kdnuggets.com/2022/02/building-visual-search-engine-part-1.html)'
- en: '[311 Call Centre Performance: Rating Service Levels](https://www.kdnuggets.com/2023/03/boxplot-outlier-311-call-center-performance.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[311呼叫中心性能：服务水平评分](https://www.kdnuggets.com/2023/03/boxplot-outlier-311-call-center-performance.html)'
- en: '[Machine Learning from Scratch: Decision Trees](https://www.kdnuggets.com/2022/11/machine-learning-scratch-decision-trees.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从零开始学习机器学习：决策树](https://www.kdnuggets.com/2022/11/machine-learning-scratch-decision-trees.html)'
