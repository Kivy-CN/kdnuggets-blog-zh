["```py\nimport dask_saturn\nfrom dask_saturn import SaturnCluster\n\ncluster = SaturnCluster(n_workers=4, scheduler_size='g4dnxlarge', worker_size='g4dn8xlarge')\nclient = Client(cluster)\nclient\n```", "```py\n[2020-10-15 18:52:56] INFO – dask-saturn | Cluster is ready\n```", "```py\ntorch.cuda.is_available() \n\n```", "```py\nTrue\n```", "```py\nclient.run(lambda: torch.cuda.is_available())\n```", "```py\n{‘tcp://10.0.24.217:45281’: True,\n‘tcp://10.0.28.232:36099’: True,\n‘tcp://10.0.3.136:40143’: True,\n‘tcp://10.0.3.239:40585’: True}\n```", "```py\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n```", "```py\n@dask.delayed\ndef preprocess(path, fs=__builtins__):\n    '''Ingest images directly from S3, apply transformations,\n    and extract the ground truth and image identifier. Accepts\n    a filepath. '''\n\n    transform = transforms.Compose([\n        transforms.Resize(256), \n        transforms.CenterCrop(250), \n        transforms.ToTensor()])\n\n    with fs.open(path, 'rb') as f:\n        img = Image.open(f).convert(\"RGB\")\n        nvis = transform(img)\n\n    truth = re.search('dogs/Images/n[0-9]+-([^/]+)/n[0-9]+_[0-9]+.jpg', path).group(1)\n    name = re.search('dogs/Images/n[0-9]+-[a-zA-Z-_]+/(n[0-9]+_[0-9]+).jpg', path).group(1)\n\n    return [name, nvis, truth]\n```", "```py\n3fpath = 's3://dask-datasets/dogs/Images/*/*.jpg'\n\nbatch_breaks = [list(batch) for batch in toolz.partition_all(60, s3.glob(s3fpath))]\n```", "```py\nimage_batches = [[preprocess(x, fs=s3) for x in y] for y in batch_breaks]\n```", "```py\n@dask.delayed\ndef reformat(batch):\n    flat_list = [item for item in batch]\n    tensors = [x[1] for x in flat_list]\n    names = [x[0] for x in flat_list]\n    labels = [x[2] for x in flat_list]\n    return [names, tensors, labels]\n\nimage_batches = [reformat(result) for result in image_batches]\n```", "```py\n@dask.delayed\ndef run_batch_to_s3(iteritem):\n    ''' Accepts iterable result of preprocessing, \n    generates inferences and evaluates. '''\n\n    with s3.open('s3://dask-datasets/dogs/imagenet1000_clsidx_to_labels.txt') as f:\n        classes = [line.strip() for line in f.readlines()]\n\n    names, images, truelabels = iteritem\n\n    images = torch.stack(images)\n... \n```", "```py\n...\n    resnet = models.resnet50(pretrained=True)\n    resnet = resnet.to(device)\n    resnet.eval()\n...\n```", "```py\n...\n    images = images.to(device)\n    pred_batch = resnet(images)\n...\n```", "```py\ndef evaluate_pred_batch(batch, gtruth, classes):\n    ''' Accepts batch of images, returns human readable predictions. '''\n    _, indices = torch.sort(batch, descending=True)\n    percentage = torch.nn.functional.softmax(batch, dim=1)[0] * 100\n\n    preds = []\n    labslist = []\n    for i in range(len(batch)):\n        pred = [(classes[idx], percentage[idx].item()) for idx in indices[i][:1]]\n        preds.append(pred)\n\n        labs = gtruth[i]\n        labslist.append(labs)\n\n    return(preds, labslist)\n```", "```py\npreds, labslist = evaluate_pred_batch(pred_batch, truelabels, classes)\n```", "```py\n...\n    for j in range(0, len(images)):\n        predicted = preds[j]\n        groundtruth = labslist[j]\n        name = names[j]\n        match = is_match(groundtruth, predicted)\n\n        outcome = {'name': name, 'ground_truth': groundtruth, 'prediction': predicted, 'evaluation': match}\n\n        # Write each result to S3 directly\n        with s3.open(f\"s3://dask-datasets/dogs/preds/{name}.pkl\", \"wb\") as f:\n            pickle.dump(outcome, f)\n...\n```", "```py\ndef evaluate_pred_batch(batch, gtruth, classes):\n    ''' Accepts batch of images, returns human readable predictions. '''\n    _, indices = torch.sort(batch, descending=True)\n    percentage = torch.nn.functional.softmax(batch, dim=1)[0] * 100\n\n    preds = []\n    labslist = []\n    for i in range(len(batch)):\n        pred = [(classes[idx], percentage[idx].item()) for idx in indices[i][:1]]\n        preds.append(pred)\n\n        labs = gtruth[i]\n        labslist.append(labs)\n\n    return(preds, labslist)\n\ndef is_match(la, ev):\n    ''' Evaluate human readable prediction against ground truth. \n    (Used in both methods)'''\n    if re.search(la.replace('_', ' '), str(ev).replace('_', ' ')):\n        match = True\n    else:\n        match = False\n    return(match)    \n\n@dask.delayed\ndef run_batch_to_s3(iteritem):\n    ''' Accepts iterable result of preprocessing, \n    generates inferences and evaluates. '''\n\n    with s3.open('s3://dask-datasets/dogs/imagenet1000_clsidx_to_labels.txt') as f:\n        classes = [line.strip() for line in f.readlines()]\n\n    names, images, truelabels = iteritem\n\n    images = torch.stack(images)\n\n    with torch.no_grad():\n        # Set up model\n        resnet = models.resnet50(pretrained=True)\n        resnet = resnet.to(device)\n        resnet.eval()\n\n        # run model on batch\n        images = images.to(device)\n        pred_batch = resnet(images)\n\n        #Evaluate batch\n        preds, labslist = evaluate_pred_batch(pred_batch, truelabels, classes)\n\n        #Organize prediction results\n        for j in range(0, len(images)):\n            predicted = preds[j]\n            groundtruth = labslist[j]\n            name = names[j]\n            match = is_match(groundtruth, predicted)\n\n            outcome = {'name': name, 'ground_truth': groundtruth, 'prediction': predicted, 'evaluation': match}\n\n            # Write each result to S3 directly\n            with s3.open(f\"s3://dask-datasets/dogs/preds/{name}.pkl\", \"wb\") as f:\n                pickle.dump(outcome, f)\n\n        return(names)\n```", "```py\nfutures = client.map(run_batch_to_s3, image_batches) \nfutures_gathered = client.gather(futures)\nfutures_computed = client.compute(futures_gathered, sync=False)\n```", "```py\nimport logging\n\nresults = []\nerrors = []\nfor fut in futures:\n    try:\n        result = fut.result()\n    except Exception as e:\n        errors.append(e)\n        logging.error(e)\n    else:\n        results.extend(result)\n```", "```py\nwith s3.open('s3://dask-datasets/dogs/preds/n02086240_1082.pkl', 'rb') as data:\n    old_list = pickle.load(data)\n    old_list\n```", "```py\n{‘name’: ‘n02086240_1082’,\n‘ground_truth’: ‘Shih-Tzu’,\n‘prediction’: [(b”203: ‘West Highland white terrier’,”, 3.0289587812148966e-05)],\n‘evaluation’: False}\n```"]