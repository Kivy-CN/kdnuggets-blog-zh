- en: 'Boosting Machine Learning Algorithms: An Overview'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/07/boosting-machine-learning-algorithms-overview.html](https://www.kdnuggets.com/2022/07/boosting-machine-learning-algorithms-overview.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Boosting Machine Learning Algorithms: An Overview](../Images/33700d14b7b9b61945cb7546be324715.png)'
  prefs: []
  type: TYPE_IMG
- en: Combing various machine learning algorithms while solving a problem usually
    results in better results. The individual algorithms are referred to as weak learners.
    Their combination results in a strong learner. A weak learner is a model that
    gives better results than a random prediction in a classification problem or the
    mean in a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The final result from these algorithms is obtained by fitting them on the training
    data and combining their predictions. In classification, the combination is done
    by voting, while in regression, it’s done via averaging.
  prefs: []
  type: TYPE_NORMAL
- en: The combination of several machine learning algorithms is referred to as **ensemble
    learning**. There are several ensemble learning techniques. In this article, we
    will focus on boosting.
  prefs: []
  type: TYPE_NORMAL
- en: '*Let’s start learning––pun intended!*'
  prefs: []
  type: TYPE_NORMAL
- en: What is Boosting?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Boosting is an ensemble learning technique that sequentially fits weaker learners
    to a dataset. Every subsequent weak learner that is fitted aims at reducing the
    errors resulting from the previous one.
  prefs: []
  type: TYPE_NORMAL
- en: How does boosting work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generally, boosting works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the initial weak learner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the weak learner to make predictions on the entire dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the prediction errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorrect predictions are assigned more weight.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build another weak learner aimed at fixing the errors of the previous learner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make predictions on the whole dataset using the new learner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat this process until the optimal results are obtained.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final model is obtained by weighting the mean of all weak learners.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s look at some algorithms that are based on the boosting framework that
    we have just discussed.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AdaBoost works by fitting one weak learner after the other. In subsequent fits,
    it gives more weight to incorrect predictions and less weight to correct predictions.
    In this way, the models learn to make predictions for the difficult classes. The
    final predictions are obtained by weighing the majority class or sum. The learning
    rate controls the contribution of each weak learner to the final prediction. 
    AdaBoost can be used for both [classification](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)
    and [regression](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor)
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn provides an [AdaBoost implementation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)
    that you can start using immediately. By default, the algorithm uses [decision
    trees](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)
    as the base estimator. In this case, a `DecisionTreeClassifier` will be fitted
    on the entire dataset first. In subsequent iterations, the fit will be done with
    incorrectly predicted instances given more weight.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To improve the performance of the model, the number of estimators, the parameters
    of the base estimator, and the learning rate should be tuned. For example, you
    can tune the maximum depth of the decision tree classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Once training is complete, the impurity-based feature importances are obtained
    via the `feature_importances_` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient tree boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gradient tree boosting is an additive ensemble learning approach that uses decision
    trees as weak learners. Additive means that the trees are added one after the
    other. Previous trees remain unchanged. When adding subsequent trees, [gradient
    descent](https://scikit-learn.org/stable/modules/sgd.html) is used to minimize
    the loss.
  prefs: []
  type: TYPE_NORMAL
- en: The quickest way to build a gradient boosting model is to use [Scikit-learn](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The algorithm can be used for [regression](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor)
    and [classification](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: eXtreme Gradient Boosting - XGBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[XGBoost](https://xgboost.readthedocs.io/en/latest/) is a popular gradient
    boosting algorithm. It uses weak regression trees as weak learners. The algorithm
    also does cross-validation and computes the feature importance.  Furthermore,
    it accepts sparse input data.'
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost offers the [DMatrix](https://xgboost.readthedocs.io/en/latest/search.html?q=DMatrix&check_keywords=yes&area=default)
    data structure that improves its performance and efficiency. XGBoost can be used
    in [R](https://www.r-project.org/), [Java](https://www.java.com/), [C++,](https://www.cplusplus.com/)
    and [Julia](https://julialang.org/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: XGBoost offers the feature importance via the `plot_importance()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: LightGBM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LightGBM is a tree-based gradient boosting algorithm that uses leaf-wise tree
    growth and not depth-wise growth.
  prefs: []
  type: TYPE_NORMAL
- en: '![LightGBM](../Images/d55af5779baecced0915cb366849095d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Leaf-wise tree growth. ](https://lightgbm.readthedocs.io/en/latest/Features.html?highlight=dart#leaf-wise-best-first-tree-growth)'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm can be used for classification and regression problems. LightGBM
    supports categorical features via the `categorical_feature` argument. One-hot
    encoding is not needed after specifying the categorical columns.
  prefs: []
  type: TYPE_NORMAL
- en: The LightGBM algorithm also has the capacity to deal with null values. This
    feature can be disabled by setting `use_missing=false`. It uses NA to represent
    null values. To use zeros set `zero_as_missing=true.`
  prefs: []
  type: TYPE_NORMAL
- en: The objective parameter is used to dictate the type of problem. For example,
    `binary` for binary classification, `regression` for regression and `multiclass`
    for multiclass problems.
  prefs: []
  type: TYPE_NORMAL
- en: When using LightGM, you’ll usually first convert the data into the [LightGBM
    Daset](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Dataset.html?highlight=lgb.Dataset)
    format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: LightGBM also allows you to specify the boosting type. Available options include
    random forests and traditional gradient boosting decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: CatBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[CatBoost](https://catboost.ai/) is a depth-wise gradient boosting library
    developed by [Yandex](https://yandex.com/). In CatBoost, a balanced tree is grown
    using oblivious trees. In these types of trees the same feature is used when making
    right and left splits at each level of the tree.'
  prefs: []
  type: TYPE_NORMAL
- en: '![CatBoost](../Images/beea2b8e36a2d499fa8aa5115efdd9a7.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Using the same feature to make left and right splits. ](https://medium.com/p/38779b0d5d9a)'
  prefs: []
  type: TYPE_NORMAL
- en: Like LightGBM, CatBoost supports categorical features, training on GPUs, and
    handling of null values. CatBoost can be used for [regression](https://catboost.ai/en/docs/concepts/python-reference_catboostregressor)
    and [classification](https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier)
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Setting `plot=true` while training visualizes the training process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article we have coved boosting algorithms and how you can apply them
    in machine learning. Specifically, we have talked about:'
  prefs: []
  type: TYPE_NORMAL
- en: What is boosting?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How boosting works.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different boosting algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How different boosting algorithms work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Happy boosting!
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Ensemble learning guide ](https://scikit-learn.org/stable/modules/ensemble.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kaggle ensembling guide](https://web.archive.org/web/20150613065532/https://mlwave.com/kaggle-ensembling-guide/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Derrick Mwiti](https://www.linkedin.com/in/mwitiderrick/)** is experienced
    in data science, machine learning, and deep learning with a keen eye for building
    machine learning communities.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Linear Machine Learning Algorithms: An Overview](https://www.kdnuggets.com/2022/07/linear-machine-learning-algorithms-overview.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Machine Learning Algorithms: An In-Depth Overview](https://www.kdnuggets.com/understanding-machine-learning-algorithms-an-indepth-overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Ultimate Guide to Mastering Seasonality and Boosting Business Results](https://www.kdnuggets.com/2023/08/media-mix-modeling-ultimate-guide-mastering-seasonality-boosting-business-results.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Labeling for Machine Learning: Market Overview, Approaches, and Tools](https://www.kdnuggets.com/2021/12/data-labeling-ml-overview-and-tools.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Evaluation Metrics: Theory and Overview](https://www.kdnuggets.com/machine-learning-evaluation-metrics-theory-and-overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Primary Supervised Learning Algorithms Used in Machine Learning](https://www.kdnuggets.com/2022/06/primary-supervised-learning-algorithms-used-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
