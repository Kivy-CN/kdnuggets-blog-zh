- en: 'Ensemble Learning: 5 Main Approaches'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习：5种主要方法
- en: 原文：[https://www.kdnuggets.com/2019/01/ensemble-learning-5-main-approaches.html](https://www.kdnuggets.com/2019/01/ensemble-learning-5-main-approaches.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/01/ensemble-learning-5-main-approaches.html](https://www.kdnuggets.com/2019/01/ensemble-learning-5-main-approaches.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Diogo Menezes Borges](https://www.linkedin.com/in/diogomenezesborges/?locale=en_US),
    Data Scientist**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Diogo Menezes Borges](https://www.linkedin.com/in/diogomenezesborges/?locale=en_US)，数据科学家**。'
- en: '*Remember a few months ago when everyone was taking wild guesses at the color
    of the dress (blue or gold) or the tennis shoe (pink or grey)? For me, Ensemble
    Learning looks a bit like that. A group of weak learners comes together to form
    a strong learner, thus increasing the accuracy of any Machine Learning model.*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*还记得几个月前每个人对裙子的颜色（蓝色还是金色）或运动鞋的颜色（粉色还是灰色）进行狂猜吗？对我来说，集成学习有点像这样。一组弱学习者汇聚成一个强学习者，从而提高了任何机器学习模型的准确性。*'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Have you ever heard about *Wisdom of the Crowd*? Not the TV Series, but the
    real term. No? Ok…so, imagine you ask a complex question to thousands of random
    people. Now, aggregate their answers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你听说过*群体智慧*吗？不是那个电视剧，而是真实的术语。没有？好吧……那么，想象一下你向成千上万的随机人提出一个复杂的问题。现在，将他们的答案汇总起来。
- en: What you will probably find is that the answer given by the majority is better
    than the expert’s answer.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会发现多数人给出的答案比专家的答案更好。
- en: '*The **wisdom of the crowd** is the collective opinion of a group of individuals
    rather than that of a single expert**—**[Wikipedia](https://en.wikipedia.org/wiki/Wisdom_of_the_crowd)*'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“**群体智慧**是一个群体的集体意见，而不是单个专家的意见”*—[维基百科](https://en.wikipedia.org/wiki/Wisdom_of_the_crowd)'
- en: Returning to the field of Machine Learning, we can apply the same idea. For
    example, if we aggregate the predictions of a group of predictors (such as classifiers
    and regressors), we will probably get a better prediction than with the best individual
    predictor.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 回到机器学习领域，我们可以应用相同的想法。例如，如果我们汇总一组预测者（例如分类器和回归器）的预测，我们可能会得到比最好的单个预测者更好的预测结果。
- en: '*A group of predictors is called an ensemble. Therefore this Machine Learning
    technique is known as Ensemble Learning. Voilá!*'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*一组预测者称为集成。因此，这种机器学习技术被称为集成学习。瞧！*'
- en: In my last post [“A breath of fresh air with Decision Trees”](https://medium.com/diogo-menezes-borges/a-breath-of-fresh-air-with-decision-trees-e660455bbfc8),
    I talked about the algorithm Decision Tree and how we can use it to make predictions.
    As you know, in Nature, a group a trees makes a forest. Hum… so that means..
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我上一篇文章[《决策树的新鲜空气》](https://medium.com/diogo-menezes-borges/a-breath-of-fresh-air-with-decision-trees-e660455bbfc8)中，我讨论了决策树算法以及我们如何使用它进行预测。正如你所知，在自然界中，一组树木组成了一个森林。嗯……这意味着……
- en: Imagine that I train a group of Decision Trees classifiers to make individual
    predictions according to different subsets of the training dataset. Later I predict
    the expected class for an observation, taking into account what the majority predicted
    for the same instance. So… that means, I am basically using the knowledge of an
    ensemble of Decision Tree or as it is mostly known a **Random Forest.**
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我训练了一组决策树分类器，根据训练数据集的不同子集进行个体预测。然后，我预测一个观察值的预期类别，考虑到多数人对同一实例的预测。因此，这意味着，我基本上是在使用一组决策树的知识，或者更常被称为**随机森林**。
- en: 'For this post we will go through the most popular Ensemble methods including *bagging,
    boosting, stacking *and a few others. Before we go into detail please have the
    following in mind:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将讨论包括*bagging、boosting、stacking*及其他一些最流行的集成方法。在详细介绍之前，请记住以下几点：
- en: '*“Ensemble methods work best when the predictors are as independent of one
    another as possible. One way to get diverse classifiers is to train them using
    very different algorithms. This increases the chance that they will make very
    different types of errors, improving the ensemble’s accuracy.”**—**“**Hands-on
    Machine Learning with Scikit-Learn & TensorFlow”, Chapter 7*'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“集成方法在预测器尽可能独立时效果最佳。获得多样化分类器的一种方法是使用非常不同的算法进行训练。这增加了它们产生不同类型错误的机会，从而提高集成的准确性。”**—**“**动手学深度学习与Scikit-Learn
    & TensorFlow，第7章*'
- en: Simple Ensemble Techniques
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 简单集成技术
- en: '**Hard Voting Classifier**'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬投票分类器**'
- en: This is the most simple example of the technique and we’ve already kindly approached
    it. Voting classifiers are usually used in classification problems. Imagine you
    trained and fitted a few classifiers (Logistic Regression classifier, SVM classifiers,
    Random Forest classifier, among others) to your training dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该技术最简单的例子，我们已经友好地介绍过。投票分类器通常用于分类问题。想象一下你训练并拟合了几个分类器（逻辑回归分类器、SVM分类器、随机森林分类器等）到你的训练数据集中。
- en: A simple way to create a better classifier is to aggregate the prediction made
    by each one and set the majority chosen as the final prediction. Basically, we
    can consider this as if we’re retrieving the mode of all predictors.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 创建更好分类器的一种简单方法是汇总每个分类器的预测，并将多数选择作为最终预测。基本上，我们可以将其视为获取所有预测器的众数。
- en: '![](../Images/47f934cb71cb7bc891c09b53aa4f05c0.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47f934cb71cb7bc891c09b53aa4f05c0.png)'
- en: '**“Hands-on Machine Learning with Scikit-Learn & TensorFlow”, Chapter 7**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**《动手学深度学习与Scikit-Learn & TensorFlow》第7章**'
- en: '**Averaging**'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均法**'
- en: The first example we’ve seen is mostly used for classification problems. Now
    let’s take a look at a technique used for Regression problems known as Averaging.
    Similar to Hard Voting, we take multiple predictions made by different algorithms
    and take its average to make the final prediction.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到的第一个例子主要用于分类问题。现在让我们来看看一种用于回归问题的技术，称为平均法。类似于硬投票，我们取多个算法做出的预测并计算其平均值以得出最终预测。
- en: '**Weighted Averaging**'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权平均**'
- en: This technique is equal to Averaging with the twist that all models are assigned
    different weights according to its importance to the final prediction.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术与平均法相同，只是所有模型根据其对最终预测的重要性分配不同的权重。
- en: Advanced Ensemble techniques
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 高级集成技术
- en: '**Stacking**'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**堆叠**'
- en: Also known as Stacked Generalisation, this technique is based on the idea of
    training a model that would perform the usual aggregation we saw previously.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 也称为堆叠泛化，这种技术基于训练一个模型，该模型将执行我们之前看到的常规聚合操作。
- en: '![](../Images/3de67bd350f8377dff6234d11b2941a2.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3de67bd350f8377dff6234d11b2941a2.png)'
- en: We have N predictors, each making its predictions and returning a final value.
    Later, the Meta Learner or Blender will take these predictions as input and make
    the final prediction.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有N个预测器，每个预测器做出预测并返回最终值。之后，Meta Learner或Blender将这些预测作为输入并做出最终预测。
- en: Let’s see how it works…
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它是如何工作的……
- en: '![](../Images/c5d9ec1621328d9551334c66854e76cf.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5d9ec1621328d9551334c66854e76cf.png)'
- en: '![](../Images/0ff34a0ef0985ecb605628f573f34add.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ff34a0ef0985ecb605628f573f34add.png)'
- en: The common approach to train the Meta Learner is a hold-out set. Firstly, you
    split the training set into two subsets. The first subset of data is used to train
    the predictors.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 训练Meta Learner的常用方法是保留集。首先，将训练集拆分为两个子集。第一个子集的数据用于训练预测器。
- en: Later, the same predictors that trained on the first subset are used to make
    predictions on the second (held-out) set. By doing this, the cleanness of the
    predictions is ensured since those algorithms never saw the data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，将在第一个子集上训练的相同预测器用于对第二个（保留的）集合进行预测。通过这样做，确保了预测的清洁性，因为这些算法从未见过数据。
- en: With these new predictions, we are able to generate a new training set which
    will be used as input features (making the new training set three-dimensional)
    and keeping the target values.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些新预测，我们可以生成一个新的训练集，这个训练集将用作输入特征（使新的训练集变为三维）并保留目标值。
- en: Therefore, in the end, the Meta Learner is trained on this new dataset and it
    predicts the target values having into account the values given by the first predictions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最后，元学习器在这个新数据集上进行训练，并考虑第一个预测提供的值来预测目标值。
- en: 'Moreover, you can train several Meta Learner using this methodology (e.g. one
    using Linear Regression, another one using Random Forest Regression, etc). To
    use more than one Meta Learner you have to split the training set into three or
    more subsets: the first to train the first layer of predictors, the second used
    to make predictions on unknown data and create a new dataset and the third one
    used to train the Meta Learner and make the predictions for the target values.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以使用这种方法训练多个元学习器（例如，一个使用线性回归，另一个使用随机森林回归，等等）。要使用多个元学习器，你必须将训练集拆分成三个或更多的子集：第一个用于训练第一层预测器，第二个用于对未知数据进行预测并创建新数据集，第三个用于训练元学习器并对目标值进行预测。
- en: '**Bagging and Pasting**'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**袋装和粘贴**'
- en: Another approach is to use the same algorithm for every predictor (e.g. all
    decision tree), however, having different random subsets of the training set allowing
    for a more generalised result.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是对每个预测器使用相同的算法（例如，所有的决策树），然而，使用不同的训练集随机子集以获得更一般化的结果。
- en: '![](../Images/26bc0060612255b816373487062371f2.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26bc0060612255b816373487062371f2.png)'
- en: '**Article “[Spatial downscaling of precipitation using adaptable random forests](https://www.researchgate.net/publication/309031320_Spatial_downscaling_of_precipitation_using_adaptable_random_forests?_sg=0FdXhCBDcpE3PGA1c_SnLcE36GH-G47WJbTr70zdEg1yUkijY1C0U1FyRmBlsfxbxdCOXKDiYDi-QiAXqOwjYua8wVUOz-M45Q)”**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**文章“[使用可适应随机森林的降尺度降水](https://www.researchgate.net/publication/309031320_Spatial_downscaling_of_precipitation_using_adaptable_random_forests?_sg=0FdXhCBDcpE3PGA1c_SnLcE36GH-G47WJbTr70zdEg1yUkijY1C0U1FyRmBlsfxbxdCOXKDiYDi-QiAXqOwjYua8wVUOz-M45Q)”**'
- en: Regarding the creation of the subsets, you can either proceed with [*replacement *](https://web.ma.utexas.edu/users/parker/sampling/repl.htm)or
    without. If we assume there is replacement, some observations maybe are present
    in more than one subset, the reason why we call this method *bagging *(short for [*bootstrap
    aggregating*](https://en.wikipedia.org/wiki/Bootstrap_aggregating)). When the
    sampling is performed without replacement we ensure that all the observations,
    in each subset, are unique thus guaranteeing there are no repeated observations
    in every subset.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 关于子集的创建，你可以选择进行[*替换*](https://web.ma.utexas.edu/users/parker/sampling/repl.htm)或不进行替换。如果我们假设有替换，一些观察可能会出现在多个子集中，这就是我们称这种方法为*袋装*（即[*自助聚合*](https://en.wikipedia.org/wiki/Bootstrap_aggregating)）。当抽样没有替换时，我们确保每个子集中的所有观察都是唯一的，从而保证每个子集中没有重复的观察。
- en: Once all the predictors are trained, the ensemble can make a prediction for
    a new instance by aggregating the predicted values of all trained predictors,
    just like we saw in the hard voting classifier. Although each individual predictor
    has a higher bias than if it were trained on the original dataset, the aggregation
    allows the reduction of both bias and variance (check my [post ](https://medium.com/diogo-menezes-borges/whats-the-trade-off-between-bias-and-variance-c05b241b15d9)on
    the bias-variance trade-off)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有预测器都训练完成，集成模型可以通过聚合所有训练过的预测器的预测值来对新实例进行预测，就像我们在硬投票分类器中看到的那样。尽管每个单独的预测器比在原始数据集上训练时具有更高的偏差，但聚合可以减少偏差和方差（查看我的[文章](https://medium.com/diogo-menezes-borges/whats-the-trade-off-between-bias-and-variance-c05b241b15d9)关于偏差-方差权衡）。
- en: With pasting, there is a high chance that these models will give the same result
    since they are getting the same input. Bootstraping introduces more diversity
    in each subset than pasting, therefore you can expect a higher bias from bagging
    which means the predictors are less correlated so the ensemble’s variance is reduced.
    Overall, the bagging model usually offers better results which explains why generally
    you hear more about bagging than pasting.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用粘贴（pasting），这些模型给出相同结果的可能性很高，因为它们获取了相同的输入。自助抽样（bootstrapping）在每个子集中引入了更多的多样性，因此你可以预期袋装（bagging）具有更高的偏差，这意味着预测器之间的相关性较低，从而减少了集成的方差。总体而言，袋装模型通常提供更好的结果，这解释了为什么你通常会听到更多关于袋装而不是粘贴的内容。
- en: '**Boosting**'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升**'
- en: If a data point is incorrectly predicted by the first model as well as the next
    (probably all models), will the combination of their results provide a better
    prediction? This is where *Boosting* comes into action.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个数据点被第一个模型以及接下来的模型（可能是所有模型）错误预测，那么它们结果的组合是否能提供更好的预测？这就是*提升*发挥作用的地方。
- en: Also known as *Hypothesis Boosting, *it refers to any Ensemble method that can
    combine weak learners into a stronger one. It’s a sequential process where each
    subsequent model attempts to fix the errors of its predecessor.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 也被称为*假设提升*，指的是任何可以将弱学习者组合成更强学习者的集成方法。这是一个顺序过程，每个后续模型试图修正其前任模型的错误。
- en: '![](../Images/b9560d9caee2ae35a7676585aaa28a88.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9560d9caee2ae35a7676585aaa28a88.png)'
- en: '**[Source](https://blog.bigml.com/2017/03/14/introduction-to-boosted-trees/)**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**[来源](https://blog.bigml.com/2017/03/14/introduction-to-boosted-trees/)**'
- en: Each model would not perform well on the entire dataset. However, they do have
    great performances in parts of the dataset. Hence, from Boosting we can expect
    that each model will actually contribute to boosting the performance of the overall
    ensemble.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型在整个数据集上的表现可能不佳。然而，它们在数据集的部分区域有很好的表现。因此，从提升中我们可以预期每个模型实际上都会有助于提升整体集成的性能。
- en: Algorithms based on Bagging and Boosting
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于袋装和提升的算法
- en: The most commonly used Ensemble Learning techniques are Bagging and Boosting.
    Here are some of the most common algorithms for each of these techniques. I’ll
    write single posts presenting each one of these algorithms in the near future.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的集成学习技术是袋装和提升。以下是这些技术中一些最常见的算法。我会在不久的将来写单独的文章介绍这些算法。
- en: '**Bagging algorithms:**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**袋装算法：**'
- en: '[Random Forest](https://medium.com/diogo-menezes-borges/random-forests-8ae226855565)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机森林](https://medium.com/diogo-menezes-borges/random-forests-8ae226855565)'
- en: '**Boosting algorithms:**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**提升算法：**'
- en: '[AdaBoost](https://medium.com/diogo-menezes-borges/boosting-with-adaboost-and-gradient-boosting-9cbab2a1af81)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AdaBoost](https://medium.com/diogo-menezes-borges/boosting-with-adaboost-and-gradient-boosting-9cbab2a1af81)'
- en: '[Gradient Boosting Machine (GBM)](https://medium.com/diogo-menezes-borges/boosting-with-adaboost-and-gradient-boosting-9cbab2a1af81)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[梯度提升机 (GBM)](https://medium.com/diogo-menezes-borges/boosting-with-adaboost-and-gradient-boosting-9cbab2a1af81)'
- en: '[XGBoost](https://medium.com/diogo-menezes-borges/boosting-with-adaboost-and-gradient-boosting-9cbab2a1af81)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost](https://medium.com/diogo-menezes-borges/boosting-with-adaboost-and-gradient-boosting-9cbab2a1af81)'
- en: '[Light GBM](https://medium.com/diogo-menezes-borges/boosting-with-adaboost-and-gradient-boosting-9cbab2a1af81)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Light GBM](https://medium.com/diogo-menezes-borges/boosting-with-adaboost-and-gradient-boosting-9cbab2a1af81)'
- en: '**Bio**: [Diogo Menezes Borges](https://www.linkedin.com/in/diogomenezesborges/?locale=en_US)
    is a Data Scientist with a background in engineering and 2 years of experience
    using predictive modeling, data processing, and data mining algorithms to solve
    challenging business problems.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介**：[Diogo Menezes Borges](https://www.linkedin.com/in/diogomenezesborges/?locale=en_US)
    是一名数据科学家，拥有工程背景和2年的经验，使用预测建模、数据处理和数据挖掘算法解决复杂的商业问题。'
- en: '[Original](https://medium.com/diogo-menezes-borges/ensemble-learning-when-everybody-takes-a-guess-i-guess-ec35f6cb4600).
    Reposted with permission.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/diogo-menezes-borges/ensemble-learning-when-everybody-takes-a-guess-i-guess-ec35f6cb4600)。已获许可转载。'
- en: '**Resources:**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：**'
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在线和基于网络的：分析、数据挖掘、数据科学、机器学习教育](https://www.kdnuggets.com/education/online.html)'
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分析、数据科学、数据挖掘和机器学习的软件](https://www.kdnuggets.com/software/index.html)'
- en: '**Related:**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Intuitive Ensemble Learning Guide with Gradient Boosting](https://www.kdnuggets.com/2018/07/intuitive-ensemble-learning-guide-gradient-boosting.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[直观的集成学习指南与梯度提升](https://www.kdnuggets.com/2018/07/intuitive-ensemble-learning-guide-gradient-boosting.html)'
- en: '[Improving the Performance of a Neural Network](https://www.kdnuggets.com/2018/05/improving-performance-neural-network.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提高神经网络性能](https://www.kdnuggets.com/2018/05/improving-performance-neural-network.html)'
- en: '[A Tour of The Top 10 Algorithms for Machine Learning Newbies](https://www.kdnuggets.com/2018/02/tour-top-10-algorithms-machine-learning-newbies.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习新手的前10大算法巡礼](https://www.kdnuggets.com/2018/02/tour-top-10-algorithms-machine-learning-newbies.html)'
- en: More On This Topic
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题更多内容
- en: '[Ensemble Learning with Examples](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[带示例的集成学习](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习技术：Python中的随机森林演练](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
- en: '[When Would Ensemble Techniques be a Good Choice?](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[何时使用集成技术是一个好选择？](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能、分析、机器学习、数据科学、深度学习……](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
- en: '[Data Labeling for Machine Learning: Market Overview, Approaches, and Tools](https://www.kdnuggets.com/2021/12/data-labeling-ml-overview-and-tools.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习的数据标注：市场概述、方法和工具](https://www.kdnuggets.com/2021/12/data-labeling-ml-overview-and-tools.html)'
- en: '[Machine Learning’s Sweet Spot: Pure Approaches in NLP and Document Analysis](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习的最佳实践：NLP 和文档分析中的纯方法](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
