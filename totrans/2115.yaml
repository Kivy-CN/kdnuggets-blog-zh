- en: Distribute and Run LLMs with llamafile in 5 Simple Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/distribute-and-run-llms-with-llamafile-in-5-simple-steps](https://www.kdnuggets.com/distribute-and-run-llms-with-llamafile-in-5-simple-steps)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Distribute and Run LLMs with llamafile in 5 Simple Steps](../Images/f0ab7be9ca7fc383ce9e47655b07be97.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: For many of us, exploring the possibilities of LLMs has felt out of reach. Whether
    it's downloading complicated software, figuring out coding, or needing powerful
    machines - getting started with LLMs can seem daunting. But just imagine, if we
    could interact with these powerful language models as easily as starting any other
    program on our computers. No installation, no coding, just click and talk. This
    accessibility is key for both developers and end-users. llamaFile emerges as a
    novel solution, merging the [llama.cpp](https://github.com/ggerganov/llama.cpp)
    with [Cosmopolitan Libc](https://github.com/jart/cosmopolitan) into a single framework.
    This framework reduces the complexity of LLMs by offering a **one-file executable
    called “llama file”**, which runs on local machines without the need for installation.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how does it work? llamaFile offers **two convenient methods** for running
    LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: The first method involves downloading the latest release of llamafile along
    with the corresponding model weights from Hugging Face. Once you have those files,
    you're good to go!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second method is even simpler - you can access [pre-existing example llamafiles](https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#other-example-llamafiles)
    that have weights built-in.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this tutorial, you will work with the llamafile of the **LLaVa model** using
    the second method. It's a 7 Billion Parameter model that is quantized to 4 bits
    that you can interact with via chat, upload images, and ask questions. The example
    llamafiles of other models are also available, but we will be working with the
    LLaVa model as its llamafile size is 3.97 GB, while Windows has a maximum executable
    file size of 4 GB. The process is simple enough, and you can run LLMs by following
    the steps mentioned below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Download the llamafile'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, you need to download the llava-v1.5-7b-q4.llamafile (3.97 GB) executable
    from the source provided [here](https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-q4.llamafile?download=true).
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Grant Execution Permission (For macOS, Linux, or BSD Users)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open your computer’s terminal and navigate to the directory where the file is
    located. Then run the following command to grant permission for your computer
    to execute this file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 3: Rename the File (For Windows Users)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are on Windows, add “.exe” to the llamafile’s name on the end. You can
    run the following command on the terminal for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 4: Run the llamafile'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Execute the llama file by the following command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '⚠️ Since MacOS uses zsh as its default shell and if you run across `zsh: exec
    format error: ./llava-v1.5-7b-q4.llamafile` error then you need to execute this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For Windows, your command may look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 5: Interact with the User Interface'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After running the llamafile, it should automatically open your default browser
    and display the user interface as shown below. If it doesn’t, open the browser
    and navigate to [http://localhost:8080](http://localhost:8080) manually.
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribute and Run LLMs with llamafile in 5 Simple Steps](../Images/42d390371b6887443db5df99300b6657.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by interacting with the interface with a simple question to provide
    some information related to the LLaVa model. Below is the response generated by
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribute and Run LLMs with llamafile in 5 Simple Steps](../Images/86d758132c338800202b92705f4f5437.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The response highlights the approach to developing the LLaVa model and its applications.
    The response generated was reasonably fast. Let’s try to implement another task.
    We will upload the following sample image of a bank card with details on it and
    extract the required information from it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribute and Run LLMs with llamafile in 5 Simple Steps](../Images/f2154617727a40577296a6d51bdb1cd4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Ruby Thompson](https://qph.cf2.quoracdn.net/main-qimg-664f0732755fc0ba072da4286668fef8-lq)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribute and Run LLMs with llamafile in 5 Simple Steps](../Images/481ad10a447dea169aa9fba0fe91b01e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Again, the response is pretty reasonable. The authors of LLaVa claim that it
    attains top-tier performance across various tasks. Feel free to explore diverse
    tasks, observe their successes and limitations, and experience the outstanding
    performance of LLaVa yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Once your interaction with the LLM is complete, you can shut down the llama
    file by returning to the terminal and pressing "Control - C".
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distributing and running LLMs has never been more straightforward. In this tutorial,
    we explained how easily you can run and experiment with different models with
    just a single executable llamafile. This not only saves time and resources but
    also expands the accessibility and real-world utility of LLMs. We hope you found
    this tutorial helpful and would love to hear your thoughts on it.  Additionally,
    if you have any questions or feedback, please don't hesitate to reach out to us.
    We're always happy to help and value your input.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs: []
  type: TYPE_NORMAL
- en: '**[](https://www.linkedin.com/in/kanwal-mehreen1/)**[Kanwal Mehreen](https://www.linkedin.com/in/kanwal-mehreen1/)****
    Kanwal is a machine learning engineer and a technical writer with a profound passion
    for data science and the intersection of AI with medicine. She co-authored the
    ebook "Maximizing Productivity with ChatGPT". As a Google Generation Scholar 2022
    for APAC, she champions diversity and academic excellence. She''s also recognized
    as a Teradata Diversity in Tech Scholar, Mitacs Globalink Research Scholar, and
    Harvard WeCode Scholar. Kanwal is an ardent advocate for change, having founded
    FEMCodes to empower women in STEM fields.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How to Package and Distribute Machine Learning Models with MLFlow](https://www.kdnuggets.com/2022/08/package-distribute-machine-learning-models-mlflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learn How to Run Alpaca-LoRA on Your Device in Just a Few Steps](https://www.kdnuggets.com/2023/05/learn-run-alpacalora-device-steps.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Schedule & Run ETLs with Jupysql and GitHub Actions](https://www.kdnuggets.com/2023/05/schedule-run-etls-jupysql-github-actions.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting Started with PyTest: Effortlessly Write and Run Tests in Python](https://www.kdnuggets.com/getting-started-with-pytest-effortlessly-write-and-run-tests-in-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Run an LLM Locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Make Python Code Run Incredibly Fast](https://www.kdnuggets.com/2021/06/make-python-code-run-incredibly-fast.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
