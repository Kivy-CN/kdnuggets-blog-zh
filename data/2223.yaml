- en: Your Features Are Important? It Doesn’t Mean They Are Good
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你的特征很重要？这并不意味着它们是好的
- en: 原文：[https://www.kdnuggets.com/your-features-are-important-it-doesnt-mean-they-are-good](https://www.kdnuggets.com/your-features-are-important-it-doesnt-mean-they-are-good)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/your-features-are-important-it-doesnt-mean-they-are-good](https://www.kdnuggets.com/your-features-are-important-it-doesnt-mean-they-are-good)
- en: '![Your Features Are Important? It Doesn’t Mean They Are Good](../Images/21354d628b72fbab815efa48350e4fcd.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![你的特征很重要？这并不意味着它们是好的](../Images/21354d628b72fbab815efa48350e4fcd.png)'
- en: '[Image by Author]'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[图片来源：作者]'
- en: “Important” and “Good” Are Not Synonyms
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “重要”和“好”不是同义词
- en: The concept of “feature importance” is widely used in machine learning as the
    most basic type of model explainability. For example, it is used in Recursive
    Feature Elimination (RFE), to iteratively drop the least important feature of
    the model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: “特征重要性”这个概念在机器学习中被广泛使用，作为最基本的模型解释性类型。例如，它被用于递归特征消除（RFE），以迭代地删除模型中最不重要的特征。
- en: However, there is a misconception about it.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于这个概念存在一种误解。
- en: '**The fact that a feature is important doesn’t imply that it is beneficial
    for the model!**'
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**一个特征重要并不意味着它对模型有益！**'
- en: Indeed, when we say that a feature is important, this simply means that the
    feature brings a high contribution to the predictions made by the model. But we
    should consider that **such contribution may be wrong**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，当我们说一个特征很重要时，这只是意味着这个特征对模型的预测贡献很高。但我们应该考虑到**这种贡献可能是错误的**。
- en: 'Take a simple example: a data scientist accidentally forgets the Customer ID
    between its model’s features. The model uses Customer ID as a highly predictive
    feature. As a consequence, this feature will have a high feature importance even
    if it is actually worsening the model, because it cannot work well on unseen data.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 举个简单的例子：一个数据科学家不小心在模型的特征中忘记了客户 ID。模型将客户 ID 作为一个高度预测的特征。因此，即使这个特征实际上使模型表现更差，因为它不能很好地处理未见过的数据，它也会有很高的特征重要性。
- en: 'To make things clearer, we will need to make a distinction between two concepts:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使事情更清楚，我们需要区分两个概念：
- en: '**Prediction Contribution**: what part of the predictions is due to the feature;
    this is equivalent to feature importance.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测贡献**：预测中有多少部分是由于特征的存在；这相当于特征重要性。'
- en: '**Error Contribution**: what part of the prediction errors is due to the presence
    of the feature in the model.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**误差贡献**：预测误差中有多少部分是由于特征在模型中存在所致。'
- en: In this article, we will see how to calculate these quantities and how to use
    them to get valuable insights about a predictive model (and to improve it).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将了解如何计算这些量，以及如何利用它们来获得关于预测模型的有价值见解（并加以改进）。
- en: 'Note: this article is focused on the regression case. If you are more interested
    in the classification case, you can read [“Which features are harmful for your
    classification model?”](https://towardsdatascience.com/which-features-are-harmful-for-your-classification-model-6227859a44a6)'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：本文集中于回归案例。如果你对分类案例更感兴趣，可以阅读[“哪些特征对你的分类模型有害？”](https://towardsdatascience.com/which-features-are-harmful-for-your-classification-model-6227859a44a6)
- en: Starting from a Toy Example
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从一个简单的例子开始
- en: Suppose we built a model to predict the income of people based on their job,
    age, and nationality. Now we use the model to make predictions on three people.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们构建了一个模型来预测人们的收入，基于他们的工作、年龄和国籍。现在我们使用模型对三个人进行预测。
- en: 'Thus, we have the ground truth, the model prediction, and the resulting error:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有了实际值、模型预测和结果误差：
- en: '![Your Features Are Important? It Doesn’t Mean They Are Good](../Images/e257229b80dd5d5b4184539d73a4a0e3.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![你的特征很重要？这并不意味着它们是好的](../Images/e257229b80dd5d5b4184539d73a4a0e3.png)'
- en: Ground truth, model prediction, and absolute error (in thousands of $). [Image
    by Author]
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 实际值、模型预测和绝对误差（以千美元计）。 [图片来源：作者]
- en: Computing “Prediction Contribution”
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算“预测贡献”
- en: When we have a predictive model, we can always decompose the model predictions
    into the contributions brought by the single features. This can be done through
    SHAP values (if you don’t know about how SHAP values work, you can read my article: [SHAP
    Values Explained Exactly How You Wished Someone Explained to You](https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一个预测模型时，我们可以将模型预测分解为单个特征带来的贡献。这可以通过 SHAP 值来完成（如果你不知道 SHAP 值的工作原理，你可以阅读我的文章：[SHAP
    值解释：正如你希望别人向你解释的那样](https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30)）。
- en: So, let’s say these are the SHAP values relative to our model for the three
    individuals.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，假设这些是相对于我们模型的三个个体的SHAP值。
- en: '![Your Features Are Important? It Doesn’t Mean They Are Good](../Images/c52c658728b48811d9811687aafb13f2.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![你的特征很重要？这并不意味着它们是好的](../Images/c52c658728b48811d9811687aafb13f2.png)'
- en: SHAP values for our model’s predictions (in thousands of $). [Image by Author]
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型预测的SHAP值（以千美元计）。 [图片由作者提供]
- en: 'The main property of SHAP values is that they are additive. This means that
    — by taking the sum of each row — we will obtain our model’s prediction for that
    individual. For instance, if we take the second row: 72k $ +3k $ -22k $ = 53k
    $, which is exactly the model’s prediction for the second individual.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP值的主要属性是它们是可加性的。这意味着——通过对每一行求和——我们将得到模型对该个体的预测。例如，如果我们取第二行：72k $ +3k $ -22k
    $ = 53k $，这正是模型对第二个体的预测。
- en: 'Now, SHAP values are a good indicator of how important a feature is for our
    predictions. Indeed, the higher the (absolute) SHAP value, the more influential
    the feature for the prediction about that specific individual. Note that I am
    talking about absolute SHAP values because the sign here doesn’t matter: a feature
    is equally important if it pushes the prediction up or down.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，SHAP值是衡量特征对我们预测的重要性的良好指标。实际上，SHAP值（绝对值）越高，该特征对特定个体的预测影响越大。注意，我说的是绝对SHAP值，因为这里的符号并不重要：一个特征无论是推动预测向上还是向下，都是同样重要的。
- en: 'Therefore, **the Prediction Contribution of a feature is equal to the mean
    of the absolute SHAP values of that feature**. If you have the SHAP values stored
    in a Pandas dataframe, this is as simple as:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，**特征的预测贡献等于该特征绝对SHAP值的平均值**。如果你在Pandas数据框中存储了SHAP值，这就非常简单：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In our example, this is the result:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，这就是结果：
- en: '![Your Features Are Important? It Doesn’t Mean They Are Good](../Images/5c54087fa75dd4dc25c2de0a1ce4683c.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![你的特征很重要？这并不意味着它们是好的](../Images/5c54087fa75dd4dc25c2de0a1ce4683c.png)'
- en: Prediction Contribution. [Image by Author]
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 预测贡献。 [图片由作者提供]
- en: As you can see, job is clearly the most important feature since, on average,
    it accounts for 71.67k $ of the final prediction. Nationality and age are respectively
    the second and the third most relevant feature.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，*job*显然是最重要的特征，因为平均而言，它占最终预测的71.67k $。国籍和年龄分别是第二和第三重要的特征。
- en: However, the fact that a given feature accounts for a relevant part of the final
    prediction doesn’t tell anything about the feature’s performance. To consider
    also this aspect, we will need to compute the “Error Contribution”.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一个特征占据了最终预测的重要部分并不能说明该特征的性能。为了考虑这个方面，我们需要计算“错误贡献”。
- en: Computing “Error Contribution”
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算“错误贡献”
- en: 'Let’s say that we want to answer the following question: “What predictions
    would the model make if it didn’t have the feature *job*?” SHAP values allow us
    to answer this question. In fact, since they are additive, it’s enough to subtract
    the SHAP values relative to the feature *job* from the predictions made by the
    model.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想回答以下问题：“如果模型没有*job*特征，会做出什么预测？”SHAP值可以让我们回答这个问题。实际上，由于它们是可加的，只需从模型的预测中减去相对于*job*特征的SHAP值即可。
- en: 'Of course, we can repeat this procedure for each feature. In Pandas:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们可以对每个特征重复这个过程。在Pandas中：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This is the outcome:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '![Your Features Are Important? It Doesn’t Mean They Are Good](../Images/b41b1dc730d672ff3d9a046f295c0de9.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![你的特征很重要？这并不意味着它们是好的](../Images/b41b1dc730d672ff3d9a046f295c0de9.png)'
- en: Predictions that we would obtain if we removed the respective feature. [Image
    by Author]
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们去掉相应特征，得到的预测。 [图片由作者提供]
- en: This means that, if we didn’t have the feature *job*, then the model would predict
    20k $ for the first individual, -19k $ for the second one, and -8k $ for the third
    one. Instead, if we didn’t have the feature *age*, the model would predict 73k
    $ for the first individual, 50k $ for the second one, and so on.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果我们没有*job*这个特征，那么模型会预测第一个个体20k $，第二个个体-19k $，第三个个体-8k $。相反，如果我们没有*age*这个特征，模型会预测第一个个体73k
    $，第二个个体50k $，等等。
- en: 'As you can see, the predictions for each individual vary a lot if we removed
    different features. As a consequence, also the prediction errors would be very
    different. We can easily compute them:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，如果我们去掉不同的特征，每个个体的预测会有很大差异。因此，预测误差也会非常不同。我们可以轻松计算它们：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The result is the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Your Features Are Important? It Doesn’t Mean They Are Good](../Images/7ce7d543c1d0b3b57dc1ca02700a5d36.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![你的特征很重要？这并不意味着它们是好的](../Images/7ce7d543c1d0b3b57dc1ca02700a5d36.png)'
- en: Absolute errors that we would obtain if we removed the respective feature. [Image
    by Author]
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们去除相应特征，我们将得到的绝对误差。[图像来自作者]
- en: These are the errors that we would obtain if we removed the respective feature.
    Intuitively, if the error is small, then removing the feature is not a problem
    — or it’s even beneficial — for the model. If the error is high, then removing
    the feature is not a good idea.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是如果我们去除相应特征将得到的误差。直观地说，如果误差很小，那么去除特征对模型不是问题——甚至是有益的。如果误差很高，那么去除特征就不是一个好主意。
- en: 'But we can do more than this. Indeed, we can compute the difference between
    the errors of the full model and the errors we would obtain without the feature:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们可以做得更多。实际上，我们可以计算完整模型误差和去除特征后误差之间的差异：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Which is:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 即：
- en: '![Your Features Are Important? It Doesn’t Mean They Are Good](../Images/ba0e7438645ab6b3692581c95aaa0742.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![你的特征很重要？这并不意味着它们是好的](../Images/ba0e7438645ab6b3692581c95aaa0742.png)'
- en: Difference between the errors of the model and the errors we would have without
    the feature. [Image by Author]
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的误差与去除特征后误差之间的差异。[图像来自作者]
- en: 'If this number is:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个数字是：
- en: negative, then the presence of the feature leads to a reduction in the prediction
    error, so the feature works well for that observation!
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是负数，那么特征的存在会导致预测误差的减少，因此该特征对该观察结果有效！
- en: positive, then the presence of the feature leads to an increase in the prediction
    error, so the feature is bad for that observation.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是正数，那么特征的存在会导致预测误差的增加，因此该特征对该观察结果不好。
- en: '**We can compute “Error Contribution” as the mean of these values, for each
    feature**. In Pandas:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们可以将每个特征的这些值的均值计算为“错误贡献”。** 在Pandas中：'
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This is the outcome:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Your Features Are Important? It Doesn’t Mean They Are Good](../Images/924112c4d0f169ad597c7b8edfe54037.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![你的特征很重要？这并不意味着它们是好的](../Images/924112c4d0f169ad597c7b8edfe54037.png)'
- en: Error Contribution. [Image by Author]
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 错误贡献。[图像来自作者]
- en: If this value is positive, then it means that, on average, the presence of the
    feature in the model leads to a higher error. Thus, without that feature, the
    prediction would have been generally better. In other words, the feature is making
    more harm than good!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个值是正数，那么这意味着，平均而言，特征在模型中的存在会导致更高的错误。因此，没有这个特征，预测结果会更好。换句话说，这个特征带来的负面影响大于正面影响！
- en: On the contrary, the more negative this value, the more beneficial the feature
    is for the predictions since its presence leads to smaller errors.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，这个值越负，特征对预测的帮助就越大，因为它的存在会导致较小的误差。
- en: Let’s try to use these concepts on a real dataset.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试在实际数据集上应用这些概念。
- en: Predicting Gold Returns
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测黄金回报
- en: Hereafter, I will use a dataset taken from [Pycaret](https://github.com/pycaret/pycaret) (a
    Python library under [MIT license](https://github.com/pycaret/pycaret/blob/master/LICENSE)).
    The dataset is called “Gold” and it contains time series of financial data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下，我将使用一个来自[Pycaret](https://github.com/pycaret/pycaret)（一个基于[MIT许可证](https://github.com/pycaret/pycaret/blob/master/LICENSE)的Python库）的数据集。该数据集名为“Gold”，包含金融数据的时间序列。
- en: '![Your Features Are Important? It Doesn’t Mean They Are Good](../Images/45597cb699e13263df4865282fb93bbc.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![你的特征很重要？这并不意味着它们是好的](../Images/45597cb699e13263df4865282fb93bbc.png)'
- en: Dataset sample. The features are all expressed in percentage, so -4.07 means
    a return of -4.07%. [Image by Author]
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集样本。特征均以百分比表示，因此-4.07意味着回报为-4.07%。[图像来自作者]
- en: 'The features consist in the returns of financial assets respectively 22, 14,
    7, and 1 days before the observation moment (“T-22”, “T-14”, “T-7”, “T-1”). Here
    is the exhaustive list of all the financial assets used as predictive features:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 特征包括观察时刻（“T-22”、“T-14”、“T-7”、“T-1”）前22天、14天、7天和1天的金融资产回报。以下是用作预测特征的所有金融资产的详尽列表：
- en: '![Your Features Are Important? It Doesn’t Mean They Are Good](../Images/05dafecc1e75f8d3fe01fe4783517006.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![你的特征很重要？这并不意味着它们是好的](../Images/05dafecc1e75f8d3fe01fe4783517006.png)'
- en: List of the available assets. Each asset is observed at time -22, -14, -7, and
    -1\. [Image by Author]
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 可用资产的列表。每个资产在时间-22、-14、-7和-1时被观察。[图像来自作者]
- en: In total, we have 120 features.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有120个特征。
- en: The goal is to predict the Gold price (return) 22 days ahead in time (“Gold_T+22”).
    Let’s take a look at the target variable.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是预测22天后的黄金价格（回报）(“Gold_T+22”)。让我们来看看目标变量。
- en: '![Your Features Are Important? It Doesn’t Mean They Are Good](../Images/37e4a2a2dd22c97851b54ca9ac65c638.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![你的特征重要吗？这并不意味着它们很好](../Images/37e4a2a2dd22c97851b54ca9ac65c638.png)'
- en: Histogram of the variable. [Image by Author]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 变量的直方图。 [图像来源：作者]
- en: 'Once I loaded the dataset, these are the steps I carried out:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦加载了数据集，我执行了以下步骤：
- en: 'Split the full dataset randomly: 33% of the rows in the training dataset, another
    33% in the validation dataset, and the remaining 33% in the test dataset.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机划分完整数据集：33%的行分配到训练数据集，另33%到验证数据集，其余33%到测试数据集。
- en: Train a LightGBM Regressor on the training dataset.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据集上训练一个LightGBM回归模型。
- en: Make predictions on training, validation, and test datasets, using the model
    trained at the previous step.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集、验证集和测试集上进行预测，使用在前一步训练的模型。
- en: Compute SHAP values of training, validation, and test datasets, using the Python
    library “shap”.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算训练集、验证集和测试集的SHAP值，使用Python库“shap”。
- en: Compute the Prediction Contribution and the Error Contribution of each feature
    on each dataset (training, validation, and test), using the code we have seen
    in the previous paragraph.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个特征在每个数据集（训练集、验证集和测试集）上的预测贡献和误差贡献，使用我们在前面段落中看到的代码。
- en: Comparing Prediction Contribution and Error Contribution
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较预测贡献和误差贡献
- en: Let’s compare the Error Contribution and the Prediction Contribution in the
    training dataset. We will use a scatter plot, so the dots identify the 120 features
    of the model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较训练数据集中的误差贡献和预测贡献。我们将使用散点图，点表示模型的120个特征。
- en: '![Your Features Are Important? It Doesn’t Mean They Are Good](../Images/48a771f0c711c4c3a4e47a00865a0cc7.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![你的特征重要吗？这并不意味着它们很好](../Images/48a771f0c711c4c3a4e47a00865a0cc7.png)'
- en: Prediction Contribution vs. Error Contribution (on the Training dataset). [Image
    by Author]
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 预测贡献与误差贡献（在训练数据集上）。 [图像来源：作者]
- en: There is a highly negative correlation between Prediction Contribution and Error
    Contribution in the training set.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练集中，预测贡献和误差贡献之间存在高度负相关。
- en: And this makes sense: **since the model learns on the training dataset, it tends
    to attribute high importance (i.e. high Prediction Contribution) to those features
    that lead to a great reduction in the prediction error (i.e. highly negative Error
    Contribution)**.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是有道理的：**因为模型在训练数据集上学习，它倾向于将高重要性（即高预测贡献）赋予那些导致预测误差大幅减少的特征（即高负误差贡献）**。
- en: But this doesn’t add much to our knowledge, right?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并没有增加太多我们的知识，对吧？
- en: Indeed, what really matters to us is the validation dataset. The validation
    dataset is in fact the best proxy we can have about how our features will behave
    on new data. So, let’s make the same comparison on the validation set.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们真正关心的是验证数据集。验证数据集实际上是我们关于特征在新数据上表现的最佳代理。因此，让我们在验证集上进行相同的比较。
- en: '![Your Features Are Important? It Doesn’t Mean They Are Good](../Images/1680c235aae3b3781b8f75f45921aac5.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![你的特征重要吗？这并不意味着它们很好](../Images/1680c235aae3b3781b8f75f45921aac5.png)'
- en: Prediction Contribution vs. Error Contribution (on the Validation dataset).
    [Image by Author]
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 预测贡献与误差贡献（在验证数据集上）。 [图像来源：作者]
- en: From this plot, we can extract some much more interesting information.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个图表中，我们可以提取一些更有趣的信息。
- en: The features in the lower right part of the plot are those to which our model
    is correctly assigning high importance since they actually bring a reduction in
    the prediction error.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图表右下角的特征是模型正确地分配了高重要性的特征，因为它们实际减少了预测误差。
- en: Also, note that “Gold_T-22” (the return of gold 22 days before the observation
    period) is working really well compared to the importance that the model is attributing
    to it. This means that **this feature is possibly underfitting**. And this piece
    of information is particularly interesting since gold is the asset we are trying
    to predict (“Gold_T+22”).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，需要注意的是“Gold_T-22”（观测期前22天的黄金回报）与模型赋予它的重要性相比，效果非常好。这意味着**这个特征可能存在欠拟合**。这一信息特别有趣，因为黄金是我们试图预测的资产（“Gold_T+22”）。
- en: On the other hand, **the features that have an Error Contribution above 0 are
    making our predictions worse**. For instance, “US Bond ETF_T-1” on average changes
    the model prediction by 0.092% (Prediction Contribution), but it leads the model
    to make a prediction on average 0.013% (Error Contribution) worse than it would
    have been without that feature.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**具有大于0的错误贡献的特征正在使我们的预测变差**。例如，“US Bond ETF_T-1”平均会将模型预测值改变0.092%（预测贡献），但会导致模型预测比没有该特征时差0.013%（错误贡献）。
- en: We may suppose that **all the features with a high Error Contribution (compared
    to their Prediction Contribution) are probably overfitting **or, in general, they
    have different behavior in the training set and in the validation set.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以假设**所有高错误贡献（与其预测贡献相比）的特征可能正在过拟合**，或者通常，它们在训练集和验证集上的表现有所不同。
- en: Let’s see which features have the largest Error Contribution.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看哪些特征具有最大的错误贡献。
- en: '![Your Features Are Important? It Doesn’t Mean They Are Good](../Images/2414e0859e0164b763ed5ea4dd86437b.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![你的特征很重要？这并不意味着它们是好的](../Images/2414e0859e0164b763ed5ea4dd86437b.png)'
- en: Features sorted by decreasing Error Contribution. [Image by Author]
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 按错误贡献递减排序的特征。 [图像由作者提供]
- en: 'And now the features with the lowest Error Contribution:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来看错误贡献最低的特征：
- en: '![Your Features Are Important? It Doesn’t Mean They Are Good](../Images/2aa994302667d8b314b65a7be5025fc7.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![你的特征很重要？这并不意味着它们是好的](../Images/2aa994302667d8b314b65a7be5025fc7.png)'
- en: Features sorted by increasing Error Contribution. [Image by Author]
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 按错误贡献递增排序的特征。 [图像由作者提供]
- en: Interestingly, we may observe that all the features with higher Error Contribution
    are relative to T-1 (1 day before the observation moment), whereas almost all
    the features with smaller Error Contribution are relative to T-22 (22 days before
    the observation moment).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们可以观察到所有具有较高错误贡献的特征相对于T-1（观察时刻前1天），而几乎所有具有较小错误贡献的特征相对于T-22（观察时刻前22天）。
- en: This seems to indicate that **the most recent features are prone to overfitting,
    whereas the features more distant in time tend to generalize better**.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎表明**最新的特征更容易过拟合，而时间较久的特征往往具有更好的泛化能力**。
- en: Note that, without Error Contribution, we would never have known this insight.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，没有错误贡献，我们永远不会知道这一点。
- en: RFE Using Error Contribution
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用错误贡献的RFE
- en: Traditional Recursive Feature Elimination (RFE) methods are based on the removal
    of unimportant features. This is equivalent to removing the features with a small
    Prediction Contribution first.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的递归特征消除（RFE）方法基于移除不重要的特征。这相当于首先移除预测贡献较小的特征。
- en: However, based on what we said in the previous paragraph, it would make more
    sense to remove the features with the highest Error Contribution first.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，根据我们在前一段中所说的，首先移除具有最高错误贡献的特征会更有意义。
- en: 'To check whether our intuition is verified, let’s compare the two approaches:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的直觉，让我们比较这两种方法：
- en: '**Traditional RFE: removing useless features first **(lowest Prediction Contribution).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传统RFE：首先移除无用的特征**（最低的预测贡献）。'
- en: '**Our RFE: removing harmful features** **first **(highest Error Contribution).'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我们的RFE：首先移除有害的特征**（最高的错误贡献）。'
- en: 'Let’s see the results on the validation set:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看验证集上的结果：
- en: '![Your Features Are Important? It Doesn’t Mean They Are Good](../Images/bc42fe32d852095ee010ce90ac7716d9.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![你的特征很重要？这并不意味着它们是好的](../Images/bc42fe32d852095ee010ce90ac7716d9.png)'
- en: Mean Absolute Error of the two strategies on the validation set. [Image by Author]
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 两种策略在验证集上的平均绝对误差。 [图像由作者提供]
- en: 'The best iteration for each method has been circled: it’s the model with 19
    features for the traditional RFE (blue line) and the model with 17 features for
    our RFE (orange line).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 每种方法的最佳迭代已被圈出：传统的RFE（蓝线）是具有19个特征的模型，而我们的RFE（橙线）是具有17个特征的模型。
- en: 'In general, it seems that our method works well: removing the feature with
    the highest Error Contribution leads to a consistently smaller MAE compared to
    removing the feature with the highest Prediction Contribution.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，看来我们的方法效果很好：移除具有最高错误贡献的特征会导致MAE consistently 较小，而不是移除具有最高预测贡献的特征。
- en: However, you may think that this works well just because we are overfitting
    the validation set. After all, we are interested in the result that we will obtain
    on the test set.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你可能会认为这只是因为我们过拟合了验证集。毕竟，我们更关心的是在测试集上得到的结果。
- en: So let’s see the same comparison on the test set.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们在测试集上看一下相同的比较。
- en: '![Your Features Are Important? It Doesn’t Mean They Are Good](../Images/28c58e356255b572ec9c61b6f89c8663.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![你的特征重要吗？这并不意味着它们好](../Images/28c58e356255b572ec9c61b6f89c8663.png)'
- en: Mean Absolute Error of the two strategies on the test set. [Image by Author]
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 两种策略在测试集上的平均绝对误差。[图片来源作者]
- en: The result is similar to the previous one. Even if there is less distance between
    the two lines, the MAE obtained by removing the highest Error Contributor is clearly
    better than the MAE by obtained removing the lowest Prediction Contributor.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与之前类似。即使两条线之间的距离较小，去除最高错误贡献者所得到的MAE明显优于去除最低预测贡献者所得到的MAE。
- en: 'Since we selected the models leading to the smallest MAE on the validation
    set, let’s see their outcome on the test set:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们选择了在验证集上产生最小MAE的模型，让我们看看它们在测试集上的表现：
- en: 'RFE-Prediction Contribution (19 features). MAE on test set: 2.04.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RFE-预测贡献（19个特征）。测试集上的MAE：2.04。
- en: 'RFE-Error Contribution (17 features). MAE on test set: 1.94.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RFE-错误贡献（17个特征）。测试集上的MAE：1.94。
- en: So the best MAE using our method is 5% better compared to traditional RFE!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，使用我们的方法得到的最佳MAE比传统RFE好5%！
- en: Conclusions
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The concept of feature importance plays a fundamental role in machine learning.
    However, the notion of “importance” is often mistaken for “goodness”.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性的概念在机器学习中扮演着基础性的角色。然而，“重要性”这一概念常被误解为“好处”。
- en: 'In order to distinguish between these two aspects we have introduced two concepts:
    Prediction Contribution and Error Contribution. Both concepts are based on the
    SHAP values of the validation dataset, and in the article we have seen the Python
    code to compute them.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了区分这两个方面，我们引入了两个概念：预测贡献和错误贡献。这两个概念基于验证数据集的SHAP值，文章中我们已经看到计算它们的Python代码。
- en: We have also tried them on a real financial dataset (in which the task is predicting
    the price of Gold) and proved that Recursive Feature Elimination based on Error
    Contribution leads to a 5% better Mean Absolute Error compared to traditional
    RFE based on Prediction Contribution.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在一个真实的金融数据集上进行了尝试（任务是预测黄金价格），并证明基于错误贡献的递归特征消除比基于预测贡献的传统RFE具有5%的更好平均绝对误差。
- en: '*All the code used for this article can be found in *[*this notebook*](https://github.com/smazzanti/tds_features_important_doesnt_mean_good/blob/main/regression.ipynb)*.*'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文所用的所有代码可以在*[*这个笔记本*](https://github.com/smazzanti/tds_features_important_doesnt_mean_good/blob/main/regression.ipynb)*中找到。*'
- en: '*Thank you for reading!*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢阅读！*'
- en: '**[Samuele Mazzanti](https://www.linkedin.com/in/samuelemazzanti/)** is Lead
    Data Scientist at Jakala and currently lives in Rome. He graduated in Statistics
    and his main research interests concern machine learning applications for the
    industry. He is also a freelance content creator.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**[萨缪埃尔·马赞提](https://www.linkedin.com/in/samuelemazzanti/)** 是Jakala的首席数据科学家，目前居住在罗马。他拥有统计学学位，主要研究兴趣包括工业中的机器学习应用。他也是一名自由内容创作者。'
- en: '[Original](https://towardsdatascience.com/your-features-are-important-it-doesnt-mean-they-are-good-ff468ae2e3d4).
    Reposted with permission.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/your-features-are-important-it-doesnt-mean-they-are-good-ff468ae2e3d4)。转载经许可。'
- en: '* * *'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三名课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全领域的职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织IT需求'
- en: '* * *'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[What are Vector Databases and Why Are They Important for LLMs?](https://www.kdnuggets.com/2023/06/vector-databases-important-llms.html)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是向量数据库，它们为什么对LLMs重要？](https://www.kdnuggets.com/2023/06/vector-databases-important-llms.html)'
- en: '[What are Large Language Models and How Do They Work?](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[大型语言模型是什么，它们是如何工作的？](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
- en: '[What Are Foundation Models and How Do They Work?](https://www.kdnuggets.com/2023/05/foundation-models-work.html)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基础模型是什么，它们是如何工作的？](https://www.kdnuggets.com/2023/05/foundation-models-work.html)'
- en: '[What Can AI-Powered RPA and IA Mean For Businesses?](https://www.kdnuggets.com/2022/12/aipowered-rpa-ia-mean-businesses.html)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能驱动的 RPA 和 IA 对企业意味着什么？](https://www.kdnuggets.com/2022/12/aipowered-rpa-ia-mean-businesses.html)'
- en: '[Machine Learning is Not Like Your Brain Part Seven: What Neurons…](https://www.kdnuggets.com/2022/08/machine-learning-like-brain-part-seven-neurons-good.html)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习不像你的大脑 第七部分：神经元…](https://www.kdnuggets.com/2022/08/machine-learning-like-brain-part-seven-neurons-good.html)'
- en: '[5 ChatGPT Features to Boost your Daily Work](https://www.kdnuggets.com/2023/05/5-chatgpt-features-boost-daily-work.html)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5 个 ChatGPT 功能提升你的日常工作](https://www.kdnuggets.com/2023/05/5-chatgpt-features-boost-daily-work.html)'
