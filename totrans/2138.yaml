- en: Strategies for Optimizing Performance and Costs When Using Large Language Models
    in the Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/strategies-for-optimizing-performance-and-costs-when-using-large-language-models-in-the-cloud](https://www.kdnuggets.com/strategies-for-optimizing-performance-and-costs-when-using-large-language-models-in-the-cloud)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Strategies for Optimizing Performance and Costs When Using Large Language
    Models in the Cloud](../Images/99ce9c90cb6e9401eac8daa858d85add.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [pch.vector](https://www.freepik.com/free-vector/money-income-attraction_9176032.htm#query=cost&position=29&from_view=search&track=sph&uuid=61aa0541-882f-4e86-b49d-e7cc8e315f4b)
    on [Freepik](https://www.freepik.com/)
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Model (LLM) has recently started to find their foot in the business,
    and it will expand even further. As the company began understanding the benefits
    of implementing the LLM, the data team would adjust the model to the business
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The optimal path for the business is to utilize a cloud platform to scale any
    LLM requirements that the business needs. However, many hurdles could hinder LLM
    performance in the cloud and increase the usage cost. It is certainly what we
    want to avoid in the business.
  prefs: []
  type: TYPE_NORMAL
- en: That’s why this article will try to outline a strategy you could use to optimize
    the performance of LLM in the cloud while taking care of the cost. What’s the
    strategy? Let’s get into it.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Having a Clear Budget Plan
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We must understand our financial condition before implementing any strategy
    to optimize performance and costs. How much budget we are willing to invest in
    the LLM will become our limit. A higher budget could lead to more significant
    performance results but might not be optimal if it doesn’t support the business.
  prefs: []
  type: TYPE_NORMAL
- en: The budget plan needs extensive discussion with various stakeholders so it would
    not become a waste. Identify the critical focus your business wants to solve and
    assess if LLM is worth investing in.
  prefs: []
  type: TYPE_NORMAL
- en: The strategy also applies to any solo business or individual. Having a budget
    for the LLM that you are willing to spend would help your financial problem in
    the long run.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Decide the Right Model Size and Hardware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the advancement of research, there are many kinds of LLMs that we can choose
    to solve our problem. With a smaller parameter model, it would be faster to optimize
    but might not have the best ability to solve your business problems. While a bigger
    model has a more excellent knowledge base and creativity, it costs more to compute.
  prefs: []
  type: TYPE_NORMAL
- en: There are trade-offs between the performance and cost with the change in the
    LLM size, which we need to take into account when we decide on the model. Do we
    need to have bigger parameter models that have better performance but require
    higher cost, or vice versa? It’s a question we need to ask. So, try to assess
    your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the cloud Hardware could affect the performance as well. Better
    GPU memory might have a faster response time, allow for more complex models, and
    reduce latency. However, higher memory means higher cost.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Choose the Suitable Inference Options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on the cloud platform, there would be many choices for the inferences.
    Comparing your application workload requirements, the option you want to choose
    might be different as well. However, inference could also affect the cost usage
    as the number of resources is different for each option.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we take an example from [Amazon SageMaker Inferences Options](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html),
    your inference options are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-Time Inference**. The inference processes the response instantly when
    input comes. It’s usually the inferences used in real-time, such as chatbot, translator,
    etc. Because it always requires low latency, the application would need high computing
    resources even in the low-demand period. This would mean that LLM with Real-Time
    inference could lead to higher costs without any benefit if the demand isn’t there.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Serverless Inference.** This inference is where the cloud platform scales
    and allocates the resources dynamically as required. The performance might suffer
    as there would be slight latency for each time the resources are initiated for
    each request. But, it’s the most cost-effective as we only pay for what we use.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Batch Transform**. The inference is where we process the request in batches.
    This means that the inference is only suitable for offline processes as we don’t
    process the request immediately. It might not be suitable for any application
    that requires an instant process as the delay would always be there, but it doesn’t
    cost much.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Asynchronous Inference**. This inference is suitable for background tasks
    because it runs the inference task in the background while the results are retrieved
    later. Performance-wise, it’s suitable for models that require a long processing
    time as it can handle various tasks concurrently in the background. Cost-wise,
    it could be effective as well because of the better resource allocation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try to assess what your application needs, so you have the most effective inference
    option.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Construct an Effective Prompts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLM is a model with a particular case, as the number of tokens affects the cost
    we would need to pay. That’s why we need to build a prompt effectively that uses
    the minimum token either for the input or the output while still maintaining the
    output quality.
  prefs: []
  type: TYPE_NORMAL
- en: Try to build a prompt that specifies a certain amount of paragraph output or
    use a concluding paragraph such as “summarize,” “concise,” and any others. Also,
    precisely construct the input prompt to generate the output you need. Don’t let
    the LLM model generate more than you need.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Caching Responses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There would be information that would be repeatedly asked and have the same
    responses every time. To reduce the number of queries, we can cache all the typical
    information in the database and call them when it’s required.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the data is stored in a vector database such as Pinecone or Weaviate,
    but cloud platform should have their vector database as well. The response that
    we want to cache would converted into vector forms and stored for future queries.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few challenges when we want to cache the responses effectively,
    as we need to manage policies where the cache response is inadequate to answer
    the input query. Also, some caches are similar to each other, which could result
    in a wrong response. Manage the response well and have an adequate database that
    could help reduce costs.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LLM that we deploy might end up costing us too much and have inaccurate performance
    if we don’t treat them right. That’s why here are some strategies you could employ
    to optimize the performance and cost of your LLM in the cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: Have a clear budget plan,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decide the right model size and hardware,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the suitable inference options,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct effective prompts,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Caching responses.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** is a data science
    assistant manager and data writer. While working full-time at Allianz Indonesia,
    he loves to share Python and data tips via social media and writing media. Cornellius
    writes on a variety of AI and machine learning topics.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[The Best Strategies for Fine-Tuning Large Language Models](https://www.kdnuggets.com/the-best-strategies-for-fine-tuning-large-language-models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Make Large Language Models Play Nice with Your Software…](https://www.kdnuggets.com/how-to-make-large-language-models-play-nice-with-your-software-using-langchain)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How a Level System can Help Forecast AI Costs](https://www.kdnuggets.com/2022/03/level-system-help-forecast-ai-costs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Optimizing Your LLM for Performance and Scalability](https://www.kdnuggets.com/optimizing-your-llm-for-performance-and-scalability)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are Large Language Models and How Do They Work?](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Optimizing Python Code Performance: A Deep Dive into Python Profilers](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
