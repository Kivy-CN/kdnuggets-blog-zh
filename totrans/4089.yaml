- en: 'Optimizing Python Code Performance: A Deep Dive into Python Profilers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Optimizing Python Code Performance: A Deep Dive into Python Profilers](../Images/3f0cd6354634d9283392f4e057c0025d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Although Python is one of the most widely used programming languages, when it
    comes to large data sets it often suffers from poor execution times. Profiling
    is one of the methods to dynamically monitor the performance of your code and
    identify the pitfalls. These pitfalls may indicate the presence of bugs or poorly
    written code that is consuming a lot of system resources. Using Profilers will
    provide detailed statistics of your program that you can use to optimize your
    code for better performance. Let's take a look at some of the Python Profilers
    along with their examples.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. cProfile
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: cProfile is a built-in profiler in Python that traces every function call in
    your program. It provides detailed information about how frequently a function
    was called, and its average execution times. As it comes with the standard Python
    library so we do not need to install it explicitly. However, it is not suitable
    for profiling live data as it traps every single function call and generates a
    lot of statistics by default.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Output**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '| ncalls | tottime | percall | cumtime | percall | percall filename:lineno(function)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.000 | 0.000 | 0.002 | 0.002 | <string>:1(<module>) |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.002 | 0.002 | 0.002 | 0.002 | cprofile.py:3(sum_) |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.000 | 0.000 | 0.002 | 0.002 | {built-in method builtins.exec} |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.000 | 0.000 | 0.000 | 0.000 | {method ''disable'' of ''_lsprof.Profiler''
    objects} |'
  prefs: []
  type: TYPE_TB
- en: As you can see from the output, the cProfile module provides a lot of information
    about the function's performance.
  prefs: []
  type: TYPE_NORMAL
- en: ncalls =  Number of times the function was called
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tottime =  Total time spent in the function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: percall = Total time spent per call
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cumtime =  Cumulative time spent in this and all sub-functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: percall = Cumulative time spent per call.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2\. Line Profiler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Line Profiler is a powerful python module that performs line-by-line profiling
    of your code. Sometimes, the hotspot in your code may be a single line and it
    is not easy to locate it from the source code directly.  Line Profiler is valuable
    in identifying how much time is taken by each line to execute and which sections
    need the most attention for optimization. However, it does not come with the standard
    python library and needs to be installed using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Output**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '| Line # | Hits | Time | Per Hit | % Time | Line Contents |'
  prefs: []
  type: TYPE_TB
- en: '| 2 |  |  |  |  | def sum_arrays(): |'
  prefs: []
  type: TYPE_TB
- en: '| 3 |  |  |  |  | # creating large arrays   |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1 | 168563.0  | 168563.0  | 30.0 | arr1 = [1] * (10 ** 6)  |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 1 | 3583.0 | 3583.0 | 0.6 | arr2 = [2] * (2 * 10 ** 7) |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 1 | 389997.0  | 389997.0  | 69.4 | return arr1 + arr2 |'
  prefs: []
  type: TYPE_TB
- en: 'Line #  = Line number in your code file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hits  = No of times it was executed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time  = Total time spent to execute the line
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Per Hit = Average time spent per hit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '% Time = Percentage of time spent on the line relative to the total time of
    function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Line Contents = Actual Source Code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3\. Memory Profiler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Memory profiler is a python profiler that tracks the memory allocation of your
    code. It can also generate flame graphs to help analyze memory usage and identify
    the memory leaks in your code. It is also useful to identify the hotspot regions
    that are causing a lot of allocations because python applications are often prone
    to memory management issues. Memory profilers profile the line-by-line statistics
    about the memory consumption and it needs to be installed using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Output**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '| Line # | Mem usage | Increment | Occurrences | Line Contents |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 21.7 MiB  | 21.7 MiB  | 1 | def avg_marks(): |'
  prefs: []
  type: TYPE_TB
- en: '| 5 |  |  |  | # Genrating Random marks for 50 students for each section |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 21.8 MiB | 0.0 MiB  | 1 | sec_a = random.sample(range(0, 100), 50) |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 21.8 MiB | 0.0 MiB  | 1 | sec_b = random.sample(range(0, 100), 50) |'
  prefs: []
  type: TYPE_TB
- en: '| 8 |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 9 |  |  |  | # combined average marks of two sections |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 21.8 MiB | 0.0 MiB  | 1 | avg_a = sum(sec_a) / len(sec_a) |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 21.8 MiB | 0.0 MiB  | 1 | avg_b =  sum(sec_b) / len(sec_b) |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 21.8 MiB | 0.0 MiB  | 1 | combined_avg = (avg_a + avg_b)/2 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 21.8 MiB | 0.0 MiB  | 1 | return combined_avg |'
  prefs: []
  type: TYPE_TB
- en: 'Line #  = Line number in your code file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mem usage = Memory usage of Python Interpreter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increment  =  Difference in memory consumed of the current line to previous
    one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Occurences = No of times the code line was executed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Line Contents = Actual Source Code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4\. Timeit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Timeit is a built-in Python library that is specifically designed for evaluating
    the performance of small code snippets. It is a powerful tool that can help you
    identify and optimize the performance bottlenecks in your code, allowing you to
    write faster and more efficient code. Different implementations of an algorithm
    can also be compared using the timeit module but the downside is that only the
    individual lines of blocks of code can be analyzed using it.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Output**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Its usage is limited to only evaluating the smaller code snippets. One important
    thing to note is that it displays different times each time the code snippet is
    run. This is because you may have other processes running on your computer and
    the allocation of the resources may vary from one run to the other, making it
    difficult to control all the variables and achieve the same processing time for
    each run.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Yappi
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yappi is a python profiler that allows you to easily identify performance bottlenecks.
    It is written in C, making it one of the most efficient profilers available. It
    has a customizable API that lets you profile only the specific parts of your code
    that you need to focus on, giving you more control over the profiling process.
    Its ability to profile concurrent coroutines provides an in-depth understanding
    of how your code is functioning.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Note:** Install yappi using this command: `pip install yappi`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Output**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '| name | ncall | tsub | ttot | tavg |'
  prefs: []
  type: TYPE_TB
- en: '| ..lers\yappiProfiler.py:4 sum_arrays | 1 | 0.109375 | 0.109375 | 0.109375
    |'
  prefs: []
  type: TYPE_TB
- en: '| builtins. next | 1 | 0.000000 | 0.000000 | 0.000000 |'
  prefs: []
  type: TYPE_TB
- en: '| .. _GeneratorContextManager.__exit__ | 1 | 0.000000 | 0.000000 | 0.000000
    |'
  prefs: []
  type: TYPE_TB
- en: '--------- Thread Stats -----------'
  prefs: []
  type: TYPE_NORMAL
- en: '| name | id | tid | ttot | scnt |'
  prefs: []
  type: TYPE_TB
- en: '| _MainThread | 0 | 15148 | 0.187500  | 1 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Remember to name your modules differently for the built-in modules. Otherwise,
    the import will import your module(i.e your python file) instead of the real built-in
    modules.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By using these profilers, developers can identify bottlenecks in their code
    and decide which implementation is best. With the right tools and a little bit
    of know-how, anyone can take their Python code to the next level of performance.
    So, get ready to optimize your Python performance optimization and watch it soar
    to new heights!
  prefs: []
  type: TYPE_NORMAL
- en: I am glad that you decided to read this article and I hope it has been a valuable
    experience for you.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Kanwal Mehreen](https://www.linkedin.com/in/kanwal-mehreen1)** is an aspiring
    software developer with a keen interest in data science and applications of AI
    in medicine. Kanwal was selected as the Google Generation Scholar 2022 for the
    APAC region. Kanwal loves to share technical knowledge by writing articles on
    trending topics, and is passionate about improving the representation of women
    in tech industry.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[A Deep Dive into GPT Models: Evolution & Performance Comparison](https://www.kdnuggets.com/2023/05/deep-dive-gpt-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Optimizing Your LLM for Performance and Scalability](https://www.kdnuggets.com/optimizing-your-llm-for-performance-and-scalability)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Strategies for Optimizing Performance and Costs When Using Large…](https://www.kdnuggets.com/strategies-for-optimizing-performance-and-costs-when-using-large-language-models-in-the-cloud)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unveiling Neural Magic: A Dive into Activation Functions](https://www.kdnuggets.com/unveiling-neural-magic-a-dive-into-activation-functions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Dive into the Future with Kaggle''s AI Report 2023 – See What''s Hot](https://www.kdnuggets.com/dive-into-the-future-with-kaggle-ai-report-2023-see-what-hot)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Optimizing Genes with a Genetic Algorithm](https://www.kdnuggets.com/2022/04/optimizing-genes-genetic-algorithm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
