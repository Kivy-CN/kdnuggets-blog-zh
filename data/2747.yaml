- en: 'A Classification Project in Machine Learning: a gentle step-by-step guide'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的分类项目：一个温和的逐步指南
- en: 原文：[https://www.kdnuggets.com/2020/06/classification-project-machine-learning-guide.html](https://www.kdnuggets.com/2020/06/classification-project-machine-learning-guide.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/06/classification-project-machine-learning-guide.html](https://www.kdnuggets.com/2020/06/classification-project-machine-learning-guide.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '![](../Images/da0440bea369d85c1810c4ab99f84b89.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da0440bea369d85c1810c4ab99f84b89.png)'
- en: 'Classification is one of the main kinds of projects you can face in the world
    of Data Science and Machine Learning. Here is Wikipedia’s definition:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 分类是数据科学和机器学习领域中你可能遇到的主要项目之一。以下是维基百科的定义：
- en: '*Classification is the problem of identifying to which of a set of categories
    (sub-populations) a new observation belongs, on the basis of a training set of
    data containing observations (or instances) whose category membership is known.
    Examples are assigning a given email to the “spam” or “non-spam.”*'
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*分类是识别新观察数据属于一组类别（子群体）中的哪个类别的问题，基于包含已知类别成员的数据训练集。举例来说，就是将给定的电子邮件分配到“垃圾邮件”或“非垃圾邮件”中。*'
- en: 'For this post, I’ll go through a project from my [General Assembly’s Immersive
    in Data Science](https://generalassemb.ly/education/data-science-immersive/).
    In this, I explored different machine learning classification models to predict
    four salary categories for Data Science job posts using publications from Indeed.co.uk:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将讲解一个来自[General Assembly的数据科学沉浸课程](https://generalassemb.ly/education/data-science-immersive/)的项目。在这个项目中，我探索了不同的机器学习分类模型，以预测来自Indeed.co.uk的Data
    Science职位的四个薪资类别：
- en: Salary below percentile 25%
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第25百分位以下的薪资
- en: Salary between percentile 25 and 50%
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第25至50百分位之间的薪资
- en: Salary between percentile 50 and 75%
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第50至75百分位之间的薪资
- en: Salary above percentile 75%
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第75百分位以上的薪资
- en: We won’t be able to go through every single aspect of the project, but be aware
    that the entire repository is available on my [GitHub profile](https://github.com/gonzaferreiro).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法逐一讨论项目的每一个方面，但请注意，整个代码库可以在我的[GitHub个人资料](https://github.com/gonzaferreiro)上找到。
- en: 'First Stage: Scraping and Cleaning'
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第一阶段：数据抓取和清洗
- en: First and foremost, no project will ever be anything without data. So I started
    by scraping [Indeed.co.uk](https://www.indeed.co.uk/jobs?q=data+scientist) in
    order to obtain a list of job posts looking for ‘data scientists’ in several cities
    of the UK. I won’t cover how to actually do the scraping here, but I used the
    same techniques and tools mentioned in another post of mine: [Web scraping in
    five minutes](https://towardsdatascience.com/web-scraping-in-5-minutes-1caceca13b6c?source=friends_link&sk=3d2c281449fc6584e4efb272245f8865).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，没有数据，任何项目都无从谈起。因此，我首先通过抓取[Indeed.co.uk](https://www.indeed.co.uk/jobs?q=data+scientist)以获取在英国多个城市寻找“数据科学家”的职位列表。我不会在这里详细讲解如何进行抓取，但我使用了我另一篇文章中提到的相同技术和工具：[五分钟网络抓取](https://towardsdatascience.com/web-scraping-in-5-minutes-1caceca13b6c?source=friends_link&sk=3d2c281449fc6584e4efb272245f8865)。
- en: It’s worth mention though that even though web scraping is great and very useful
    for those working in data science, always check the completeness of your data
    once you finish scraping. For example, in this case, having the job post salary
    was, of course, key. However, not all publications on Indeed include salary, so
    it was necessary to scrap thousands of pages and job posts in order to have at
    least 1000 job posts that contain a salary.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，尽管网络抓取对数据科学工作者非常有用，但完成抓取后一定要检查数据的完整性。例如，在这个案例中，职位薪资当然是关键。然而，并不是所有的职位信息都包含薪资，因此需要抓取成千上万的页面和职位信息，以确保至少有1000个职位包含薪资信息。
- en: Working with scraped data usually also involves lots of feature engineering
    to add some value from the data we already have. For example, for this project,
    I developed a ‘Seniority’ feature, which is created from the Title and Summary
    of each publication, using two different lists with words belonging to senior
    or junior levels of jobs. If any word of each level was present, either on the
    job title, in the summary, then the corresponding seniority level was assigned.
    If none of the words were in those features, the job post was assigned as a middle-level.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 处理抓取的数据通常还涉及大量的特征工程，以从我们已有的数据中添加一些价值。例如，对于这个项目，我开发了一个“资历”特征，该特征由每篇出版物的标题和摘要生成，使用了两个不同的单词列表，分别属于高级或初级职位。如果职位标题或摘要中出现了任何一个层级的单词，则分配相应的资历层级。如果这些特征中没有任何单词，则该职位被分配为中级层级。
- en: 'Second Stage: Modelling'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第二阶段：建模
- en: 'I started this stage exploring three different models:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我开始这个阶段时探索了三种不同的模型：
- en: '**A KNN model with bagging**: KNN stands for a K-Nearest Neighbours model.
    This works by checking the class of the nearest points to the one being predicted,
    in order to classify it. Combining it with bagging, we can improve the stability
    and accuracy while also reducing variance and helping to avoid overfitting.*How?* Bagging
    is an ensemble method — a technique that combines the predictions from multiple
    machine learning algorithms to make more accurate predictions than any individual
    model. Although it’s usually applied to decision tree methods, it can be used
    with any type of method.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个带有装袋的 KNN 模型**：KNN 代表 K-Nearest Neighbours 模型。这通过检查最接近被预测点的点的类别来工作，以对其进行分类。将其与装袋结合，我们可以提高稳定性和准确性，同时减少方差并帮助避免过拟合。*怎么做？*
    装袋是一种集成方法——一种结合多个机器学习算法的预测以比任何单一模型更准确地预测的技术。尽管通常应用于决策树方法，但它可以与任何类型的方法一起使用。'
- en: '**A Decision Tree model with boosting**: in this case a decision tree works
    as a flowchart-like structure in which each internal node represents a “test”
    on an attribute (e.g., whether a coin flip comes up heads or tails), each branch
    represents the outcome of the test, and each leaf node represents a class label
    and a decision taken. The paths from the root to the leaf represent classification
    rules. In this model, although boosting is a very different method than bagging,
    it is also an ensemble method — one that works by building a model from the training
    data, then creating a second model that attempts to correct the errors from the
    first model. Models are added until the training set is predicted perfectly, or
    a maximum number of models are added.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个带有提升的决策树模型**：在这种情况下，决策树像一个流程图结构，其中每个内部节点代表对属性的“测试”（例如，硬币翻转是正面还是反面），每个分支代表测试的结果，每个叶子节点代表一个类标签和一个做出的决策。从根到叶的路径代表分类规则。在这个模型中，尽管提升是一种与装袋截然不同的方法，但它也是一种集成方法——它通过从训练数据中建立一个模型，然后创建第二个模型以试图纠正第一个模型的错误来工作。模型会不断增加，直到训练集被完美预测，或添加到最大模型数。'
- en: Since these two models are highly dependent on the given hyperparameters, you’ll
    probably want to use *GridSearch *in order to optimize them as much as possible. *[GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) *is
    simply a tool that trains several models looking for the best parameters from
    a given list of parameters and values.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两个模型高度依赖给定的超参数，你可能会希望使用*GridSearch*来尽可能地优化它们。*[GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)*只是一个工具，它通过训练多个模型来寻找从给定参数和数值列表中得到的最佳参数。
- en: So, for example, for creating a Decision Tree model with boosting and *GridSearch *you
    would take the following steps.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，例如，创建一个带有提升和*GridSearch*的决策树模型，你需要执行以下步骤。
- en: '**1\. Instantiate the model**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 实例化模型**'
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**2\. Instantiate the ensemble method algorithm**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 实例化集成方法算法**'
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**3\. Instantiate GridSearch and specify the parameters to be tested**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 实例化 GridSearch 并指定要测试的参数**'
- en: 'When using *GridSearch *you can get the available parameters to be tuned just
    by calling *get_params()* over the previously instantiated model:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*GridSearch*时，你可以通过调用*get_params()*来获取可以调整的可用参数：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Remember: you can always get more detail about how to optimize any hyperparameters
    in Sklearn’s documentation. For example, here is the [decision trees doc](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住：你总是可以在 Sklearn 的文档中获取有关如何优化任何超参数的更多细节。例如，以下是[决策树文档](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)。
- en: Finally, let’s import *GridSearch*, specify the parameters wanted and instantiate
    the object. Be aware that sklearn’s *GridSearchCV *includes the cross-validation
    within the algorithm, so you will have to specify the number of CV to be done
    too,
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们导入*GridSearch*，指定所需的参数并实例化对象。请注意，sklearn 的*GridSearchCV*在算法中包含了交叉验证，因此你还需要指定要进行的
    CV 数量，
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**4\. Fit your combined GridSearch and check the results**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 拟合你的组合 GridSearch 并检查结果**'
- en: 'Fitting the *GridSearch *is like fitting any model:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合*GridSearch*就像拟合任何模型一样：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once it’s done you can check the best parameters to see if you still have an
    opportunity to optimize any of them. Just run the following piece of code:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，你可以检查最佳参数，看看是否还有机会优化它们。只需运行以下代码：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As in any mode, you can use *.score()* and *.predict()* using the *GridSearchCV *object.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 和任何模式一样，你可以使用 *.score()* 和 *.predict()* 使用 *GridSearchCV* 对象。
- en: 'Stage Three: Feature Importance'
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第三阶段：特征重要性
- en: After modeling, the next stage is always analyzing how our model is performing
    and why it is doing what it’s doing.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模之后，下一个阶段总是分析我们的模型表现如何以及为什么会这样。
- en: However, if you’ve had the chance to work with ensemble methods, you probably
    already know that these algorithms are usually known as “black-box models.” These
    models lack explicability and interpretability since the way they usually work
    implies one or several layers of a machine making decisions without human supervision,
    apart from a group of rules or parameters set. More often than not, not even the
    most expert professionals in the field can understand the function that is actually
    created by, for example, training a neural network.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你有机会使用过集成方法，你可能已经知道这些算法通常被称为“黑箱模型”。这些模型缺乏解释性和可解释性，因为它们通常的工作方式涉及一个或多个层的机器在没有人工监督的情况下做出决策，除了设置的一组规则或参数。往往，甚至领域中最专业的人员也无法理解通过训练神经网络实际创建的函数。
- en: In this sense, some of the most classical machine learning models were actually
    better. That’s why, for the sake of this post, we’ll be analyzing the feature
    importance of our project using a classic Logistic Regression. However, if you’re
    interested in knowing how to analyze feature importance for a black-box model,
    in [this other article of mine](https://towardsdatascience.com/unboxing-machine-learning-feature-importance-for-black-box-models-ea12268ddb23?source=friends_link&sk=52d12526d6c199d78d680ad05118449d), I
    explored a tool for doing just that.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个意义上说，一些最经典的机器学习模型实际上更好。这就是为什么在本文中，我们将使用经典的逻辑回归分析我们项目的特征重要性。然而，如果你有兴趣了解如何分析黑箱模型的特征重要性，在[我另外一篇文章](https://towardsdatascience.com/unboxing-machine-learning-feature-importance-for-black-box-models-ea12268ddb23?source=friends_link&sk=52d12526d6c199d78d680ad05118449d)中，我探讨了一个工具来实现这一点。
- en: 'Starting from a Logistic Regression model, getting the feature importance is
    as easy as calling:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从逻辑回归模型开始，获取特征重要性就像调用以下内容一样简单：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: A neat way of seeing the overall feature importance is by creating a *DataFrame *with
    the feature importance for each class. I like to do it using the absolute value
    for each feature, in order to see the absolute impact each one has in the model.
    However, mind that if you want to analyze specifically how each feature helps
    to increase or decrease the possibility of being each class, you should take the
    original value, whether it is negative or positive.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 查看整体特征重要性的一个简洁方法是创建一个 *DataFrame*，包含每个类别的特征重要性。我喜欢使用每个特征的绝对值，以便查看每个特征对模型的绝对影响。然而，请注意，如果你想具体分析每个特征如何帮助增加或减少属于每个类别的可能性，你应该使用原始值，无论是负值还是正值。
- en: 'Remember we were trying to predict four classes, so this is how we should create
    the Pandas *DataFrame*:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 记住我们在尝试预测四个类别，因此我们应该这样创建 Pandas *DataFrame*：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can finally put everything in plots and see how each class behaves:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以最终将所有内容放入图表中，查看每个类别的表现：
- en: '![](../Images/afac478005552c36df2b63bb8f2ae51a.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afac478005552c36df2b63bb8f2ae51a.png)'
- en: 'Even though perhaps the size of the labels doesn’t help, we can conclude from
    these plots that the following features of our dataset are relevant when predicting
    the salary category:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管标签的大小可能没有帮助，但我们可以从这些图表中得出结论，我们数据集中的以下特征在预测薪资类别时是相关的：
- en: 'Seniority: as we can see, the tree levels created impact very strongly in all
    categories, being the first coefficients in terms of absolute size.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级别：正如我们所见，树的层级对所有类别都有很强的影响，是绝对大小方面的第一个系数。
- en: In the second place, comes the *Job_Type* directly scraped from Indeed
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次是直接从 Indeed 抓取的 *Job_Type*
- en: 'Finally, for all the salary categories, there are two job titles scraped and
    cleaned from indeed: Web Content Specialist and Test Engineer.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，对于所有薪资类别，有两个职位从 indeed 上抓取并清洗过来：网页内容专家和测试工程师。
- en: This dataset contains hundreds of features, but it’s nice to see there’s a clear
    trend throughout the categories!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含数百个特征，但很高兴看到在各类别中有明显的趋势！
- en: 'Stage Four: Conclusions and Trustworthiness'
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第四阶段：结论和可信度
- en: In the end, the only thing left is to evaluate the performance of our model.
    For this, we can use several metrics. Unfortunately, going through all the possible
    metrics in a classification problem would be too long for this post. However,
    I can refer you to a very good one here in Medium, giving good details about all
    the key metrics. Enjoy it [here](https://medium.com/thalus-ai/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，剩下的唯一任务就是评估我们模型的表现。为此，我们可以使用多个指标。不幸的是，全面讲解分类问题中的所有可能指标对于这篇文章来说太长了。然而，我可以在这里推荐一个非常好的
    Medium 文章，它详细介绍了所有关键指标。请在[这里](https://medium.com/thalus-ai/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b)享受它。
- en: 'As can read in Mohammed’s story linked above, the Confusion Matrix is the mother
    concept involving all the rest of the metrics. In short, it has the true labels
    or categories on one axis and the predicted ones on the other. In the end, we’d
    like to have a diagonal match in between our predictions and the real labels,
    with ideally zero or few cases mismatching. The metrics library from Sklearn has
    a beautiful and simple representation that we can plot just by feeding the algorithm
    with the real label and our predictions:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 正如上面链接的 Mohammed 故事中所述，混淆矩阵是包含所有其他指标的核心概念。简而言之，它在一个轴上显示真实标签或类别，在另一个轴上显示预测标签。最终，我们希望预测和真实标签之间有对角线匹配，理想情况下，匹配的情况为零或极少。Sklearn
    的指标库提供了一个美观且简单的表示方式，我们只需将真实标签和我们的预测输入算法即可绘制：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Using this library, we can see in the following plots that, for this project,
    both the train and test groups were predicted with a solid accuracy throughout
    the four salary categories:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个库，我们可以在以下图中看到，对于这个项目，训练组和测试组在四个薪水类别中都预测得相当准确：
- en: '![](../Images/7b651a046f3d4d7ce4ecc1a77d89c30f.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b651a046f3d4d7ce4ecc1a77d89c30f.png)'
- en: One important final clarification is that, although our final model seems to
    be accurate, it works well to predict categories when the importance of them is
    equal, and we don’t have the need to ponder any class or classes.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的最终澄清是，尽管我们的最终模型似乎很准确，但它在预测类别时效果良好，当类别的重要性相等时，我们不需要对任何类别进行加权。
- en: For example, if we were creating this model for a company, for which it would
    be more consequential to tell a person incorrectly that they would get a *low *salary
    job than to tell a client incorrectly that they would get a *high *salary job,
    our model would struggle, since it wouldn’t be able to predict all the positive
    values of a class as positive, without predicting a lot of negative values incorrectly
    as well. In that case, we should work another way around this problem — for example,
    by creating a model with weighted categories.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们为一家公司创建这个模型，在这种情况下，错误地告知某人他们将获得*低*薪水工作比错误地告诉客户他们将获得*高*薪水工作更为严重，我们的模型可能会遇到困难，因为它无法准确预测所有正类值为正类，同时也无法避免错误地预测大量负类值。在这种情况下，我们应该另辟蹊径，例如，创建一个具有加权类别的模型。
- en: '[Original](https://medium.com/better-programming/facing-a-classification-project-in-machine-learning-462b319873de).
    Reposted with permission.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/better-programming/facing-a-classification-project-in-machine-learning-462b319873de)。经许可转载。'
- en: '**Bio:** After 5+ years of experience in eCommerce and Marketing across multiple
    industries, [Gonzalo Ferreiro Volpi](https://www.linkedin.com/in/gferreirovolpi/)
    pivoted into the world of Data Science and Machine Learning, and currently works
    at Ravelin Technology using a combination of machine learning and human insights
    to tackle fraud in eCommerce.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：** 在多个行业拥有5年以上的电子商务和营销经验之后，[Gonzalo Ferreiro Volpi](https://www.linkedin.com/in/gferreirovolpi/)
    转向了数据科学和机器学习领域，目前在 Ravelin Technology 工作，利用机器学习和人类洞察来解决电子商务中的欺诈问题。'
- en: '**Related:**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Model Evaluation Metrics in Machine Learning](https://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的模型评估指标](https://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html)'
- en: '[Linear to Logistic Regression, Explained Step by Step](https://www.kdnuggets.com/2020/03/linear-logistic-regression-explained.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性回归到逻辑回归，逐步解析](https://www.kdnuggets.com/2020/03/linear-logistic-regression-explained.html)'
- en: '[Idiot’s Guide to Precision, Recall, and Confusion Matrix](https://www.kdnuggets.com/2020/01/guide-precision-recall-confusion-matrix.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[傻瓜指南：精准率、召回率与混淆矩阵](https://www.kdnuggets.com/2020/01/guide-precision-recall-confusion-matrix.html)'
- en: '* * *'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在 IT 领域'
- en: '* * *'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[How To Structure a Data Science Project: A Step-by-Step Guide](https://www.kdnuggets.com/2022/05/structure-data-science-project-stepbystep-guide.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何结构化数据科学项目：逐步指南](https://www.kdnuggets.com/2022/05/structure-data-science-project-stepbystep-guide.html)'
- en: '[A Gentle Introduction to Natural Language Processing](https://www.kdnuggets.com/2022/06/gentle-introduction-natural-language-processing.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理的温和介绍](https://www.kdnuggets.com/2022/06/gentle-introduction-natural-language-processing.html)'
- en: '[A Gentle Introduction to Support Vector Machines](https://www.kdnuggets.com/2023/07/gentle-introduction-support-vector-machines.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[支持向量机的温和介绍](https://www.kdnuggets.com/2023/07/gentle-introduction-support-vector-machines.html)'
- en: '[A Guide On How To Become A Data Scientist (Step By Step Approach)](https://www.kdnuggets.com/2021/05/guide-become-data-scientist.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何成为数据科学家的指南（逐步方法）](https://www.kdnuggets.com/2021/05/guide-become-data-scientist.html)'
- en: '[A Step-by-Step Guide to Web Scraping with Python and Beautiful Soup](https://www.kdnuggets.com/2023/04/stepbystep-guide-web-scraping-python-beautiful-soup.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 和 Beautiful Soup 进行网页抓取的逐步指南](https://www.kdnuggets.com/2023/04/stepbystep-guide-web-scraping-python-beautiful-soup.html)'
- en: '[Text-2-Video Generation: Step-by-Step Guide](https://www.kdnuggets.com/2023/08/text2video-generation-stepbystep-guide.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本到视频生成：逐步指南](https://www.kdnuggets.com/2023/08/text2video-generation-stepbystep-guide.html)'
