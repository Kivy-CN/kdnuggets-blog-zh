- en: Building a Wikipedia Text Corpus for Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个用于自然语言处理的维基百科文本语料库
- en: 原文：[https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html](https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html](https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html)
- en: One of the first things required for natural language processing (NLP) tasks
    is a corpus. In linguistics and NLP, **corpus** (literally Latin for body) refers
    to a collection of texts. Such collections may be formed of a single language
    of texts, or can span multiple languages -- there are numerous reasons for which
    multilingual corpora (the plural of corpus) may be useful. Corpora may also consist
    of themed texts (historical, Biblical, etc.). Corpora are generally solely used
    for statistical linguistic analysis and hypothesis testing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自然语言处理 (NLP) 任务，所需的第一件事之一是一个语料库。在语言学和 NLP 中，**语料库**（字面意思是拉丁语中的“身体”）指的是一组文本。这些集合可以由单一语言的文本组成，也可以跨越多种语言——多语言语料库（语料库的复数形式）可能有许多用途。语料库还可以由主题文本（历史的、圣经的等）组成。语料库通常仅用于统计语言分析和假设检验。
- en: The good thing is that the internet is filled with text, and in many cases this
    text is collected and well oganized, even if it requires some finessing into a
    more usable, precisely-defined format. Wikipedia, in particular, is a rich source
    of well-organized textual data. It's also a vast collection of knowledge, and
    the unhampered mind can dream up all sorts of uses for just such a body of text.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，互联网充满了文本，而且在许多情况下，这些文本已经被收集和组织得很好，即使它需要一些调整以适应更可用、更精确定义的格式。特别是维基百科，是一个组织良好的丰富文本数据源。它也是一个庞大的知识集合，开放的头脑可以为这样的文本集合构思各种用途。
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在的组织的 IT 需求'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: What we will do here is build a corpus from the set of English Wikipedia articles,
    which is freely and conveniently available online.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里从一组英文维基百科文章中构建一个语料库，这些文章可以在网上自由和方便地获取。
- en: '![Wikipedia](../Images/8f2962b58d464bf97d168a4617f22220.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![维基百科](../Images/8f2962b58d464bf97d168a4617f22220.png)'
- en: '**Install gensim**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装 gensim**'
- en: 'In order to easily build a text corpus void of the Wikipedia article markup,
    we will use [gensim](https://radimrehurek.com/gensim/index.html), a [topic modeling](https://en.wikipedia.org/wiki/Topic_model)
    library for Python. Specifically, the `gensim.corpora.wikicorpus.WikiCorpus` class
    is made just for this task:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了轻松构建一个没有维基百科文章标记的文本语料库，我们将使用 [gensim](https://radimrehurek.com/gensim/index.html)，这是一个用于
    Python 的 [主题建模](https://en.wikipedia.org/wiki/Topic_model) 库。具体来说，`gensim.corpora.wikicorpus.WikiCorpus`
    类正是为这个任务而设计的：
- en: Construct a corpus from a Wikipedia (or other MediaWiki-based) database dump.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从维基百科（或其他基于 MediaWiki 的）数据库转储中构建一个语料库。
- en: 'In order to properly progress through the following steps, you will need to
    have [gensim installed](https://radimrehurek.com/gensim/install.html). It''s a
    simple enough process; using pip:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确完成以下步骤，你需要 [安装 gensim](https://radimrehurek.com/gensim/install.html)。这是一个相对简单的过程；使用
    pip：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Moving on...
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 继续...
- en: '**Download the Wikipedia Dump File**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**下载维基百科转储文件**'
- en: A Wikipedia dump file is also required for this procedure, quite obviously.
    The latest such files can be found [here](https://dumps.wikimedia.org/enwiki/latest/).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，此过程还需要一个维基百科转储文件。最新的文件可以在 [这里](https://dumps.wikimedia.org/enwiki/latest/)
    找到。
- en: 'A warning: the latest such English Wikipedia database dump file is ~14 GB in
    size, so downloading, storing, and processing said file is not exactly trivial.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 提个警告：最新的英文维基百科数据库转储文件大小约为 14 GB，因此下载、存储和处理该文件并非易事。
- en: The file I aquired and used for this task was `enwiki-latest-pages-articles.xml.bz2`.
    Go ahead and download it or another similar file to use in the next steps.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我获得并用于此任务的文件是`enwiki-latest-pages-articles.xml.bz2`。下载它或其他类似文件以用于接下来的步骤。
- en: '**Make the Corpus**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**创建语料库**'
- en: I wrote a simple Python script (with inspiration from [here](https://github.com/panyang/Wikipedia_Word2vec/blob/master/v1/process_wiki.py))
    to build the corpus by stripping all Wikipedia markup from the articles, using
    gensim. You can read up on the WikiCorpus class (mentioned above) [here](https://radimrehurek.com/gensim/install.html).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我写了一个简单的Python脚本（灵感来自于[这里](https://github.com/panyang/Wikipedia_Word2vec/blob/master/v1/process_wiki.py)），通过去除文章中的所有维基百科标记来构建语料库，使用gensim库。你可以在[这里](https://radimrehurek.com/gensim/install.html)阅读关于WikiCorpus类的更多信息（如上所述）。
- en: 'The code is pretty straightforward: the Wikipedia dump file is opened and read
    article by article using the `get_texts()` method of the `WikiCorpus` class, all
    of which are ultimately written to a single text file. Both the Wikipedia dump
    file and the resulting corpus file must be specified on the command line.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 代码非常简单明了：使用`WikiCorpus`类的`get_texts()`方法逐篇打开和读取维基百科的转储文件，所有这些最终都写入一个文本文件中。维基百科转储文件和最终的语料库文件都必须在命令行中指定。
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After several hours, the above code leaves me with a corpus file named `wiki_en.txt`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 几小时后，上述代码生成了一个名为`wiki_en.txt`的语料库文件。
- en: '**Check the Corpus**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**检查语料库**'
- en: A second script then checks the corpus text file we just built.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个脚本检查我们刚刚构建的语料库文本文件。
- en: Now, keep in mind that this large Wikipedia dump file then resulted in a very
    large corpus file. Given its enormous size, you may have dificulty reading the
    full file into memory at one time.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，请记住，这个大型的维基百科转储文件最终会生成一个非常大的语料库文件。由于其巨大的尺寸，你可能会在一次性将整个文件读入内存时遇到困难。
- en: This script, then, starts by reading 50 lines -- which equates to 50 full articles
    -- from the text file and outputting them to the terminal, after which you can
    press a key to output another 50, or type 'STOP' to quit. If you *do* stop, the
    script then proceeds to load the entire file into memory. Which could be a problem
    for you. You can, however, verify the text by batches of lines, in order to satisfy
    your curiousity that something good happened as a result of running the first
    script.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本开始时会从文本文件中读取50行——相当于50篇完整的文章——并将它们输出到终端，然后你可以按一个键输出另外50行，或者输入‘STOP’以退出。如果你*确实*停止，脚本将继续将整个文件加载到内存中。这可能会对你造成问题。然而，你可以通过批量检查文本，以满足你对运行第一个脚本后有所收获的好奇心。
- en: If you are planning on working on such a large text file, you may need some
    workarounds for its large size in comparison to your machine's memory.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划处理这样一个大的文本文件，可能需要一些变通方法来应对其相对于你计算机内存的巨大尺寸。
- en: The corpus file must be specified at the command line to execute.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 必须在命令行中指定语料库文件以执行。
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And that's it. Some simple code to accomplish what gensim makes a simple task.
    Now that you are armed with an ample corpus, the natural language processing world
    is your oyster. Time for something fun.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。一些简单的代码实现了gensim使之变得简单的任务。现在你拥有了一个充足的语料库，自然语言处理的世界就是你的舞台。是时候做些有趣的事情了。
- en: '**Related**:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关**：'
- en: '[A Framework for Approaching Textual Data Science Tasks](/2017/11/framework-approaching-textual-data-tasks.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[处理文本数据科学任务的框架](/2017/11/framework-approaching-textual-data-tasks.html)'
- en: '[Natural Language Processing Key Terms, Explained](/2017/02/natural-language-processing-key-terms-explained.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理关键术语解释](/2017/02/natural-language-processing-key-terms-explained.html)'
- en: '[5 Free Resources for Getting Started with Deep Learning for Natural Language
    Processing](/2017/07/5-free-resources-getting-started-deep-learning-nlp.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5个免费资源帮助你开始深度学习自然语言处理](/2017/07/5-free-resources-getting-started-deep-learning-nlp.html)'
- en: More On This Topic
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[N-gram Language Modeling in Natural Language Processing](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理中的N-gram语言建模](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
- en: '[Natural Language Processing Key Terms, Explained](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理关键术语解释](https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html)'
- en: '[Data Representation for Natural Language Processing Tasks](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理任务的数据表示](https://www.kdnuggets.com/2018/11/data-representation-natural-language-processing.html)'
- en: '[Transfer Learning for Image Recognition and Natural Language Processing](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[图像识别和自然语言处理的迁移学习](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
- en: '[How to Start Using Natural Language Processing With PyTorch](https://www.kdnuggets.com/2022/04/start-natural-language-processing-pytorch.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何开始使用 PyTorch 进行自然语言处理](https://www.kdnuggets.com/2022/04/start-natural-language-processing-pytorch.html)'
- en: '[A Gentle Introduction to Natural Language Processing](https://www.kdnuggets.com/2022/06/gentle-introduction-natural-language-processing.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理的温和入门](https://www.kdnuggets.com/2022/06/gentle-introduction-natural-language-processing.html)'
