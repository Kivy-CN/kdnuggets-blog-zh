- en: Research Guide for Video Frame Interpolation with Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习视频帧插值研究指南
- en: 原文：[https://www.kdnuggets.com/2019/10/research-guide-video-frame-interpolation-deep-learning.html](https://www.kdnuggets.com/2019/10/research-guide-video-frame-interpolation-deep-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/10/research-guide-video-frame-interpolation-deep-learning.html](https://www.kdnuggets.com/2019/10/research-guide-video-frame-interpolation-deep-learning.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: In this research guide, we’ll look at deep learning papers aimed at synthesizing
    video frames within an existing video. This could be in between video frames,
    known as *interpolation,* or after them, known as *extrapolation*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究指南中，我们将深入探讨旨在合成现有视频中的视频帧的深度学习论文。这可能是在视频帧之间的*插值*，也可能是在视频帧之后的*外推*。
- en: The better part of this guide will cover *interpolation. Interpolation *is useful
    in software editing tools as well as in generating video animations. It can also
    be used to generate clear video frames in sections where a video is blurred.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南的较大部分将涉及*插值技术*。插值在软件编辑工具以及生成视频动画中都很有用。它还可以用于生成清晰的视频帧，尤其是在视频模糊的部分。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您组织的 IT 需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Video frame interpolation is a very common task, especially in film and video
    production. [Optical flow](https://en.wikipedia.org/wiki/Optical_flow) is one
    of the common tactics used in solving this problem. Optical Flow Estimation is
    the process of estimating the motion of each pixel in a sequence of frames. In
    this paper, we’ll look at advanced methods of video frame interpolation using
    deep learning techniques.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 视频帧插值是一项非常常见的任务，特别是在电影和视频制作中。[光流](https://en.wikipedia.org/wiki/Optical_flow)是解决这个问题的常见方法之一。光流估计是估算每个像素在一系列帧中的运动的过程。在本文中，我们将深入探讨使用深度学习技术的视频帧插值的先进方法。
- en: Video Frame Interpolation via Adaptive Separable Convolution (ICCV, 2017)
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自适应可分离卷积的视频帧插值（ICCV, 2017）
- en: In this paper, the authors propose a deep fully convolutional neural network
    that’s fed with two input frames and estimates pairs of 1D kernels for all pixels.
    The method is capable of estimating kernels and synthesizing the entire video
    frame at once. This makes it possible to incorporate perceptual loss to train
    the neural network, in order to produce visually appealing frames.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，作者提出了一种深度全卷积神经网络，输入两个帧，并估计所有像素的一维卷积核对。这种方法能够一次性估计卷积核并合成整个视频帧。这使得可以引入感知损失来训练神经网络，从而生成视觉上令人愉悦的帧。
- en: '[**Video Frame Interpolation via Adaptive Separable Convolution**](https://arxiv.org/abs/1708.01692v1?source=post_page-----519ab2eb3dda----------------------)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[**自适应可分离卷积的视频帧插值**](https://arxiv.org/abs/1708.01692v1?source=post_page-----519ab2eb3dda----------------------)'
- en: Standard video frame interpolation methods first estimate optical flow between
    input frames and then synthesize an…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的视频帧插值方法首先估计输入帧之间的光流，然后合成一个…
- en: The paper introduces a spatially-adaptive separable convolution technique, which
    aims to interpolate a new frame in the middle of two video frames. The convolution-based
    interpolation method then estimates a pair of 2D convolution kernels. This is
    then used to convolve the two video frames in order to compute the color of the
    output pixel.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 论文介绍了一种空间自适应可分离卷积技术，旨在插值生成两个视频帧之间的新帧。卷积基础的插值方法然后估计一对 2D 卷积核。这些卷积核被用来对两个视频帧进行卷积，以计算输出像素的颜色。
- en: The pixel-dependent kernels capture both motion and re-sampling information
    that’s required for interpolation. Four sets of 1D kernels are estimated by directing
    the information flow into four sub-networks. Each of the subnetworks estimates
    one kernel. The Rectified Linear Unit is used with the 3x3 convolutional layers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 像素依赖的内核捕捉了插值所需的运动和重采样信息。通过将信息流导入四个子网络，估计了四组1D内核。每个子网络估计一个内核。使用了3x3卷积层的**修正线性单元**。
- en: '![](../Images/295c861179ce968d140212eef910baae.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/295c861179ce968d140212eef910baae.png)'
- en: '![Figure](../Images/8d5f28e5e6fbe6460428e07315727a06.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/8d5f28e5e6fbe6460428e07315727a06.png)'
- en: '[source](https://arxiv.org/abs/1708.01692v1)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1708.01692v1)'
- en: The network was trained using the [AdaMax](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adamax) optimizer
    with a learning rate of 0.001 and a mini-batch size of 16 samples. Training videos
    were obtained from various YouTube channels such as “Tom Scott”, “Casey Neistat”,
    “Linus Tech Tips”, and “Austin Evans”.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 网络使用[AdaMax](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adamax)优化器进行训练，学习率为0.001，迷你批量大小为16个样本。训练视频来自于多个YouTube频道，如“Tom
    Scott”、“Casey Neistat”、“Linus Tech Tips”和“Austin Evans”。
- en: Data augmentation was performed by random cropping to ensure that the network
    isn’t biased. Implementation of the convolutional neural network was done using
    Torch. Here’s how this model performs in comparison to other models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通过随机裁剪进行数据增强，以确保网络不偏向某一类。卷积神经网络的实现使用了Torch。以下是该模型与其他模型的比较。
- en: '![](../Images/8c5358a2c0978c33ee007d811933bc3a.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c5358a2c0978c33ee007d811933bc3a.png)'
- en: '![Figure](../Images/b22c44bbc772ca60d09fbda679fec8ef.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/b22c44bbc772ca60d09fbda679fec8ef.png)'
- en: '[source](https://arxiv.org/abs/1708.01692v1)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1708.01692v1)'
- en: Video Frame Interpolation via Adaptive Convolution (CVPR 2017)
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自适应卷积的视频帧插值（CVPR 2017）
- en: This paper presents a method that combines motion estimation and pixel synthesis
    into a single process for video frame interpolation. A deep fully convolutional
    neural network is implemented to estimate a spatially-adaptive convolution kernel
    for each pixel.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种将运动估计和像素合成结合成单一过程的视频帧插值方法。实现了一个深度全卷积神经网络，以为每个像素估计空间自适应卷积内核。
- en: '[**Video Frame Interpolation via Adaptive Convolution**](https://arxiv.org/abs/1703.07514?source=post_page-----519ab2eb3dda----------------------)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[**自适应卷积的视频帧插值**](https://arxiv.org/abs/1703.07514?source=post_page-----519ab2eb3dda----------------------)'
- en: 'Video frame interpolation typically involves two steps: motion estimation and
    pixel synthesis. Such a two-step approach…'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 视频帧插值通常包括两个步骤：运动估计和像素合成。这种两步法...
- en: For a pixel in the interpolated frame, the deep neural network takes two receptive
    field patches centered at that pixel as input and estimates the convolution kernel.
    The convolution kernel is used to convolve with the input patches to synthesize
    the output pixel. Given two video frames, this model aims at creating a temporarily
    frame in between them.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于插值帧中的一个像素，深度神经网络以该像素为中心的两个接收场补丁作为输入，估计卷积内核。该卷积内核用于与输入补丁进行卷积，以合成输出像素。给定两个视频帧，该模型旨在创建它们之间的临时帧。
- en: '![Figure](../Images/d6d09265a926ee228a03cc59ff08bb29.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/d6d09265a926ee228a03cc59ff08bb29.png)'
- en: '[source](https://arxiv.org/abs/1703.07514)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1703.07514)'
- en: This method estimates a convolution kernel directly and uses that to convolve
    the two frames to interpolate the pixel color. Pixel synthesis is accomplished
    by the convolution kernel capturing motion and re-sampling coefficients. Pixel
    interpolation as convolution enables pixel synthesis to be done in a single step,
    which makes this approach more robust.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法直接估计卷积内核，并使用该内核对两个帧进行卷积以插值像素颜色。像素合成通过卷积内核捕捉运动和重采样系数来实现。卷积作为像素插值使得像素合成可以在一步中完成，从而使这种方法更具鲁棒性。
- en: '![](../Images/91d8525c12b2988ec531a569df401c4a.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91d8525c12b2988ec531a569df401c4a.png)'
- en: The convolutional neural network is made up of several convolutional layers
    and down convolutions as alternatives to max-pooling layers. For regularization,
    the authors use ReLUs as activations and batch normalization. The table below
    is an illustration of the architecture of this network.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络由若干卷积层和下卷积组成，作为最大池化层的替代。为了正则化，作者使用**ReLUs**作为激活函数，并进行批量归一化。下表展示了该网络的架构。
- en: '![Figure](../Images/0b569373cf0ef890b74b03606d048b1e.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/0b569373cf0ef890b74b03606d048b1e.png)'
- en: '[source](https://arxiv.org/abs/1703.07514)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[源](https://arxiv.org/abs/1703.07514)'
- en: 'The model is implemented using Torch. Here’s the performance of the model:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用Torch实现。以下是模型的性能：
- en: '![Figure](../Images/640639e4d8c9fee3c8f70459a0d13edd.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/640639e4d8c9fee3c8f70459a0d13edd.png)'
- en: '[source](https://arxiv.org/abs/1703.07514)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[源](https://arxiv.org/abs/1703.07514)'
- en: Video processing techniques like interpolation make many computer vision applications
    possible — not just on servers and in the cloud, but on mobile devices, too. [Learn
    more about how Fritz can teach your mobile devices to see.](https://www.fritz.ai/features/?utm_campaign=vidinterp&utm_source=heartbeat)
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 视频处理技术，如插值，使许多计算机视觉应用成为可能——不仅在服务器和云端，还在移动设备上。[了解更多关于Fritz如何教会你的移动设备“看”](https://www.fritz.ai/features/?utm_campaign=vidinterp&utm_source=heartbeat)。
- en: Video Frame Synthesis using Deep Voxel Flow (ICCV 2017)
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度体素流的视频帧合成（ICCV 2017）
- en: The authors of this paper propose a deep neural network that learns to synthesize
    video frames by flowing pixel values from existing ones. This paper combines the
    strengths of generative convolutional neural networks and optical flow to solve
    this problem.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的作者提出了一种深度神经网络，通过从现有帧流动像素值来合成视频帧。该论文结合了生成卷积神经网络和光流的优点来解决这个问题。
- en: '[**Video Frame Synthesis using Deep Voxel Flow**](https://arxiv.org/abs/1702.02463?source=post_page-----519ab2eb3dda----------------------)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[**深度体素流的视频帧合成**](https://arxiv.org/abs/1702.02463?source=post_page-----519ab2eb3dda----------------------)'
- en: We address the problem of synthesizing new video frames in an existing video,
    either in-between existing frames…
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解决了在现有视频中合成新视频帧的问题，无论是在现有帧之间…
- en: The network used in this model is trained in an unsupervised fashion. Pixels
    are generated by interpolating pixel values from frames that are close by. This
    network includes a voxel flow layer across space and time in the input video.
    Trilinear interpolation across the input video volume generates the final pixel.
    The network is trained on the [UCF-101 dataset](https://www.crcv.ucf.edu/data/UCF101.php) and
    tested on various videos.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本模型中使用的网络以无监督的方式进行训练。像素通过插值来自相邻帧的像素值来生成。该网络包括一个跨空间和时间的体素流层。通过在输入视频体积上进行三线性插值生成最终像素。该网络在[UCF-101数据集](https://www.crcv.ucf.edu/data/UCF101.php)上进行训练，并在各种视频上进行测试。
- en: '![Figure](../Images/0117bd62895e4943c1c6707632e19b4e.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/0117bd62895e4943c1c6707632e19b4e.png)'
- en: '[source](https://arxiv.org/pdf/1702.02463.pdf)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[源](https://arxiv.org/pdf/1702.02463.pdf)'
- en: Their proposed model, Deep Voxel Flow (DVF), is an end-to-end, fully differentiable
    network for video frame synthesis. DVF adopts a fully-convolutional encoder-decoder
    architecture, containing three convolution layers, three deconvolution layers,
    and one bottleneck layer. In the training process of this model, two frames are
    provided as input and the remaining frame is used as a reconstruction target.
    The method is self-supervised and learns to reconstruct a frame by borrowing voxels
    from frames that are nearby. This leads to results that are sharper and more realistic.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 他们提出的模型，深度体素流（DVF），是一个端到端的、完全可微的网络，用于视频帧合成。DVF采用完全卷积的编码器-解码器架构，包含三个卷积层、三个反卷积层和一个瓶颈层。在该模型的训练过程中，提供两个帧作为输入，剩余的帧作为重建目标。该方法是自监督的，通过借用相邻帧的体素来学习重建帧。这使得结果更清晰、更真实。
- en: '![Figure](../Images/727ec9aeb8875e6d17bcfc7c3e5dade6.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/727ec9aeb8875e6d17bcfc7c3e5dade6.png)'
- en: '[source](https://arxiv.org/pdf/1702.02463.pdf)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[源](https://arxiv.org/pdf/1702.02463.pdf)'
- en: The authors use Peak Signal to Noise Ratio (PSN) and Structural Similarity Index
    (SSIM) for analyzing the quality of the interpolated image. Below are the results
    they achieved.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用峰值信噪比（PSN）和结构相似性指数（SSIM）来分析插值图像的质量。以下是他们取得的结果。
- en: '![Figure](../Images/21c8da83a9178796d4a0e14a32da8a2e.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/21c8da83a9178796d4a0e14a32da8a2e.png)'
- en: '[source](https://arxiv.org/pdf/1702.02463.pdf)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[源](https://arxiv.org/pdf/1702.02463.pdf)'
- en: Long-Term Video Interpolation with Bidirectional Predictive Network (2017)
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 长期视频插值与双向预测网络（2017）
- en: This paper addresses the challenge of generating multiple frames between two
    non-consecutive frames in videos. The authors present a deep bidirectional predictive
    network (BiPN) that predicts intermediate frames from two opposite directions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本文解决了在视频中生成两个非连续帧之间多个帧的挑战。作者提出了一种深度双向预测网络（BiPN），该网络从两个相反的方向预测中间帧。
- en: '[**Long-Term Video Interpolation with Bidirectional Predictive Network**](https://arxiv.org/abs/1706.03947?source=post_page-----519ab2eb3dda----------------------)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[**长时间视频插值与双向预测网络**](https://arxiv.org/abs/1706.03947?source=post_page-----519ab2eb3dda----------------------)'
- en: This paper considers the challenging task of long-term video interpolation.
    Unlike most existing methods that only…
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本文考虑了长时间视频插值的挑战性任务。与大多数现有方法仅…
- en: The authors train a convolutional encoder-decoder network given two nonconsecutive
    frames. The network is trained to regress the missing intermediate frames from
    two opposite directions. The network consists of a bi-directional encoder-decoder
    that predicts the *future-forward* from the start frame and predicts the *past-backward* from
    the end frame all at the same time.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 作者训练了一个卷积编码器-解码器网络，给定两个不连续的帧。该网络被训练以从两个相反的方向回归缺失的中间帧。网络由一个双向编码器-解码器组成，同时从起始帧预测*未来-前向*，并从结束帧预测*过去-反向*。
- en: The model is evaluated on a synthetic dataset Moving 2D Shapes and a natural
    video dataset UCF101.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在合成数据集“移动2D形状”和自然视频数据集UCF101上进行了评估。
- en: '![Figure](../Images/5549fd68b4f69e76c54c16733f4e9965.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/5549fd68b4f69e76c54c16733f4e9965.png)'
- en: '[source](https://arxiv.org/abs/1706.03947)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1706.03947)'
- en: The BiPN architecture is an encoder-decoder pipeline with a bidirectional encoder
    and a single decoder. A latent frame representation is produced by the bidirectional
    encoder through encoding information from the start frame and end frame.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: BiPN架构是一个编码器-解码器管道，具有双向编码器和单个解码器。双向编码器通过从起始帧和结束帧中编码信息来生成潜在帧表示。
- en: The multiple missing frames are predicted by the decoder after taking the feature
    representations as input. The forward and reverse encoders consist of several
    convolutional layers, each with a rectified linear unit (ReLU).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器在接收特征表示作为输入后，预测了多个缺失的帧。前向和反向编码器由多个卷积层组成，每个卷积层都具有修正线性单元（ReLU）。
- en: The decoder is composed of a series of up-convolutional layers and ReLUs. The
    decoder outputs a feature map with the size of *l ×h×w ×c *as the prediction of
    the target in-between frames, where l is the length of frames to be predicted,
    h, w and c are the height, width and the number of channels for each frame, respectively.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器由一系列上卷积层和ReLU组成。解码器输出一个特征图，其大小为 *l ×h×w ×c* 作为中间帧的目标预测，其中l是要预测的帧长度，h、w和c分别是每帧的高度、宽度和通道数。
- en: '![](../Images/c3b195c4fb4f59987af4bc2208756ad3.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3b195c4fb4f59987af4bc2208756ad3.png)'
- en: '![Figure](../Images/6b636709e32911d866dae097e5855c09.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/6b636709e32911d866dae097e5855c09.png)'
- en: '[source](https://arxiv.org/abs/1706.03947)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1706.03947)'
- en: The model is implemented using [TensorFlow](https://www.tensorflow.org/) and
    deployed on the [Tesla K80 GPU](https://www.nvidia.com/en-gb/data-center/tesla-k80/).
    The model has been tested using the UCF101 dataset for natural high-resolution
    videos.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用 [TensorFlow](https://www.tensorflow.org/) 实现，并部署在 [Tesla K80 GPU](https://www.nvidia.com/en-gb/data-center/tesla-k80/)上。该模型已经使用UCF101数据集进行了自然高分辨率视频的测试。
- en: '![Figure](../Images/1cb5696326a800d9fcf2f87501d04a30.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/1cb5696326a800d9fcf2f87501d04a30.png)'
- en: '[close](https://arxiv.org/abs/1706.03947)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[关闭](https://arxiv.org/abs/1706.03947)'
- en: The authors use Peak Signal to Noise Ratio (PSN) and Structural Similarity Index
    (SSIM) for analyzing the quality of the interpolated frames. Below are the results
    they achieved.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用峰值信噪比（PSNR）和结构相似性指数（SSIM）来分析插值帧的质量。以下是他们取得的结果。
- en: '![](../Images/20ca7445006889891bf741486816952f.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20ca7445006889891bf741486816952f.png)'
- en: PhaseNet for Video Frame Interpolation (CVPR 2018)
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PhaseNet用于视频帧插值（CVPR 2018）
- en: PhaseNet consists of a neural network decoder that estimates the phase decomposition
    of the intermediate frame. The architecture is a neural network that combines
    the phase-based approach with a learning framework. The network proposed in this
    paper aims to synthesize an intermediate image given two neighboring images as
    input.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: PhaseNet由一个神经网络解码器组成，该解码器估计中间帧的相位分解。该架构是一个结合了基于相位的方法与学习框架的神经网络。本文提出的网络旨在给定两个相邻图像作为输入时合成一个中间图像。
- en: '![Figure](../Images/73984ebb070ee13bfa9eb459fd7b5d9a.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/73984ebb070ee13bfa9eb459fd7b5d9a.png)'
- en: '[source](https://arxiv.org/abs/1804.00884)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1804.00884)'
- en: '[**PhaseNet for Video Frame Interpolation**](https://arxiv.org/abs/1804.00884?source=post_page-----519ab2eb3dda----------------------)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[**PhaseNet用于视频帧插值**](https://arxiv.org/abs/1804.00884?source=post_page-----519ab2eb3dda----------------------)'
- en: Most approaches for video frame interpolation require accurate dense correspondences
    to synthesize an in-between frame…
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数视频帧插值方法需要准确的密集对应关系来合成中间帧…
- en: PhaseNet is designed as a decoder-only network, hence increasing its resolution
    level by level. At each level, the corresponding decomposition information is
    incorporated. Apart from the lowest level, all other levels are structurally identical.
    Information from the previous level is also included at each level.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: PhaseNet被设计为仅解码器网络，从而逐级提高其分辨率。在每个级别，都会结合相应的分解信息。除了最低级别外，所有其他级别在结构上都是相同的。每个级别还包含来自前一个级别的信息。
- en: The input to the network is the response from the steerable pyramid decomposition
    of the two input frames, consisting of the phase and amplitude values for each
    pixel at each level. These values are normalized before being passed through the
    network.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的输入是来自两个输入帧的可调金字塔分解的响应，包括每个像素在每个级别的相位和幅度值。这些值在通过网络之前会进行归一化处理。
- en: '![Figure](../Images/73d32597b732bd598002964f1d815427.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/73d32597b732bd598002964f1d815427.png)'
- en: '[source](https://arxiv.org/abs/1804.00884)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1804.00884)'
- en: Each resolution level has a PhaseNet block that takes the decomposition values
    from the input images as its input. It also takes in the resized feature maps
    and the resized predicted values from the previous level. This information is
    then passed through two convolution layers, each followed by batch normalization
    and ReLU nonlinearity.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分辨率级别都有一个PhaseNet块，该块以分解值作为输入。它还接受来自上一层的缩放特征图和缩放预测值。这些信息随后通过两个卷积层，每个卷积层后跟批归一化和ReLU非线性激活。
- en: Each convolution produces 64 feature maps. After each PhaseNet block, values
    of the in-between frame decomposition are predicted. This is done by passing the
    output feature maps of the PhaseNet block through one convolution layer with size
    1 x 1.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 每个卷积生成64个特征图。每个PhaseNet块之后，预测中间帧分解值。这是通过将PhaseNet块的输出特征图通过一个1 x 1的卷积层来完成的。
- en: This is followed by the hyperbolic tangent function to predict output values.
    The decomposition values of the intermediate image are then computed from these
    values. Now the intermediate image can be reconstructed.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这之后使用双曲正切函数来预测输出值。然后根据这些值计算中间图像的分解值。现在可以重建中间图像。
- en: '![Figure](../Images/d08094f9bf507f42069ee9342c515fc1.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/d08094f9bf507f42069ee9342c515fc1.png)'
- en: '[source](https://arxiv.org/abs/1804.00884)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1804.00884)'
- en: Training of this network is done using triplets of frames from the [DAVIS video
    dataset.](https://davischallenge.org/)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络的训练使用了来自[DAVIS视频数据集](https://davischallenge.org/)的帧三元组。
- en: '![Figure](../Images/19d6fa55751bca761cb46bc05e707992.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/19d6fa55751bca761cb46bc05e707992.png)'
- en: '[source](https://arxiv.org/abs/1804.00884)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1804.00884)'
- en: Here are some of the error measurements obtained for this model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是为此模型获得的一些误差测量结果。
- en: '![Figure](../Images/f2d7978e4f1cd016cb676eb2ae0594d3.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/f2d7978e4f1cd016cb676eb2ae0594d3.png)'
- en: '[source](https://arxiv.org/abs/1804.00884)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1804.00884)'
- en: 'Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video
    Interpolation (CVPR 2018)'
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Super SloMo：高质量估计视频插值的多个中间帧（CVPR 2018）
- en: The authors of this paper propose an end-end-end convolution neural network
    for variable-length multi-frame video interpolation. In this model, motion interception
    and occlusion reasoning are jointly modeled.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文的作者提出了一种端到端卷积神经网络，用于可变长度的多帧视频插值。在这个模型中，运动拦截和遮挡推理被联合建模。
- en: Bidirectional optical flows between input images are computed using a [U-Net
    architecture](https://en.wikipedia.org/wiki/U-Net). The flows are then linearly
    combined at each time step to approximate the intermediate bi-directional optical
    flows. The approximated optical flows are refined using another U-Net.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像之间的双向光流是使用[U-Net架构](https://en.wikipedia.org/wiki/U-Net)计算的。然后在每个时间步长将这些光流线性组合，以逼近中间的双向光流。使用另一个U-Net对近似的光流进行精细化。
- en: This U-Net also predicts soft visibility maps. The two input images are then
    warped and linearly joined to form each intermediate frame. This approach is able
    to produce many intermediate frames as needed because the learned network parameters
    are time-independent.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个U-Net还预测了软可见性图。然后将两个输入图像进行扭曲和线性合成，形成每个中间帧。这种方法能够根据需要生成多个中间帧，因为学习到的网络参数是时间无关的。
- en: '[**Super SloMo: High Quality Estimation of Multiple Intermediate Frames for
    Video Interpolation**](https://arxiv.org/abs/1712.00080v2?source=post_page-----519ab2eb3dda----------------------)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[**超级慢动作：视频插值的多中间帧高质量估计**](https://arxiv.org/abs/1712.00080v2?source=post_page-----519ab2eb3dda----------------------)'
- en: Given two consecutive frames, video interpolation aims at generating intermediate
    frame(s) to form both spatially and…
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个连续帧，视频插值旨在生成中间帧，以形成空间上和…
- en: In this network, a flow computation CNN is used to first estimate the bi-directional
    optical flow between the two input images. This is then linearly joined to approximate
    the required intermediate optical flow in order to warp input images.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个网络中，使用了流计算 CNN 来首先估计两个输入图像之间的双向光流。然后将其线性结合以近似所需的中间光流，以便对输入图像进行扭曲。
- en: The network is trained by collecting 240-FPS videos from YouTube and hand-held
    cameras. The trained model is evaluated on several datasets including the [Middlebury](http://vision.middlebury.edu/flow/data/), [UCF101](https://www.crcv.ucf.edu/data/UCF101.php/), [slow
    flow dataset](http://www.cvlibs.net/projects/slow_flow/), and [high-frame-rate
    MPI Sintel.](http://sintel.is.tue.mpg.de/) The unsupervised optical flow results
    were also evaluated on the [KITTI 2012 optical flow benchmark](http://www.cvlibs.net/).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 网络通过收集来自 YouTube 和手持摄像机的 240FPS 视频进行训练。训练后的模型在多个数据集上进行了评估，包括 [Middlebury](http://vision.middlebury.edu/flow/data/)、[UCF101](https://www.crcv.ucf.edu/data/UCF101.php/)、[慢速流数据集](http://www.cvlibs.net/projects/slow_flow/)
    和 [高帧率 MPI Sintel](http://sintel.is.tue.mpg.de/)。无监督光流结果也在 [KITTI 2012 光流基准](http://www.cvlibs.net/)
    上进行了评估。
- en: '![Figure](../Images/dffa1e76f54a3ac7c62cc9026840e4e8.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/dffa1e76f54a3ac7c62cc9026840e4e8.png)'
- en: '[source](https://arxiv.org/pdf/1712.00080v2.pdf)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1712.00080v2.pdf)'
- en: The U-Net used in this architecture is fully convolutional and consists of an
    encoder and a decoder. There are skip connections between the encoder and the
    decoder features at the same spatial resolution. There are six hierarchies in
    the encoder made up of two convolutional and Leaky ReLU layers.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构中使用的 U-Net 是完全卷积的，由一个编码器和一个解码器组成。编码器和解码器在相同空间分辨率下有跳跃连接。编码器中有六个层次，由两个卷积层和
    Leaky ReLU 层组成。
- en: An average pooling layer with a stride of 2 is used to decrease the spatial
    dimension at each hierarchy, except the last one. The decoder section has five
    hierarchies. The beginning of each hierarchy is a bilinear upsampling layer that’s
    used to increase the spatial dimension by a factor of 2\. This is followed by
    two convolutional and Leaky ReLU layers. 7 x 7 kernels are used in the first two
    convolutional layers and 5 x 5 layers in the second hierarchy. The remaining part
    of the network uses 3 x 3 convolution kernels.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用步幅为 2 的平均池化层在每个层次上减少空间维度，除了最后一个层次。解码器部分有五个层次。每个层次的开头是一个双线性上采样层，用于将空间维度增加 2
    倍。接下来是两个卷积层和 Leaky ReLU 层。在前两个卷积层中使用 7 x 7 的卷积核，在第二个层次中使用 5 x 5 的卷积层。网络的其余部分使用
    3 x 3 的卷积核。
- en: '![Figure](../Images/256dc68a5cb874d52cf5cc0eb622cab5.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/256dc68a5cb874d52cf5cc0eb622cab5.png)'
- en: '[source](https://arxiv.org/pdf/1712.00080v2.pdf)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1712.00080v2.pdf)'
- en: 'Here’s the performance of the said model on the UCF101 and Adobe datasets:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是该模型在 UCF101 和 Adobe 数据集上的表现：
- en: '![](../Images/4934c92495e6e0c565efeab107fb0125.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4934c92495e6e0c565efeab107fb0125.png)'
- en: '![Figure](../Images/87a8f2430bfc3305f41e928ae5c506b8.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/87a8f2430bfc3305f41e928ae5c506b8.png)'
- en: '[source](https://arxiv.org/pdf/1712.00080v2.pdf)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1712.00080v2.pdf)'
- en: Depth-Aware Video Frame Interpolation (CVPR 2019)
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度感知视频帧插值（CVPR 2019）
- en: This paper proposes a video frame interpolation method that detects occlusion
    by exploring depth information. The authors develop a depth-aware flow projection
    layer that synthesizes immediate flows that sample closer objects than ones that
    are far away.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种视频帧插值方法，通过探索深度信息来检测遮挡。作者开发了一个深度感知流投影层，该层合成了比远处物体更靠近的即时流。
- en: Learning of hierarchical features is done by gathering contextual information
    from neighboring pixels. The output frame is then generated by warping the input
    frames, depth maps, and contextual features based on the optical flow and local
    interpolation kernels.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从邻近像素收集上下文信息来学习层次特征。然后通过基于光流和局部插值核扭曲输入帧、深度图和上下文特征来生成输出帧。
- en: The authors propose a Depth-Aware video frame INterpolation (DAIN) model that
    effectively exploits the optical flow, local interpolation kernels, depth maps,
    and contextual features to generate high-quality video frames.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提出了一种深度感知视频帧插值（DAIN）模型，该模型有效利用光流、局部插值内核、深度图和上下文特征来生成高质量的视频帧。
- en: '[**Depth-Aware Video Frame Interpolation**](https://arxiv.org/abs/1904.00830v1?source=post_page-----519ab2eb3dda----------------------)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[**深度感知视频帧插值**](https://arxiv.org/abs/1904.00830v1?source=post_page-----519ab2eb3dda----------------------)'
- en: Video frame interpolation aims to synthesize nonexistent frames in-between the
    original frames. While significant…
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 视频帧插值旨在合成原始帧之间不存在的帧。虽然显著…
- en: The model uses [PWC-Net](https://github.com/NVlabs/PWC-Net/tree/master/PyTorch) as
    its flow estimation network. The flow estimation network is initialized from the
    pre-trained PWC-Net. For the depth estimation network, the authors use the hourglass
    architecture. The depth estimation network is also initialized from a pre-trained
    version. Contextual information is obtained by using a pre-trained [ResNet](https://arxiv.org/abs/1512.03385).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用 [PWC-Net](https://github.com/NVlabs/PWC-Net/tree/master/PyTorch) 作为流估计网络。流估计网络从预训练的
    PWC-Net 初始化。对于深度估计网络，作者使用了 hourglass 架构。深度估计网络也从预训练版本初始化。上下文信息通过使用预训练的 [ResNet](https://arxiv.org/abs/1512.03385)来获取。
- en: '![Figure](../Images/e99fa95eea1f4c0e9219fb12559d1109.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/e99fa95eea1f4c0e9219fb12559d1109.png)'
- en: '[source](https://arxiv.org/pdf/1904.00830v1.pdf)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1904.00830v1.pdf)'
- en: The authors build a context extraction network with one 7x 7 convolutional layer
    and two residual blocks without any normalization layer. A hierarchical feature
    is then obtained by concatenating the features from the first convolution layer
    and the two residual blocks. Training the context extraction network from scratch
    ensures that it learns effective contextual features for video frame interpolation.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 作者构建了一个上下文提取网络，该网络包含一个 7x7 卷积层和两个残差块，没有任何归一化层。然后通过将来自第一个卷积层和两个残差块的特征进行拼接，获得了分层特征。从头开始训练上下文提取网络确保其学习有效的上下文特征以进行视频帧插值。
- en: '![Figure](../Images/ff82f12a037f14222c94a49c61a03949.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/ff82f12a037f14222c94a49c61a03949.png)'
- en: '[source](https://arxiv.org/pdf/1904.00830v1.pdf)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1904.00830v1.pdf)'
- en: For the kernel estimation and adaptive warping layers, the authors use a U-Net
    architecture to estimate 4 x 4 local kernels for each pixel. The depth-aware flow
    projection layer generates the interpolation kernels and intermediate flows. The
    adaptive warping layer is adopted to warp the input frames, depth maps, and contextual
    features.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于内核估计和自适应变形层，作者使用了 U-Net 架构来为每个像素估计 4 x 4 的局部内核。深度感知流投影层生成插值内核和中间流。自适应变形层被采用来变形输入帧、深度图和上下文特征。
- en: The final frame output is generated from a frame synthesis network. The network
    takes the warped input frames, warped depth maps, contextual features, projected
    flows, and interpolation kernels as its input. In order to ensure that the network
    predicts residuals between the ground-truth frame and the blended frame, the two
    warped frames are linearly blended.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最终帧输出是通过帧合成网络生成的。该网络以变形的输入帧、变形的深度图、上下文特征、投影流和插值内核作为输入。为了确保网络预测真实帧和混合帧之间的残差，两个变形帧被线性混合。
- en: The model is trained on the [Vimeo90K dataset](http://toflow.csail.mit.edu/) with
    AdaMax as the optimization strategy. The results obtained are shown below.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在 [Vimeo90K 数据集](http://toflow.csail.mit.edu/)上进行训练，优化策略采用 AdaMax。下方展示了获得的结果。
- en: '![Figure](../Images/8442686e881cfad008c066de4f6715fb.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/8442686e881cfad008c066de4f6715fb.png)'
- en: '[source](https://arxiv.org/pdf/1904.00830v1.pdf)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1904.00830v1.pdf)'
- en: Frame Interpolation with Multi-Scale Deep Loss Functions and Generative Adversarial
    Networks (2019)
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多尺度深度损失函数与生成对抗网络的帧插值（2019）
- en: In this paper, the authors propose a multi-scale generative adversarial network
    for frame interpolation (FIGAN). The efficiency of this network is maximized by
    a multiscale residual estimation module, where the predicted flow and synthesized
    frame are constructed in a corse-to-fine fashion.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，作者提出了一种用于帧插值的多尺度生成对抗网络（FIGAN）。通过多尺度残差估计模块最大化了该网络的效率，其中预测的流和合成的帧以粗到细的方式构建。
- en: The quality of the synthesized intermediate video frames is improved by the
    fact that the network is jointly supervised at different levels with a perceptual
    loss that’s made up of an adversarial and two content losses. The network is evaluated
    on 60fps videos from YouTube.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 合成的中间视频帧的质量得到了提升，这是因为网络在不同级别上共同监督，并且使用了由对抗损失和两个内容损失组成的感知损失。该网络在YouTube上的60fps视频上进行了评估。
- en: '![Figure](../Images/11f8fae9eb978c37a03b5182d808f688.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/11f8fae9eb978c37a03b5182d808f688.png)'
- en: '[source](https://arxiv.org/pdf/1711.06045.pdf)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[源](https://arxiv.org/pdf/1711.06045.pdf)'
- en: '[**Frame Interpolation with Multi-Scale Deep Loss Functions and Generative
    Adversarial Networks**](https://arxiv.org/abs/1711.06045?source=post_page-----519ab2eb3dda----------------------)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[**多尺度深度损失函数和生成对抗网络的帧插值**](https://arxiv.org/abs/1711.06045?source=post_page-----519ab2eb3dda----------------------)'
- en: Frame interpolation attempts to synthesise frames given one or more consecutive
    video frames. In recent years, deep…
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 帧插值尝试从一个或多个连续的视频帧中合成帧。近年来，深度…
- en: The proposed model is made up of a trainable CNN architecture that directly
    estimates an interpolated frame from two input frames. Synthesis features are
    obtained by building a pyramidal structure and estimating optical flow between
    two frames at different scales. The synthesis refinement module is made up of
    a CNN that enables the joint processing of the synthesized image with the original
    input frames that produced it.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的模型由一个可训练的CNN架构组成，该架构直接从两个输入帧中估算一个插值帧。合成特征是通过构建一个金字塔结构并在不同尺度下估计两个帧之间的光流来获得的。合成精炼模块由一个CNN组成，它能够将合成图像与生成它的原始输入帧共同处理。
- en: '![Figure](../Images/10329dde81ce4c02ba745b0b53476f8a.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/10329dde81ce4c02ba745b0b53476f8a.png)'
- en: '[source](https://arxiv.org/pdf/1711.06045.pdf)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[源](https://arxiv.org/pdf/1711.06045.pdf)'
- en: Some of the results obtained from this network are shown below.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 从该网络中获得的一些结果如下所示。
- en: '![Figure](../Images/690f8df9e936d1ef74636d28034af166.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/690f8df9e936d1ef74636d28034af166.png)'
- en: '[source](https://arxiv.org/pdf/1711.06045.pdf)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[源](https://arxiv.org/pdf/1711.06045.pdf)'
- en: Conclusion
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: We should now be up to speed on some of the most common — and a couple of very
    recent — techniques for performing video frame interpolation in a variety of contexts.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在应该了解了一些最常见的—以及几个非常新的—用于在各种背景下执行视频帧插值的技术。
- en: The papers/abstracts mentioned and linked to above also contain links to their
    code implementations. We’d be happy to see the results you obtain after testing
    them.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 上面提到并链接的论文/摘要还包含其代码实现的链接。我们很高兴看到你在测试它们后的结果。
- en: '**Bio: [Derrick Mwiti](https://derrickmwiti.com/)** is a data analyst, a writer,
    and a mentor. He is driven by delivering great results in every task, and is a
    mentor at Lapid Leaders Africa.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介： [德里克·穆伊提](https://derrickmwiti.com/)** 是一名数据分析师、作家和导师。他致力于在每项任务中提供卓越的成果，并且是Lapid
    Leaders Africa的导师。'
- en: '[Original](https://heartbeat.fritz.ai/research-guide-for-video-frame-interpolation-with-deep-learning-519ab2eb3dda).
    Reposted with permission.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://heartbeat.fritz.ai/research-guide-for-video-frame-interpolation-with-deep-learning-519ab2eb3dda).
    经许可转载。'
- en: '**Related:**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Research Guide for Neural Architecture Search](/2019/10/research-guide-neural-architecture-search.html)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[神经架构搜索研究指南](/2019/10/research-guide-neural-architecture-search.html)'
- en: '[A 2019 Guide to Speech Synthesis with Deep Learning](/2019/09/2019-guide-speech-synthesis-deep-learning.html)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2019年深度学习语音合成指南](/2019/09/2019-guide-speech-synthesis-deep-learning.html)'
- en: '[A 2019 Guide for Automatic Speech Recognition](/2019/09/2019-guide-automatic-speech-recognition.html)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2019年自动语音识别指南](/2019/09/2019-guide-automatic-speech-recognition.html)'
- en: More On This Topic
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Customize Your Data Frame Column Names in Python](https://www.kdnuggets.com/2022/08/customize-data-frame-column-names-python.html)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在Python中自定义数据框列名](https://www.kdnuggets.com/2022/08/customize-data-frame-column-names-python.html)'
- en: '[How to Deal with Missing Data Using Interpolation Techniques in Pandas](https://www.kdnuggets.com/how-to-deal-with-missing-data-using-interpolation-techniques-in-pandas)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用Pandas中的插值技术处理缺失数据](https://www.kdnuggets.com/how-to-deal-with-missing-data-using-interpolation-techniques-in-pandas)'
- en: '[Bounding Box Deep Learning: The Future of Video Annotation](https://www.kdnuggets.com/2022/07/bounding-box-deep-learning-future-video-annotation.html)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[边界框深度学习：视频标注的未来](https://www.kdnuggets.com/2022/07/bounding-box-deep-learning-future-video-annotation.html)'
- en: '[Text-2-Video Generation: Step-by-Step Guide](https://www.kdnuggets.com/2023/08/text2video-generation-stepbystep-guide.html)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本到视频生成：逐步指南](https://www.kdnuggets.com/2023/08/text2video-generation-stepbystep-guide.html)'
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能、分析、机器学习、数据科学、深度学习……](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
- en: '[A Solid Plan for Learning Data Science, Machine Learning, and Deep Learning](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学、机器学习和深度学习的坚实计划](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
