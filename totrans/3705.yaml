- en: Concepts You Should Know Before Getting Into Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/01/concepts-know-getting-transformer.html](https://www.kdnuggets.com/2023/01/concepts-know-getting-transformer.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1. Input Embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks learn through numbers, so each word will be mapped to vectors
    to represent a particular word. The embedding layer can be thought of as a lookup
    table that stores word embeddings and retrieves them using indices.
  prefs: []
  type: TYPE_NORMAL
- en: '![Concepts You should know before Getting into Transformer](../Images/67feca653a9f1a4d43414738c0e6b6f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Words that have the same meaning will be close in terms of euclidian distance/cosine
    similarity. for example, in the below word representation, “Saturday”,” Sunday”,
    and” Monday” is associated with the same concept, so we can see that the words
    are resulting similar.
  prefs: []
  type: TYPE_NORMAL
- en: '![Concepts You should know before Getting into Transformer](../Images/201bac3fee572f0382e3a1605ae44365.png)'
  prefs: []
  type: TYPE_IMG
- en: 2. Positional Encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The determining the position of the word, Why do we need to determine the position
    of word? because, the transformer encoder has no recurrence like recurrent neural
    networks,we must add some information about the positions into the input embeddings.
    This is done using positional encoding. The authors of the paper used the following
    functions to model the position of a word.
  prefs: []
  type: TYPE_NORMAL
- en: '![Concepts You should know before Getting into Transformer](../Images/e86a3b77649ce1de4a74d3a9e149b254.png)'
  prefs: []
  type: TYPE_IMG
- en: We will try to explain positional Encoding.
  prefs: []
  type: TYPE_NORMAL
- en: '![Concepts You should know before Getting into Transformer](../Images/8c335ba900729d8d71f07c0f03a72a52.png)'
  prefs: []
  type: TYPE_IMG
- en: Here “pos” refers to the position of the “word” in the sequence. P0 refers to
    the position embedding of the first word; “d” means the size of the word/token
    embedding. In this example d=5\. Finally, “i” refers to each of the 5 individual
    dimensions of the embedding (i.e. 0, 1,2,3,4)
  prefs: []
  type: TYPE_NORMAL
- en: if “i” vary in the equation above, you will get a bunch of curves with varying
    frequencies. Reading off the position embedding values against different frequencies,
    giving different values at different embedding dimensions for P0 and P4.
  prefs: []
  type: TYPE_NORMAL
- en: 3. Scaled Dot-Product Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![Concepts You should know before Getting into Transformer](../Images/092ad7293c501dd51b03be1655769cda.png)'
  prefs: []
  type: TYPE_IMG
- en: In this **query, Q** represents a vector word, the **keys K** are all other
    words in the sentence, and **value V** represents the vector of the word.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of attention is to calculate the importance of the key term compared
    to the query term related to the same person/thing or concept.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, V is equal to Q.
  prefs: []
  type: TYPE_NORMAL
- en: '**The attention mechanism gives us the importance of the word in a sentence.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Concepts You should know before Getting into Transformer](../Images/11e009ff8e36ce45e26988feaa571c1f.png)'
  prefs: []
  type: TYPE_IMG
- en: When we compute the normalized dot product between the query and the keys, we
    get a tensor that represents the relative importance of each other word for the
    query.
  prefs: []
  type: TYPE_NORMAL
- en: '![Concepts You should know before Getting into Transformer](../Images/6d70bf90709da9833ad5a3f4f45e9709.png)'
  prefs: []
  type: TYPE_IMG
- en: When computing the dot product between Q and K.T, we try to estimate how the
    vectors (i.e words between query and keys) are aligned and return a weight for
    each word in the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we normalize the result squared of d_k and The softmax function regularizes
    the terms and rescales them between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we multiply the result( i.e weights) by the value (i.e all words) to
    reduce the importance of non-relevant words and focus only on the most important
    words.
  prefs: []
  type: TYPE_NORMAL
- en: 4. Residual Connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The multi-headed attention output vector is added to the original positional
    input embedding. This is called a residual connection/skip connection. The output
    of the residual connection goes through layer normalization. The normalized residual
    output is passed through a pointwise feed-forward network for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: '![Concepts You should know before Getting into Transformer](../Images/e270c87fda8f765ea81be84c3e8ea92a.png)'
  prefs: []
  type: TYPE_IMG
- en: 5. Mask
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The mask is a matrix that’s the same size as the attention scores filled with
    values of 0’s and negative infinities.
  prefs: []
  type: TYPE_NORMAL
- en: '![Concepts You should know before Getting into Transformer](../Images/7d8fa80c3bb2bdc1dd4e0a5fadb3cff1.png)'
  prefs: []
  type: TYPE_IMG
- en: The reason for the mask is that once you take the softmax of the masked scores,
    the negative infinities get zero, leaving zero attention scores for future tokens.
  prefs: []
  type: TYPE_NORMAL
- en: This tells the model to put no focus on those words.
  prefs: []
  type: TYPE_NORMAL
- en: 6. Softmax function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of the softmax function is to grab real numbers(positive and negative)
    and turn them into positive numbers which sum to 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![Concepts You should know before Getting into Transformer](../Images/dd21079a15c752d55a2ef69f4a0e0b3c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**[Ravikumar Naduvin](https://www.linkedin.com/in/ravikumar-mn/)** is busy
    in building and understanding NLP tasks using PyTorch.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://ravikumarmn.github.io/concepts-you-should-know-before-getting-into/).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Concepts You Should Know About Gradient Descent and Cost Function](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 SQL Concepts You Should Know For Data Science](https://www.kdnuggets.com/2022/11/7-sql-concepts-needed-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets™ News 22:n03, Jan 19: A Deep Look Into 13 Data…](https://www.kdnuggets.com/2022/n03.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Google Recommends You do Before Taking Their Machine Learning…](https://www.kdnuggets.com/2021/10/google-recommends-before-machine-learning-data-science-course.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Read This Before You Take Any Free Data Science Course](https://www.kdnuggets.com/read-this-before-you-take-any-free-data-science-course)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, April 13: Python Libraries Data Scientists Should…](https://www.kdnuggets.com/2022/n15.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
