["```py\ntask = TaskModelTrain()\nprint(d6tflow.preview(task, clip_params=True))\n```", "```py\n## \n## └─--[TaskModelTrain-{'data_generation': 'random'}[more] ([94mPENDING[0m)]\n##    |--[TaskBuildNetwork-{'data_generation': 'random'}[more] ([94mPENDING[0m)]\n##    |  └─--[TaskGetTrainDataset-{'data_generation': 'random'}[more] ([94mPENDING[0m)]\n##    |--[TaskGetTrainDataset-{'data_generation': 'random'}[more] ([94mPENDING[0m)]\n##    └─--[TaskGetTestDataset-{'data_generation': 'random'}[more] ([94mPENDING[0m)]\n## None\n```", "```py\n# ***BEFORE***\n# see dlrm_s_pytorch.py\n\ndef train_model():\n    data = loadData()\n    dlrm = DLRM_Net([...])\n    model = dlrm.train(data) \n    torch.save({model},'model.pickle')\n\nif __name__ == \"__main__\":\n\n    parser.add_argument(\"--load-model\")\n    if load_model:\n        model = torch.load('model.pickle')\n    else:\n        model = train_model()\n```", "```py\n# ***AFTER***\n# see flow_tasks.py\n\nclass TaskModelTrain(d6tflow.tasks.TaskPickle):\n\n    def requires(self):  # define dependencies\n        return {'data': TaskPrepareData(), 'model': TaskBuildNetwork()}\n\n    def run(self):\n        data = self.input()['data'].load() # easily load input data\n        dlrm = self.input()['model'].load()\n        model = dlrm.train(data) \n        self.save(model) # easily save trained model as pickle\n\nif __name__ == \"__main__\":\n    if TaskModelTrain().complete(): # load ouput if task was run\n        model = TaskModelTrain().output().load()\n```", "```py\n# ***BEFORE***\n# dlrm_s_pytorch.py\n\nif __name__ == \"__main__\":\n    # define model parameters\n    parser.add_argument(\"--learning-rate\", type=float, default=0.01)\n    parser.add_argument(\"--nepochs\", type=int, default=1)\n    # manually specify filename\n    parser.add_argument(\"--save-model\", type=str, default=\"\") \n    model = train_model()\n    torch.save(model, args.save_model)\n\n# ***AFTER***\n# see flow_tasks.py\n\nclass TaskModelTrain(d6tflow.tasks.TaskPickle):\n\n    # define model parameters\n    learning_rate = luigi.FloatParameter(default = 0.01)\n    num_epochs = luigi.IntParameter(default = 1)\n    # filename is determined automatically\n\n    def run(self):\n        data = self.input()['data'].load()\n        dlrm = self.input()['model'].load()\n\n        # use learning_rate param\n        optimizer = torch.optim.SGD(dlrm.parameters(), lr=self.learning_rate)        \n        # use num_epochs param\n        while k < self.num_epochs: \n            optimizer.step()\n        model = optimizer.get_model()\n        self.save(model) # automatically save model, seperately for each parameter config\n```", "```py\nmodel1 = TaskModelTrain().output().load() # use default num_epochs=1\nprint_accuracy(model1)\nmodel2 = TaskModelTrain(num_epochs=10).output().load()\nprint_accuracy(model2)\n```", "```py\nclass TaskGetTrainDataset(d6tflow.tasks.TaskPickle):\n    mini_batch_size = luigi.FloatParameter(default = 1)\n    # [...]\n\n@d6tflow.inherit(TaskGetTrainDataset)\nclass TaskModelTrain(d6tflow.tasks.TaskPickle):\n    # no need to repeat parameters\n    pass\n```", "```py\ntask = TaskModelTrain() # or task = TaskModelTrain(num_epochs=10)\nd6tflow.preview(task)\nd6tflow.run(task)\n```", "```py\nmodel1 = TaskModelTrain().output().load()\nprint_accuracy(model1)\n```", "```py\nmodel1 = TaskModelTrain().output().load() # use default num_epochs=1\nprint_accuracy(model1)\nmodel2 = TaskModelTrain(num_epochs=10).output().load()\nprint_accuracy(model2)\n```", "```py\nTaskGetTrainDataset().invalidate()\n\n# or\nd6tflow.run(task, forced=TaskGetTrainDataset())\n```"]