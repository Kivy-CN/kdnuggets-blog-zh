- en: The Benefits & Examples of Using Apache Spark with PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/04/benefits-apache-spark-pyspark.html](https://www.kdnuggets.com/2020/04/benefits-apache-spark-pyspark.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**What is Apache Spark? **'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Apache Spark](https://spark.apache.org/) is one of the hottest new trends
    in the technology domain. It is the framework with probably the highest potential
    to realize the fruit of the marriage between Big Data and Machine Learning.'
  prefs: []
  type: TYPE_NORMAL
- en: It runs fast (up to 100x faster than traditional [Hadoop MapReduce](https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm) due
    to in-memory operation, offers robust, distributed, fault-tolerant data objects
    (called [RDD](https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm)),
    and integrates beautifully with the world of machine learning and graph analytics
    through supplementary packages like [Mlib](https://spark.apache.org/mllib/) and [GraphX](https://spark.apache.org/graphx/).
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Benefits & Examples of Using Apache Spark with PySpark in Python | Apache
    Spark](../Images/30cb3911f9df0322fd922f31ff23852a.png)'
  prefs: []
  type: TYPE_IMG
- en: Spark is implemented on [Hadoop/HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html) and
    written mostly in [Scala](https://www.scala-lang.org/), a functional programming
    language, similar to Java. In fact, Scala needs the latest Java installation on
    your system and runs on JVM. However, for most beginners, Scala is not a language
    that they learn first to venture into the world of data science. Fortunately,
    Spark provides a wonderful Python integration, called **PySpark**, which lets
    Python programmers to interface with the Spark framework and learn how to manipulate
    data at scale and work with objects and algorithms over a distributed file system.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will learn the basics of PySpark. There are a lot of concepts
    (constantly evolving and introduced), and therefore, we just focus on fundamentals
    with a few simple examples. Readers are encouraged to build on these and explore
    more on their own.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Short History of Apache Spark**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apache Spark started as a research project at the UC Berkeley AMPLab in 2009,
    and was open sourced in early 2010\. It was a class project at UC Berkeley. Idea
    was to build a cluster management framework, which can support different kinds
    of cluster computing systems. Many of the ideas behind the system were presented
    in various research papers over the years. After being released, Spark grew into
    a broad developer community, and moved to the Apache Software Foundation in 2013\.
    Today, the project is developed collaboratively by a community of hundreds of
    developers from hundreds of organizations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Spark is Not a Programming Language**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One thing to remember is that Spark is not a programming language like Python
    or Java. It is a general-purpose distributed data processing engine, suitable
    for use in a wide range of circumstances. It is particularly useful for big data
    processing both at scale and with high speed.
  prefs: []
  type: TYPE_NORMAL
- en: Application developers and data scientists generally incorporate Spark into
    their applications to rapidly query, analyze, and transform data at scale. Some
    of the tasks that are most frequently associated with Spark, include, – ETL and
    SQL batch jobs across large data sets (often of terabytes of size), – processing
    of streaming data from IoT devices and nodes, data from various sensors, financial
    and transactional systems of all kinds, and – machine learning tasks for e-commerce
    or IT applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'At its core, Spark builds on top of the Hadoop/HDFS framework for handling
    distributed files. It is mostly implemented with Scala, a functional language
    variant of Java. There is a core Spark data processing engine, but on top of that,
    there are many libraries developed for SQL-type query analysis, distributed machine
    learning, large-scale graph computation, and streaming data processing. Multiple
    programming languages are supported by Spark in the form of easy interface libraries:
    Java, Python, Scala, and R.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spark Uses the MapReduce Paradigm for Distributed Processing**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The basic idea of distributed processing is to divide the data chunks into small
    manageable pieces (including some filtering and sorting), bring the computation
    close to the data i.e. use small nodes of a large cluster for specific jobs and
    then re-combine them back. The dividing portion is called the ‘Map’ action and
    the recombination is called the ‘Reduce’ action. Together, they make the famous
    ‘MapReduce’ paradigm, which was introduced by Google around 2004 (see the [original
    paper here](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: For example, if a file has 100 records to be processed, 100 mappers can run
    together to process one record each. Or maybe 50 mappers can run together to process
    two records each. After all the mappers complete processing, the framework shuffles
    and sorts the results before passing them on to the reducers. A reducer cannot
    start while a mapper is still in progress. All the map output values that have
    the same key are assigned to a single reducer, which then aggregates the values
    for that key.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to Setup PySpark**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you’re already familiar with Python and libraries such as Pandas and Numpy,
    then PySpark is a great extension/framework to learn in order to create more scalable,
    data-intensive analyses and pipelines by utilizing the power of Spark in the background.
  prefs: []
  type: TYPE_NORMAL
- en: The exact process of installing and setting up PySpark environment (on a standalone
    machine) is somewhat involved and can vary slightly depending on your system and
    environment. The goal is to get your regular Jupyter data science environment
    working with Spark in the background using the PySpark package.
  prefs: []
  type: TYPE_NORMAL
- en: '[**This article**](https://medium.com/free-code-camp/how-to-set-up-pyspark-for-your-jupyter-notebook-7399dd3cb389) on
    Medium provides more details on the step-by-step setup process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/881536150793aa5d361af9092960c036.png)'
  prefs: []
  type: TYPE_IMG
- en: Alternatively, you can use Databricks setup for practicing Spark. This company
    was created by the original creators of Spark and have an excellent ready-to-launch
    environment to do distributed analysis with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: But the idea is always the same. You are distributing (and replicating) your
    large dataset in small fixed chunks over many nodes. You then bring the compute
    engine close to them so that the whole operation is parallelized, fault-tolerant
    and scalable.
  prefs: []
  type: TYPE_NORMAL
- en: By working with PySpark and Jupyter notebook, you can learn all these concepts
    without spending anything on AWS or Databricks platform. You can also easily interface
    with SparkSQL and MLlib for database manipulation and machine learning. It will
    be much easier to start working with real-life large clusters if you have internalized
    these concepts beforehand!
  prefs: []
  type: TYPE_NORMAL
- en: '**Resilient Distributed Dataset (RDD) and SparkContext**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many Spark programs revolve around the concept of a resilient distributed dataset
    (RDD), which is a fault-tolerant collection of elements that can be operated on
    in parallel. SparkContext resides in the Driver program and manages the distributed
    data over the worker nodes through the cluster manager. The good thing about using
    PySpark is that all this complexity of data partitioning and task management is
    handled automatically at the back and the programmer can focus on the specific
    analytics or machine learning job itself.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Benefits & Examples of Using Apache Spark with PySpark in Python | Rdd-1](../Images/5bb0f6532941bb2fa5a40915e804a5df.png)'
  prefs: []
  type: TYPE_IMG
- en: '*rdd-1*'
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways to create RDDs–parallelizing an existing collection in your
    driver program, or referencing a dataset in an external storage system, such as
    a shared file- system, HDFS, HBase, or any data source offering a Hadoop InputFormat.
  prefs: []
  type: TYPE_NORMAL
- en: For illustration with a Python-based approach, we will give examples of the
    first type here. We can create a simple Python array of 20 random integers (between
    0 and 10), using Numpy random.randint(), and then create an RDD object as following,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '***Note the ‘4’ in the argument*****. It denotes 4 computing cores (in your
    local machine) to be used for this ****SparkContext**** object**. If we check
    the type of the RDD object, we get the following,'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Opposite to parallelization is the collection (with collect()) which brings
    all the distributed elements and returns them to the head node.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: But A is no longer a simple Numpy array. We can use the glom() method to check
    how the partitions are created.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now stop the SC and reinitialize it with 2 cores and see what happens when you
    repeat the process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '***The RDD is now distributed over two chunks, not four! ***'
  prefs: []
  type: TYPE_NORMAL
- en: '**You have learned about the first step in distributed data analytics i.e. controlling
    how your data is partitioned over smaller chunks for further processing**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Some Examples of Basic Operations with RDD & PySpark**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Count the elements**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**The first element (****first****) and the first few elements (****take****)**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Removing duplicates with using ****distinct**'
  prefs: []
  type: TYPE_NORMAL
- en: '*NOTE*: This operation requires a **shuffle** in order to detect duplication
    across partitions. So, it is a slow operation. Don’t overdo it.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**To sum all the elements use ****reduce**** method**'
  prefs: []
  type: TYPE_NORMAL
- en: Note the use of a lambda function in this,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Or the direct ****sum()**** method**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Finding maximum element by ****reduce**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Finding longest word in a blob of text**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Use ****filter**** for logic-based filtering**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Write regular Python functions to use with ****reduce()**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note here the x < y does a lexicographic comparison and determines that Macintosh is
    larger than computers!
  prefs: []
  type: TYPE_NORMAL
- en: '**Mapping operation with a lambda function with PySpark**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**Mapping with a regular Python function in PySpark**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**groupby**** returns a RDD of grouped elements (iterable) as per a given group
    operation**'
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we use a list-comprehension along with the groupby to
    create a list of two elements, each having a header (the result of the lambda
    function, simple modulo 2 here), and a sorted list of the elements which gave
    rise to that result. You can imagine easily that this kind of seperation can come
    particularly handy for processing data which needs to be binned/canned out based
    on particular operation performed over them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**Using ****histogram**'
  prefs: []
  type: TYPE_NORMAL
- en: The histogram() method takes a list of bins/buckets and returns a tuple with
    result of the histogram (binning),
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '**Set operations**'
  prefs: []
  type: TYPE_NORMAL
- en: You can also do regular set operations on RDDs like – union(), intersection(), subtract(),
    or cartesian().
  prefs: []
  type: TYPE_NORMAL
- en: Check out [**this Jupyter notebook**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/SparkContext%20and%20RDD%20Basics.ipynb) for
    more examples.
  prefs: []
  type: TYPE_NORMAL
- en: '**Lazy evaluation with PySpark (and *****Caching*****)**'
  prefs: []
  type: TYPE_NORMAL
- en: Lazy evaluation is an evaluation/computation strategy which prepares a detailed
    step-by-step internal map of the execution pipeline for a computing task, but
    delays the final execution until when it is absolutely needed. This strategy is
    at the heart of Spark for speeding up many parallelized Big Data operations.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use two CPU cores for this example,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '**Make a RDD with 1 million elements**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '**Some computing function – ****taketime**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Check how much time is taken by taketime function**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Remember this result, the taketime() function took a wall time of 31.5 us. Of
    course, the exact number will depend on the machine you are working on.
  prefs: []
  type: TYPE_NORMAL
- en: '**Now do the map operation on the function**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '*How come each **taketime** function takes 45.8 us but the map operation with
    a 1 million elements RDD also took similar time?*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Because of lazy evaluation i.e. nothing was computed in the previous step,
    just a plan of execution was made**. The variable interim does not point to a
    data structure, instead it points to a plan of execution, expressed as a dependency
    graph. The dependency graph defines how RDDs are computed from each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The actual execution by ****reduce**** method**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: So, the wall time here is 15.6 seconds. Remember, the taketime() function had
    a wall time of 31.5 us? Therefore, we expect the total time to be on the order
    of ~ 31 seconds for a 1-million array. Because of parallel operation on two cores,
    it took ~ 15 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have not saved (materialized) any intermediate results in interim, so
    another simple operation (e.g. counting elements > 0) will take almost same time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '**Caching to reduce computation time on similar operation (spending memory)**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember the dependency graph that we built in the previous step? We can run
    the same computation as before with cache method to tell the dependency graph
    to plan for caching.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The first computation will not improve, but it caches the interim result,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Now run the same filter method with the help of cached result,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Wow! The compute time came down to less than a second from 12 seconds earlier!
    This way, caching and parallelization with lazy excution, is the core feature
    of programming with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataframe and SparkSQL**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apart from the RDD, the second key data structure in the Spark framework, is
    the *DataFrame*. If you have done work with Python Pandas or R DataFrame, the
    concept may seem familiar.
  prefs: []
  type: TYPE_NORMAL
- en: 'A DataFrame is a distributed collection of rows under named columns. It is
    conceptually equivalent to a table in a relational database, an Excel sheet with
    Column headers, or a data frame in R/Python, but with richer optimizations under
    the hood. DataFrames can be constructed from a wide array of sources such as:
    structured data files, tables in Hive, external databases, or existing RDDs. It
    also shares some common characteristics with RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Immutable in nature : We can create DataFrame / RDD once but can’t change it.
    And we can transform a DataFrame / RDD after applying transformations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lazy Evaluations: Which means that a task is not executed until an action is
    performed. Distributed: RDD and DataFrame both are distributed in nature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![The Benefits & Examples of Using Apache Spark with PySpark in Python | DataFrame
    in PySpark](../Images/197e1ce09b894ea1628c7da1c8d3c87d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Advantages of the DataFrame**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DataFrames are designed for processing large collection of structured or semi-structured
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observations in Spark DataFrame are organised under named columns, which helps
    Apache Spark to understand the schema of a DataFrame. This helps Spark optimize
    execution plan on these queries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataFrame in Apache Spark has the ability to handle petabytes of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataFrame has a support for wide range of data format and sources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has API support for different languages like Python, R, Scala, Java.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataFrame basics example**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For fundamentals and typical usage examples of DataFrames, please see the following
    Jupyter Notebooks,
  prefs: []
  type: TYPE_NORMAL
- en: '[**Spark DataFrame basics**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/Dataframe_basics.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Spark DataFrame operations**](https://github.com/tirthajyoti/Spark-with-Python/blob/masterDataFrame_operations_basics.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '**SparkSQL Helps to Bridge the Gap for PySpark**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Relational data stores are easy to build and query. Users and developers often
    prefer writing easy-to-interpret, declarative queries in a human-like readable
    language such as SQL. However, as data starts increasing in volume and variety,
    the relational approach does not scale well enough for building Big Data applications
    and analytical systems.
  prefs: []
  type: TYPE_NORMAL
- en: We have had success in the domain of Big Data analytics with Hadoop and the
    MapReduce paradigm. This was powerful, but often slow, and gave users a low-level, **procedural
    programming interface** that required people to write a lot of code for even very
    simple data transformations. However, once Spark was released, it really revolutionized
    the way Big Data analytics was done with a focus on in-memory computing, fault
    tolerance, high-level abstractions, and ease of use.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL essentially tries to bridge the gap between the two models we mentioned
    previously—the relational and procedural models. Spark SQL works through the DataFrame
    API that can perform relational operations on both external data sources and Spark’s
    built-in distributed collections—at scale!
  prefs: []
  type: TYPE_NORMAL
- en: '![The Benefits & Examples of Using Apache Spark with PySpark in Python | Spark
    SQL](../Images/fbbca3e99c3728cb51c0284181460269.png)'
  prefs: []
  type: TYPE_IMG
- en: Why is Spark SQL so fast and optimized? The reason is because of a new extensible
    optimizer, **Catalyst**, based on functional programming constructs in Scala.
    Catalyst supports both rule-based and cost-based optimization. While extensible
    optimizers have been proposed in the past, they have typically required a complex
    domain-specific language to specify rules. Usually, this leads to having a significant
    learning curve and maintenance burden. In contrast, Catalyst uses standard features
    of the Scala programming language, such as pattern-matching, to let developers
    use the full programming language while still making rules easy to specify.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the following Jupyter notebook for an introduction to Database
    operations with SparkSQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**SparkSQL database operations basics**](https://github.com/tirthajyoti/Spark-with-Python/blob/master/Dataframe_SQL_query.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: '**How Are You Going to Use PySpark in Your Project? **'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We covered the fundamentals of the Apache Spark ecosystem and how it works along
    with some basic usage examples of core data structure RDD with the Python interface
    PySpark. Also, DataFrame and SparkSQL were discussed along with reference links
    for example code notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: There is so much more to learn and experiment with Apache Spark being used with
    Python. The [PySpark website is](https://spark.apache.org/docs/latest/api/python/index.html) a
    good reference to have on your radar, and they make regular updates and enhancements–so
    keep an eye on that.
  prefs: []
  type: TYPE_NORMAL
- en: And, if you are interested in doing large-scale, distributed machine learning
    with Apache Spark, then check out the [MLLib portion of the PySpark ecosystem](https://spark.apache.org/mllib/).
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://blog.exxactcorp.com/the-benefits-examples-of-using-apache-spark-with-pyspark-using-python/).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Learn how to use PySpark in under 5 minutes (Installation + Tutorial)](/2019/08/learn-pyspark-installation-tutorial.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Time Series Analysis: A Simple Example with KNIME and Spark](/2019/10/time-series-analysis-simple-example-knime-spark.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Spark NLP 101: LightPipeline](/2019/11/spark-nlp-101-lightpipeline.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Data Validation for PySpark Applications using Pandera](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Benefits Of Becoming A Data-First Enterprise](https://www.kdnuggets.com/2022/07/benefits-becoming-datafirst-enterprise.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3 Benefits to A/B Testing (+ Where to Get Started)](https://www.kdnuggets.com/2022/08/sphere-3-benefits-ab-testing-get-started.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Benefits of Natural Language AI for Content Creators](https://www.kdnuggets.com/2022/08/benefits-natural-language-ai-content-creators.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PySpark for Data Science](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Picking Examples to Understand Machine Learning Model](https://www.kdnuggets.com/2022/11/picking-examples-understand-machine-learning-model.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
