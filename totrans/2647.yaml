- en: All Machine Learning Algorithms You Should Know in 2021
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/01/machine-learning-algorithms-2021.html](https://www.kdnuggets.com/2021/01/machine-learning-algorithms-2021.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8eaa944a2ca4608d78159c07d9972a2f.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText).*'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: As my knowledge in machine learning grows, so does the number of machine learning
    algorithms! This article will cover machine learning algorithms that are commonly
    used in the data science community.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that I’ll be elaborating on some algorithms more than others simply
    because this article would be as long as a book if I thoroughly explained every
    algorithm! I’m also going to try to minimize the amount of math in this article
    because I know it can be pretty daunting for those who aren’t mathematically savvy.
    Instead, I’ll try to give a concise summary of each and point out some of the
    key features.
  prefs: []
  type: TYPE_NORMAL
- en: With that in mind, I’m going to start with some of the more fundamental algorithms
    and then dive into some newer algorithms like CatBoost, Gradient Boost, and XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linear Regression is one of the most fundamental algorithms used to model relationships
    between a dependent variable and one or more independent variables. In simpler
    terms, it involves finding the ‘line of best fit’ that represents two or more
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: The line of best fit is found by minimizing the squared distances between the
    points and the line of best fit — this is known as minimizing the sum of squared
    residuals. A residual is simply equal to the predicted value minus the actual
    value.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e05d6134a52fd2abe790ba257d5cb2e.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image created by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: In case it doesn’t make sense yet, consider the image above. Comparing the green
    line of best fit to the red line, notice how the vertical lines (the residuals)
    are much bigger for the green line than the red line. This makes sense because
    the green line is so far away from the points that it isn’t a good representation
    of the data at all!
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about the math behind linear regression, I would start
    off with [Brilliant’s explanation](https://brilliant.org/wiki/linear-regression/#:~:text=Linear%20regression%20is%20a%20technique,a%20linear%20relationship%20between%20them.&text=The%20best%2Dfitting%20linear%20relationship%20between%20the%20variables%20x%20and%20y.).
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Logistic regression is similar to linear regression but is used to model the
    probability of a discrete number of outcomes, typically two. At a glance, logistic
    regression sounds much more complicated than linear regression but really only
    has one extra step.
  prefs: []
  type: TYPE_NORMAL
- en: First, you calculate a score using an equation similar to the equation for the
    line of best fit for linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6fd0167a72ce61116b19064a3222332f.png)'
  prefs: []
  type: TYPE_IMG
- en: The extra step is feeding the score that you previously calculated in the sigmoid
    function below so that you get a probability in return. This probability can then
    be converted to a binary output, either 1 or 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f67b60747da4919edc1b698dccb894e9.png)'
  prefs: []
  type: TYPE_IMG
- en: To find the weights of the initial equation and calculate the score, methods
    like gradient descent or maximum likelihood are used. Since it’s beyond the scope
    of this article, I won’t go into much more detail, but now you know how it works!
  prefs: []
  type: TYPE_NORMAL
- en: K-Nearest Neighbors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/31cb440d1b26ee334db8d8f57ba0a71c.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image created by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: K-nearest neighbors is a simple idea. First, you start off with data that is
    already classified (i.e., the red and blue data points). Then when you add a new
    data point, you classify it by looking at the k nearest classified points. Whichever
    class gets the most votes determines what the new point gets classified as.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, if we set *k*=1, we can see that the first nearest point to the
    grey sample is a red data point. Therefore, the point would be classified as red.
  prefs: []
  type: TYPE_NORMAL
- en: Something to keep in mind is that if the value of *k* is set too low, it can
    be subject to outliers. On the other hand, if the value of *k* is set too high,
    then it might overlook classes with only a few samples.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Naive Bayes](https://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html)
    is a classification algorithm. This means that Naive Bayes is used when the output
    variable is discrete.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Naive Bayes can seem like a daunting algorithm because it requires preliminary
    mathematical knowledge in conditional probability and Bayes Theorem, but it’s
    an extremely simple and ‘naive’ concept, which I’ll do my best to explain with
    an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b255beeaffd37abf899784964331ddf7.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image created by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have input data on the characteristics of the weather (outlook, temperature,
    humidity, windy) and whether you played golf or not (i.e., last column).
  prefs: []
  type: TYPE_NORMAL
- en: What Naive Bayes essentially does is compare the **proportion** between each
    input variable and the categories in the output variable. This can be shown in
    the table below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da6017b20498fbfb145852c1b8c36fc6.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image created by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: To give an example to help you read this, in the **temperature** section, it
    was hot for two days out of the nine days that you played golf (i.e., yes).
  prefs: []
  type: TYPE_NORMAL
- en: In mathematical terms, you can write this as **the probability of it being hot
    GIVEN that you played golf. **The mathematical notation is P(hot|yes). This is
    known as conditional probability and is essential to understand the rest of what
    I’m about to say.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have this, then you can predict whether you’ll play golf or not for
    any combination of weather characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that we have a new day with the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Outlook: sunny'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Temperature: mild'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Humidity: normal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Windy: false'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, we’ll calculate the probability that you will play golf given X, P(yes|X),
    followed by the probability that you won’t play golf given X, P(no|X).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the chart above, we can get the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0dfa3242d96bc275ad2811180c495a56.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can simply input this information into the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e144ad25596ebfdb821104880152173.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, you would complete the same sequence of steps for P(no|X).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca6c2ec598ed7fee6ff411c3ee4efd7e.png)'
  prefs: []
  type: TYPE_IMG
- en: Because P(yes|X) > P(no|X), then you can predict that this person would play
    golf given that the outlook is sunny, the temperature is mild, the humidity is
    normal, and it’s not windy.
  prefs: []
  type: TYPE_NORMAL
- en: This is the essence of Naive Bayes!
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/7de55836d123b70c6f6dd0e9f7f818fe.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image created by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: ASupport Vector Machine is a supervised classification technique that can actually
    get pretty complicated but is pretty intuitive at the most fundamental level.
    For the sake of this article, we’ll keep it pretty high level.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume that there are two classes of data. A support vector machine will
    find a** hyperplane **or a boundary between the two classes of data that maximizes
    the margin between the two classes (see above). There are many planes that can
    separate the two classes, but only one plane can maximize the margin or distance
    between the classes.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to get into the math behind support vector machines, check out this [series
    of articles](https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/).
  prefs: []
  type: TYPE_NORMAL
- en: Decision Tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/ac0429fc3849c937de001e97421a9d47.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image created by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before understanding random forests, there are a couple of terms that you’ll
    need to know:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ensemble learning**is a method where multiple learning algorithms are used
    in conjunction. The purpose of doing so is that it allows you to achieve higher
    predictive performance than if you were to use an individual algorithm by itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bootstrap sampling**is a resampling method that uses random sampling with
    replacement. It sounds complicated but trust me when I say it’s REALLY simple
    — [read more about it here](https://towardsdatascience.com/what-is-bootstrap-sampling-in-machine-learning-and-why-is-it-important-a5bb90cbd89a).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bagging**when you use the aggregate of the bootstrapped datasets to make
    a decision — I dedicated an article to this topic, so feel free to check it out [here](https://towardsdatascience.com/ensemble-learning-bagging-and-boosting-explained-in-3-minutes-2e6d2240ae21) if
    this doesn’t make complete sense.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you understand these terms let’s dive into it.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests are an ensemble learning technique that builds off of decision
    trees. Random forests involve creating multiple decision trees using bootstrapped
    datasets of the original data and randomly selecting a subset of variables at
    each step of the decision tree. The model then selects the mode of all of the
    predictions of each decision tree (bagging). What’s the point of this? By relying
    on a “majority wins” model, it reduces the risk of error from an individual tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/18580c1ee180ad6d725b4911003ab70c.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Image created by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we created one decision tree, the third one, it would predict
    0\. But if we relied on the mode of all 4 decision trees, the predicted value
    would be 1\. This is the power of random forests!
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AdaBoost, or Adaptive Boost, is also an ensemble algorithm that leverages bagging
    and boostingmethods to develop an enhanced predictor.
  prefs: []
  type: TYPE_NORMAL
- en: 'AdaBoost is similar to Random Forests in the sense that the predictions are
    taken from many decision trees. However, there are three main differences that
    make AdaBoost unique:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4383db4a9efe4c4de296ff65beaac8f1.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Example of a stump.*'
  prefs: []
  type: TYPE_NORMAL
- en: First, AdaBoost creates a forest of stumps rather than trees. A stump is a tree
    that is made of only one node and two leaves (like the image above).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second, the stumps that are created are not equally weighted in the final decision
    (final prediction). Stumps that create more errors will have less say in the final
    decision.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, the order in which the stumps are made is important because each stump
    aims to reduce the errors that the previous stump(s) made.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In essence, AdaBoost takes a more iterative approach in the sense that it seeks
    to iteratively improve from the mistakes that the previous stump(s) made.
  prefs: []
  type: TYPE_NORMAL
- en: '[*If you want to learn more about the underlying math behind AdaBoost, check
    out my article ‘A Mathematical Explanation of AdaBoost in 5 Minutes’.*](https://towardsdatascience.com/a-mathematical-explanation-of-adaboost-4b0c20ce4382)'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Boost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It’s no surprise that Gradient **Boost**is also an ensemble algorithm that
    uses boostingmethods to develop an enhanced predictor. In many ways, Gradient
    Boost is similar to AdaBoost, but there are a couple of key differences:'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike AdaBoost, which builds stumps, Gradient Boost builds trees with usually
    8–32 leaves.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient Boost views the boosting problem as an optimization problem, where
    it uses a loss function and tries to minimize the error. This is why it’s called ***Gradient***boost,
    as it’s inspired by gradient descent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, the trees are used to predict the residuals of the samples (predicted
    minus actual).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the last point may have been confusing, all that you need to know is that
    Gradient Boost starts by building one tree to try to fit the data, and the subsequent
    trees built after aim to reduce the residuals (error). It does this by concentrating
    on the areas where the existing learners performed poorly, similar to AdaBoost.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'XGBoost is one of the most popular and widely used algorithms today because
    it is simply so powerful. It is similar to Gradient Boost but has a few extra
    features that make it that much stronger including:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A proportional shrinking of leaf nodes (pruning) **— used to improve the
    generalization of the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Newton Boosting**— provides a direct route to the minima than gradient descent,
    making it much faster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**An extra randomization parameter **— reduces the correlation between trees,
    ultimately improving the strength of the ensemble'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unique penalization of trees**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I strongly recommend that you watch [StatQuest’s video](https://www.youtube.com/watch?v=OtD8wVaFm6E&t=3s) to
    understand how the algorithm works in greater detail.
  prefs: []
  type: TYPE_NORMAL
- en: LightGBM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you thought XGBoost was the best algorithm out there, think again. LightGBM
    is another type of boosting algorithm that has shown to be faster and sometimes
    more accurate than XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: What makes LightGBM different is that it uses a unique technique called Gradient-based
    One-Side Sampling (GOSS) to filter out the data instances to find a split value.
    This is different than XGBoost, which uses pre-sorted and histogram-based algorithms
    to find the best split.
  prefs: []
  type: TYPE_NORMAL
- en: '[*Read more about Light GBM vs. XGBoost here*](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)*!*'
  prefs: []
  type: TYPE_NORMAL
- en: CatBoost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'CatBoost is another algorithm based on Gradient Descent that has a few subtle
    differences that make it unique:'
  prefs: []
  type: TYPE_NORMAL
- en: CatBoost implements symmetric trees, which help in decreasing prediction time,
    and it also has a shallower tree-depth by default (six)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CatBoost leverages random permutations similar to the way XGBoost has a randomization
    parameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike XGBoost however, CatBoost handles categorical features more elegantly,
    using concepts like ordered boosting and response coding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, what makes CatBoost so powerful is its low latency requirements, which
    translates to it being around **eight times faster** than XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: '*If you want to read about CatBoost in greater detail, check out this *[*article*](https://medium.com/@hanishsidhu/whats-so-special-about-catboost-335d64d754ae)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Thanks for Reading!**'
  prefs: []
  type: TYPE_NORMAL
- en: If you made it to the end, congrats! You should now have a better idea of all
    of the different machine learning algorithms out there.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t feel discouraged if you had a harder time understanding the last few algorithms
    — not only are they more complex, but they’re also relatively new! So stay tuned
    for more resources that will go into these algorithms in greater depth.
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/all-machine-learning-algorithms-you-should-know-in-2021-2e357dd494c7).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Key Data Science Algorithms Explained: From k-means to k-medoids clustering](https://www.kdnuggets.com/2020/12/algorithms-explained-k-means-k-medoids-clustering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Top Machine Learning Algorithm Explained: Support Vector Machines (SVM)](https://www.kdnuggets.com/2020/03/machine-learning-algorithm-svm-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Decision Tree Algorithm, Explained](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
