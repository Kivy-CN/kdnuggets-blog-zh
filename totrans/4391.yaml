- en: Roadmap to Natural Language Processing (NLP)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）的路线图
- en: 原文：[https://www.kdnuggets.com/2020/10/roadmap-natural-language-processing-nlp.html](https://www.kdnuggets.com/2020/10/roadmap-natural-language-processing-nlp.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/10/roadmap-natural-language-processing-nlp.html](https://www.kdnuggets.com/2020/10/roadmap-natural-language-processing-nlp.html)
- en: '[comments](#comments)![Figure](../Images/b8ef7d8ca3d872590868efad60b8c60d.png)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)![图](../Images/b8ef7d8ca3d872590868efad60b8c60d.png)'
- en: Photo by [Kelly Sikkema](https://unsplash.com/@kellysikkema?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Kelly Sikkema](https://unsplash.com/@kellysikkema?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍
- en: Due to the development of [Big Data](https://towardsdatascience.com/big-data-analysis-spark-and-hadoop-a11ba591c057) during
    the last decade. organizations are now faced with analysing large amounts of data
    coming from a wide variety of sources on a daily basis.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 由于过去十年中[大数据](https://towardsdatascience.com/big-data-analysis-spark-and-hadoop-a11ba591c057)的发展，组织现在每天面临分析来自各种来源的大量数据的任务。
- en: Natural Language Processing (NLP) is the area of research in Artificial Intelligence
    focused on processing and using Text and Speech data to create smart machines
    and create insights.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是人工智能研究的一个领域，专注于处理和使用文本和语音数据以创建智能机器并生成洞察。
- en: One of nowadays most interesting NLP application is creating machines able to
    discuss with humans about complex topics. [IBM Project Debater](https://www.research.ibm.com/artificial-intelligence/project-debater/) represents
    so far one of the most successful approaches in this area.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 目前最有趣的NLP应用之一是创建能够与人类讨论复杂话题的机器。[IBM项目辩论者](https://www.research.ibm.com/artificial-intelligence/project-debater/)迄今为止代表了这一领域最成功的方法之一。
- en: 'Video 1: IBM Project Debater'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 视频1：IBM项目辩论者
- en: Preprocessing Techniques
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理技术
- en: 'Some of the most common techniques which are applied in order to prepare text
    data for inference are:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最常用的技术用于准备文本数据以供推理使用：
- en: '**Tokenization: **is used to segment the input text into its constituents words
    (tokens). In this way, it becomes easier to then convert our data into a numerical
    format.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词：** 用于将输入文本分割成其组成单词（标记）。这样，更容易将数据转换为数字格式。'
- en: '**Stop Words Removal: **is applied in order to remove from our text all the
    prepositions (eg. “an”, “the”, etc…) which can just be considered as a source
    of noise in our data (since they do not carry additional informative information
    in our data).'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停止词去除：** 应用于从文本中去除所有介词（例如“an”，“the”等），这些介词只是数据中的噪音源（因为它们不会带来额外的信息）。'
- en: '**Stemming: **is finally used in order to get rid of all the affixes in our
    data (eg. prefixes or suffixes). In this way, it can in fact become much easier
    for our algorithm to not consider as distinguished words which have actually similar
    meaning (eg. insight ~ insightful).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词干提取：** 最终用于去除数据中的所有词缀（例如前缀或后缀）。这样，算法可以更容易地将具有相似含义的词（例如 insight ~ insightful）视为不同的词。'
- en: All of these preprocessing techniques can be easily applied to different types
    of texts using standard Python NLP libraries such as [NLTK](https://www.nltk.org/) and [Spacy](https://spacy.io/).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些预处理技术可以通过标准的Python NLP库，如[NLTK](https://www.nltk.org/)和[Spacy](https://spacy.io/)，轻松应用于不同类型的文本。
- en: Additionally, in order to extrapolate the language syntax and structure of our
    text, we can make use of techniques such as Parts of Speech (POS) Tagging and
    Shallow Parsing (Figure 1). Using these techniques, in fact, we explicitly tag
    each word with its lexical category (which is based on the phrase syntactic context).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了推断我们文本的语言语法和结构，我们可以利用如词性（POS）标注和浅层解析等技术（见图1）。实际上，运用这些技术，我们会明确地将每个单词标记上其词汇类别（基于短语的句法上下文）。
- en: '![Figure](../Images/951cdc5d92cd5446926fe321dc97c9ef.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/951cdc5d92cd5446926fe321dc97c9ef.png)'
- en: 'Figure 1: Parts of Speech Tagging Example [1].'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：词性标注示例 [1]。
- en: Modelling Techniques
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 建模技术
- en: Bag of Words
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词袋模型
- en: Bag of Words is a technique used in Natural Language Processing and [Computer
    Vision](https://towardsdatascience.com/roadmap-to-computer-vision-79106beb8be4) in
    order to create new features for training classifiers (Figure 2). This technique
    is implemented by constructing a histogram counting all the words in our document
    (not taking into account the word order and syntax rules).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型是一种用于自然语言处理和[计算机视觉](https://towardsdatascience.com/roadmap-to-computer-vision-79106beb8be4)的技术，用于创建用于训练分类器的新特征（图2）。该技术通过构建直方图来计算文档中的所有词汇（不考虑词序和语法规则）。
- en: '![Figure](../Images/1a4c7a49429f7625947a25b13bb77af0.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1a4c7a49429f7625947a25b13bb77af0.png)'
- en: 'Figure 2: Bag of Words [2]'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：词袋模型 [2]
- en: One of the main problems which can limit the efficacy of this technique is the
    presence of prepositions, pronouns, articles, etc… in our text. In fact, these
    can all be considered as words which are likely to appear frequently in our text
    even without necessarily being really informative in finding out what are the
    main characteristics and topics in our document.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术可能受限的主要问题之一是文本中介词、代词、冠词等的存在。实际上，这些词可能在我们的文本中频繁出现，但不一定对找出文档的主要特征和主题有实际信息。
- en: In order to solve this type of problem, a technique called “Term Frequency-Inverse
    Document Frequency” (TFIDF) is commonly used. TFIDF aims to rescale the words
    count frequency in our text by considering how frequently each of the words in
    our text appears overall in a large sample of texts. Using this technique, we
    will then reward words (scaling up their frequency value) which appear quite commonly
    in our text but rarely in other texts, while punishing words (scaling down their
    frequency value) which appear frequently in both our text and other texts (such
    as prepositions, pronouns, etc…).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这种问题，通常使用一种称为“词频-逆文档频率”（TFIDF）的技术。TFIDF旨在通过考虑文本中每个词在大量文本样本中的总体出现频率来重新调整词频。使用这种技术，我们将奖励那些在我们的文本中出现但在其他文本中很少出现的词（提升其频率值），而惩罚那些在我们的文本和其他文本中都频繁出现的词（如介词、代词等）。
- en: Latent Dirichlet Allocation (LDA)
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配（LDA）
- en: Latent Dirichlet Allocation (LDA) is a type of Topic Modelling technique. Topic
    Modelling is a field of research focused on finding out ways to cluster documents
    in order to discover latent distinguishing markers which can characterize them
    based on their content (Figure 3). Therefore, Topic Modelling can also be considered
    in this ambit as a [dimensionality reduction technique](https://towardsdatascience.com/feature-extraction-techniques-d619b56e31be) since
    it allows us to reduce our initial data to a limited set of clusters.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配（LDA）是一种主题建模技术。主题建模是一个研究领域，专注于发现聚类文档的方法，以便根据内容发现潜在的区分标志（图3）。因此，主题建模在这个范围内也可以被视为一种[降维技术](https://towardsdatascience.com/feature-extraction-techniques-d619b56e31be)，因为它允许我们将初始数据减少到一个有限的聚类集。
- en: '![Figure](../Images/cdec39b743e04d96d3f4444ce0fb6fb0.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/cdec39b743e04d96d3f4444ce0fb6fb0.png)'
- en: 'Figure 3: Topic Modelling [3]'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：主题建模 [3]
- en: Latent Dirichlet Allocation (LDA) is an unsupervised learning technique used
    to find out latent topics which can characterize different documents and cluster
    together similar ones. This algorithm takes as input the number ***N*** of topics
    which are believed exists and then groups the different documents into ***N*** clusters
    of documents which are closely related to each other.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配（LDA）是一种无监督学习技术，用于发现可以表征不同文档的潜在主题，并将相似的文档聚集在一起。该算法输入的是被认为存在的主题数***N***，然后将不同的文档分组到***N***个彼此紧密相关的文档聚类中。
- en: What characterises LDA from other clustering techniques such as K-Means Clustering
    is that LDA is a soft-clustering technique (each document is assigned to a cluster
    based on a probability distribution). For example, a document can be assigned
    to a Cluster A because the algorithm determines that it is 80% likely that this
    document belongs to this class, while still taking into account that some characteristics
    embedded into this document (the remaining 20%) are more likely to belong instead
    to a second Cluster B.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LDA与其他聚类技术如K均值聚类的区别在于，LDA是一种软聚类技术（每个文档根据概率分布分配到一个聚类中）。例如，文档可以被分配到聚类A，因为算法确定该文档80%的可能性属于该类，同时也考虑到该文档中嵌入的一些特征（剩余的20%）更有可能属于第二个聚类B。
- en: Word Embeddings
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Word Embeddings are one of the most common ways to encode words as vectors of
    numbers which can then fed in into our Machine Learning models for inference.
    Word Embeddings aim to reliably transform our words into a vector space so that
    similar words are represented by similar vectors.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是将词编码为数值向量的最常见方法之一，然后将这些向量输入到我们的机器学习模型中进行推断。词嵌入旨在可靠地将词转换为向量空间，以便相似的词由相似的向量表示。
- en: '![Figure](../Images/1d23da0d54d8af70e47056a3cf0967c9.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/1d23da0d54d8af70e47056a3cf0967c9.png)'
- en: 'Figure 4: Word Embedding [4]'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：词嵌入 [4]
- en: 'Nowadays, there are three main techniques used in order to create Word Embeddings:
    [Word2Vec](https://en.wikipedia.org/wiki/Word2vec), [GloVe](https://en.wikipedia.org/wiki/GloVe_(machine_learning))
    and [fastText](https://en.wikipedia.org/wiki/FastText). All these three techniques,
    use a shallow neural network in order to create the desired word embedding.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，有三种主要技术用于创建词嵌入：[Word2Vec](https://en.wikipedia.org/wiki/Word2vec)、[GloVe](https://en.wikipedia.org/wiki/GloVe_(machine_learning))和[fastText](https://en.wikipedia.org/wiki/FastText)。这三种技术都使用浅层神经网络来创建所需的词嵌入。
- en: In case you can be interested in finding out more about how Word Embeddings
    works, [this article is a great place where to start.](https://machinelearningmastery.com/what-are-word-embeddings/)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣了解更多关于词嵌入的工作原理，[这篇文章是一个很好的起点。](https://machinelearningmastery.com/what-are-word-embeddings/)
- en: Sentiment Analysis
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 情感分析
- en: Sentiment Analysis is an NLP technique commonly used in order to understand
    if some form of text expresses positive, negative or neutral sentiment about a
    topic. This can be particularly useful to do when for example trying to find out
    what is the general public opinion (through online reviews, tweets, etc…) about
    a topic, product or a company.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是一种自然语言处理技术，通常用于理解某种文本是否表达了对一个话题的正面、负面或中性情感。这在尝试了解公众对一个话题、产品或公司的总体看法（通过在线评论、推文等）时尤其有用。
- en: In sentiment analysis, sentiments in texts are usually represented as a value
    between -1 (negative sentiment) and 1 (positive sentiment) referred to as polarity.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在情感分析中，文本中的情感通常表示为介于-1（负面情感）和1（正面情感）之间的值，称为极性。
- en: Sentiment Analysis can be considered as an Unsupervised Learning technique since
    we are not usually provided with handcrafted labels for our data. In order to
    overcome this obstacle, we make use of prelabeled lexicons (a book of words) which
    had been created to quantify the sentiment of a large number of words in different
    contexts. Some examples of widely used lexicons in sentiment analysis are [TextBlob](https://github.com/sloria/TextBlob/tree/eb08c120d364e908646731d60b4e4c6c1712ff63) and [VADER](https://github.com/cjhutto/vaderSentiment).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析可以被视为一种无监督学习技术，因为我们通常没有为数据提供手工标签。为了克服这一障碍，我们使用了预标记的词典（一个词汇书），这些词典已经创建用来量化大量词在不同上下文中的情感。一些广泛使用的情感分析词典包括[TextBlob](https://github.com/sloria/TextBlob/tree/eb08c120d364e908646731d60b4e4c6c1712ff63)和[VADER](https://github.com/cjhutto/vaderSentiment)。
- en: Transformers
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Transformer
- en: '[Transformers](http://jalammar.github.io/illustrated-transformer/) represent
    the current state of the art NLP models in order to analyse text data. Some examples
    of widely known Transformers models are [BERT](https://arxiv.org/abs/1810.04805) and [GTP2](https://openai.com/blog/better-language-models/).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[Transformer](http://jalammar.github.io/illustrated-transformer/)代表了当前最先进的自然语言处理模型，用于分析文本数据。一些广为人知的Transformer模型包括[BERT](https://arxiv.org/abs/1810.04805)和[GTP2](https://openai.com/blog/better-language-models/)。'
- en: Before the creation of Transformers, Recurrent Neural Networks (RNNs) represented
    the most efficient way to analyse sequentially text data for prediction but this
    approach found quite difficult to reliably make use of long term dependencies
    (eg. our network might find difficult to understand if a word fed in several iterations
    ago might result to be useful for the current iteration).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在Transformer出现之前，递归神经网络（RNNs）代表了分析序列化文本数据以进行预测的最有效方法，但这种方法在可靠地利用长期依赖性方面相当困难（例如，我们的网络可能会发现很难理解几个迭代前输入的一个词对当前迭代的有用性）。
- en: Transformers successfully managed to overcome this limitation thanks to a mechanism
    called [Attention](https://arxiv.org/pdf/1706.03762.pdf) (which is used in order
    to determine which parts of the text to focus on and give more weight). Additionally,
    Transformers made easier to process text data in parallel rather than sequentially
    (therefore improving execution speed).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 成功克服了这一限制，得益于一种叫做 [Attention](https://arxiv.org/pdf/1706.03762.pdf) 的机制（该机制用于确定文本中需要关注的部分并给予更多权重）。此外，Transformers
    使得并行处理文本数据变得更容易，而不是顺序处理（从而提高了执行速度）。
- en: Transformers can nowadays be easily implemented in Python thanks to [Hugging
    Face library](https://huggingface.co/).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以通过 [Hugging Face 库](https://huggingface.co/)轻松在 Python 中实现 Transformers。
- en: Text Prediction Demonstration
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本预测演示
- en: Text prediction is one of the tasks which can be easily implemented using Transformers
    such as GPT2\. In this example, we will give as input a quote from “[The Shadow
    of the Wind](https://en.wikipedia.org/wiki/The_Shadow_of_the_Wind)” by Carlos
    Ruiz Zafón and our transformer will then generate other 50 characters which should
    logically follow our input data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 文本预测是可以通过 Transformers 轻松实现的任务之一，例如 GPT2。在这个示例中，我们将输入 Carlos Ruiz Zafón 的《[风之影](https://en.wikipedia.org/wiki/The_Shadow_of_the_Wind)》中的一句话，然后我们的
    Transformer 将生成另外 50 个字符，这些字符应该逻辑上跟随我们的输入数据。
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As can be seen from our example output shown above, our GPT2 model performed
    quite well in creating a resealable continuation for our input string.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面展示的示例输出中可以看出，我们的 GPT2 模型在为输入字符串创建可续写的内容方面表现得相当出色。
- en: An example notebook which you can run in order to generate your own text is
    available at [this link.](https://drive.google.com/open?id=1UVfieBsf4gb6J_s_FaRA93D0ueO4-7JE)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例笔记本可以运行以生成你自己的文本，访问 [此链接](https://drive.google.com/open?id=1UVfieBsf4gb6J_s_FaRA93D0ueO4-7JE)。
- en: '*I hope you enjoyed this article, thank you for reading!*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*希望你喜欢这篇文章，感谢阅读！*'
- en: Contacts
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 联系方式
- en: 'If you want to keep updated with my latest articles and projects [follow me
    on Medium](https://medium.com/@pierpaoloippolito28?source=post_page---------------------------) and
    subscribe to my [mailing list](http://eepurl.com/gwO-Dr?source=post_page---------------------------).
    These are some of my contacts details:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想保持更新我的最新文章和项目， [请关注我在 Medium 上](https://medium.com/@pierpaoloippolito28?source=post_page---------------------------) 并订阅我的 [邮件列表](http://eepurl.com/gwO-Dr?source=post_page---------------------------)。以下是我的一些联系方式：
- en: '[Linkedin](https://uk.linkedin.com/in/pier-paolo-ippolito-202917146?source=post_page---------------------------)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Linkedin](https://uk.linkedin.com/in/pier-paolo-ippolito-202917146?source=post_page---------------------------)'
- en: '[Personal Blog](https://pierpaolo28.github.io/blog/?source=post_page---------------------------)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[个人博客](https://pierpaolo28.github.io/blog/?source=post_page---------------------------)'
- en: '[Personal Website](https://pierpaolo28.github.io/?source=post_page---------------------------)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[个人网站](https://pierpaolo28.github.io/?source=post_page---------------------------)'
- en: '[Medium Profile](https://towardsdatascience.com/@pierpaoloippolito28?source=post_page---------------------------)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Medium 个人资料](https://towardsdatascience.com/@pierpaoloippolito28?source=post_page---------------------------)'
- en: '[GitHub](https://github.com/pierpaolo28?source=post_page---------------------------)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub](https://github.com/pierpaolo28?source=post_page---------------------------)'
- en: '[Kaggle](https://www.kaggle.com/pierpaolo28?source=post_page---------------------------)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kaggle](https://www.kaggle.com/pierpaolo28?source=post_page---------------------------)'
- en: Bibliography
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Extract Custom Keywords using NLTK POS tagger in python, Thinkinfi, Anindya
    Naskar. Accessed at: [https://www.thinkinfi.com/2018/10/extract-custom-entity-using-nltk-pos.html](https://www.thinkinfi.com/2018/10/extract-custom-entity-using-nltk-pos.html)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 使用 NLTK POS 标注器在 Python 中提取自定义关键词，Thinkinfi，Anindya Naskar。访问地址：[https://www.thinkinfi.com/2018/10/extract-custom-entity-using-nltk-pos.html](https://www.thinkinfi.com/2018/10/extract-custom-entity-using-nltk-pos.html)'
- en: '[2] Comparison of word bag model BoW and word set model SoW, ProgrammerSought.
    Accessed at: [http://www.programmersought.com/article/4304366575/;jsessionid=0187F8E68A22612555B437068028C012](http://www.programmersought.com/article/4304366575/;jsessionid=0187F8E68A22612555B437068028C012)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] BoW 和 SoW 词袋模型的比较，ProgrammerSought。访问地址：[http://www.programmersought.com/article/4304366575/;jsessionid=0187F8E68A22612555B437068028C012](http://www.programmersought.com/article/4304366575/;jsessionid=0187F8E68A22612555B437068028C012)'
- en: '[3] Topic Modeling: Art of Storytelling in NLP,'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 主题建模：NLP 中的讲故事艺术，'
- en: TechnovativeThinker. Accessed at: [https://medium.com/@MageshDominator/topic-modeling-art-of-storytelling-in-nlp-4dc83e96a987](https://medium.com/@MageshDominator/topic-modeling-art-of-storytelling-in-nlp-4dc83e96a987)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: TechnovativeThinker。访问地址：[https://medium.com/@MageshDominator/topic-modeling-art-of-storytelling-in-nlp-4dc83e96a987](https://medium.com/@MageshDominator/topic-modeling-art-of-storytelling-in-nlp-4dc83e96a987)
- en: '[4] Word Mover’s Embedding: Universal Text Embedding from Word2Vec, IBM Research
    Blog. Accessed at: [https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/](https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 词移动嵌入：来自Word2Vec的通用文本嵌入，IBM研究博客。访问链接：[https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/](https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/)'
- en: '**Bio: [Pier Paolo Ippolito](https://www.linkedin.com/in/pierpaolo28/)** is
    a Data Scientist and MSc in Artificial Intelligence graduate from the University
    of Southampton. He has a strong interest in AI advancements and machine learning
    applications (such as finance and medicine). Connect with him on [Linkedin](https://www.linkedin.com/in/pierpaolo28/).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介： [Pier Paolo Ippolito](https://www.linkedin.com/in/pierpaolo28/)** 是一位数据科学家和南安普顿大学人工智能硕士毕业生。他对AI进展和机器学习应用（如金融和医学）有浓厚的兴趣。可以在[Linkedin](https://www.linkedin.com/in/pierpaolo28/)上与他联系。'
- en: '[Original](https://towardsdatascience.com/roadmap-to-natural-language-processing-nlp-38a81dcff3a6).
    Reposted with permission.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/roadmap-to-natural-language-processing-nlp-38a81dcff3a6)。经授权转载。'
- en: '**Related:**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Accelerated Natural Language Processing: A Free Course From Amazon](/2020/08/accelerated-nlp-free-amazon-machine-learning-university.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[加速自然语言处理：来自亚马逊的免费课程](/2020/08/accelerated-nlp-free-amazon-machine-learning-university.html)'
- en: '[An Introduction to NLP and 5 Tips for Raising Your Game](/2020/09/introduction-nlp-5-tips-raising-your-game.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NLP简介及提升技能的5个技巧](/2020/09/introduction-nlp-5-tips-raising-your-game.html)'
- en: '[Getting Started with PyTorch](/2020/10/getting-started-pytorch.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开始使用PyTorch](/2020/10/getting-started-pytorch.html)'
- en: '* * *'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT工作'
- en: '* * *'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么让Python成为初创企业理想的编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并寻找目标来…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9亿美元AI失败的分析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学的统计学顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家都应该知道的三个R库（即使你使用Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
