- en: How to Know if a Neural Network is Right for Your Machine Learning Initiative
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/11/neural-network-right-machine-learning-initiative.html](https://www.kdnuggets.com/2020/11/neural-network-right-machine-learning-initiative.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By Frank Fineis, Lead Data Scientist at [Avatria](http://www.avatria.com/)**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/144a940379294e3e3f816ae0f54df5a4.png)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning models (aka neural nets) now power everything from self-driving
    cars to video recommendations on a YouTube feed, having grown very popular over
    the last couple of years. Despite their popularity, the technology is known to
    have some drawbacks, such as the deep learning “[reproducibility crisis](https://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis/)”—
    as it is very common for researchers at one to be unable to recreate a set of
    results published by another, even on the same data set.  Additionally, the steep
    costs of deep learning would give any company pause, as the [FAANG](https://en.wikipedia.org/wiki/Big_Tech#FAANG) companies
    have spent [over $30,000](https://medium.com/syncedreview/the-staggering-cost-of-training-sota-ai-models-e329e80fa82) to
    train just a single (very) deep net.  Even the largest tech companies on the planet
    struggle with the scale, depth, and complexity of venturing into neural nets,
    while the same problems are even more pronounced for smaller data science organizations
    as neural nets  can be both time-and cost-prohibitive. Also, there is no guarantee
    that neural nets will be able to outperform benchmark models like logistic regression
    or gradient-boosted ones, as neural nets are finicky and typically require added
    data and engineering complexities.
  prefs: []
  type: TYPE_NORMAL
- en: With these concerns in mind, it is important to remember that there must be
    a **business reason** for even considering neural nets and it should not be because
    the C-Suite is feeling a bad case of FOMO.
  prefs: []
  type: TYPE_NORMAL
- en: 'When thinking about the need for neural nets, it''s useful to think about your
    data as coming in three flavors:'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous variables, which consist of numeric, decimal-pointed values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Categorical variables, which offer a limited number of possible values, typically
    semantic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Text sequences, which provide unstructured semantic information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decision tree-based algorithms are not the most efficient method for handling
    text sequences or categorical variables with many different values, which can
    require a company to employ creative ways to encode these values into numeric
    features for their models. This can also mean *a lot* of manual feature engineering
    work, as this type of approach adds a lot of complexity to model pipelines when
    the number of potential values exceeds a handful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural networks generally have a much easier time learning so-called “sparse
    features,” which is why any organization thinking about venturing into deep learning
    might want to consider these tips before doing so:'
  prefs: []
  type: TYPE_NORMAL
- en: Try to stick with pre-built, plug-and-play solutions (at least at first), such
    as models that have been pre-assembled with [TensorFlow](https://www.tensorflow.org/).
    It's always tempting to react to an exciting new paper by attempting to pursue
    the absolute SOTA, but if the model doesn’t come from within a package with at
    least some regular users on GitHub/GitLab, it’s probably not going to work well
    out of the box for your dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Avoid implementing any algorithm that does not come with any accompanying code. [Paperswithcode.com](https://paperswithcode.com/) keeps
    up with the SOTA in deep learning/AI and conveniently links the papers describing
    neural net architectures and their implementations on GitHub. If your team is
    considering a model based on a paper that does not come with a corresponding open-sourced
    implementation (like this paper on a neural net architecture called [seq2slate](https://paperswithcode.com/paper/seq2slate-re-ranking-and-slate-optimization)
    from Google), then don’t. It will be very difficult and time consuming to recreate
    the model and match performance claims made in the paper, and likely means that
    very few people are actually using what the paper proposes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Don't give up if you don't see immediate results! We can't stress enough the
    value of keeping precise records of configurations and results. There may be dozens
    of hyperparameters to tune, and while some of them will have little effect on
    the outcome, some will change your results dramatically. Keep a careful eye on
    your choice of optimizer and learning rate—this combination can make all the difference
    between whether training will make zero, little, or a lot of progress. A good
    rule of thumb is to start with a very small learning rate to check for loss decrease
    during training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training neural nets can take a long time to train using just CPUs, but training
    on a GPU (graphical processor unit) can speed up training by a factor of 10\.
    Get access to a free GPU on [Google Colab](https://colab.research.google.com/notebooks/gpu.ipynb),
    which conveniently comes with the latest version of TensorFlow already installed. When
    evaluating a new model, your data science team should generally do a feature extraction
    and feature engineering on your own VPC, and then upload training sets to Google
    Drive to train models in Colab notebooks. This keeps costs down and avoids additional
    infrastructure for dev ops to maintain. Finally, using TensorFlow's tensorboard
    utility, data scientists can study model training results directly from the cloud
    and compare all of their different experiments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write reusable code. It’s much too easy to tweak one of the dozens of neural
    net hyperparameters or to subtly manipulate the training set in a way that greatly
    impacts model results in a notebook development setting and lose track of which
    settings led to which results. But maintaining central dataset creation and model
    training scripts allows you to record the best settings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Bio: [Frank Fineis](https://www.linkedin.com/in/frank-fineis-41770277/)**
    is the Lead Data Scientist at [Avatria](http://www.avatria.com/), a digital commerce
    firm and developer of e-commerce solutions. The company builds data-driven products
    that leverage machine learning to provide actionable insights for its B2C and
    B2B customer''s e-commerce needs.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Can Neural Networks Show Imagination? DeepMind Thinks They Can](/2020/09/deepmind-neural-networks-show-imagination.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Looking Inside The Blackbox: How To Trick A Neural Network](/2020/09/inside-blackbox-trick-neural-network.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Python Libraries for Deep Learning, Natural Language Processing & Computer
    Vision](/2020/11/top-python-libraries-deep-learning-natural-language-processing-computer-vision.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[RedPajama Project: An Open-Source Initiative to Democratizing LLMs](https://www.kdnuggets.com/2023/06/redpajama-project-opensource-initiative-democratizing-llms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learn Deep Learning by Building 15 Neural Network Projects in 2022](https://www.kdnuggets.com/2022/01/15-neural-network-projects-build-2022.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Neural Network Optimization with AIMET](https://www.kdnuggets.com/2022/04/qualcomm-neural-network-optimization-aimet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a Convolutional Neural Network with PyTorch](https://www.kdnuggets.com/building-a-convolutional-neural-network-with-pytorch)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Importance of Permutation in Neural Network Predictions](https://www.kdnuggets.com/2022/12/importance-permutation-neural-network-predictions.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
