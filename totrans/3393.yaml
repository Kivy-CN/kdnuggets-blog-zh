- en: How to Rank 10% in Your First Kaggle Competition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在第一次Kaggle比赛中排名前10%
- en: 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/3)
- en: '**Model Training**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型训练**'
- en: We can improve a model’s performance by tuning its parameters. A model usually
    have many parameters, but only a few of them are significant to its performance.
    For example, the most important parameters for a random forset is the number of
    trees in the forest and the maximum number of features used in developing each
    tree. **We need to understand how models work and what impact does each parameter
    have to the model’s performance, be it accuracy, robustness or speed.**
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调节模型参数来提高模型的性能。一个模型通常有很多参数，但其中只有少数对性能有显著影响。例如，随机森林中最重要的参数是森林中的树木数量和在每棵树中使用的最大特征数。**我们需要了解模型如何工作，以及每个参数对模型性能的影响，无论是准确性、鲁棒性还是速度。**
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的IT需求'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Normally we would find the best set of parameters by a process called **[grid
    search](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html))**.
    Actually what it does is simply iterating through all the possible combinations
    and find the best one.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们会通过一个叫做**[网格搜索](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html)**的过程来找到最佳参数集。实际上，它所做的只是遍历所有可能的组合并找到最佳的那个。
- en: By the way, random forest usually reach optimum when `max_features` is set to
    the square root of the total number of features.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，当`max_features`设置为特征总数的平方根时，随机森林通常能达到最优。
- en: 'Here I’d like to stress some points about tuning XGB. These parameters are
    generally considered to have real impacts on its performance:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我想强调一些关于调节XGB的要点。这些参数通常被认为对性能有实际影响：
- en: '`eta`: Step size used in updating weights. Lower `eta` means slower training
    but better convergence.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta`：用于更新权重的步长。较低的`eta`意味着训练速度较慢，但收敛性更好。'
- en: '`num_round`: Total number of iterations.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_round`：总迭代次数。'
- en: '`subsample`: The ratio of training data used in each iteration. This is to
    combat overfitting.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subsample`：每次迭代中使用的训练数据比例。这是为了防止过拟合。'
- en: '`colsample_bytree`: The ratio of features used in each iteration. This is like `max_features` in `RandomForestClassifier`.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`colsample_bytree`：每次迭代中使用的特征比例。这类似于`RandomForestClassifier`中的`max_features`。'
- en: '`max_depth`: The maximum depth of each tree. Unlike random forest, **gradient
    boosting would eventually overfit if we do not limit its depth**.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：每棵树的最大深度。与随机森林不同，**梯度提升算法如果不限制其深度，最终会过拟合**。'
- en: '`early_stopping_rounds`: If we don’t see an increase of validation score for
    a given number of iterations, the algorithm will stop early. This is to combat
    overfitting, too.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`early_stopping_rounds`：如果在给定的迭代次数内验证分数没有提高，算法将提前停止。这也是为了防止过拟合。'
- en: 'Usual tuning steps:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的调优步骤：
- en: Reserve a portion of training set as the validation set.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保留一部分训练集作为验证集。
- en: Set `eta` to a relatively high value (e.g. 0.05 ~ 0.1), `num_round` to 300 ~
    500.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`eta`设置为相对较高的值（例如0.05 ~ 0.1），将`num_round`设置为300 ~ 500。
- en: Use grid search to find the best combination of other parameters.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用网格搜索来找到其他参数的最佳组合。
- en: Gradually lower `eta` until we reach the optimum.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逐渐降低`eta`直到达到最优。
- en: '**Use the validation set as `watch_list` to re-train the model with the best
    parameters. Observe how score changes on validation set in each iteration. Find
    the optimal value for `early_stopping_rounds`.**'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用验证集作为`watch_list`以最佳参数重新训练模型。观察每次迭代中验证集上的得分变化。找到`early_stopping_rounds`的最佳值。**'
- en: Finally, note that models with randomness all have a parameter like `seed` or `random_state` to
    control the random seed. **You must record this** with all other parameters when
    you get a good model. Otherwise you wouldn’t be able to reproduce it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，所有具有随机性的模型都有一个类似于`seed`或`random_state`的参数来控制随机种子。**你必须记录这个**以及所有其他参数，以便在获得一个好的模型时能够重现它。否则你将无法复现它。
- en: '**Cross Validation**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉验证**'
- en: '**[Cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))** is
    an essential step in model training. It tells us whether our model is at high
    risk of overfitting. In many competitions, public LB scores are not very reliable.
    Often when we improve the model and get a better local CV score, the LB score
    becomes worse. **It is widely believed that we should trust our CV scores under
    such situation.**Ideally we would want **CV scores obtained by different approaches
    to improve in sync with each other and with the LB score**, but this is not always
    possible.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**[交叉验证](https://en.wikipedia.org/wiki/Cross-validation_(statistics))** 是模型训练中的一个关键步骤。它告诉我们我们的模型是否有较高的过拟合风险。在许多比赛中，公共LB分数并不非常可靠。经常地，当我们改进模型并获得更好的本地CV分数时，LB分数反而会变差。**在这种情况下，人们普遍认为我们应该信任我们的CV分数。**
    理想情况下，我们希望**通过不同方法获得的CV分数与LB分数同步提高**，但这并非总是可能的。'
- en: Usually **5-fold CV** is good enough. If we use more folds, the CV score would
    become more reliable, but the training takes longer to finish as well. However,
    we shouldn’t use too many folds if our training data is limited. Otherwise we
    would have too few samples in each fold to guarantee statistical significance.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通常**5折交叉验证（5-fold CV）**已经足够好。如果我们使用更多的折，CV分数将变得更可靠，但训练也需要更长时间完成。然而，如果我们的训练数据有限，我们不应使用太多的折。否则每个折中的样本太少，无法保证统计显著性。
- en: How to do CV properly is not a trivial problem. It requires constant experiment
    and case-by-case discussion. Many Kagglers share their CV approaches (like [this
    one](https://www.kaggle.com/c/telstra-recruiting-network/forums/t/19277/what-is-your-cross-validation-method))
    after competitions when they feel that reliable CV is not easy.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如何正确进行CV不是一个简单的问题。它需要不断的实验和逐个案例讨论。许多Kaggler在比赛后分享他们的CV方法（例如[this one](https://www.kaggle.com/c/telstra-recruiting-network/forums/t/19277/what-is-your-cross-validation-method)），因为他们认为可靠的CV并不容易。
- en: '**Ensemble Generation**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成生成（Ensemble Generation）**'
- en: '[Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning) refers
    to the technique of combining different models. It **reduces both bias and variance
    of the final model** (you can find a proof [here](http://link.springer.com/chapter/10.1007%2F3-540-33019-4_19)),
    thus **increasing the score and reducing the risk of overfitting**. Recently it
    became virtually impossible to win prize without using ensemble in Kaggle competitions.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[集成学习](https://en.wikipedia.org/wiki/Ensemble_learning)指的是结合不同模型的技术。它**减少了最终模型的偏差和方差**（你可以在[这里](http://link.springer.com/chapter/10.1007%2F3-540-33019-4_19)找到证明），从而**提高了得分并减少了过拟合的风险**。最近，没有使用集成方法几乎无法在Kaggle竞赛中获奖。'
- en: 'Common approaches of ensemble learning are:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习的常见方法包括：
- en: '**Bagging**: Use different random subsets of training data to train each base
    model. Then all the base models vote to generate the final predictions. This is
    how random forest works.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**装袋（Bagging）**：使用训练数据的不同随机子集来训练每个基模型。然后，所有基模型投票生成最终预测。这就是随机森林的工作方式。'
- en: '**Boosting**: Train base models iteratively, modify the weights of training
    samples according to the last iteration. This is how gradient boosted trees work.
    (Actually it’s not the whole story. Apart from boosting, GBTs try to learn the
    residuals of earlier iterations.) It performs better than bagging but is more
    prone to overfitting.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升（Boosting）**：迭代训练基模型，根据上一次迭代修改训练样本的权重。这就是梯度提升树的工作方式。（实际上这并不是全部。除了提升，GBT还试图学习早期迭代的残差。）它的表现优于装袋，但更容易过拟合。'
- en: '**Blending**: Use non-overlapping data to train different base models and take
    a weighted average of them to obtain the final predictions. This is easy to implement
    but uses less data.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合（Blending）**：使用不重叠的数据训练不同的基模型，并对它们的加权平均以获得最终预测。这种方法易于实现，但使用的数据较少。'
- en: '**Stacking**: To be discussed next.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**堆叠（Stacking）**：将在下一部分讨论。'
- en: 'In theory, for the ensemble to perform well, two factors matter:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，要使集成模型表现良好，两个因素很重要：
- en: '**Base models should be as unrelated as possibly**. This is why we tend to
    include non-tree-based models in the ensemble even though they don’t perform as
    well. The math says that the greater the diversity, and less bias in the final
    ensemble.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础模型应该尽可能无关**。这就是为什么我们倾向于在集成中包含非树模型，即使它们的表现不如其他模型。数学上说，模型的多样性越大，最终集成中的偏差越小。'
- en: '**Performance of base models shouldn’t differ to much.**'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础模型的表现不应相差太大**。'
- en: Actually we have a **trade-off** here. In practice we may end up with highly
    related models of comparable performances. Yet we ensemble them anyway because
    it usually increase the overall performance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们在这里有一个**权衡**。在实践中，我们可能会得到表现相当的高度相关的模型。但我们仍然将它们集成在一起，因为这通常会提高整体性能。
- en: '**Stacking**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**堆叠**'
- en: 'Compared with blending, stacking makes better use of training data. Here’s
    a diagram of how it works:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 与混合方法相比，堆叠方法更好地利用了训练数据。以下是其工作原理的图示：
- en: '[![Stacking](../Images/fb2fd03a6a8cf5851969a1254b354966.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-diagram-stacking.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[![堆叠](../Images/fb2fd03a6a8cf5851969a1254b354966.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-diagram-stacking.jpg)'
- en: '*(Taken from [Faron](https://www.kaggle.com/mmueller). Many thanks!)*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*(摘自 [Faron](https://www.kaggle.com/mmueller)。非常感谢！)*'
- en: 'It’s much like cross validation. Take 5-fold stacking as an example. First
    we split the training data into 5 folds. Next we will do 5 iterations. In each
    iteration, train every base model on 4 folds and predict on the hold-out fold. **You
    have to keep the predictions on the testing data as well.** This way, in each
    iteration every base model will make predictions on 1 fold of the training data
    and all of the testing data. After 5 iterations we will obtain a matrix of shape `#(samples
    in training data) X #(base models)`. This matrix is then fed to the stacker (it’s
    just another model) in the second level. After the stacker is fitted, use the
    predictions on testing data by base models (**each base model is trained 5 times,
    therefore we have to take an average to obtain a matrix of the same shape**) as
    the input for the stacker and obtain our final predictions.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '这很像交叉验证。以5折堆叠为例。首先，我们将训练数据分成5折。接下来，我们将进行5次迭代。在每次迭代中，在4折上训练每个基础模型，并在保留折上进行预测。**你还必须保留在测试数据上的预测。**
    这样，在每次迭代中，每个基础模型将对训练数据中的1折和所有测试数据进行预测。经过5次迭代后，我们将得到一个形状为`#(训练数据样本数) X #(基础模型数)`的矩阵。这个矩阵随后会被送到第二层的堆叠器（它只是另一个模型）。在堆叠器训练完成后，使用基础模型在测试数据上的预测（**每个基础模型训练5次，因此我们需要取平均值以获得相同形状的矩阵**）作为堆叠器的输入，从而获得我们的最终预测。'
- en: 'Maybe it’s better to just show the codes:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 也许直接展示代码会更好：
- en: Prize winners usually have larger and much more complicated ensembles. For beginner,
    implementing a correct 5-fold stacking is good enough.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励获得者通常有更大且复杂得多的集成。对于初学者来说，实施一个正确的5折堆叠就足够了。
- en: '***Pipeline**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '***管道***'
- en: 'We can see that the workflow for a Kaggle competition is quite complex, especially
    for model selection and ensemble. Ideally, we need a highly automated pipeline
    capable of:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，Kaggle比赛的工作流程相当复杂，尤其是在模型选择和集成方面。理想情况下，我们需要一个能够高度自动化的管道，具备以下功能：
- en: '**Modularized feature transformations**. We only need to write a few lines
    of codes (or better, rules / DSLs) and the new feature is added to the training
    set.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模块化特征转换**。我们只需写几行代码（或者更好的是，使用规则/领域特定语言），新的特征就会添加到训练集中。'
- en: '**Automated grid search**. We only need to set up models and parameter grid,
    the search will be run and the best parameters will be recorded.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化网格搜索**。我们只需设置模型和参数网格，搜索将会运行，并记录最佳参数。'
- en: '**Automated ensemble selection**. Use K best models for training the ensemble
    as soon as we put another base model into the pool.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化集成选择**。当我们将另一个基础模型添加到池中时，使用K个最佳模型进行集成训练。'
- en: For beginners, the first one is not very important because the number of features
    is quite manageable; the third one is not important either because typically we
    only do several ensembles at the end of the competition. But the second one is
    good to have because**manually recording the performance and parameters of each
    model is time-consuming and error-prone**.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于初学者来说，第一个因素不太重要，因为特征数量相对可管理；第三个因素也不重要，因为通常我们在比赛结束时只做几次集成。但第二个因素是值得注意的，因为**手动记录每个模型的性能和参数既耗时又容易出错**。
- en: '[Chenglong Chen](https://www.kaggle.com/chenglongchen), the winner of [Crowdflower
    Search Results Relevance](https://www.kaggle.com/c/crowdflower-search-relevance),
    once released his pipeline on [GitHub](https://github.com/ChenglongChen/Kaggle_CrowdFlower).
    It’s very complete and efficient. Yet it’s very hard to understand and extract
    all his logic to build a general framework. This is something you might want to
    do when you have plenty of time.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[陈成龙](https://www.kaggle.com/chenglongchen)，曾获得 [Crowdflower 搜索结果相关性](https://www.kaggle.com/c/crowdflower-search-relevance)
    的奖项，曾在 [GitHub](https://github.com/ChenglongChen/Kaggle_CrowdFlower) 上发布过他的工作流程。这个流程非常完整且高效，但理解和提取他的所有逻辑以构建一个通用框架是非常困难的。这是你在有充足时间时可能想做的事情。'
- en: More On This Topic
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[How LinkedIn Uses Machine Learning To Rank Your Feed](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LinkedIn 如何利用机器学习来排序你的动态](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)'
- en: '[How to Get Your First Job in Data Science without Any Work Experience](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在没有任何工作经验的情况下获得你的第一份数据科学工作](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)'
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 TensorFlow 和 Keras 构建并训练你的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
- en: '[It''s alive! Build your first robots with Python and some cheap,…](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[它活了！用 Python 和一些便宜的组件打造你的第一个机器人](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
- en: '[From Zero to Hero: Create Your First ML Model with PyTorch](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从零到英雄：用 PyTorch 创建你的第一个 ML 模型](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
- en: '[Deploying Your First Machine Learning Model](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[部署你的第一个机器学习模型](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
