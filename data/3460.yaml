- en: How to Select Support Vector Machine Kernels
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何选择支持向量机核
- en: 原文：[https://www.kdnuggets.com/2016/06/select-support-vector-machine-kernels.html](https://www.kdnuggets.com/2016/06/select-support-vector-machine-kernels.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/06/select-support-vector-machine-kernels.html](https://www.kdnuggets.com/2016/06/select-support-vector-machine-kernels.html)
- en: Given an arbitrary dataset, you typically don't know which kernel may work best.
    I recommend starting with the simplest hypothesis space first -- given that you
    don't know much about your data -- and work your way up towards the more complex
    hypothesis spaces. So, the linear kernel works fine if your dataset if linearly
    separable; however, if your dataset isn't linearly separable, a linear kernel
    isn't going to cut it (almost in a literal sense ;)).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个任意的数据集，你通常不知道哪个核可能效果最好。我建议首先从最简单的假设空间开始——因为你对数据了解不多——然后逐步过渡到更复杂的假设空间。因此，如果你的数据集线性可分，线性核表现良好；然而，如果你的数据集不是线性可分的，线性核就不够用了（几乎是字面上的意思；）。
- en: 'For simplicity (and visualization purposes), let''s assume our dataset consists
    of 2 dimensions only. Below, I plotted the decision regions of a linear SVM on
    2 features of the iris dataset:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化（以及可视化目的），我们假设我们的数据集仅包含2个维度。下面，我绘制了在鸢尾花数据集的2个特征上线性SVM的决策区域：
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前3个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全领域的职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在IT方面'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[![Selecting SVM Kernels](../Images/de1b68ee23dbaeb7c0c14a146503b9ac.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/select_svm_kernels/1.png)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[![选择SVM核](../Images/de1b68ee23dbaeb7c0c14a146503b9ac.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/select_svm_kernels/1.png)'
- en: 'This works perfectly fine. And here comes the RBF kernel SVM:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这完全可以正常工作。接下来是RBF核SVM：
- en: '[![Selecting SVM Kernels](../Images/280238071d4d7b2a7fc0f4dc03f07fc5.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/select_svm_kernels/2.png)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[![选择SVM核](../Images/280238071d4d7b2a7fc0f4dc03f07fc5.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/select_svm_kernels/2.png)'
- en: Now, it looks like both linear and RBF kernel SVM would work equally well on
    this dataset. So, why prefer the simpler, linear hypothesis? Think of Occam's
    Razor in this particular case. Linear SVM is a parametric model, an RBF kernel
    SVM isn't, and the complexity of the latter grows with the size of the training
    set. Not only is it more expensive to train an RBF kernel SVM, but you also have
    to keep the kernel matrix around, and the projection into this "infinite" higher
    dimensional space where the data becomes linearly separable is more expensive
    as well during prediction. Furthermore, you have more hyperparameters to tune,
    so model selection is more expensive as well! And finally, it's much easier to
    overfit a complex model!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，看起来线性和RBF核SVM在这个数据集上都可以表现得很好。那么，为什么要偏好更简单的线性假设呢？在这种情况下考虑奥卡姆剃刀原则。线性SVM是一个参数模型，而RBF核SVM则不是，后者的复杂性随着训练集的大小增加。不仅训练RBF核SVM更昂贵，你还需要保留核矩阵，并且在预测时将数据投影到这个“无限”的更高维空间使数据变得线性可分的代价也更高。此外，你还需要调整更多的超参数，因此模型选择也更昂贵！最后，更复杂的模型更容易过拟合！
- en: 'Okay, what I''ve said above sounds all very negative regarding kernel methods,
    but it really depends on the dataset. E.g., if your data is not linearly separable,
    it doesn''t make sense to use a linear classifier:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我刚才说的关于核方法的内容听起来都非常负面，但这实际上取决于数据集。例如，如果你的数据不是线性可分的，那么使用线性分类器就没有意义了：
- en: '[![Selecting SVM Kernels](../Images/608cb886ec38dfe4372fc3f8fbcd5f8c.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/select_svm_kernels/3.png)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[![选择SVM核](../Images/608cb886ec38dfe4372fc3f8fbcd5f8c.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/select_svm_kernels/3.png)'
- en: 'In this case, a RBF kernel would make so much more sense:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，RBF核会更有意义：
- en: '[![Selecting SVM Kernels](../Images/92340a2e611d807e5f8813582e2ad5a4.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/select_svm_kernels/4.png)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[![选择SVM核](../Images/92340a2e611d807e5f8813582e2ad5a4.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/select_svm_kernels/4.png)'
- en: 'In any case, I wouldn''t bother too much about the polynomial kernel. In practice,
    it is less useful for efficiency (computational as well as predictive) performance
    reasons. So, the rule of thumb is: use linear SVMs (or logistic regression) for
    linear problems, and nonlinear kernels such as the Radial Basis Function kernel
    for non-linear problems.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我不会过于纠结于多项式核。在实践中，它在效率（计算和预测）方面的作用较小。因此，经验法则是：对于线性问题使用线性SVM（或逻辑回归），对于非线性问题使用如径向基函数核等非线性核。
- en: 'The RBF kernel SVM decision region is actually also a linear decision region.
    What RBF kernel SVM actually does is to create non-linear combinations of your
    features to uplift your samples onto a higher-dimensional feature space where
    you can use a linear decision boundary to separate your classes:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: RBF核SVM的决策区域实际上也是一个线性决策区域。RBF核SVM所做的实际上是创建特征的非线性组合，将样本提升到一个更高维的特征空间，在那里你可以使用线性决策边界来分隔你的类别：
- en: '[![Selecting SVM Kernels](../Images/472009ef232672ddbd1aad7f899eb0d9.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/select_svm_kernels/5.png)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[![选择SVM核](../Images/472009ef232672ddbd1aad7f899eb0d9.png)](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/select_svm_kernels/5.png)'
- en: 'Okay, above, I walked you through an intuitive example where we can visualize
    our data in 2 dimensions ... but what do we do in a real-world problem, i.e.,
    a dataset with more than 2 dimensions? Here, we want to keep an eye on our objective
    function: minimizing the hinge-loss. We would setup a hyperparameter search (grid
    search, for example) and compare different kernels to each other. Based on the
    loss function (or a performance metric such as accuracy, F1, MCC, ROC auc, etc.)
    we could determine which kernel is "appropriate" for the given task.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，上面我给你展示了一个直观的例子，我们可以在2维空间中可视化我们的数据……但在实际问题中，即数据集有超过2个维度时，我们该如何处理？在这里，我们需要关注我们的目标函数：最小化铰链损失。我们会设置一个超参数搜索（例如网格搜索），并将不同的核函数相互比较。根据损失函数（或像准确率、F1、MCC、ROC
    auc等性能指标），我们可以确定哪个核函数对于给定任务是“合适的”。
- en: '**Bio: [Sebastian Raschka](https://twitter.com/rasbt)** is a ''Data Scientist''
    and Machine Learning enthusiast with a big passion for Python & open source. Author
    of ''[Python Machine Learning](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning)''.
    Michigan State University.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**简历： [塞巴斯蒂安·拉施卡](https://twitter.com/rasbt)** 是一位‘数据科学家’和机器学习爱好者，对Python及开源充满热情。《[Python机器学习](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning)》的作者。密歇根州立大学。'
- en: '[Original](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/select_svm_kernels.md).
    Reposted with permission.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/select_svm_kernels.md)。经许可转载。'
- en: '**Related:**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[When Does Deep Learning Work Better Than SVMs or Random Forests?](/2016/04/deep-learning-vs-svm-random-forest.html)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习何时比SVM或随机森林更有效？](/2016/04/deep-learning-vs-svm-random-forest.html)'
- en: '[The Development of Classification as a Learning Machine](/2016/04/development-classification-learning-machine.html)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类作为学习机器的发展](/2016/04/development-classification-learning-machine.html)'
- en: '[Why Implement Machine Learning Algorithms From Scratch?](/2016/05/implement-machine-learning-algorithms-scratch.html)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么从头实现机器学习算法？](/2016/05/implement-machine-learning-algorithms-scratch.html)'
- en: More On This Topic
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Support Vector Machines: An Intuitive Approach](https://www.kdnuggets.com/2022/08/support-vector-machines-intuitive-approach.html)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[支持向量机：一种直观的方法](https://www.kdnuggets.com/2022/08/support-vector-machines-intuitive-approach.html)'
- en: '[A Gentle Introduction to Support Vector Machines](https://www.kdnuggets.com/2023/07/gentle-introduction-support-vector-machines.html)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[支持向量机的温和介绍](https://www.kdnuggets.com/2023/07/gentle-introduction-support-vector-machines.html)'
- en: '[How Semantic Vector Search Transforms Customer Support Interactions](https://www.kdnuggets.com/how-semantic-vector-search-transforms-customer-support-interactions)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[语义向量搜索如何改变客户支持互动](https://www.kdnuggets.com/how-semantic-vector-search-transforms-customer-support-interactions)'
- en: '[Python Vector Databases and Vector Indexes: Architecting LLM Apps](https://www.kdnuggets.com/2023/08/python-vector-databases-vector-indexes-architecting-llm-apps.html)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 向量数据库和向量索引：构建 LLM 应用](https://www.kdnuggets.com/2023/08/python-vector-databases-vector-indexes-architecting-llm-apps.html)'
- en: '[How to Correctly Select a Sample From a Huge Dataset in Machine Learning](https://www.kdnuggets.com/2019/05/sample-huge-dataset-machine-learning.html)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在机器学习中从巨大的数据集中正确选择样本](https://www.kdnuggets.com/2019/05/sample-huge-dataset-machine-learning.html)'
- en: '[How to Select Rows and Columns in Pandas Using [ ], .loc, iloc, .at…](https://www.kdnuggets.com/2019/06/select-rows-columns-pandas.html)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在 Pandas 中使用 [ ]、.loc、iloc、.at 等选择行和列](https://www.kdnuggets.com/2019/06/select-rows-columns-pandas.html)'
