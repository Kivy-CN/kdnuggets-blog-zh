- en: The Most Important Fundamentals of PyTorch you Should Know
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/06/fundamentals-pytorch.html](https://www.kdnuggets.com/2020/06/fundamentals-pytorch.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d2969ec5e746ad4540d7f8cf3b322008.png)'
  prefs: []
  type: TYPE_IMG
- en: Fundamentals of PyTorch – Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since it was introduced by the Facebook AI Research (FAIR) team, back in early
    2017, [PyTorch](https://www.exxactcorp.com/PyTorch) has become a highly popular
    and widely used Deep Learning (DL) framework. Since the humble beginning, it has
    caught the attention of serious AI researchers and practitioners around the world,
    both in industry and academia, and has matured significantly over the years.
  prefs: []
  type: TYPE_NORMAL
- en: Scores of DL enthusiasts and professionals started their journey with the Google
    TensorFlow (TF), but the learning curve with base TensorFlow has always been steep.
    On the other hand, PyTorch has approached DL programming in an intuitive fashion
    since the beginning, focusing on fundamental linear algebra and data flow operations
    in a manner that is easily understood and amenable to step-by-step learning.
  prefs: []
  type: TYPE_NORMAL
- en: Due to this modular approach, building and experimenting with complex DL architectures
    has been much easier with PyTorch than following the somewhat rigid framework
    of TF and TF-based tools. Moreover, PyTorch was built to integrate seamlessly
    with the numerical computing infrastructure of the Python ecosystem and Python
    being the lingua franca of data science and machine learning, it has ridden over
    that wave of increasing popularity.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Operations with PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Tensors](https://www.kdnuggets.com/2018/05/wtf-tensor.html) are at the heart
    of any DL framework. PyTorch provides tremendous flexibility to a programmer about
    how to create, combine, and process tensors as they flow through a network (called
    computational graph) paired with a relatively high-level, object-oriented API.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What are Tensors?**'
  prefs: []
  type: TYPE_NORMAL
- en: Representing data (e.g., about the physical world or some business process)
    for Machine Learning (ML), in particular for DNN, is accomplished via a data/mathematical
    structure known as the *tensor*. A tensor is a container that can house data in *N* dimensions.
    A tensor is often used interchangeably with another more familiar mathematical
    object *matrix* (which is specifically a 2-dimensional tensor). In fact, tensors
    are generalizations of 2-dimensional matrices to *N*-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: In simplistic terms, one can think of scalar-vectors-matrices- tensors as a
    flow.
  prefs: []
  type: TYPE_NORMAL
- en: Scalar are 0-dimensional tensors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectors are 1-dimensional tensors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrices are 2-dimensional tensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensors are generalized N-dimensional *tensors*. N can be anything from 3 to
    infinity…
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often, these dimensions are also called *ranks*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/230f7ae97d3d12688a50fdee8f7e4a3d.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Fig 1: Tensors of various dimensions (ranks) ([Image source](https://medium.com/mlait/tensors-representation-of-data-in-neural-networks-bbe8a711b93b)).*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Why Are Tensors Important for ML and DL?**'
  prefs: []
  type: TYPE_NORMAL
- en: Think of a supervised ML problem. You are given a table of data with some labels
    (could be numerical entities or binary classification such as Yes/No answers).
    For ML algorithms to process it, the data must be fed as a mathematical object.
    A table is naturally equivalent to a 2-D matrix where an individual row (or instance)
    or individual column (or feature) can be treated as a 1-D vector.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, a black-and-white image can be treated as a 2-D matrix containing
    numbers 0 or 1\. This can be fed into a neural network for image classification
    or segmentation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: A time-series or sequence data (e.g., ECG data from a monitoring machine or
    a stock market price tracking data stream) is another example of 2-D data where
    one dimension (time) is fixed.
  prefs: []
  type: TYPE_NORMAL
- en: These are examples of using 2-D tensors in classical ML (e.g., linear regression,
    support vector machines, decision trees, etc.) and DL algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Going beyond 2-D, a color or grayscale image can be treated as a 3-D tensor
    where each pixel is associated with a so-called ‘color-channel’ – a vector of
    3 numbers representing intensities in the Red-Green-Blue (RGB) spectrum. This
    is an example of a 3-D tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, videos can be thought of as sequences of color images (or frames)
    in time and can be thought of as 4-D tensors.
  prefs: []
  type: TYPE_NORMAL
- en: In short, all kinds of data from the physical world, sensors and instruments,
    business and finance, scientific or social experiments, can be easily represented
    by multi-dimensional tensors to make them amenable for processing by ML/DL algorithms
    inside a computing machine.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how PyTorch defines and handles tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and Converting Tensors in PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tensors can be defined from a Python list as follows,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a645f3d7902d5ce23a854604d8b7a32.png)'
  prefs: []
  type: TYPE_IMG
- en: Actual elements can be accessed and indexed as follows,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/176f1784554eb13d6d89b77dea3bd98c.png)'
  prefs: []
  type: TYPE_IMG
- en: Tensors with specific data types can be created easily (e.g., floating points),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa5c22461ae055d6eeba75130f44bded.png)'
  prefs: []
  type: TYPE_IMG
- en: Size and dimensions can be read easily,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed711fb8f4d40821ee2a5059feb39b7b.png)'
  prefs: []
  type: TYPE_IMG
- en: We can change the view of a tensor. Let us start with a 1-dimensional tensor
    as follows,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69d4901515ecbf307f803ebda11f0e4e.png)'
  prefs: []
  type: TYPE_IMG
- en: Then change the view to a 2-D tensor,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6edac16b9d2c56f0f1901236417dc92d.png)'
  prefs: []
  type: TYPE_IMG
- en: Changing back and forth between a PyTorch tensor and a NumPy array is easy and
    efficient.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a1ea52f3b7034fde69e0e3edbfd6d2f.png)'
  prefs: []
  type: TYPE_IMG
- en: Converting from a Pandas series object is also easy,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b56ef4ea2a7282ecb74362422c9d3a3a.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, converting back to a Python list can be accomplished,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4dd65f6c4260e45877800ed96e338c72.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Vector and matrix mathematics with PyTorch tensors**'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch provides an easy-to-understand API and programmatic toolbox to manipulate
    tensors mathematically. We show basic operations with 1-D and 2-D tensors here.
  prefs: []
  type: TYPE_NORMAL
- en: Simple vector addition,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/792db69dd3d8147151c0489df387b266.png)'
  prefs: []
  type: TYPE_IMG
- en: Vector multiplication with a scalar,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2585875d695b37dae47730cf28dae5bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear combination,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a452cd3fe0cc1313c43d1d34154fa4f.png)'
  prefs: []
  type: TYPE_IMG
- en: Element-wise product,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/650b8f02bbdc14f58b39dc29346e57b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Dot product,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7ddef771720eb6ea283cff17f1f34dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Adding a scalar to every element of a tensor, i.e., broadcasting,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7fd9c5cab8f1dd8f93a69d0f1311a6cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating 2-D tensor from list of lists,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1423ffffc615874e9c2f1bccf51152a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Slicing and indexing of matrix elements,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b5cfe3f44046093ae52160707c246ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Matrix multiplication,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9a5efb99bbea4ad4012f1630eee0e75.png)'
  prefs: []
  type: TYPE_IMG
- en: Matrix transpose,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b7c3c5960a6476daafb186a9008df0d.png)'
  prefs: []
  type: TYPE_IMG
- en: Matrix inverse and determinant,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84422c9a7221bfd93314a8f2c68dbb53.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Autograd: Automatic differentiation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Neural network training and prediction involves taking derivatives of various
    functions (tensor-valued) over and over. The Tensor object supports the magical
    Autograd feature, i.e., automatic differentiation, which is achieved by tracking
    and storing all the operations performed on the tensor while it flows through
    a network. You can watch this wonderful tutorial video for a visual explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: The Pytorch autograd [**official documentation is here**](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html).
  prefs: []
  type: TYPE_NORMAL
- en: We show simple examples to illustrate the autograd feature of PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0f3e5c1f6916685796339bd0e5c3f05f.png)'
  prefs: []
  type: TYPE_IMG
- en: We define a generic function and a tensor variable ***x***, then define another
    variable ***y*** assigning it to the function of ***x***.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17fb29b0b9c5c79077442c3b983a802f.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, we use a special **backward()** method on ***y*** to take the derivative
    and calculate the derivative value at the given value of ***x***.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e7047c8e0a61295c48b8e7c569618f9.png)'
  prefs: []
  type: TYPE_IMG
- en: We can also deal with partial derivatives!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ad48aa7236ee4fe387a62d85e7ab384.png)'
  prefs: []
  type: TYPE_IMG
- en: We can define ***u*** and ***v*** as tensor variables, define a function combining
    them, apply the **backward()** method, and calculate the partial derivatives.
    See below,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0bf12a5f2ee5ce5649bd279fb6244ab3.png)'
  prefs: []
  type: TYPE_IMG
- en: PyTorch computes derivatives of scalar functions only, but if we pass a vector,
    then essentially it computes derivatives element-wise and stores them in an array
    of the same dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96c8ad30783fa05147414bf1b920dabb.png)'
  prefs: []
  type: TYPE_IMG
- en: The following code will calculate the derivative with respect to the three constituent
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b4908eab9b5f412f72ff6e338d39de71.png)'
  prefs: []
  type: TYPE_IMG
- en: We can show the plot of the derivative. Note, a derivative of a quadratic function
    is a straight line, tangent to the parabolic curve.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a3bdfd29fc4271ac66efab2f21cfb77.png)'
  prefs: []
  type: TYPE_IMG
- en: Building a Full-Fledged Neural Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apart from the tensors and automatic differentiation ability, there are few
    more core components/features of PyTorch that come together for a deep neural
    network definition.
  prefs: []
  type: TYPE_NORMAL
- en: The core components of PyTorch that will be used for building the neural classifier
    are,
  prefs: []
  type: TYPE_NORMAL
- en: The **Tensor** (the central data structure in PyTorch)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Autograd** feature of the Tensor (automatic differentiation formula baked
    into the
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **nn.Module** class that is used to build any other neural classifier class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Optimizer** (of course, there are many of them to choose from)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Loss** function (a big selection is available for your choice)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have already described in detail the Tensor and the Autograd. Let us quickly
    discuss the other components,
  prefs: []
  type: TYPE_NORMAL
- en: '**The nn.Module Class**'
  prefs: []
  type: TYPE_NORMAL
- en: In PyTorch, we construct a neural network by defining it as a custom class.
    However, instead of deriving from the native Python object, this class inherits
    from the [nn.Module class](https://pytorch.org/docs/stable/nn.html). This imbues
    the neural net class with useful properties and powerful methods. This way, the
    full power of Object-Oriented-Programming (OOP) can be maintained while working
    with neural net models. We will see a full example of such a class definition
    in our article.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Loss Function**'
  prefs: []
  type: TYPE_NORMAL
- en: In a neural network architecture and operation, the loss functions define how
    far the final prediction of the neural net is from the ground truth (given labels/classes
    or data for supervised training). The quantitative measure of loss helps drive
    the network to move closer to the configuration (the optimal settings of the weights
    of the neurons), which classifies the given dataset best or predicts the numerical
    output with the least total error.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch offers all the usual loss functions for classification and regression
    tasks —
  prefs: []
  type: TYPE_NORMAL
- en: binary and multi-class cross-entropy,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mean squared and mean absolute errors,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: smooth L1 loss,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: neg log-likelihood loss, and even
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kullback-Leibler divergence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**A detailed discussion of these can be found in this article.**](https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7)'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Optimizer**'
  prefs: []
  type: TYPE_NORMAL
- en: Optimization of the weights to achieve the lowest loss is at the heart of the
    backpropagation algorithm for training a neural network. PyTorch offers a plethora
    of optimizers to do the job, exposed through the torch.optim module —
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic gradient descent (SGD),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adam, Adadelta, Adagrad, SpareAdam,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L-BFGS,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RMSprop, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Look at this article**](https://blog.exxactcorp.com/activation-functions-and-optimizers-for-deep-learning-models/) to
    know more about activation functions and optimizers, used in modern deep neural
    networks.'
  prefs: []
  type: TYPE_NORMAL
- en: The Five-Step-Process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using these components, we will build the classifier in five simple steps,
  prefs: []
  type: TYPE_NORMAL
- en: Construct our neural network as our custom class (inherited from the **nn.Module** class),
    complete with hidden layer tensors and forward method for propagating the input
    tensor through various layers and activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Propagate the feature (from a dataset) tensor through the network using this **forward()** method
    — say we get an output tensor as a result
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the loss by comparing the output to the ground truth and using built-in
    loss functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Propagate the gradient of the loss using the automatic differentiation ability
    (**Autograd**) with the backward method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the weights of the network using the gradient of the loss — this is accomplished
    by executing one step of the so-called optimizer — **optimizer.step()**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And that’s it. This five-step process constitutes **one complete epoch of training**.
    We just repeat it a bunch of times to drive down the loss and obtain high classification
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The idea looks like following,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ddaf1cc441b4dd42f218c0b85151a2f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Hands-on example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s suppose we want to build and train the following 2-layer neural network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1e16c7faacd7079046dc18db4961113.png)'
  prefs: []
  type: TYPE_IMG
- en: We start with the class definition,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b41c6384b2a1c9ab8c9cc90980137ce3.png)'
  prefs: []
  type: TYPE_IMG
- en: We can define a variable as an object belonging to this class and print the
    summary.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc4381932a3ae01a5f0d67e5f74501f8.png)'
  prefs: []
  type: TYPE_IMG
- en: We choose the Binary cross-entropy loss,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b82c0484793c825febc6aea49f12c68c.png)'
  prefs: []
  type: TYPE_IMG
- en: Let us run the input dataset through the neural net model we have defined, i.e., **forward
    pass once and compute the output probabilities**. As the weights have been initialized
    as random, we will see random output probabilities (mostly close to 0.5). **This
    network has not been trained yet**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c93b460e863ae9dc8d82fb7f41a3fe2e.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/d1ad6dbac80d3fe8267d06985d9a7a50.png)'
  prefs: []
  type: TYPE_IMG
- en: We define the optimizer,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55175acd3a200fc0dbb21da83e19cf9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we show how to do forward and backward passes with one step of optimizer. **This
    set of code can be found at the heart of any PyTorch neural net model**. We follow
    another five-step process,
  prefs: []
  type: TYPE_NORMAL
- en: reset the gradients to zero (to prevent the accumulation of grads)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: forward pass the tensors through the layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: calculate the loss tensor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: calculate the gradients of the loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: update the weights by incrementing the optimizer by one step (in the direction
    of the negative gradient)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The five steps above** are exactly what you can observe and read about in all
    the theoretical discussion (and in the textbooks) on neural nets and deep learning**.
    And, with PyTorch, you are able to implement this process with deceptively simple
    code, step-by-step.
  prefs: []
  type: TYPE_NORMAL
- en: The code is shown below,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c331585b9ea3bbc0bd384a69e1a8bfb6.png)'
  prefs: []
  type: TYPE_IMG
- en: When **we run the same type of code over a loop (for multiple epochs)**, we
    can observe the familiar loss-curve going down, i.e., the neural network getting
    trained gradually.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab330f2f41dab4503f83d3df06a60b91.png)'
  prefs: []
  type: TYPE_IMG
- en: After training for 200 epochs, we can look at the probability distributions
    again directly to see how the neural network output probabilities are now different
    (trying to match with the true data distributions).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/268637fd77fe58320394ddec8b27666c.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary of PyTorch Fundamentals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyTorch is a great package for reaching out to the heart of a neural net and
    customizing it for your application or trying out bold new ideas with the architecture,
    optimization, and mechanics of the network.
  prefs: []
  type: TYPE_NORMAL
- en: You can easily build complex interconnected networks, try out novel activation
    functions, mix and match custom loss functions, etc. The core ideas of computation
    graphs, easy auto-differentiation, and forward and backward flow of tensors will
    come in handy for any of your neural network definitions and optimization.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we summarized a few key steps which can be followed to quickly
    build a neural network for classification or regression tasks. We also showed
    how neat ideas could be easily tried out with this framework.
  prefs: []
  type: TYPE_NORMAL
- en: All the code for this article [**can be found here in this Github repo**](https://github.com/tirthajyoti/PyTorch_Machine_Learning).
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://blog.exxactcorp.com/the-most-important-fundamentals-of-pytorch-you-should-know/).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Build PyTorch Models Easily Using torchlayers](https://www.kdnuggets.com/2020/04/pytorch-models-torchlayers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenAI is Adopting PyTorch… They Aren’t Alone](https://www.kdnuggets.com/2020/01/openai-pytorch-adoption.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Gentle Introduction to PyTorch 1.2](https://www.kdnuggets.com/2019/09/gentle-introduction-pytorch-12.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
