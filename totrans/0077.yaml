- en: 'Fasten Your Seatbelt: Falcon 180B is Here!'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/fasten-your-seatbelt-falcon-180b-is-here](https://www.kdnuggets.com/fasten-your-seatbelt-falcon-180b-is-here)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Fasten Your Seatbelt: Falcon 180B is Here!](../Images/e65a7e76e2f4c174d5c28fe9e2c34569.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: A few months ago, we learnt about [Falcon LLM](/2023/06/falcon-llm-new-king-llms.html),
    which was founded by the [Technology Innovation Institute](https://www.tii.ae/)
    (TII), a company part of the Abu Dhabi Government’s Advanced Technology Research
    Council. Fast forward a few months, they’ve just got even bigger and better -
    literally, so much bigger.
  prefs: []
  type: TYPE_NORMAL
- en: 'Falcon 180B: All You Need to Know'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Falcon 180B is the largest openly available language model, with 180 billion
    parameters. Yes, that’s right, you read correctly - 180 billion. It was trained
    on 3.5 trillion tokens using TII's RefinedWeb dataset. This represents the longest
    single-epoch pre-training for an open model.
  prefs: []
  type: TYPE_NORMAL
- en: But it’s not just about the size of the model that we’re going to focus on here,
    it’s also about the power and potential behind it. Falcon 180B is creating new
    standards with Large language models (LLMs) when it comes to capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The models that are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Falcon 180B](https://huggingface.co/tiiuae/falcon-180B)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Falcon 180B Chat](https://huggingface.co/tiiuae/falcon-180B-chat)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Falcon-180B Base model is a causal decoder-only model. I would recommend
    using this model for further fine-tuning your own data.
  prefs: []
  type: TYPE_NORMAL
- en: The Falcon-180B-Chat model has similarities to the base version but goes in
    a bit deeper by fine-tuning using a mix of Ultrachat, Platypus, and Airoboros
    instruction (chat) datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Falcon 180B scaled up for its predecessor Falcon 40B, with new capabilities
    such as multiquery attention for enhanced scalability. The model used 4096 GPUs
    on Amazon SageMaker and was trained on 3.5 trillion tokens. This is roughly around
    7,000,000 GPU hours. This means that Falcon 180B is 2.5x faster than LLMs such
    as Llama 2 and was trained on 4x more computing.
  prefs: []
  type: TYPE_NORMAL
- en: Wow, that’s a lot.
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset used for Falcon 180B was predominantly sourced (85%) from RefinedWeb,
    as well as being trained on a mix of curated data such as technical papers, conversations,
    and some elements of code.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The part you all want to know - how is Falcon 180B doing amongst its competitors?
  prefs: []
  type: TYPE_NORMAL
- en: Falcon 180B is currently the best openly released LLM to date (September 2023).
    It has been shown to outperform Llama 2 70B and OpenAI’s GPT-3.5 on [MMLU](https://paperswithcode.com/dataset/mmlu).
    It typically sits somewhere between GPT 3.5 and GPT 4.
  prefs: []
  type: TYPE_NORMAL
- en: '![Fasten Your Seatbelt: Falcon 180B is Here!](../Images/2b578ab8e51a55f1336154eed1d9b9a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [HuggingFace Falcon 180B](https://huggingface.co/blog/falcon-180b)
  prefs: []
  type: TYPE_NORMAL
- en: Falcon 180B ranked 68.74 on the Hugging Face Leaderboard, making it the highest-scoring
    openly released pre-trained LLM where it surpassed Meta’s LLaMA 2 which was at
    67.35.
  prefs: []
  type: TYPE_NORMAL
- en: How to use Falcon 180B?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the developer and natural language processing (NLP) enthusiasts out there,
    Falcon 180B is available on the Hugging Face ecosystem, starting with Transformers
    version 4.33.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, as you can imagine due to the model’s size, you will need to take
    into consideration hardware requirements. To get a better understanding of the
    hardware requirements, HuggingFace ran tests needed to run the model for different
    use cases, as shown in the image below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Fasten Your Seatbelt: Falcon 180B is Here!](../Images/424f925c66da18a9be9a9153c7800098.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [HuggingFace Falcon 180B](https://huggingface.co/blog/falcon-180b)
  prefs: []
  type: TYPE_NORMAL
- en: 'If you would like to give it a test and play around with it, you can try out
    Falcon 180B through the demo by clicking on this link: [Falcon 180B Demo](https://huggingface.co/spaces/tiiuae/falcon-180b-demo).'
  prefs: []
  type: TYPE_NORMAL
- en: Falcon 180B vs ChatGPT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model has some serious hardware requirements which are not easily accessible
    to everybody. However, based on other people's findings on testing both Falcon
    180B against ChatGPT by asking them the same questions, ChatGPT took the win.
  prefs: []
  type: TYPE_NORMAL
- en: It performed well on code generation, however, it needs a boost on text extraction
    and summarization.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping it up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’ve had a chance to play around with it, let us know what your findings
    were against other LLMs. Is Falcon 180B worth all the hype that’s around it as
    it is currently the largest publicly available model on the Hugging Face model
    hub?
  prefs: []
  type: TYPE_NORMAL
- en: Well, it seems to be as it has shown to be at the top of the charts for open-access
    models, and models like PaLM-2, a run for their money. We’ll find out sooner or
    later.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.linkedin.com/in/nisha-arya-ahmed/)****[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)****
    is a data scientist, freelance technical writer, and an editor and community manager
    for KDnuggets. She is particularly interested in providing data science career
    advice or tutorials and theory-based knowledge around data science. Nisha covers
    a wide range of topics and wishes to explore the different ways artificial intelligence
    can benefit the longevity of human life. A keen learner, Nisha seeks to broaden
    her tech knowledge and writing skills, while helping guide others.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Falcon LLM: The New King of Open-Source LLMs](https://www.kdnuggets.com/2023/06/falcon-llm-new-king-llms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Want to Use Your Data Skills to Solve Global Problems? Here’s What…](https://www.kdnuggets.com/2022/04/jhu-want-data-skills-solve-global-problems.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[I Used ChatGPT (Every Day) for 5 Months. Here Are Some Hidden Gems…](https://www.kdnuggets.com/2023/07/used-chatgpt-every-day-5-months-hidden-gems-change-life.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Here Are the AI Tools I Use Along With My Skills to Make $10,000…](https://www.kdnuggets.com/2023/07/ai-tools-along-skills-make-10000-monthly-bs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unable to Land a Data Science Job? Here’s Why](https://www.kdnuggets.com/2022/01/unable-land-data-science-job.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science is Overrated, Here’s Why](https://www.kdnuggets.com/2022/06/data-science-overrated.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
