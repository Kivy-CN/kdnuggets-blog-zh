- en: Gradient Boosting in TensorFlow vs XGBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/01/gradient-boosting-tensorflow-vs-xgboost.html](https://www.kdnuggets.com/2018/01/gradient-boosting-tensorflow-vs-xgboost.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Nicolò Valigi](https://www.linkedin.com/in/nicolovaligi/), Founder of
    AI Academy**'
  prefs: []
  type: TYPE_NORMAL
- en: Tensorflow 1.4 was released a few weeks ago with an implementation of Gradient
    Boosting, called **TensorFlow Boosted Trees (TFBT)**. Unfortunately, the [paper](https://arxiv.org/abs/1710.11555) does
    not have any benchmarks, so I ran some against XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: For many Kaggle-style data mining problems, XGBoost has been the go-to solution
    since its release in 2016\. It's probably as close to an out-of-the-box machine
    learning algorithm as you can get today, as it gracefully handles un-normalized
    or missing data, while being accurate and fast to train.
  prefs: []
  type: TYPE_NORMAL
- en: The code to reproduce the results in this article is [on GitHub](https://github.com/nicolov/gradient_boosting_tensorflow_xgboost).
  prefs: []
  type: TYPE_NORMAL
- en: The experiment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I wanted a decently sized dataset to test the scalability of the two solutions,
    so I picked the **airlines dataset** available [here](http://stat-computing.org/dataexpo/2009/).
    It has around 120 million data points for all commercial flights within the USA
    from 1987 to 2008\. The features include origin and destination airports, date
    and time of departure, arline, and flight distance. I set up a straightforward
    binary classification task that tries to predict whether a flight would be more
    than 15 minutes late.
  prefs: []
  type: TYPE_NORMAL
- en: 'I sampled 100k flights from 2006 for the training set, and 100k flights from
    2007 for the test set. Sadly, roughly 20% of flights were more than 15 minutes
    late, a fact that doesn''t reflect well on the airline industry :D. It''s easy
    to see how strongly departure time throughout the day correlates with the likelihood
    of delay:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5441b90ecfb58bced314c9b7eb110c9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I did not do any feature engineering, so the list of features is very basic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: I used the scikit-style wrapper for XGBoost, which makes training and prediction
    from NumPy arrays a two-line affair ([code](https://github.com/nicolov/gradient_boosting_tensorflow_xgboost/blob/master/do_xgboost.py)).
    For TensorFlow, I used `tf.Experiment`, `tf.learn.runner`, and the NumPy input
    functions to save some boilerplate ([code](https://github.com/nicolov/gradient_boosting_tensorflow_xgboost/blob/master/do_tensorflow.py)).
    TODO
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I started out with XGBoost and a decent guess at the hyperparameters, and immediately
    got an [AUC score](https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it) I
    was happy with. When I tried the same settings on TensorFlow Boosted Trees, I
    didn't even have enough patience for the training to end!
  prefs: []
  type: TYPE_NORMAL
- en: 'While I kept `num_trees=50` and `learning_rate=0.1` for both models, I ended
    up having to tweak the TF Boosted Trees `examples_per_layer` knob using an hold-out
    set. It''s likely that this is related to the novel *layer-by-layer* learning
    algorithm featured in the TFBT paper, but I haven''t dug in deeper. As a starting
    point for comparison, I selected two values (1k and 5k) that yielded similar training
    times and accuracy to XGBoost. Here''s how the results look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/edf8da04192ca1cbff995991242c0161.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/7acb4573030b1a09dddcca330f037844.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Accuracy numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Training runtime:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Neither of the two settings shown for TensorFlow could match the training time/accuracy
    of XGBoost. Besides the disadvantage in `user` time (total CPU time used), it
    also seems that TensorFlow isn't very effective at parallelizing on multiple cores
    either, leading to a massive gap in `total` (i.e. wall) time too. XGBoost has
    no trouble loading 16 of the 32 cores in my box (and can do better when using
    more trees), whereas TensorFlow uses less than 4\. I guess that the whole "distributed
    TF" toolbox could be used to make TFBT scale better, but that seems overkill just
    to make full use of a *single* server.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With a few hours of tweaking, I couldn't get TensorFlow's Boosted Trees implementation
    to match XGBoost's results, neither in training time nor accuracy. This immediately
    disqualifies it from many of the quick-n-dirty projects I would use XGBoost for.
    The limited parallelism of the implementation also means that it wouldn't scale
    up to big datasets either.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Boosted Trees might make sense within an infrastructure that's heavily
    invested in TensorFlow tooling already. TensorBoard and the data loading pipeline
    are two features that work fine on Boosted Trees too and could easily migrated
    from other Deep Learning projects based on TensorFlow. However, TF Boosted Trees
    won't be very useful in most cases until the implementation can match the performance
    of XGBoost (or I learn how to tune it).
  prefs: []
  type: TYPE_NORMAL
- en: To reproduce my results, get the [training code on GitHub](https://github.com/nicolov/gradient_boosting_tensorflow_xgboost).
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Nicolò Valigi](https://www.linkedin.com/in/nicolovaligi/)** is fascinated
    by the code and infrastructure that power machine learning. He founded AI Academy,
    a consultancy to help companies learn about AI, and is a Senior Software Engineer
    at Cruise Automation, where he takes care of the self-driving car platform.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://nicolovaligi.com/gradient-boosting-tensorflow-xgboost.html).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[XGBoost: A Concise Technical Overview](/2017/10/xgboost-concise-technical-overview.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Lessons Learned From Benchmarking Fast Machine Learning Algorithms](/2017/08/lessons-benchmarking-fast-machine-learning-algorithms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Simple XGBoost Tutorial Using the Iris Dataset](/2017/03/simple-xgboost-tutorial-iris-dataset.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How to Speed Up XGBoost Model Training](https://www.kdnuggets.com/2021/12/speed-xgboost-model-training.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Boosting Machine Learning Algorithms: An Overview](https://www.kdnuggets.com/2022/07/boosting-machine-learning-algorithms-overview.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are the Assumptions of XGBoost?](https://www.kdnuggets.com/2022/08/assumptions-xgboost.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Tuning XGBoost Hyperparameters](https://www.kdnuggets.com/2022/08/tuning-xgboost-hyperparameters.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Leveraging XGBoost for Time-Series Forecasting](https://www.kdnuggets.com/2023/08/leveraging-xgboost-timeseries-forecasting.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Ultimate Guide to Mastering Seasonality and Boosting Business Results](https://www.kdnuggets.com/2023/08/media-mix-modeling-ultimate-guide-mastering-seasonality-boosting-business-results.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
