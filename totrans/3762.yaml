- en: Deep Learning Can be Applied to Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/01/deep-learning-applied-natural-language-processing.html](https://www.kdnuggets.com/2017/01/deep-learning-applied-natural-language-processing.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)![](../Images/d7c1e236ba300848f00cebec470c16f1.png)'
  prefs: []
  type: TYPE_IMG
- en: '*[Image credit](https://unsplash.com/collections/144450/music-learning-blog-collateral?photo=gN_nIUnjYJI)*'
  prefs: []
  type: TYPE_NORMAL
- en: There is an article going around the rounds at LinkedIn that attempts to make
    an argument against the use of Deep Learning in the domain of NLP. The article
    written by Riza Berkan “[Is Google Hyping it? Why Deep Learning cannot be Applied
    to Natural Languages Easily](http://www.linkedin.com/pulse/google-hyping-why-deep-learning-cannot-applied-easily-berkan-ph-d?trk=hp-feed-article-title-comment)”
    has several arguments about DL cannot possibly work and that Google is exaggerating
    its claims. The latter argument is of course borderline conspiracy theory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yannick Vesley has written a rebuttal “[Neural Networks are Quite Neat: a Reply
    to Riza Berkan](https://www.linkedin.com/pulse/neural-networks-quite-neat-reply-riza-berkan-yannick-versley?deepLinkCommentId=6221869083437072384&anchorTime=1483409186231&trk=hb_ntf_MEGAPHONE_REPLY_TOP_LEVEL_COMMENT)”
    where he makes his arguments on each point that Berkan makes. Vesley’s points
    are on the mark, however one can not ignore the feeling that DL theory has a few
    unexplained parts in it.'
  prefs: []
  type: TYPE_NORMAL
- en: However, before I do get into that, I think it is very important for readers
    to understand that DL currently is an experimental science. That is, DL capabilities
    are actually discovered by researchers by surprise. There are certainly a lot
    of engineering that goes into the optimization and improvement of these machines.
    However, its capabilities are ‘unreasonably effective’, in short, we don’t have
    very good theories to explain its capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is clear that there are gaps in understanding are in at least 3 open questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How is DL able to search high dimensional discrete spaces?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is DL able to perform generalization if it appears to be performing rote
    memorization?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does (1) and (2) arise from simple components?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Berkan’s arguments exploit our current lack of a solid explanation with his
    own alternative approach. He is arguing that a symbolicist approach is the road
    to salvation. Unfortunately, no where in his arguments does he reveal the brittleness
    of the symbolicist approach, the lack of generalization and the lack of scalability.
    Has anyone created a rule based system that is able to classify images based on
    low level features that rivals DL? I don’t think so.
  prefs: []
  type: TYPE_NORMAL
- en: DL practitioners, however, aren’t stopping their work just because they don’t
    have air tight theoretical foundations. DL works and works surprisingly well.
    DL at is present state is an experimental science and it is absolutely clear that
    there is something going on underneath the covers that we don’t fully understand.
    A lack of understanding however does not invalidate the approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the issues better, I wrote in an earlier article about “[Architecture
    Ilities found in Deep Learning Systems](https://medium.com/intuitionmachine/architecture-ilities-for-deep-learning-12a3ff9bec4e#.rpjhp26b0)”.
    I basically spell out the 3 capabilities in DL:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Expressibility** — This quality describes how well a machine can approximate
    universal functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trainability** — How well and quickly a DL system can learn its problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generalizability** — How well machine can perform predictions on data that
    it has not been trained on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are of course other capabilities that also need to be considered in DL:
    Interpretability, modularity, transferability, latency, adversarial stability
    and security. But these are the main ones.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get our bearing right about explaining all of these, we have to consider
    the latest experimental evidences. I’ve written about this here “[Rethinking Generalization](https://medium.com/intuitionmachine/rethinking-generalization-in-deep-learning-ec66ed684ace#.h418bb3it)”
    which I summarize again:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ICLR 2017 submission “[Understanding Deep Learning required Rethinking
    Generalization](https://openreview.net/pdf?id=Sy8gdB9xx)“ is certainly going to
    disrupt our understanding of Deep Learning . Here is a summary of what the had
    discovered through experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The effective capacity of neural networks is large enough for a brute-force
    memorization of the entire data set.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2\. Even optimization on random labels remains easy. In fact, training time
    increases only by a small constant factor compared with training on the true labels.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3\. Randomizing labels is solely a data transformation, leaving all other properties
    of the learning problem unchanged.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The point here that surprises most Machine Learning practitioners is the ‘brute-force
    memorization’. See, ML has always been about curve fitting. In curve fitting you
    find a sparse set of parameters that describe your curve and you use that to fit
    the data. The generalization that comes into play relates to the ability to interpolate
    between points. The major disconnect here is that DL have exhibited impressive
    generalization, yet it cannot possibly work if we consider them as just memory
    stores.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if we consider them as holographic memory stores, then that problem
    of generalization has a decent explanation. In “[Deep Learning are Holographic
    Memories](https://medium.com/intuitionmachine/deep-learning-machines-are-holographic-memories-258272422995#.zgf8bjj77)”
    I point out the experimental evidence that:'
  prefs: []
  type: TYPE_NORMAL
- en: The Swapout learning procedure which tells us that if you sample any subnetwork
    of the entire network the resulting prediction will be the similar to any other
    subnetwork you look sample. Just like holographic memory where you can slice of
    pieces and still recreate the whole.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'As it turns out, the universe itself is driven by a similar theory called the
    Holographic Principle. In fact, this serves as a very good base camp to begin
    a more solid explanation of the capabilities of Deep Learning. I introduce the
    “[The Holographic Principle: Why Deep Learning Works](https://medium.com/intuitionmachine/the-holographic-principle-and-deep-learning-52c2d6da8d9#.fsgytefke)”
    where I introduce a technical approach of using Tensor Networks that performs
    a reduction of the high dimensional problem space into a space that is computable
    within acceptable response times.'
  prefs: []
  type: TYPE_NORMAL
- en: So going back again to the question about wether NLP can be handled by Deep
    Learning approaches. We certainly know that it can work, afterall, are you not
    reading and comprehending this text?
  prefs: []
  type: TYPE_NORMAL
- en: 'There certainly is a lot of confusion in the ranks of expert data scientists
    and ML practitioners. I was aware of the existence of this “push back” when I
    wrote: “[11 Arguments that Experts get Wrong about Deep Learning](https://www.linkedin.com/pulse/11-arguments-experts-get-wrong-deeplearning-carlos-e-perez?articleId=6219628719263215617)”.
    However, Deep Learning likely can be best explained by a simple intuition that
    can be [explained to a five year old](https://medium.com/intuitionmachine/deep-learning-explained-to-a-five-year-old-25919b0bf889#.weckq9twr):'
  prefs: []
  type: TYPE_NORMAL
- en: '**DE3p Larenn1g wrok smliair to hOw biarns wrok.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tehse mahcnies wrok by s33nig f22Uy pa773rns and cnonc3t1ng t3Hm t0 fU22y cnoc3tps.
    T3hy wRok l4y3r by ly43r, j5ut lK1e A f1l73r, t4k1NG cmopl3x sc3n3s aNd br3k41ng
    tH3m dwon itno s1pmLe iD34s.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A symbolic system cannot read this, however a human can.
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2015, Chris Manning, an NLP practitioner wrote about the concerns of the
    field regarding Deep Learning (see: [Computational Linguistics and Deep Learning](http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00239)).
    It is very important to take note of his arguments since his arguments are not
    in conflict with the capabilities of Deep Learning. His two arguments why NLP
    experts need not worry are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) It just has to be wonderful for our field for the smartest and most influential
    people in machine learning to be saying that NLP is the problem area to focus
    on; and (2) Our field is the domain science of language technology; it’s not about
    the best method of machine learning — the central issue remains the domain problems.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The first argument isn’t a criticism of Deep Learning. The second argument explains
    that he doesn’t believe in one-size-fits-all generic machine learning that works
    for all domains. That is not in conflict with the above Holographic Principle
    approach that indicates the importance of the network structure.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude, I hope this article puts an end to the discussion that DL is not
    applicable to NLP.
  prefs: []
  type: TYPE_NORMAL
- en: 'If perhaps you still aren’t convinced, then maybe Chris Manning himself should
    convince you himself:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Carlos Perez](http://www.linkedin.com/in/ceperez)** is a software developer
    presently writing a book on "Design Patterns for Deep Learning". This is where
    he sources his ideas for his blog posts.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/intuitionmachine/why-deep-learning-can-be-applied-to-natural-languages-46c74a6f861f#.wc79wpm3g).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Why Deep Learning is Radically Different From Machine Learning](/2016/12/deep-learning-radically-different-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Five Capability Levels of Deep Learning Intelligence](/2016/12/5-capability-levels-deep-learning-intelligence.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Game Theory Reveals the Future of Deep Learning](/2016/12/game-theory-reveals-future-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
