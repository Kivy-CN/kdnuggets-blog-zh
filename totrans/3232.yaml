- en: Ensemble Learning to Improve Machine Learning Results
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习以改善机器学习结果
- en: 原文：[https://www.kdnuggets.com/2017/09/ensemble-learning-improve-machine-learning-results.html](https://www.kdnuggets.com/2017/09/ensemble-learning-improve-machine-learning-results.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/09/ensemble-learning-improve-machine-learning-results.html](https://www.kdnuggets.com/2017/09/ensemble-learning-improve-machine-learning-results.html)
- en: '**By Vadim Smolyakov, [Statsbot](https://statsbot.co/).**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 Vadim Smolyakov 提供，[Statsbot](https://statsbot.co/)。**'
- en: '*Ensemble learning helps improve machine learning results by combining several
    models. This approach allows the production of better predictive performance compared
    to a single model. That is why ensemble methods placed first in many prestigious
    machine learning competitions, such as the Netflix Competition, KDD 2009, and
    Kaggle.*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*集成学习通过结合多个模型来帮助提高机器学习结果。这种方法能够比单一模型产生更好的预测性能。这就是为什么集成方法在许多著名的机器学习竞赛中获胜，例如
    Netflix 竞赛、KDD 2009 和 Kaggle。*'
- en: '*The *[*Statsbot*](http://statsbot.co/?utm_source=kdnuggets)* team wanted to
    give you the advantage of this approach and asked a data scientist, Vadim Smolyakov,
    to dive into three basic ensemble learning techniques.*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*[*Statsbot*](http://statsbot.co/?utm_source=kdnuggets)* 团队希望为你提供这种方法的优势，并请数据科学家
    Vadim Smolyakov 深入探讨三种基本的集成学习技术。*'
- en: Ensemble methods are meta-algorithms that combine several machine learning techniques
    into one predictive model in order to **decrease** **variance** (bagging), **bias** (boosting),
    or **improve predictions** (stacking).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法是将几种机器学习技术组合成一个预测模型的元算法，以**减少****方差**（Bagging）、**偏差**（Boosting）或**改善预测**（Stacking）。
- en: 'Ensemble methods can be divided into two groups:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法可以分为两类：
- en: '*sequential* ensemble methods where the base learners are generated sequentially
    (e.g. AdaBoost).'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*序列*集成方法，其中基础学习器是按序生成的（例如 AdaBoost）。'
- en: The basic motivation of sequential methods is to** exploit the dependence between
    the base learners.** The overall performance can be boosted by weighing previously
    mislabeled examples with higher weight.
  id: totrans-8
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 序列方法的基本动机是**利用基础学习器之间的依赖性**。通过加大对之前标记错误样本的权重，可以提升整体性能。
- en: '*parallel* ensemble methods where the base learners are generated in parallel
    (e.g. Random Forest).'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*并行*集成方法，其中基础学习器是并行生成的（例如随机森林）。'
- en: The basic motivation of parallel methods is to **exploit independence between
    the base learners** since the error can be reduced dramatically by averaging.
  id: totrans-10
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 并行方法的基本动机是**利用基础学习器之间的独立性**，因为通过平均可以显著减少误差。
- en: Most ensemble methods use a single base learning algorithm to produce homogeneous
    base learners, i.e. learners of the same type, leading to *homogeneous ensembles*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数集成方法使用单一基础学习算法来生成同质基础学习器，即相同类型的学习器，从而形成*同质集成*。
- en: There are also some methods that use heterogeneous learners, i.e. learners of
    different types, leading to *heterogeneous ensembles*. In order for ensemble methods
    to be more accurate than any of its individual members, the base learners have
    to be as accurate as possible and as diverse as possible.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些方法使用异质学习器，即不同类型的学习器，从而形成*异质集成*。为了使集成方法比任何单个成员都更准确，基础学习器必须尽可能准确且尽可能多样化。
- en: Bagging
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Bagging
- en: 'Bagging stands for bootstrap aggregation. One way to reduce the variance of
    an estimate is to average together multiple estimates. For example, we can train
    M different trees on different subsets of the data (chosen randomly with replacement)
    and compute the ensemble:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging 代表自助聚合。减少估计方差的一种方法是对多个估计值进行平均。例如，我们可以在数据的不同子集（随机选择且允许重复）上训练 M 棵不同的树，并计算集成：
- en: '![](../Images/2cceadad307359316c9dd527421af74e.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cceadad307359316c9dd527421af74e.png)'
- en: Bagging uses bootstrap sampling to obtain the data subsets for training the
    base learners. For aggregating the outputs of base learners, bagging uses *voting
    for classification* and *averaging for regression*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging 使用自助抽样来获取训练基础学习器的数据子集。为了汇总基础学习器的输出，Bagging 使用*分类投票*和*回归平均*。
- en: 'We can study bagging in the context of classification on the Iris dataset.
    We can choose two base estimators: a decision tree and a k-NN classifier. Figure
    1 shows the learned decision boundary of the base estimators as well as their
    bagging ensembles applied to the Iris dataset.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 Iris 数据集的分类背景下研究 Bagging。我们可以选择两个基础估计器：决策树和 k-NN 分类器。图 1 显示了基础估计器学习到的决策边界，以及它们的
    Bagging 集成应用于 Iris 数据集的效果。
- en: 'Accuracy: 0.63 (+/- 0.02) [Decision Tree]'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率：0.63 (+/- 0.02) [决策树]
- en: 'Accuracy: 0.70 (+/- 0.02) [K-NN]'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率：0.70 (+/- 0.02) [K-NN]
- en: 'Accuracy: 0.64 (+/- 0.01) [Bagging Tree]'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率：0.64 (+/- 0.01) [袋装树]
- en: 'Accuracy: 0.59 (+/- 0.07) [Bagging K-NN]'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率：0.59 (+/- 0.07) [袋装K-NN]
- en: '![](../Images/33f9ea715faa9a1a1c6673eb09c5d0c9.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33f9ea715faa9a1a1c6673eb09c5d0c9.png)'
- en: The decision tree shows the axes’ parallel boundaries, while the k=1 nearest
    neighbors fit closely to the data points. The bagging ensembles were trained using
    10 base estimators with 0.8 subsampling of training data and 0.8 subsampling of
    features.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树显示了轴的平行边界，而k=1的最近邻则紧密地拟合数据点。袋装集成是使用10个基本估计器训练的，训练数据的子样本为0.8，特征的子样本也为0.8。
- en: The decision tree bagging ensemble achieved higher accuracy in comparison to
    the k-NN bagging ensemble. K-NN are less sensitive to perturbation on training
    samples and therefore they are called stable learners.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树袋装集成在准确率上优于k-NN袋装集成。K-NN对训练样本的扰动不太敏感，因此被称为稳定的学习器。
- en: '*Combining stable learners is less advantageous since the ensemble will not
    help improve generalization performance.*'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*结合稳定的学习器并不太有优势，因为集成不会有助于提高泛化性能。*'
- en: The figure also shows how the test accuracy improves with the size of the ensemble.
    Based on cross-validation results, we can see the accuracy increases until approximately
    10 base estimators and then plateaus afterwards. Thus, adding base estimators
    beyond 10 only increases computational complexity without accuracy gains for the
    Iris dataset.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图中还显示了测试准确率如何随着集成大小的增加而提高。根据交叉验证结果，我们可以看到准确率在大约10个基本估计器时上升，然后保持平稳。因此，超过10个基本估计器只会增加计算复杂性，而不会提高Iris数据集的准确率。
- en: We can also see the learning curves for the bagging tree ensemble. Notice an
    average error of 0.3 on the training data and a U-shaped error curve for the testing
    data. The smallest gap between training and test errors occurs at around 80% of
    the training set size.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到袋装树集成的学习曲线。注意训练数据的平均误差为0.3，以及测试数据的U型误差曲线。训练和测试误差之间的最小差距发生在训练集大小的约80%时。
- en: '*A commonly used class of ensemble algorithms are forests of randomized trees.*'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*一种常用的集成算法类别是随机化树的森林。*'
- en: In **random forests**, each tree in the ensemble is built from a sample drawn
    with replacement (i.e. a bootstrap sample) from the training set. In addition,
    instead of using all the features, a random subset of features is selected, further
    randomizing the tree.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在**随机森林**中，集合中的每棵树都是从训练集中通过替换抽样（即自助样本）构建的。此外，除了使用所有特征之外，还会随机选择特征的子集，从而进一步随机化树的结构。
- en: As a result, the bias of the forest increases slightly, but due to the averaging
    of less correlated trees, its variance decreases, resulting in an overall better
    model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，森林的偏差略微增加，但由于对相关性较低的树进行平均，方差降低，从而整体上模型表现更好。
- en: '![](../Images/47a13d5d5d421a4b111aa4691973b497.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47a13d5d5d421a4b111aa4691973b497.png)'
- en: 'In an **extremely randomized trees** algorithm randomness goes one step further:
    the splitting thresholds are randomized. Instead of looking for the most discriminative
    threshold, thresholds are drawn at random for each candidate feature and the best
    of these randomly-generated thresholds is picked as the splitting rule. This usually
    allows reduction of the variance of the model a bit more, at the expense of a
    slightly greater increase in bias.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在**极端随机树**算法中，随机性进一步提升：分裂阈值被随机化。不是寻找最具判别力的阈值，而是为每个候选特征随机抽取阈值，并选择这些随机生成的阈值中最好的作为分裂规则。这通常可以稍微降低模型的方差，但偏差会略微增加。
- en: '**Boosting**'
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**提升（Boosting）**'
- en: Boosting refers to a family of algorithms that are able to convert weak learners
    to strong learners. The main principle of boosting is to fit a sequence of weak
    learners− models that are only slightly better than random guessing, such as small
    decision trees− to weighted versions of the data. More weight is given to examples
    that were misclassified by earlier rounds.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 提升（Boosting）指的是一类能够将弱学习器转换为强学习器的算法。提升的主要原理是将一系列弱学习器（即稍微优于随机猜测的模型，如小型决策树）拟合到加权版本的数据中。对早期轮次错误分类的示例赋予更多权重。
- en: The predictions are then combined through a weighted majority vote (classification)
    or a weighted sum (regression) to produce the final prediction. The principal
    difference between boosting and the committee methods, such as bagging, is that
    base learners are trained in sequence on a weighted version of the data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过加权多数投票（分类）或加权求和（回归）来组合预测，以产生最终预测。提升方法与委员会方法（如 bagging）的主要区别在于基学习器是在数据的加权版本上按顺序训练的。
- en: The algorithm below describes the most widely used form of boosting algorithm
    called **AdaBoost**,which stands for adaptive boosting.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的算法描述了最广泛使用的提升算法形式，即 **AdaBoost**，它代表自适应提升。
- en: '![](../Images/8a53b5e180f2642f01752d190c29b478.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a53b5e180f2642f01752d190c29b478.png)'
- en: We see that the first base classifier y1(x) is trained using weighting coefficients
    that are all equal. In subsequent boosting rounds, the weighting coefficients
    are increased for data points that are misclassified and decreased for data points
    that are correctly classified.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到第一个基分类器 y1(x) 是使用所有相等的权重系数进行训练的。在随后的提升回合中，权重系数会增加对误分类的数据点，减少对正确分类的数据点的权重。
- en: The quantity epsilon represents a weighted error rate of each of the base classifiers.
    Therefore, the weighting coefficients alpha give greater weight to the more accurate
    classifiers.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数量 epsilon 代表每个基分类器的加权错误率。因此，权重系数 alpha 给予更准确的分类器更大的权重。
- en: '![](../Images/66139941634db7f9ed0861cb2c93e1a9.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66139941634db7f9ed0861cb2c93e1a9.png)'
- en: The AdaBoost algorithm is illustrated in the figure above. Each base learner
    consists of a decision tree with depth 1, thus classifying the data based on a
    feature threshold that partitions the space into two regions separated by a linear
    decision surface that is parallel to one of the axes. The figure also shows how
    the test accuracy improves with the size of the ensemble and the learning curves
    for training and testing data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 算法如上图所示。每个基学习器由深度为 1 的决策树组成，从而基于特征阈值对数据进行分类，该阈值将空间分成由一个与轴平行的线性决策面分隔的两个区域。图中还显示了随着集合大小的增加，测试准确度如何提高，以及训练和测试数据的学习曲线。
- en: '**Gradient Tree Boosting** is a generalization of boosting to arbitrary differentiable
    loss functions. It can be used for both regression and classification problems.
    Gradient Boosting builds the model in a sequential way.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度树提升** 是对提升方法的一种推广，适用于任意可微损失函数。它可用于回归和分类问题。梯度提升以顺序方式构建模型。'
- en: '![](../Images/e584f410462ef7eb0adbaaab2e999d99.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e584f410462ef7eb0adbaaab2e999d99.png)'
- en: 'At each stage the decision tree hm(x) is chosen to minimize a loss function
    L given the current model Fm-1(x):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个阶段，选择决策树 hm(x) 来最小化给定当前模型 Fm-1(x) 的损失函数 L：
- en: '![](../Images/6bb6db30a3c9eda743f67859a8b19ec8.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bb6db30a3c9eda743f67859a8b19ec8.png)'
- en: The algorithms for regression and classification differ in the type of loss
    function used.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 回归和分类的算法在使用的损失函数类型上有所不同。
- en: '* * *'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT'
- en: '* * *'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Ensemble Learning with Examples](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[带示例的集成学习](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习技术：Python 中随机森林的详细介绍](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
- en: '[When Would Ensemble Techniques be a Good Choice?](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[何时选择集成技术？](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
- en: '[The Ultimate Guide to Mastering Seasonality and Boosting Business Results](https://www.kdnuggets.com/2023/08/media-mix-modeling-ultimate-guide-mastering-seasonality-boosting-business-results.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握季节性并提升业务结果的终极指南](https://www.kdnuggets.com/2023/08/media-mix-modeling-ultimate-guide-mastering-seasonality-boosting-business-results.html)'
- en: '[7 Ways to Improve Your Machine Learning Models](https://www.kdnuggets.com/7-ways-to-improve-your-machine-learning-models)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提升机器学习模型的7种方法](https://www.kdnuggets.com/7-ways-to-improve-your-machine-learning-models)'
- en: '[Why Humbling Yourself Will Improve Your Data Science Skills](https://www.kdnuggets.com/2022/01/humbling-improve-data-science-skills.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么谦逊自己能提高你的数据科学技能](https://www.kdnuggets.com/2022/01/humbling-improve-data-science-skills.html)'
