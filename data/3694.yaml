- en: 'Multi-label NLP: An Analysis of Class Imbalance and Loss Function Approaches'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多标签自然语言处理：类别不平衡和损失函数方法的分析
- en: 原文：[https://www.kdnuggets.com/2023/03/multilabel-nlp-analysis-class-imbalance-loss-function-approaches.html](https://www.kdnuggets.com/2023/03/multilabel-nlp-analysis-class-imbalance-loss-function-approaches.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/03/multilabel-nlp-analysis-class-imbalance-loss-function-approaches.html](https://www.kdnuggets.com/2023/03/multilabel-nlp-analysis-class-imbalance-loss-function-approaches.html)
- en: Multi-label NLP refers to the task of assigning multiple labels to a given text
    input, rather than just one label. In traditional NLP tasks, such as text classification
    or sentiment analysis, each input is typically assigned a single label based on
    its content. However, in many real-world scenarios, a piece of text can belong
    to multiple categories or express multiple sentiments simultaneously.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签自然语言处理（NLP）指的是将多个标签分配给给定的文本输入，而不仅仅是一个标签。在传统的NLP任务中，例如文本分类或情感分析，每个输入通常根据其内容分配一个标签。然而，在许多实际场景中，一段文本可能同时属于多个类别或表达多种情感。
- en: Multi-label NLP is important because it allows us to capture more nuanced and
    complex information from text data. For example, in the domain of customer feedback
    analysis, a customer review may express both positive and negative sentiments
    at the same time, or it may touch upon multiple aspects of a product or service.
    By assigning multiple labels to such inputs, we can gain a more comprehensive
    understanding of the customer's feedback and take more targeted actions to address
    their concerns.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签自然语言处理很重要，因为它允许我们从文本数据中捕捉更细致和复杂的信息。例如，在客户反馈分析领域，一条客户评论可能同时表达积极和消极的情感，或者可能涉及产品或服务的多个方面。通过为这些输入分配多个标签，我们可以更全面地理解客户的反馈，并采取更有针对性的措施来解决他们的关切。
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织IT'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This article delves into a noteworthy case of Provectus’ use of multi-label
    NLP.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文深入探讨了Provectus使用多标签NLP的一个值得注意的案例。
- en: '**Context:**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**背景：**'
- en: A client approached us with a request to help them [automate labeling documents
    of a certain type](https://provectus.com/case-studies/automating-document-processing-hcls-ai/).
    At first glance, the task appeared to be straightforward and easily solved. However,
    as we worked on the case, we encountered a dataset with inconsistent annotations.
    Though our customer had faced challenges with varying class numbers and changes
    in their review team over time, they had invested significant efforts into creating
    a diverse dataset with a range of annotations. While there existed some imbalances
    and uncertainties in the labels, this dataset provided a valuable opportunity
    for analysis and further exploration.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一位客户找到我们，请求帮助[自动标记某种类型的文档](https://provectus.com/case-studies/automating-document-processing-hcls-ai/)。乍一看，这个任务似乎简单易解。然而，当我们处理这个案例时，遇到了一个注释不一致的数据集。尽管我们的客户面临了类数变化和审查团队随时间变化的挑战，他们仍然投入了大量精力来创建一个具有多种注释的多样化数据集。虽然标签中存在一些不平衡和不确定性，但这个数据集为分析和进一步探索提供了宝贵的机会。
- en: Let’s take a closer look at the dataset, explore the metrics and our approach,
    and recap how Provectus solved the problem of multi-label text classification.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地查看数据集，探讨指标和我们的方法，并回顾Provectus如何解决多标签文本分类的问题。
- en: Overview of the Dataset
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集概述
- en: The dataset has 14,354 observations, with 124 unique classes (labels). Our task
    is to assign one or multiple classes to every observation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含14,354个观测值，124个独特的类别（标签）。我们的任务是为每个观测值分配一个或多个类别。
- en: Table 1 provides descriptive statistics for the dataset.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 表1提供了数据集的描述统计信息。
- en: On average, we have about two classes per observation, with an average of 261
    different texts describing a single class.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，我们每个观察值大约有两个类别，每个类别有大约 261 个不同的文本描述。
- en: '![Multi-label NLP: An Analysis of Class Imbalance and Loss Function Approaches](../Images/bb3651bc604356e68fe1c4edcc7a7745.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![多标签 NLP：类别不平衡和损失函数方法的分析](../Images/bb3651bc604356e68fe1c4edcc7a7745.png)'
- en: 'Table 1: Dataset Statistic'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：数据集统计
- en: In Figure 1, we see the distribution of classes in the top graph, and we have
    a certain number of HEAD labels with the highest frequency of occurrence in the
    dataset. Also note that the majority of classes have a low frequency of occurrence.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 1 中，我们看到顶部图中的类别分布，我们在数据集中有一定数量的 HEAD 标签，其出现频率最高。还需注意，大多数类别的出现频率较低。
- en: '![Multi-label NLP: An Analysis of Class Imbalance and Loss Function Approaches](../Images/60e2e51874303c662937949527a7bb6f.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![多标签 NLP：类别不平衡和损失函数方法的分析](../Images/60e2e51874303c662937949527a7bb6f.png)'
- en: In the bottom graph we see that there is frequent overlap between the classes
    that are best represented in the dataset, and the classes that have low significance.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在底部图中，我们看到在数据集中，最佳表示的类别与低显著性类别之间存在频繁的重叠。
- en: We changed the process of splitting the dataset into train/val/test sets. Instead
    of using a traditional method, we have employed iterative stratification, to provide
    a well-balanced distribution of evidence of label relations. For that, we used
    [Scikit Multi-learn](http://scikit.ml/api/skmultilearn.model_selection.iterative_stratification.html)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们改变了将数据集拆分为训练/验证/测试集的过程。我们没有使用传统方法，而是采用了迭代分层抽样，以提供标签关系的良好平衡分布。为此，我们使用了 [Scikit
    Multi-learn](http://scikit.ml/api/skmultilearn.model_selection.iterative_stratification.html)。
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We obtained the following distribution:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得了以下分布：
- en: The training dataset contains 60% of the data and covers all 124 labels
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练数据集包含 60% 的数据，并涵盖所有 124 个标签。
- en: The validation dataset contains 20% of the data and covers all 124 labels
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证数据集包含 20% 的数据，并涵盖所有 124 个标签。
- en: The test dataset contains 20% of the data and covers all 124 labels
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试数据集包含 20% 的数据，并涵盖所有 124 个标签。
- en: Metrics Applied
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用的指标
- en: Multi-label classification is a type of supervised machine learning algorithm
    that allows us to assign multiple labels to a single data sample. It differs from
    binary classification where the model predicts only two categories, and multi-class
    classification where the model predicts only one out of multiple classes for a
    sample.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签分类是一种监督机器学习算法，它允许我们为单个数据样本分配多个标签。它不同于二分类，后者模型仅预测两个类别，也不同于多类别分类，后者模型仅预测一个样本的多个类别之一。
- en: Evaluation metrics for multi-label classification performance are inherently
    different from those used in multi-class (or binary) classification due to the
    inherent differences of the classification problem. More detailed information
    can be found on Wikipedia.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签分类性能的评估指标与多类别（或二分类）分类中的指标本质上不同，因为分类问题的固有差异。更多详细信息可以在维基百科上找到。
- en: 'We selected metrics that are most suitable for us:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了最适合我们的指标：
- en: '**Precision** measures the proportion of true positive predictions among the
    total positive predictions made by the model.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**精确度** 衡量模型所做的所有正预测中真实正预测的比例。'
- en: '**Recall** measures the proportion of true positive predictions among all actual
    positive samples.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**召回率** 衡量所有实际正样本中真实正预测的比例。'
- en: '**F1-score** is the harmonic mean of precision and recall, which helps to restore
    balance between the two.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**F1 分数** 是精确度和召回率的调和平均数，有助于恢复两者之间的平衡。'
- en: '**Hamming loss** is the fraction of labels that are incorrectly predicted'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**汉明损失** 是预测错误的标签的比例。'
- en: We also track **the number of predicted labels** in the set { defined as count
    for labels, for which we achieve an F1 score > 0}.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还跟踪**预测标签的数量**，在集合 {定义为标签的计数，F1 分数 > 0 的标签}。
- en: K. I. S. S. Approach
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K. I. S. S. 方法
- en: Multi-Label Classification is a type of supervised learning problem where a
    single instance or example can be associated with multiple labels or classifications,
    as opposed to traditional single-label classification, where each instance is
    only associated with a single class label.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签分类是一种监督学习问题，其中单个实例或示例可以与多个标签或分类相关联，与传统的单标签分类不同，在单标签分类中，每个实例仅与单一类别标签相关联。
- en: 'To solve multi-label classification problems, there are two main categories
    of techniques:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 解决多标签分类问题的主要技术有两个类别：
- en: Problem transformation methods
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题转换方法
- en: Algorithm adaptation methods
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法适应方法
- en: Problem transformation methods enable us to transform multi-label classification
    tasks into multiple single-label classification tasks. For example, the Binary
    Relevance (BR) baseline approach treats every label as a separate binary classification
    problem. In this case, the multi-label problem is transformed into multiple single-label
    problems.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 问题转换方法使我们能够将多标签分类任务转化为多个单标签分类任务。例如，二进制相关（BR）基线方法将每个标签视为一个独立的二进制分类问题。在这种情况下，多标签问题被转化为多个单标签问题。
- en: Algorithm adaptation methods modify the algorithms themselves to handle multi-label
    data natively, without transforming the task into multiple single-label classification
    tasks. An example of this approach is [the BERT model](https://arxiv.org/abs/1810.04805),
    which is a pre-trained transformer-based language model that can be fine-tuned
    for various NLP tasks, including multi-label text classification. BERT is designed
    to handle multi-label data directly, without the need for problem transformation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 算法适应方法修改算法本身，以本地处理多标签数据，而不是将任务转化为多个单标签分类任务。这种方法的一个例子是[ BERT模型](https://arxiv.org/abs/1810.04805)，它是一个预训练的基于变换器的语言模型，可以针对各种NLP任务进行微调，包括多标签文本分类。BERT设计用来直接处理多标签数据，无需问题转换。
- en: In the context of using BERT for multi-label text classification, the standard
    approach is to use Binary Cross-Entropy (BCE) loss as the loss function. BCE loss
    is a commonly used loss function for binary classification problems and can be
    easily extended to handle multi-label classification problems by computing the
    loss for each label independently, and then summing the losses. In this case,
    the BCE loss function measures the error between predicted probabilities and true
    labels, where predicted probabilities are obtained from the final sigmoid activation
    layer in the BERT model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用BERT进行多标签文本分类的背景下，标准方法是使用二进制交叉熵（BCE）损失作为损失函数。BCE损失是二进制分类问题中常用的损失函数，可以通过独立计算每个标签的损失，然后将损失总和来轻松扩展到处理多标签分类问题。在这种情况下，BCE损失函数测量预测概率和真实标签之间的误差，其中预测概率来自BERT模型中的最终sigmoid激活层。
- en: Now, let's take a closer look at Figure 2 below.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更深入地查看下图中的图2。
- en: '![Multi-label NLP: An Analysis of Class Imbalance and Loss Function Approaches](../Images/4971951b46f1f03a6b6529986e8bff95.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![多标签NLP：类别不平衡和损失函数方法分析](../Images/4971951b46f1f03a6b6529986e8bff95.png)'
- en: Figure 2\. Metrics for baseline models
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 基线模型指标
- en: 'The graph on the left shows a comparison of metrics for a “baseline: BERT”
    and “baseline: ML”. Thus, it can be seen that for “baseline: BERT”, the F1 and
    Recall scores are approximately 1.5 times higher, while the Precision for “baseline:
    ML” is 2 times higher than that of model 1\. By analyzing the overall percentage
    of predicted classes shown on the right, we see that “baseline: BERT” predicted
    classes more than 10 times that of “baseline: ML”.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧图表展示了“基线：BERT”和“基线：ML”指标的对比。因此，可以看出，对于“基线：BERT”，F1和召回率分数约高出1.5倍，而“基线：ML”的精准率是模型1的2倍。通过分析右侧显示的预测类别的整体百分比，我们看到“基线：BERT”预测的类别是“基线：ML”的10倍以上。
- en: 'Because the maximum result for the “baseline: BERT” is less than 50% of all
    classes, the results are quite discouraging. Let’s figure out how to improve these
    results.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于“基线：BERT”的最大结果不到所有类别的50%，结果相当令人沮丧。让我们想办法改善这些结果。
- en: Golden Ratio of Approaches
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法的黄金比例
- en: Based on the outstanding article ["Balancing Methods for Multi-label Text Classification
    with Long-Tailed Class Distribution"](https://arxiv.org/abs/2109.04712), we learned
    that distribution-balanced loss may be the most suitable approach for us.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 根据杰出文章[“具有长尾类别分布的多标签文本分类平衡方法”](https://arxiv.org/abs/2109.04712)，我们了解到分布平衡损失可能是最适合我们的方案。
- en: Distribution-balanced loss
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布平衡损失
- en: Distribution-balanced loss is a technique used in multi-label text classification
    problems to address imbalances in class distribution. In these problems, some
    classes have a much higher frequency of occurrence compared to others, resulting
    in model bias toward these more frequent classes.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 分布平衡损失是一种用于多标签文本分类问题的技术，用于解决类别分布中的不平衡问题。在这些问题中，一些类别的出现频率远高于其他类别，导致模型偏向于这些更频繁的类别。
- en: To address this issue, distribution-balanced loss aims to balance the contribution
    of each sample in the loss function. This is achieved by re-weighting the loss
    of each sample based on the inverse of its frequency of occurrence in the dataset.
    By doing so, the contribution of less frequent classes is increased, and the contribution
    of more frequent classes is decreased, thus balancing the overall class distribution.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，分布平衡损失旨在平衡每个样本在损失函数中的贡献。这是通过根据样本在数据集中的出现频率的倒数重新加权每个样本的损失来实现的。通过这样做，较少出现类别的贡献增加，较多出现类别的贡献减少，从而平衡整体类别分布。
- en: This technique has been shown to be effective in improving the performance of
    models on long-tailed class distribution problems. By reducing the impact of frequent
    classes and increasing the impact of infrequent classes, the model is able to
    better capture patterns in the data and produce more balanced predictions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术已被证明在改善长尾类别分布问题的模型性能方面有效。通过减少频繁类别的影响和增加不频繁类别的影响，模型能够更好地捕捉数据中的模式，并产生更平衡的预测。
- en: '![Multi-label NLP: An Analysis of Class Imbalance and Loss Function Approaches](../Images/7b53f29d83fccde71f82595269c4f134.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![多标签 NLP：类别不平衡和损失函数方法的分析](../Images/7b53f29d83fccde71f82595269c4f134.png)'
- en: Implementation of Resample Class
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 重采样类的实现
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: DBLoss
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: DBLoss
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: By closely investigating the dataset, we have concluded that the parameter ![Equation](../Images/9d0e5bd3c7e47b028bb4c39c81a7c7ce.png)
    = 0.405.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过深入调查数据集，我们得出参数 ![Equation](../Images/9d0e5bd3c7e47b028bb4c39c81a7c7ce.png)
    = 0.405。
- en: Threshold tuning
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阈值调整
- en: Another step in improving our model was the process of tuning the threshold,
    both in the training stage, and in the validation and testing stages. We calculated
    the dependencies of metrics such as f1-score, precision, and recall on the threshold
    level, and we selected the threshold based on the highest metric score. Below
    you can see the function implementation of this process.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 改进模型的另一个步骤是调整阈值的过程，包括在训练阶段、验证阶段和测试阶段。我们计算了诸如 F1 分数、精确度和召回率等指标对阈值水平的依赖关系，并根据最高的指标分数选择了阈值。下面可以看到这一过程的函数实现。
- en: 'Optimization of the F1 score by tuning the threshold:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整阈值优化 F1 分数：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Evaluation and comparison with baseline
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准线的评估和比较
- en: 'These approaches allowed us to train a new model and obtain the following result,
    which is compared to the baseline: BERT in Figure 3 below.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法使我们能够训练一个新模型，并获得以下结果，与基准线比较：下图中的 BERT。
- en: '![Multi-label NLP: An Analysis of Class Imbalance and Loss Function Approaches](../Images/784e95194919a213efd587587a54fb47.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![多标签 NLP：类别不平衡和损失函数方法的分析](../Images/784e95194919a213efd587587a54fb47.png)'
- en: Figure 3\. Comparison metrics by baseline and newer approach.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 基准线和新方法的比较指标。
- en: 'By comparing the metrics that are relevant for classification, we see a significant
    increase in performance measures almost by 5-6 times:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较与分类相关的指标，我们发现性能指标几乎提高了5-6倍：
- en: The F1 score increased from 12% → 55%, while Precision increased from 9% → 59%
    and Recall increased from 15% → 51%.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: F1 分数从 12% 提升至 55%，而精确度从 9% 提升至 59%，召回率从 15% 提升至 51%。
- en: With the changes shown in the right graph in Figure 3, we can now predict 80%
    of the classes.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图 3 右侧图中的变化，我们现在可以预测 80% 的类别。
- en: Slices of classes
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类别切片
- en: 'We divided our labels into four groups: HEAD, MEDIUM, TAIL, and ZERO. Each
    group contains labels with a similar amount of supporting data observations.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将标签分为四组：HEAD、MEDIUM、TAIL 和 ZERO。每组包含具有相似数量的支持数据观测值的标签。
- en: As seen in Figure 4, the distributions of the groups are distinct. The rose
    box (HEAD) has a negatively skewed distribution, the middlebox (MEDIUM) has a
    positively skewed distribution, and the green box (TAIL) appears to have a normal
    distribution.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 4 所示，各组的分布是不同的。玫瑰盒（HEAD）具有负偏态分布，中间盒（MEDIUM）具有正偏态分布，绿色盒子（TAIL）似乎具有正态分布。
- en: All groups also have outliers, which are points outside the whiskers in the
    box plot. The HEAD group has a major impact on a MAJOR class.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 所有组中也有离群值，它们是箱形图中胡须外的点。HEAD 组对 MAJOR 类有重大影响。
- en: Additionally, we have identified a separate group named "ZERO" which contains
    labels that the model was unable to learn and cannot recognize due to the minimal
    number of occurrences in the dataset (less than 3% of all observations).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还识别出一个名为“ZERO”的单独组，其中包含模型无法学习且由于数据集中出现次数极少（少于 3% 的所有观察值）而无法识别的标签。
- en: '![Multi-label NLP: An Analysis of Class Imbalance and Loss Function Approaches](../Images/723740823a636593c7ebb8fe04cf09ac.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![多标签 NLP：类别不平衡和损失函数方法分析](../Images/723740823a636593c7ebb8fe04cf09ac.png)'
- en: Figure 4\. Label counts vs. groups
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 标签数量与组
- en: Table 2 provides information about metrics per each group of labels for the
    test subset of data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2 提供了每个标签组在测试子集数据中的指标信息。
- en: '![Multi-label NLP: An Analysis of Class Imbalance and Loss Function Approaches](../Images/8966526cd3060ab95b9b0f377314041e.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![多标签 NLP：类别不平衡和损失函数方法分析](../Images/8966526cd3060ab95b9b0f377314041e.png)'
- en: Table 2\. Metrics per group.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 各组指标。
- en: 'The HEAD group contains 21 labels with an average of 112 support observations
    per label. This group is impacted by outliers and, due to its high representation
    in the dataset, its metrics are high: F1 - 73%, Precision - 71%, Recall - 75%.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: HEAD 组包含 21 个标签，每个标签的平均支持观察值为 112。由于其在数据集中的高表示度，该组受到离群值的影响，其指标较高：F1 - 73%，精准度
    - 71%，召回率 - 75%。
- en: 'The MEDIUM group consists of 44 labels with an average support of 67 observations,
    which is approximately two times lower than the HEAD group. The metrics for this
    group are expected to decrease by 50%: F1 - 52%, Precision - 56%, Recall - 51%.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MEDIUM 组由 44 个标签组成，每个标签的平均支持观察值为 67，大约是 HEAD 组的一半。该组的指标预计会下降 50%：F1 - 52%，精准度
    - 56%，召回率 - 51%。
- en: 'The TAIL group has the largest number of classes, but all are poorly represented
    in the dataset, with an average of 40 support observations per label. As a result,
    the metrics drop significantly: F1 - 21%, Precision - 32%, Recall - 16%.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TAIL 组拥有最多的类别，但所有类别在数据集中表现都很差，每个标签的平均支持观察值为 40。因此，指标显著下降：F1 - 21%，精准度 - 32%，召回率
    - 16%。
- en: The ZERO group includes classes that the model cannot recognize at all, potentially
    due to their low occurrence in the dataset. Each of the 24 labels in this group
    has an average of 7 support observations.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ZERO 组包括模型无法识别的类别，这可能是由于这些类别在数据集中出现频率较低。该组中的 24 个标签每个平均有 7 个支持观察值。
- en: Figure 5 visualizes the information presented in Table 2, providing a visual
    representation of the metrics per group of labels.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5 可视化了表 2 中的信息，提供了每个标签组指标的视觉表现。
- en: '![Multi-label NLP: An Analysis of Class Imbalance and Loss Function Approaches](../Images/44dde3b61f89dc50bd17ecf406545bfd.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![多标签 NLP：类别不平衡和损失函数方法分析](../Images/44dde3b61f89dc50bd17ecf406545bfd.png)'
- en: Figure 5\. Metrics vs. label groups. All ZERO values = 0.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 指标与标签组。所有 ZERO 值 = 0。
- en: Conclusion
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this comprehensive article, we have demonstrated that a seemingly simple
    task of multi-label text classification can be challenging when traditional methods
    are applied. We have proposed the use of distribution-balancing loss functions
    to tackle the issue of class imbalance.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇综合文章中，我们展示了在应用传统方法时，表面上看似简单的多标签文本分类任务可能具有挑战性。我们提出了使用分布平衡损失函数来解决类别不平衡问题。
- en: We have compared the performance of our proposed approach to the classic method,
    and evaluated it using real-world business metrics. The results demonstrate that
    utilizing loss functions to address class imbalances and label co-occurrences
    offer a viable solution for multi-label text classification.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将提出的方法与经典方法进行了比较，并使用实际业务指标进行了评估。结果表明，利用损失函数来解决类别不平衡和标签共现问题为多标签文本分类提供了一种可行的解决方案。
- en: The proposed use case highlights the importance of considering different approaches
    and techniques when dealing with multi-label text classification, and the potential
    benefits of distribution-balancing loss functions in addressing class imbalances.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的使用案例突显了在处理多标签文本分类时考虑不同方法和技术的重要性，以及分布平衡损失函数在解决类别不平衡问题上的潜在好处。
- en: '*If you are facing a similar issue and seeking to* [*streamline document processing
    operations*](https://provectus.com/intelligent-document-processing/) *within your
    organization, please contact me or the Provectus team. We will be happy to assist
    you in finding more efficient methods for automating your processes.*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你面临类似问题并寻求* [*优化文档处理操作*](https://provectus.com/intelligent-document-processing/)
    *在你的组织中，请联系我或 Provectus 团队。我们将乐意帮助你寻找更高效的自动化方法。*'
- en: '[**Oleksii Babych**](https://www.linkedin.com/in/babycholeks/) is a Machine
    Learning Engineer at Provectus. With a background in physics, he possesses excellent
    analytical and math skills, and has gained valuable experience through scientific
    research and international conference presentations, including SPIE Photonics
    West. Oleksii specializes in creating end-to-end, large-scale AI/ML solutions
    for healthcare and fintech industries. He is involved in every stage of the ML
    development life cycle, from identifying business problems to deploying and running
    production ML models.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[**奥列克西·巴比奇**](https://www.linkedin.com/in/babycholeks/) 是 Provectus 的机器学习工程师。凭借物理学背景，他具备出色的分析和数学技能，并通过科学研究和国际会议演讲（包括
    SPIE Photonics West）获得了宝贵的经验。奥列克西专注于为医疗保健和金融科技行业创建端到端的大规模 AI/ML 解决方案。他参与机器学习开发生命周期的每个阶段，从识别业务问题到部署和运行生产
    ML 模型。'
- en: '[**Rinat Akhmetov**](https://www.linkedin.com/in/rinat-akhmetov/) is the ML
    Solution Architect at Provectus. With a solid practical background in Machine
    Learning (especially in Computer Vision), Rinat is a nerd, data enthusiast, software
    engineer, and workaholic whose second biggest passion is programming. At Provectus,
    Rinat is in charge of the discovery and proof of concept phases, and leads the
    execution of complex AI projects.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[**里纳特·阿赫梅托夫**](https://www.linkedin.com/in/rinat-akhmetov/) 是 Provectus 的机器学习解决方案架构师。凭借扎实的机器学习（尤其是计算机视觉）实践背景，里纳特是一个极客、数据爱好者、软件工程师和工作狂，他的第二大激情是编程。在
    Provectus，里纳特负责发现和概念验证阶段，并领导复杂 AI 项目的执行。'
- en: More On This Topic
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Multilabel Classification: An Introduction with Python''s Scikit-Learn](https://www.kdnuggets.com/2023/08/multilabel-classification-introduction-python-scikitlearn.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多标签分类：使用 Python 的 Scikit-Learn 的介绍](https://www.kdnuggets.com/2023/08/multilabel-classification-introduction-python-scikitlearn.html)'
- en: '[Loss Functions: An Explainer](https://www.kdnuggets.com/2022/03/loss-functions-explainer.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[损失函数：解释](https://www.kdnuggets.com/2022/03/loss-functions-explainer.html)'
- en: '[Machine Learning’s Sweet Spot: Pure Approaches in NLP and Document Analysis](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习的甜蜜点：NLP 和文档分析中的纯粹方法](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
- en: '[Unsupervised Disentangled Representation Learning in Class…](https://www.kdnuggets.com/2023/01/unsupervised-disentangled-representation-learning-class-imbalanced-dataset-elastic-infogan.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[无监督解缠表示学习在类不平衡数据集中的应用](https://www.kdnuggets.com/2023/01/unsupervised-disentangled-representation-learning-class-imbalanced-dataset-elastic-infogan.html)'
- en: '[Get World-class Data Science Learning with DataCamp at 25% off](https://www.kdnuggets.com/2023/03/datacamp-world-class-data-science-learning.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过 DataCamp 享受世界级数据科学学习，立享 25% 折扣](https://www.kdnuggets.com/2023/03/datacamp-world-class-data-science-learning.html)'
- en: '[5 Concepts You Should Know About Gradient Descent and Cost Function](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你需要了解的关于梯度下降和成本函数的 5 个概念](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html)'
