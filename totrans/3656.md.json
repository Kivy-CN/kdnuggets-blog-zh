["```py\n# LangChain & LLM\nfrom langchain.llms import OpenAI\nfrom langchain import PromptTemplate\nfrom langchain.chains.summarize import load_summarize_chain\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n#Wikipedia API\nimport wikipediaapi\n```", "```py\n# Create our prompt string.\ntemplate = \"\"\"\n%INSTRUCTIONS:\nPlease summarize the following text.\nAlways use easy-to-understand vocabulary so an elementary school student can understand.\n\n%TEXT:\n{input_text}\n\"\"\"\n\nNow we define the LLM we want to work with - OpenAIâ€™s GPT in my case -  and the prompt template. \n\n# The default model is already 'text-davinci-003', but it can be changed.\nllm = OpenAI(temperature=0, model_name='text-davinci-003', openai_api_key=openai_api_key)\n\n# Create a LangChain prompt template that we can insert values to later\nprompt = PromptTemplate(\n   input_variables=[\"input_text\"],\n   template=template,\n) \n```", "```py\nclass Chain(BaseModel, ABC):\n    \"\"\"Base interface that all chains should implement.\"\"\"\n    memory: BaseMemory\n    callbacks: Callbacks\n    def __call__(\n        self,\n        inputs: Any,\n        return_only_outputs: bool = False,\n        callbacks: Callbacks = None,\n    ) -> Dict[str, Any]:\n        ... \n```", "```py\n# To help construct our Chat Messages\nfrom langchain.schema import HumanMessage\nfrom langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n\n# We will be using a chat model, defaults to gpt-3.5-turbo\nfrom langchain.chat_models import ChatOpenAI\n\n# To parse outputs and get structured data back\nfrom langchain.output_parsers import StructuredOutputParser, ResponseSchema\n\nchat_model = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo', openai_api_key=openai_api_key)\n```", "```py\n# The schema I want out\nresponse_schemas = [\n   ResponseSchema(name=\"product\", description=\"The name of the product to be bought\"),\n   ResponseSchema(name=\"brand\", description=  \"The brand of the product.\")\n]\nAnd then I generate an output_parser object that takes as an input my response_schema. \n# The parser that will look for the LLM output in my schema and return it back to me\noutput_parser = StructuredOutputParser.from_response_schemas(response_schemas) \n```"]