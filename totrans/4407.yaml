- en: The Most Complete Guide to PyTorch for Data Scientists
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学家的 PyTorch 最完整指南
- en: 原文：[https://www.kdnuggets.com/2020/09/most-complete-guide-pytorch-data-scientists.html](https://www.kdnuggets.com/2020/09/most-complete-guide-pytorch-data-scientists.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/09/most-complete-guide-pytorch-data-scientists.html](https://www.kdnuggets.com/2020/09/most-complete-guide-pytorch-data-scientists.html)
- en: '[comments](#comments)![Header](../Images/43c438dc46265ffdd4bc06ff3d0f68a3.png)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)![Header](../Images/43c438dc46265ffdd4bc06ff3d0f68a3.png)'
- en: '***PyTorch*** has sort of became one of the de facto standards for creating
    Neural Networks now, and I love its interface. Yet, it is somehow a little difficult
    for beginners to get a hold of.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '***PyTorch*** 已经成为创建神经网络的事实标准之一，我非常喜欢它的接口。然而，对于初学者来说，掌握它还是有点困难的。'
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三名课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: I remember picking PyTorch up only after some extensive experimentation a couple
    of years back. To tell you the truth, it took me a lot of time to pick it up but
    am I glad that I moved from [Keras to PyTorch](https://towardsdatascience.com/moving-from-keras-to-pytorch-f0d4fff4ce79).* With
    its high customizability and pythonic syntax, *PyTorch is just a joy to work with,
    and I would recommend it to anyone who wants to do some heavy lifting with Deep
    Learning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我记得几年前在经过一番广泛的实验之后才开始接触 PyTorch。说实话，花了我很长时间才掌握它，但我很高兴从 [Keras 转到 PyTorch](https://towardsdatascience.com/moving-from-keras-to-pytorch-f0d4fff4ce79)。*凭借其高度的自定义性和
    Python 风格的语法，*PyTorch 使用起来非常愉快，我会推荐给任何想在深度学习中做些重型工作的朋友。
- en: So, in this PyTorch guide, ***I will try to ease some of the pain with PyTorch
    for starters*** and go through some of the most important classes and modules
    that you will require while creating any Neural Network with Pytorch.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这本 PyTorch 指南中，***我将尝试缓解初学者在使用 PyTorch 时的一些困难***，并逐步讲解在使用 PyTorch 创建任何神经网络时所需的一些最重要的类和模块。
- en: But, that is not to say that this is aimed at beginners only as ***I will also
    talk about the*** ***high customizability PyTorch provides and will talk about
    custom Layers, Datasets, Dataloaders, and Loss functions***.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，这并不意味着这仅仅针对初学者，因为***我还将讨论*** ***PyTorch 提供的高度自定义性，并将讨论自定义层、数据集、数据加载器和损失函数***。
- en: So let’s get some coffee ☕ ️and start it up.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们喝杯咖啡 ☕ ️ 开始吧。
- en: Tensors
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 张量
- en: '[Tensors](https://www.kdnuggets.com/2018/05/wtf-tensor.html) are the basic
    building blocks in PyTorch and put very simply, they are NumPy arrays but on GPU.
    In this part, I will list down some of the most used operations we can use while
    working with Tensors. This is by no means an exhaustive list of operations you
    can do with Tensors, but it is helpful to understand what tensors are before going
    towards the more exciting parts.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[张量](https://www.kdnuggets.com/2018/05/wtf-tensor.html) 是 PyTorch 中的基本构建块，简单来说，它们是
    GPU 上的 NumPy 数组。在这一部分，我将列出一些我们在处理张量时最常用的操作。这绝不是张量操作的详尽列表，但了解张量是什么是非常有帮助的，然后再进入更激动人心的部分。'
- en: 1\. Create a Tensor
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1. 创建张量
- en: We can create a PyTorch tensor in multiple ways. This includes converting to
    tensor from a NumPy array. Below is just a small gist with some examples to start
    with, but you can do a whole lot of [more things](https://pytorch.org/docs/stable/tensors.html) with
    tensors just like you can do with NumPy arrays.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过多种方式创建 PyTorch 张量。这包括从 NumPy 数组转换为张量。下面只是一些示例的小概要，你可以像在 NumPy 数组中做的那样，使用张量做很多其他事情，更多内容请参考
    [这里](https://pytorch.org/docs/stable/tensors.html)。
- en: '![Image for post](../Images/be8c3218e6ad5ab93ddbc41fa18b206d.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Image for post](../Images/be8c3218e6ad5ab93ddbc41fa18b206d.png)'
- en: 2\. Tensor Operations
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2. 张量操作
- en: Again, there are a lot of operations you can do on these tensors. The full list
    of functions can be found [here](https://pytorch.org/docs/stable/torch.html?highlight=mm#math-operations).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这些张量上可以执行很多操作。完整的函数列表可以在[这里](https://pytorch.org/docs/stable/torch.html?highlight=mm#math-operations)找到。
- en: '![Image for post](../Images/ba8d46e45d52acf8c3009404823db6fd.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/ba8d46e45d52acf8c3009404823db6fd.png)'
- en: '**Note: **What are PyTorch Variables? In the previous versions of Pytorch,
    Tensor and Variables used to be different and provided different functionality,
    but now the Variable API is [deprecated](https://pytorch.org/docs/stable/autograd.html#variable-deprecated),
    and all methods for variables work with Tensors. So, if you don’t know about them,
    it’s fine as they re not needed, and if you know them, you can forget about them.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** PyTorch 变量是什么？在之前的PyTorch版本中，Tensor和Variables是不同的，并提供不同的功能，但现在变量API已[弃用](https://pytorch.org/docs/stable/autograd.html#variable-deprecated)，所有变量的方法都与张量一起工作。因此，如果你不了解它们也没关系，因为它们不再需要；如果你了解它们，可以忘记它们。'
- en: The nn.Module
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: nn.Module
- en: '![Figure](../Images/cbff37d276c1915bc74c150f60bef3d3.png)Photo by [Fernand
    De Canne](https://unsplash.com/@fernanddecanne?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/cbff37d276c1915bc74c150f60bef3d3.png)照片由[Fernand De Canne](https://unsplash.com/@fernanddecanne?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)提供'
- en: Here comes the fun part as we are now going to talk about some of the most used
    constructs in Pytorch while creating deep learning projects. nn.Module lets you
    create your Deep Learning models as a class. You can inherit from `nn.Module`
    to define any model as a class. Every model class necessarily contains an `__init__` procedure
    block and a block for the `forward` pass.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里来到了有趣的部分，我们将讨论一些在创建深度学习项目时最常用的构造。nn.Module允许你将深度学习模型创建为类。你可以从`nn.Module`继承，以类的形式定义任何模型。每个模型类必然包含一个`__init__`过程块和一个`forward`传递块。
- en: In the `__init__` part, the user can define all the layers the network is going
    to have but doesn't yet define how those layers would be connected to each other.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`__init__`部分，用户可以定义网络将要拥有的所有层，但尚未定义这些层如何相互连接。
- en: In the `forward` pass block, the user defines how data flows from one layer
    to another inside the network.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`forward`传递块中，用户定义了数据如何在网络中的一个层流向另一个层。
- en: 'So, put simply, any network we define will look like:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，我们定义的任何网络都将如下所示：
- en: Here we have defined a very simple Network that takes an input of size 784 and
    passes it through two linear layers in a sequential manner. But the thing to note
    is that we can define any sort of calculation while defining the forward pass,
    and that makes PyTorch highly customizable for research purposes. For example,
    in our crazy experimentation mode, we might have used the below network where
    we arbitrarily attach our layers. Here we send back the output from the second
    linear layer back again to the first one after adding the input to it(skip connection)
    back again(I honestly don’t know what that will do).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个非常简单的网络，它接受大小为784的输入，并将其按顺序通过两个线性层。但需要注意的是，在定义前向传播时，我们可以定义任何类型的计算，这使得PyTorch在研究中具有极高的自定义性。例如，在我们的疯狂实验模式中，我们可能会使用以下网络，其中我们任意地附加层。在这里，我们将第二个线性层的输出再返回到第一个线性层，并在将输入加到其中后（跳跃连接）再次返回（老实说，我不知道这会有什么效果）。
- en: We can also check if the neural network forward pass works. I usually do that
    by first creating some random input and just passing that through the network
    I have created.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以检查神经网络前向传播是否有效。我通常通过首先创建一些随机输入，并将其传递到我创建的网络中来做到这一点。
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A word about Layers
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于层的说明
- en: Pytorch is pretty powerful, and you can actually create any new experimental
    layer by yourself using `nn.Module`. For example, rather than using the predefined
    Linear Layer `nn.Linear` from Pytorch above, we could have created our **custom
    linear layer**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch非常强大，你实际上可以使用`nn.Module`自己创建任何新的实验层。例如，与上面使用的预定义线性层`nn.Linear`相比，我们可以创建我们**自定义的线性层**。
- en: 'You can see how we wrap our weights tensor in `nn.Parameter.` This is done
    to make the tensor to be considered as a model parameter. From PyTorch [docs](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们如何将权重张量包装在`nn.Parameter.`中。这是为了使张量被视为模型参数。来自PyTorch的[文档](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter)：
- en: Parameters are `[*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)` subclasses,
    that have a very special property when used with `*Module*` - when they’re assigned
    as Module attributes they are automatically added to the list of its parameters,
    and will appear in `*parameters()*` iterator
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参数是 `[*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)`
    子类，这些子类在与 `*Module*` 一起使用时具有非常特殊的属性——当它们被分配为模块属性时，会自动添加到其参数列表中，并会出现在 `*parameters()*`
    迭代器中。
- en: As you will later see, the `model.parameters()` iterator will be an input to
    the optimizer. But more on that later.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你稍后会看到的，`model.parameters()` 迭代器将作为优化器的输入。不过，稍后会详细讲解。
- en: Right now, we can now use this custom layer in any PyTorch network, just like
    any other layer.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以像使用其他层一样在任何 PyTorch 网络中使用这个自定义层。
- en: 'But then again, Pytorch would not be so widely used if it didn’t provide a
    lot of ready to made layers used very frequently in wide varieties of Neural Network
    architectures. Some examples are: `[nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)`,
    `[nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)`,
    `[nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)`,
    `[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)`, `[nn.BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d)`,
    `[nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout)`,
    `[nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding)`, `[nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU)/[nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM)`,
    `[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax)`, `[nn.LogSoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax)`,
    `[nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention)`,
    `[nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder)`,
    `[nn.TransformerDecoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder)`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，如果 Pytorch 没有提供许多常用的现成层，它也不会被如此广泛使用。这里有一些例子：`[nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)`、`[nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)`、`[nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)`、`[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)`、`[nn.BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d)`、`[nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout)`、`[nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding)`、`[nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU)/[nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM)`、`[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax)`、`[nn.LogSoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax)`、`[nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention)`、`[nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder)`、`[nn.TransformerDecoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder)`
- en: I have linked all the layers to their source where you could read all about
    them, but to show how I usually try to understand a layer and read the docs, I
    would try to look at a very simple convolutional layer here.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经链接了所有层的来源，你可以阅读所有相关内容，但为了展示我通常如何尝试理解一层并阅读文档，我将尝试查看一个非常简单的卷积层。
- en: '![Image for post](../Images/c6dbe2d59cfe06a6d60c85ce6574c285.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![Image for post](../Images/c6dbe2d59cfe06a6d60c85ce6574c285.png)'
- en: So, a Conv2d Layer needs as input an Image of height H and width W, with `Cin` channels.
    Now, for the first layer in a convnet, the number of `in_channels` would be 3
    (RGB), and the number of `out_channels` can be defined by the user. The `kernel_size`
    mostly used is 3x3, and the `stride` normally used is 1.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个 Conv2d 层需要输入一个高度为 H、宽度为 W、具有 `Cin` 通道的图像。现在，对于卷积网络中的第一层，`in_channels`
    的数量将为 3（RGB），`out_channels` 的数量可以由用户定义。常用的 `kernel_size` 是 3x3，通常使用的 `stride`
    是 1。
- en: 'To check a new layer which I don’t know much about, I usually try to see the
    input as well as output for the layer like below where I would first initialize
    the layer:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查我不太了解的新层，我通常会尝试查看该层的输入和输出，如下所示，其中我会先初始化该层：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: And then pass some random input through it. Here 100 is the batch size.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过它传递一些随机输入。这里的 100 是批次大小。
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: So, we get the output from the convolution operation as required, and I have
    sufficient information on how to use this layer in any Neural Network I design.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们按要求从卷积操作中获得输出，并且我对如何在我设计的任何神经网络中使用这一层有足够的信息。
- en: Datasets and DataLoaders
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集和数据加载器
- en: 'How would we pass data to our Neural nets while training or while testing?
    We can definitely pass tensors as we have done above, but Pytorch also provides
    us with pre-built Datasets to make it easier for us to pass data to our neural
    nets. You can check out the complete list of datasets provided at [torchvision.datasets](https://pytorch.org/docs/stable/torchvision/datasets.html) and [torchtext.datasets](https://pytorch.org/text/datasets.html).
    But, to give a concrete example for datasets, let’s say we had to pass images
    to an Image Neural net using a folder which has images in this structure:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练或测试期间，我们如何将数据传递给神经网络？我们当然可以像上面那样传递张量，但 Pytorch 还提供了预构建的数据集，以便更方便地将数据传递给神经网络。你可以查看在[torchvision.datasets](https://pytorch.org/docs/stable/torchvision/datasets.html)和[torchtext.datasets](https://pytorch.org/text/datasets.html)提供的完整数据集列表。但是，为了给出一个具体的数据集示例，假设我们需要使用一个包含图像的文件夹将图像传递给图像神经网络，这些图像的结构如下：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can use `torchvision.datasets.ImageFolder` dataset to get an example image
    like below:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`torchvision.datasets.ImageFolder`数据集来获取一个示例图像，如下所示：
- en: '![Image for post](../Images/8f36e3e7f1066b9c820066cbac26ddbf.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![帖子图像](../Images/8f36e3e7f1066b9c820066cbac26ddbf.png)'
- en: 'This dataset has 847 images, and we can get an image and its label using an
    index. Now we can pass images one by one to any image neural network using a for
    loop:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集有 847 张图像，我们可以使用索引来获取图像及其标签。现在我们可以通过 for 循环将图像一个一个地传递给任何图像神经网络：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***But that is not optimal. We want to do batching. ***We can actually write
    some more code to append images and labels in a batch and then pass it to the
    Neural network. But Pytorch provides us with a utility iterator `torch.utils.data.DataLoader` to
    do precisely that. Now we can simply wrap our `train_dataset` in the Dataloader,
    and we will get batches instead of individual examples.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '***但这不是最优的。我们希望进行批量处理。*** 我们实际上可以编写一些代码来将图像和标签追加到一个批次中，然后将其传递给神经网络。但是 Pytorch
    提供了一个实用的迭代器`torch.utils.data.DataLoader`来精确完成这项工作。现在我们可以简单地将我们的`train_dataset`包装在
    Dataloader 中，这样我们将获得批次而不是单个样本。'
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can simply iterate with batches using:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地使用批次进行迭代，如下所示：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'So actually, the whole process of using datasets and Dataloaders becomes:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 所以实际上，使用数据集和数据加载器的整个过程变成了：
- en: You can look at this particular example in action in my previous blogpost on
    Image classification using Deep Learning [here](https://towardsdatascience.com/end-to-end-pipeline-for-setting-up-multiclass-image-classification-for-data-scientists-2e051081d41c).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我之前关于使用深度学习进行图像分类的博客文章中查看这个特定示例：[这里](https://towardsdatascience.com/end-to-end-pipeline-for-setting-up-multiclass-image-classification-for-data-scientists-2e051081d41c)。
- en: This is great, and Pytorch does provide a lot of functionality out of the box.
    But the main power of Pytorch comes with its immense customization. We can also
    create our own custom datasets if the datasets provided by PyTorch don’t fit our
    use case.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这很好，Pytorch 确实提供了大量开箱即用的功能。但 Pytorch 的主要优势在于其巨大的自定义能力。如果 Pytorch 提供的数据集不符合我们的使用情况，我们还可以创建自己的自定义数据集。
- en: Understanding Custom Datasets
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解自定义数据集
- en: To write our custom datasets, we can make use of the abstract class `torch.utils.data.Dataset` provided
    by Pytorch. We need to inherit this `Dataset` class and need to define two methods
    to create a custom Dataset.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要编写我们的自定义数据集，我们可以利用 Pytorch 提供的抽象类`torch.utils.data.Dataset`。我们需要继承这个`Dataset`类，并定义两个方法来创建自定义数据集。
- en: '`__len__` : a function that returns the size of the dataset. This one is pretty
    simple to write in most cases.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__len__`：一个函数，返回数据集的大小。这个在大多数情况下都比较简单。'
- en: '`__getitem__`: a function that takes as input an index `i` and returns the
    sample at index `i`.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__getitem__`：一个函数，输入一个索引`i`，并返回索引`i`处的样本。'
- en: For example, we can create a simple custom dataset that returns an image and
    a label from a folder. See that most of the tasks are happening in `__init__` part
    where we use `glob.glob` to get image names and do some general preprocessing.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以创建一个简单的自定义数据集，它从一个文件夹中返回图像和标签。请注意，大多数任务都发生在`__init__`部分，我们使用`glob.glob`来获取图像名称并进行一些常规预处理。
- en: Also, note that we open our images one at a time in the` __getitem__` method
    and not while initializing. This is not done in `__init__` because we don't want
    to load all our images in the memory and just need to load the required ones.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，我们在`__getitem__`方法中逐个打开图像，而不是在初始化时。这么做是因为我们不想将所有图像加载到内存中，只需加载所需的图像即可。
- en: We can now use this dataset with the utility `Dataloader` just like before.
    It works just like the previous dataset provided by PyTorch but without some utility
    functions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以像以前一样使用这个数据集和`Dataloader`工具。它的工作原理与PyTorch提供的以前的数据集相同，但没有一些实用函数。
- en: Understanding Custom DataLoaders
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解自定义DataLoaders
- en: '**This particular section is a little advanced and can be skipped going through
    this post as it will not be needed in a lot of situations.** But I am adding it
    for completeness here.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**这一部分有点复杂，可以在浏览此帖时跳过，因为在很多情况下并不需要。** 但我在这里添加它以便完整。'
- en: So let’s say you are looking to provide batches to a network that processes
    text input, and the network could take sequences with any sequence size as long
    as the size remains constant in the batch. For example, we can have a BiLSTM network
    that can process sequences of any length. It’s alright if you don’t understand
    the layers used in it right now; just know that it can process sequences with
    variable sizes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想为一个处理文本输入的网络提供批次，而该网络可以处理任意序列大小的输入，只要批次内的大小保持不变。例如，我们可以有一个BiLSTM网络，可以处理任何长度的序列。如果你现在还不理解其中使用的层没关系；只要知道它可以处理变长序列即可。
- en: This network expects its input to be of shape (`batch_size`, `seq_length`) and
    works with any `seq_length`. We can check this by passing our model two random
    batches with different sequence lengths(10 and 25).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络期望其输入形状为(`batch_size`, `seq_length`)，并且可以处理任何`seq_length`。我们可以通过将两个具有不同序列长度（10和25）的随机批次传递给模型来检查这一点。
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, we want to provide tight batches to this model, such that each batch has
    the same sequence length based on the max sequence length in the batch to minimize
    padding. This has an added benefit of making the neural net run faster. It was,
    in fact, one of the methods used in the winning submission of the Quora Insincere
    challenge in Kaggle, where running time was of utmost importance.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们希望将这个模型提供紧凑的批次，使得每个批次的序列长度根据批次中的最大序列长度来保持一致，以最小化填充。这还有一个附加的好处，就是使神经网络运行更快。事实上，这正是Kaggle
    Quora Insincere挑战赛获胜提交中使用的一种方法，因为运行时间至关重要。
- en: So, how do we do this? Let’s write a very simple custom dataset class first.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们该怎么做呢？首先，让我们写一个非常简单的自定义数据集类。
- en: Also, let’s generate some random data which we will use with this custom Dataset.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，让我们生成一些随机数据，用于这个自定义数据集。
- en: '![Figure](../Images/09c6e3ad648b1c216c3dffeec8a2eb70.png)Example of one random
    sequence and label. Each integer in the sequence corresponds to a word in the
    sentence.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![图](../Images/09c6e3ad648b1c216c3dffeec8a2eb70.png)随机序列和标签的示例。序列中的每个整数对应于句子中的一个单词。'
- en: 'We can use the custom dataset now using:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用自定义数据集：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If we now try to use the Dataloader on this dataset with `batch_size`>1, we
    will get an error. Why is that?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在尝试在这个数据集上使用`batch_size`>1的Dataloader，我们将遇到一个错误。这是为什么呢？
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Image for post](../Images/f16ed32a2e9e8eb50c11b791e59b59ff.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![用于帖子中的图像](../Images/f16ed32a2e9e8eb50c11b791e59b59ff.png)'
- en: This happens because the sequences have different lengths, and our data loader
    expects our sequences of the same length. Remember that in the previous image
    example, we resized all images to size 224 using the transforms, so we didn’t
    face this error.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为序列长度不同，而我们的数据加载器期望所有序列具有相同的长度。请记住，在之前的图像示例中，我们使用转换将所有图像调整为224的大小，因此我们没有遇到这个错误。
- en: '***So, how do we iterate through this dataset so that each batch has sequences
    with the same length, but different batches may have different sequence lengths?***'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '***那么，我们该如何迭代这个数据集，以便每个批次具有相同长度的序列，但不同批次可能有不同的序列长度呢？***'
- en: We can use `collate_fn` parameter in the DataLoader that lets us define how
    to stack sequences in a particular batch. To use this, we need to define a function
    that takes as input a batch and returns (`x_batch`, `y_batch` ) with padded sequence
    lengths based on `max_sequence_length` in the batch. The functions I have used
    in the below function are simple NumPy operations. Also, the function is properly
    commented so you can understand what is happening.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 DataLoader 中使用`collate_fn`参数，定义如何在特定批次中堆叠序列。要使用它，我们需要定义一个函数，该函数以批次为输入，并根据批次中的`max_sequence_length`返回（`x_batch`，`y_batch`）与填充后的序列长度。下面函数中使用的函数是简单的
    NumPy 操作。此外，函数中有适当的注释，以便你理解发生了什么。
- en: 'We can now use this `collate_fn` with our Dataloader as:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将这个`collate_fn`与我们的数据加载器一起使用：
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Figure](../Images/071eb46c3a18b3d3e7c289a3cb5d1471.png)See that the batches
    have different sequence lengths now'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图](../Images/071eb46c3a18b3d3e7c289a3cb5d1471.png)现在可以看到批次的序列长度不同了'
- en: It will work this time as we have provided a custom `collate_fn.` And see that
    the batches have different sequence lengths now. Thus we would be able to train
    our BiLSTM using variable input sizes just like we wanted.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们提供了自定义的`collate_fn`，这次它会起作用。现在我们看到批次的序列长度不同。因此，我们可以使用可变输入大小来训练我们的 BiLSTM，正如我们希望的那样。
- en: Training a Neural Network
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: 'We know how to create a neural network using `nn.Module.` But how to train
    it? Any neural network that has to be trained will have a training loop that will
    look something similar to below:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道如何使用`nn.Module`创建神经网络。那么如何训练它呢？任何需要训练的神经网络都会有一个训练循环，其形式如下：
- en: 'In the above code, we are running five epochs and in each epoch:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们运行了五个周期，在每个周期中：
- en: We iterate through the dataset using a data loader.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用数据加载器迭代数据集。
- en: In each iteration, we do a forward pass using `model(x_batch)`
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每次迭代中，我们使用`model(x_batch)`进行前向传播。
- en: We calculate the Loss using a `loss_criterion`
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`loss_criterion`计算损失。
- en: We back-propagate that loss using `loss.backward()` call. We don't have to worry
    about the calculation of the gradients at all, as this simple call does it all
    for us.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`loss.backward()`调用反向传播损失。我们不需要担心梯度的计算，因为这个简单的调用会为我们完成所有工作。
- en: Take an optimizer step to change the weights in the whole network using `optimizer.step()`.
    This is where weights of the network get modified using the gradients calculated
    in `loss.backward()` call.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 采取优化器步骤来通过`optimizer.step()`更改整个网络中的权重。这时，网络的权重会使用在`loss.backward()`调用中计算的梯度进行修改。
- en: We go through the validation data loader to check the validation score/metrics.
    Before doing validation, we set the model to eval mode using `model.eval().`Please
    note we don't back-propagate losses in eval mode.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过验证数据加载器检查验证得分/指标。在进行验证之前，我们使用`model.eval()`将模型设置为评估模式。请注意，在评估模式下我们不会进行损失的反向传播。
- en: Till now, we have talked about how to use `nn.Module` to create networks and
    how to use Custom Datasets and Dataloaders with Pytorch. So let's talk about the
    various options available for Loss Functions and Optimizers.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了如何使用`nn.Module`创建网络以及如何在 Pytorch 中使用自定义数据集和数据加载器。接下来我们来讨论损失函数和优化器的各种选项。
- en: Loss functions
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失函数
- en: Pytorch provides us with a variety of [loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) for
    our most common tasks, like Classification and Regression. Some most used examples
    are `[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`,
    `[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)`,
    `[nn.KLDivLoss](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss)`
    and `[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss).`
    You can read the documentation of each loss function, but to explain how to use
    these loss functions, I will go through the example of `[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)`
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Pytorch 为我们提供了各种[损失函数](https://pytorch.org/docs/stable/nn.html#loss-functions)，用于处理最常见的任务，如分类和回归。一些常用的例子有`[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`、`[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)`、`[nn.KLDivLoss](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss)`和`[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)`。你可以阅读每个损失函数的文档，但为了说明如何使用这些损失函数，我将通过`[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)`的例子来讲解。
- en: '![Image for post](../Images/cfb45e3ebca92aab225660e6ddccde92.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![帖子图片](../Images/cfb45e3ebca92aab225660e6ddccde92.png)'
- en: 'The documentation for NLLLoss is pretty succinct. As in, this loss function
    is used for Multiclass classification, and based on the documentation:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: NLLLoss 的文档相当简洁。即，这个损失函数用于多类别分类，根据文档：
- en: the input expected needs to be of size (`batch_size` x `Num_Classes` ) — These
    are the predictions from the Neural Network we have created.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预期的输入需要是 (`batch_size` x `Num_Classes`) 的大小 —— 这些是我们创建的神经网络的预测结果。
- en: We need to have the log-probabilities of each class in the input — To get log-probabilities
    from a Neural Network, we can add a `LogSoftmax` Layer as the last layer of our
    network.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要输入中每个类别的对数概率 —— 为了从神经网络中获取对数概率，我们可以在网络的最后一层添加一个 `LogSoftmax` 层。
- en: The target needs to be a tensor of classes with class numbers in the range(0,
    C-1) where C is the number of classes.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标需要是一个类的张量，类编号在范围(0, C-1) 内，其中 C 是类别的数量。
- en: So, we can try to use this Loss function for a simple classification network.
    Please note the `LogSoftmax` layer after the final linear layer. If you don't
    want to use this `LogSoftmax` layer, you could have just used `[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以尝试在一个简单的分类网络中使用这个损失函数。请注意在最终线性层后面的 `LogSoftmax` 层。如果不想使用 `LogSoftmax`
    层，你可以直接使用 `[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`
- en: 'Let’s define a random input to pass to our network to test it:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个随机输入来测试我们的网络：
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And pass it through the model to get predictions:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将其通过模型以获取预测结果：
- en: '[PRE12]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can now get the loss as:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以得到损失值为：
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Custom Loss Function
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义损失函数
- en: Defining your custom loss functions is again a piece of cake, and you should
    be okay as long as you use tensor operations in your loss function. For example,
    here is the `customMseLoss`
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自定义损失函数再次是件简单的事，只要你在损失函数中使用张量操作就可以了。例如，这里是 `customMseLoss`
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You can use this custom loss just like before. But note that we don’t instantiate
    the loss using criterion this time as we have defined it as a function.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像以前一样使用这个自定义损失。但请注意，这次我们不使用 criterion 实例化损失，因为我们将其定义为一个函数。
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If we wanted, we could have also written it as a class using `nn.Module` ,
    and then we would have been able to use it as an object. Here is an NLLLoss custom
    example:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，我们也可以将其编写为一个使用 `nn.Module` 的类，然后将其作为对象使用。这里是一个 NLLLoss 自定义示例：
- en: Optimizers
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化器
- en: 'Once we get gradients using the `loss.backward()` call, we need to take an
    optimizer step to change the weights in the whole network. Pytorch provides a
    variety of different ready to use optimizers using the `torch.optim` module. For
    example: `[torch.optim.Adadelta](https://pytorch.org/docs/stable/optim.html#torch.optim.Adadelta)`,
    `[torch.optim.Adagrad](https://pytorch.org/docs/stable/optim.html#torch.optim.Adagrad)`,
    `[torch.optim.RMSprop](https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop)`
    and the most widely used `[torch.optim.Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam).`
    To use the most used Adam optimizer from PyTorch, we can simply instantiate it
    with:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦通过 `loss.backward()` 调用获取梯度，我们需要进行一次优化器步骤以改变整个网络中的权重。Pytorch 提供了多种现成的优化器，使用
    `torch.optim` 模块。例如：`[torch.optim.Adadelta](https://pytorch.org/docs/stable/optim.html#torch.optim.Adadelta)`、`[torch.optim.Adagrad](https://pytorch.org/docs/stable/optim.html#torch.optim.Adagrad)`、`[torch.optim.RMSprop](https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop)`
    和最广泛使用的 `[torch.optim.Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)`。要使用
    Pytorch 最常用的 Adam 优化器，我们可以简单地实例化它：
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: And then use `optimizer**.**zero_grad()` and `optimizer.step()` while training
    the model.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在训练模型时使用 `optimizer**.**zero_grad()` 和 `optimizer.step()`。
- en: I am not discussing how to write custom optimizers as it is an infrequent use
    case, but if you want to have more optimizers, do check out the [pytorch-optimizer](https://pytorch-optimizer.readthedocs.io/en/latest/) library,
    which provides a lot of other optimizers used in research papers. Also, if you
    anyhow want to create your own optimizers, you can take inspiration using the
    source code of implemented optimizers in [PyTorch](https://github.com/pytorch/pytorch/tree/master/torch/optim) or [pytorch-optimizers](https://github.com/jettify/pytorch-optimizer/tree/master/torch_optimizer).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有讨论如何编写自定义优化器，因为这是一个不常见的用例，但如果你想要更多优化器，可以查看[pytorch-optimizer](https://pytorch-optimizer.readthedocs.io/en/latest/)库，它提供了许多在研究论文中使用的其他优化器。此外，如果你确实想创建自己的优化器，可以参考[PyTorch](https://github.com/pytorch/pytorch/tree/master/torch/optim)或[pytorch-optimizers](https://github.com/jettify/pytorch-optimizer/tree/master/torch_optimizer)中实现的优化器的源代码。
- en: '![Figure](../Images/8fa38ecb11772198ed87d804bc7c312e.png)Other optimizers from [pytorch-optimizer](https://github.com/jettify/pytorch-optimizer) library'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![图像](../Images/8fa38ecb11772198ed87d804bc7c312e.png)来自[pytorch-optimizer](https://github.com/jettify/pytorch-optimizer)库的其他优化器'
- en: Using GPU/Multiple GPUs
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用GPU/多个GPU
- en: Till now, whatever we have done is on the CPU. If you want to use a GPU, you
    can put your model to GPU using `model.to('cuda')`. Or if you want to use multiple
    GPUs, you can use `nn.DataParallel`. Here is a utility function that checks the
    number of GPUs in the machine and sets up parallel training automatically using `DataParallel` if
    needed.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所做的都是在CPU上。如果你想使用GPU，可以使用`model.to('cuda')`将模型放到GPU上。或者，如果你想使用多个GPU，可以使用`nn.DataParallel`。以下是一个检查机器中GPU数量并在需要时自动使用`DataParallel`进行并行训练的实用函数。
- en: The only thing that we will need to change is that we will load our data to
    GPU while training if we have GPUs. It’s as simple as adding a few lines of code
    to our training loop.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们唯一需要改变的就是在训练时将数据加载到GPU中，如果我们有GPU的话。这就像在训练循环中添加几行代码一样简单。
- en: Conclusion
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: Pytorch provides a lot of customizability with minimal code. While at first,
    it might be hard to understand how the whole ecosystem is structured with classes,
    in the end, it is simple Python. In this post, I have tried to break down most
    of the parts you might need while using Pytorch, and I hope it makes a little
    more sense for you after reading this.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了极大的可定制性，且代码量很少。虽然一开始可能很难理解整个生态系统如何通过类来构建，但最终它就是简单的Python。在这篇文章中，我尝试拆解了你使用PyTorch时可能需要的大部分部分，希望阅读后能让你多一些理解。
- en: You can find the code for this post here on my [GitHub](https://github.com/MLWhiz/data_science_blogs/tree/master/pytorch_guide) repo,
    where I keep codes for all my blogs.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我的[GitHub](https://github.com/MLWhiz/data_science_blogs/tree/master/pytorch_guide)仓库中找到这篇文章的代码，我在这里保存了我所有博客的代码。
- en: If you want to learn more about Pytorch using a course based structure, take
    a look at the [Deep Neural Networks with PyTorch](https://www.coursera.org/learn/deep-neural-networks-with-pytorch?ranMID=40328&ranEAID=lVarvwc5BD0&ranSiteID=lVarvwc5BD0-Mh_whR0Q06RCh47zsaMVBQ&siteID=lVarvwc5BD0-Mh_whR0Q06RCh47zsaMVBQ&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0) course
    by IBM on Coursera. Also, if you want to know more about Deep Learning, I would
    like to recommend this excellent course on [Deep Learning in Computer Vision](https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0) in
    the [Advanced machine learning specialization](https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想通过课程结构深入学习Pytorch，可以查看IBM在Coursera上提供的[使用PyTorch的深度神经网络](https://www.coursera.org/learn/deep-neural-networks-with-pytorch?ranMID=40328&ranEAID=lVarvwc5BD0&ranSiteID=lVarvwc5BD0-Mh_whR0Q06RCh47zsaMVBQ&siteID=lVarvwc5BD0-Mh_whR0Q06RCh47zsaMVBQ&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0)课程。此外，如果你想了解更多关于深度学习的内容，我推荐这个在[计算机视觉中的深度学习](https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0)方面的优秀课程，它是[高级机器学习专项课程](https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0)的一部分。
- en: Thanks for the read. I am going to be writing more beginner-friendly posts in
    the future too. Follow me up at [**Medium**](https://medium.com/@rahul_agarwal?source=post_page---------------------------) or
    Subscribe to my [**blog**](http://eepurl.com/dbQnuX?source=post_page---------------------------) to
    be informed about them. As always, I welcome feedback and constructive criticism
    and can be reached on Twitter [**@mlwhiz**](https://twitter.com/MLWhiz?source=post_page---------------------------).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读。我将来还会写更多适合初学者的文章。请关注我的 [**Medium**](https://medium.com/@rahul_agarwal?source=post_page---------------------------)
    或订阅我的 [**博客**](http://eepurl.com/dbQnuX?source=post_page---------------------------)
    以获取更新。像往常一样，我欢迎反馈和建设性的批评，可以通过 Twitter [**@mlwhiz**](https://twitter.com/MLWhiz?source=post_page---------------------------)
    联系我。
- en: Also, a small disclaimer — There might be some affiliate links in this post
    to relevant resources, as sharing knowledge is never a bad idea.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，小小的免责声明——本文可能包含一些与相关资源相关的附属链接，因为分享知识从来不是坏事。
- en: '**Bio: [Rahul Agarwal](https://www.linkedin.com/in/rahulagwl/)** is Senior
    Statistical Analyst at WalmartLabs. Follow him on Twitter [@mlwhiz](https://twitter.com/MLWhiz).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Rahul Agarwal](https://www.linkedin.com/in/rahulagwl/)** 是 WalmartLabs
    的高级统计分析师。关注他的 Twitter [@mlwhiz](https://twitter.com/MLWhiz)。'
- en: '[Original](https://towardsdatascience.com/minimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b).
    Reposted with permission.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/minimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b)。经许可转载。'
- en: '**Related:**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[6 bits of advice for Data Scientists](/2019/09/advice-data-scientists.html)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学家的 6 条建议](/2019/09/advice-data-scientists.html)'
- en: '[The Hitchhiker’s Guide to Feature Extraction](/2019/06/hitchhikers-guide-feature-extraction.html)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[特征提取指南](/2019/06/hitchhikers-guide-feature-extraction.html)'
- en: '[The 5 Classification Evaluation Metrics Every Data Scientist Must Know](/2019/10/5-classification-evaluation-metrics-every-data-scientist-must-know.html)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家必须知道的 5 种分类评估指标](/2019/10/5-classification-evaluation-metrics-every-data-scientist-must-know.html)'
- en: More On This Topic
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[The Complete Free PyTorch Course for Deep Learning](https://www.kdnuggets.com/2022/10/complete-free-pytorch-course-deep-learning.html)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[完全免费的 PyTorch 深度学习课程](https://www.kdnuggets.com/2022/10/complete-free-pytorch-course-deep-learning.html)'
- en: '[How to Build a Data Science Enablement Team: A Complete Guide](https://www.kdnuggets.com/2022/10/build-data-science-enablement-team-complete-guide.html)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何建立数据科学赋能团队：完整指南](https://www.kdnuggets.com/2022/10/build-data-science-enablement-team-complete-guide.html)'
- en: '[Geocoding in Python: A Complete Guide](https://www.kdnuggets.com/2022/11/geocoding-python-complete-guide.html)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 中的地理编码：完整指南](https://www.kdnuggets.com/2022/11/geocoding-python-complete-guide.html)'
- en: '[A Complete Guide To Decision Tree Software](https://www.kdnuggets.com/2022/08/complete-guide-decision-tree-software.html)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[决策树软件完整指南](https://www.kdnuggets.com/2022/08/complete-guide-decision-tree-software.html)'
- en: '[A Practical Guide to Transfer Learning using PyTorch](https://www.kdnuggets.com/2023/06/practical-guide-transfer-learning-pytorch.html)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 PyTorch 的迁移学习实用指南](https://www.kdnuggets.com/2023/06/practical-guide-transfer-learning-pytorch.html)'
- en: '[A Beginner''s Guide to PyTorch](https://www.kdnuggets.com/a-beginners-guide-to-pytorch)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 初学者指南](https://www.kdnuggets.com/a-beginners-guide-to-pytorch)'
