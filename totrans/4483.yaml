- en: Optimize Response Time of your Machine Learning API In Production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化生产环境中机器学习 API 的响应时间
- en: 原文：[https://www.kdnuggets.com/2020/05/optimize-response-time-machine-learning-api-production.html](https://www.kdnuggets.com/2020/05/optimize-response-time-machine-learning-api-production.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/05/optimize-response-time-machine-learning-api-production.html](https://www.kdnuggets.com/2020/05/optimize-response-time-machine-learning-api-production.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Yannick Wolf](https://www.linkedin.com/in/yannick-wolff-457616124/),
    Lead Data Scientist @ Sicara**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Yannick Wolf](https://www.linkedin.com/in/yannick-wolff-457616124/)，Sicara
    领先数据科学家**'
- en: This article demonstrates how building a smarter API serving Deep Learning models
    minimizes the response time.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了如何通过构建更智能的 API 服务深度学习模型来最小化响应时间。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: <picture>![](../Images/e8acc09138d252f613079422b466a468.png)</picture>
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![](../Images/e8acc09138d252f613079422b466a468.png)</picture>
- en: 'Your team worked hard to build a Deep Learning model for a given task (let''s
    say: detecting bought products in a store thanks to Computer Vision). Good.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你的团队努力为特定任务（比如：通过计算机视觉检测商店中的购买产品）构建了一个深度学习模型。很好。
- en: 'You then developed and deployed an API that integrates this model (let''s keep
    our example: self-checkout machines would call this API). Great!'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你开发并部署了一个集成此模型的 API（让我们继续这个例子：自助结账机会调用这个 API）。太棒了！
- en: The new product is working well and you feel like all the work is done.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 新产品运行良好，你感觉所有工作都已完成。
- en: But since the manager decided to install more self-checkout machines (I really
    like this example), users have started to complain about the huge latency that
    occurs each time they are scanning a product.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但由于经理决定安装更多自助结账机（我真的很喜欢这个例子），用户开始抱怨每次扫描商品时都会出现巨大的延迟。
- en: What can you do? Buy 10x faster—and 10x more expensive—GPUs? Ask data scientists
    to try reducing the depth of the model without degrading its accuracy?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以做什么？购买速度快十倍且昂贵十倍的 GPU？让数据科学家尝试在不降低准确性的情况下减少模型的深度？
- en: Cheaper and easier solutions exist, as you will see in this article.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 更便宜、更简单的解决方案存在，如本文所示。
- en: A basic API with a big dummy model
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个基础的 API 和一个大型虚拟模型
- en: 'First of all, we''ll need a model with a long inference time to work with.
    Here is how I would do that with [TensorFlow 2](https://www.tensorflow.org/)''s
    Keras API (if you''re not familiar with this Deep Learning framework, just step
    over this piece of code):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一个具有长推理时间的模型。以下是我如何使用 [TensorFlow 2](https://www.tensorflow.org/)'s Keras
    API 来实现这一点（如果你不熟悉这个深度学习框架，可以跳过这段代码）：
- en: When testing the model on my [GeForce RTX 2080](https://www.nvidia.com/fr-fr/geforce/graphics-cards/rtx-2080/)
    GPU, I measured an inference time of 303 ms. That's what we can call a big model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的 [GeForce RTX 2080](https://www.nvidia.com/fr-fr/geforce/graphics-cards/rtx-2080/)
    GPU 上测试模型时，我测得推理时间为 303 毫秒。这就是我们可以称之为大模型的情况。
- en: Now, we need a very simple API to serve our model, with only one route to ask
    for a prediction. A very standard API framework in Python is [Flask](https://www.palletsprojects.com/p/flask/).
    That's the one I chose, along with a [WSGI HTTP Server](https://www.fullstackpython.com/wsgi-servers.html)
    called [Gunicorn](https://gunicorn.org/).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要一个非常简单的 API 来服务我们的模型，仅一个路由来请求预测。Python 中一个非常标准的 API 框架是 [Flask](https://www.palletsprojects.com/p/flask/)。这是我选择的框架，还有一个叫做
    [Gunicorn](https://gunicorn.org/) 的 [WSGI HTTP 服务器](https://www.fullstackpython.com/wsgi-servers.html)。
- en: Our unique route parses the input from the request, calls the instantiated model
    on it and sends the output back to the user.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们独特的路径解析请求中的输入，调用实例化的模型，并将输出发送回用户。
- en: 'We can run our deep learning API with the command:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令运行我们的深度学习 API：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Okay, I can now send some random numbers to my API and it responds to me with
    some other random numbers. The question is: how fast?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我可以向我的 API 发送一些随机数字，它会用其他随机数字回应我。问题是：有多快？
- en: Let's load test our API
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 让我们对我们的 API 进行负载测试
- en: We indeed want to know how fast our API responds, and especially when increasing
    the number of requests per second. Such a load test can be performed using [Locust](https://locust.io/)
    Python library.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确实想了解我们的 API 响应有多快，特别是当每秒请求数量增加时。这样的负载测试可以使用 [Locust](https://locust.io/)
    Python 库进行。
- en: 'This library works with a locustfile.py which indicates which behavior to simulate:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库与一个 locustfile.py 一起工作，指示要模拟的行为：
- en: Locust simulates several users, executing different tasks at the same time.
    In our case, we want to define only one task which is to call the /predict route
    with a random input.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Locust 模拟多个用户同时执行不同的任务。在我们的案例中，我们只想定义一个任务，即使用随机输入调用 /predict 路由。
- en: We can also parametrize the wait_time, which is the time a simulated user waits
    after having received a response from the API, before sending his next request.
    To simulate the self-checkout use case, I've set this time to be a random value
    between 1 and 3 seconds.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以参数化`wait_time`，这是一个模拟用户在接收到 API 响应后等待的时间，然后再发送下一个请求。为了模拟自助结账的用例，我将这个时间设置为
    1 到 3 秒之间的随机值。
- en: 'The number of users is chosen through Locust''s dashboard, where all the statistics
    are shown in real-time. This dashboard can be run by calling the command:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 用户数量通过 Locust 的仪表盘选择，所有统计数据都会实时显示。可以通过调用以下命令来运行此仪表盘：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Locust dashboard
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Locust 仪表盘
- en: 'So, how does our naive API react when we increase the number of users? As we
    could expect, quite badly:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，当我们增加用户数量时，我们天真的 API 会有什么反应呢？正如我们所预期的，反应非常糟糕：
- en: with 1 user, the average response time I measured was 332 ms (slightly more
    than the isolated inference time previously measured, nothing surprising)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 1 个用户时，我测量的平均响应时间是 332 ms（略高于之前测量的孤立推理时间，没什么惊讶的）
- en: 'with 5 users, this average time increased a little bit: 431 ms'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 5 个用户时，这个平均时间稍微增加了一点：431 ms
- en: with 20 users, it reached 4.1 s (more than 12 times more than with 1 user)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 20 个用户时，它达到了 4.1 秒（比 1 个用户时多了 12 倍以上）
- en: <picture>![gunicorn-flask-tensorflow-api-response-time](../Images/e9ae12e8925c2b179622755f9d2e5010.png)</picture>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![gunicorn-flask-tensorflow-api-response-time](../Images/e9ae12e8925c2b179622755f9d2e5010.png)</picture>
- en: 'These results are not so surprising when we think of how requests are handled
    by our API. By default Gunicorn launches 2 processes: a master process listen
    to incoming requests, stores them into a [FIFO](https://en.wikipedia.org/wiki/FIFO_and_LIFO_accounting#FIFO)
    queue, and sends them successively to a worker process, each time it''s available.
    The latter manages to run your code to compute the response for each request.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑到 API 是如何处理请求的，这些结果并不那么令人惊讶。默认情况下，Gunicorn 启动 2 个进程：一个主进程监听传入的请求，将它们存储在一个
    [FIFO](https://en.wikipedia.org/wiki/FIFO_and_LIFO_accounting#FIFO) 队列中，并将它们依次发送到一个工作进程，每次工作进程可用时。后者会运行您的代码来计算每个请求的响应。
- en: Since the worker process is single-threaded, requests are handled one by one.
    If 5 requests arrive simultaneously, the 5th request will receive its response
    after 5 x 300 ms = 1500 ms, which explains the high average response time with
    20 users.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于工作进程是单线程的，请求是逐一处理的。如果 5 个请求同时到达，第 5 个请求将在 5 x 300 ms = 1500 ms 后收到响应，这解释了
    20 个用户时高的平均响应时间。
- en: The more the faster
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 越多越快
- en: 'Luckily, Gunicorn provides two ways to scale APIs by increasing:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Gunicorn 提供了两种通过增加以下内容来扩展 API 的方法：
- en: the number of threads handling requests
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理请求的线程数量
- en: the number of worker processes
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作进程的数量
- en: The multi-threading option is not the one that may help us the most since TensorFlow
    would not allow a thread to access the session graph initialized in another thread.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 多线程选项可能不是对我们帮助最大的一种，因为 TensorFlow 不允许一个线程访问在另一个线程中初始化的会话图。
- en: On the other hand, setting a number of workers greater than one would create
    several processes, with the whole Flask app initialized for each worker. Each
    one instantiates its own TensorFlow session with its own model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，设置大于 1 的工作进程数量将创建多个进程，每个工作进程都初始化整个 Flask 应用程序。每个进程都会实例化自己的 TensorFlow 会话和模型。
- en: <picture>![multi-workers-gunicorn-flask-tensorflow-api-schema](../Images/1c4723ca5844a91abd1575e67f684d17.png)</picture>
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![multi-workers-gunicorn-flask-tensorflow-api-schema](../Images/1c4723ca5844a91abd1575e67f684d17.png)</picture>
- en: 'Let''s try this approach by running the following command:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过运行以下命令来尝试这种方法：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And here are the new performances we get with a multi-workers API:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在多工人API中获得的新性能：
- en: <picture>![multi-workers-gunicorn-flask-tensorflow-api-response-time](../Images/69dfec94e4ccee61dcc3858dfb78e933.png)</picture>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![多工人-gunicorn-flask-tensorflow-api响应时间](../Images/69dfec94e4ccee61dcc3858dfb78e933.png)</picture>
- en: 'You can see that the effect on response time when using 5 workers instead of
    1 is drastic: the 4.1 s average for 20 users has almost been divided by 2\. By
    2 and not by 5 as you might expect, because of the wait time between requests
    — it would have been divided by 5 if the simulated users were immediately requesting
    the API again after having received a response.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，使用5个工人而不是1个工人对响应时间的影响是巨大的：20个用户的4.1秒平均时间几乎减少了一半。减少了两倍，而不是预期的五倍，因为请求之间的等待时间——如果模拟用户在收到响应后立即重新请求API，响应时间将减少五倍。
- en: 'You may wonder why I stopped to 5 workers: why not to set-up 20 workers, to
    be able to handle all the 20 users requesting the API at the exact same time?
    The reason is that each worker is an independent process that instantiates its
    own version of the model in the GPU memory.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么我止步于5个工人：为什么不设置20个工人，以便能够处理20个用户在同一时间请求API的情况？原因是每个工人是一个独立的进程，在GPU内存中实例化其自己的模型版本。
- en: 20 workers would thus consume 20 times the size of the model just for initializing
    the weights, and more memory is then needed for inference computing. With a 2GiB
    model and an 8GiB GPU, we are a bit limited.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，20名工人仅仅在初始化权重时就会消耗20倍模型的大小，推理计算则需要更多的内存。对于一个2GiB的模型和一个8GiB的GPU来说，我们的资源有些有限。
- en: '![Figure](../Images/781550b5a91b86e5e76a34b60c897835.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/781550b5a91b86e5e76a34b60c897835.png)'
- en: Each of the 2 workers consumes 2349MiB of GPU memory
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每个工人消耗2349MiB的GPU内存
- en: It's possible to maximize the number of running workers, by fine-tuning the
    memory allocated to each of them with a TensorFlow parameter (per_process_gpu_memory_fraction
    in TensorFlow 1.x, and memory_limit in TensorFlow 2.x — I've only tested the first
    one), setting it to the minimal value which does not make predictions fail with
    an out of memory error.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整分配给每个工人的内存，可以最大化运行工人的数量，使用TensorFlow参数（TensorFlow 1.x中的per_process_gpu_memory_fraction和TensorFlow
    2.x中的memory_limit——我只测试过第一个），将其设置为不会因内存不足错误而导致预测失败的最小值。
- en: As warned by the library, reducing the available memory could theoretically
    increase inference time by disabling optimization. However, I did not personally
    notice any significant change.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如库所警告的那样，减少可用内存可能会通过禁用优化理论上增加推理时间。然而，我个人并没有注意到任何显著变化。
- en: Anyway, this will not allow us to run 20 workers, even using our 2 GPUs. So,
    let us buy 2 more? Don't panic, you have more than one string to your bow.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，这样做并不能让我们运行20个工人，即使使用我们的2个GPU。那么，买2个更多的GPU？别慌，你还有其他的解决方案。
- en: Haste makes waste
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欲速则不达
- en: 'Let us come back to the mono-worker case for now. If you think about how things
    happen when two or more requests arrive almost at the same time to this API, there
    is something unoptimized:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回到单工人的情况。如果你考虑到当两个或更多请求几乎同时到达此API时发生的情况，会发现有一些优化不到位的地方：
- en: '![Figure](../Images/06d468e84c3a8937dd518aa81510fca8.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/06d468e84c3a8937dd518aa81510fca8.png)'
- en: 3 requests arriving with 100 ms between each of them, the average response time
    is 1433 ms
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 3个请求间隔100毫秒到达，平均响应时间为1433毫秒
- en: The API passes the 3 inputs through the model one by one. However, most Deep
    Learning models, thanks to the nature of the mathematical operations they are
    made of, can process several inputs at the same time without increasing inference
    time — that's what we call batch processing, and that's what we usually do during
    the training phase.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: API将3个输入逐个通过模型。然而，由于深度学习模型的数学运算特性，大多数模型可以同时处理多个输入而不会增加推理时间——这就是我们所说的批处理，也就是我们在训练阶段通常做的。
- en: So, why not processing these 3 inputs in a batch? This would imply that after
    receiving the first request, the API waits a bit before running the model, in
    order to receive the 2 next requests before processing them all in one. This would
    increase the response time for the first request but decrease it on average.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么不将这3个输入批量处理呢？这将意味着在接收到第一个请求后，API会稍等片刻，以接收接下来的2个请求，然后一次性处理所有请求。这会增加第一个请求的响应时间，但在平均上会减少。
- en: '![Figure](../Images/c266286ba19aad8c0e6421495b0655e0.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/c266286ba19aad8c0e6421495b0655e0.png)'
- en: If the API waits 300 ms before processing requests by batch, we get an average
    response time of 600 ms
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 API 在处理批量请求之前等待 300 毫秒，我们得到的平均响应时间是 600 毫秒
- en: 'In practice, we do not know when future requests will arrive, and it''s a bit
    complex to decide how long to wait for potential next requests. A quite logical
    rule to trigger processing of queued requests is to do it:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们不知道未来请求何时到达，而且决定等待潜在下一个请求的时间有点复杂。一条相当合理的规则是触发处理排队请求的时间是：
- en: if the number of queued requests has reached a maximum value, corresponding
    to the GPU memory limit or the maximum number of users (if known)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果排队请求的数量达到最大值，通常对应于 GPU 内存限制或最大用户数（如果已知）
- en: OR
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: if the oldest request in the queue is older than a timeout value. This value
    has to be fine-tuned empirically to minimize the average response time.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果队列中最旧的请求比超时时间值还要久。这一值需要通过经验调优以最小化平均响应时间。
- en: Here is a way to implement such behavior, still with a Gunicorn - Flask - TensorFlow
    stack, using a Python queue and a thread dedicated to handling requests by batch.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一种实现这种行为的方法，仍然使用 Gunicorn - Flask - TensorFlow 堆栈，使用 Python 队列和专门处理批量请求的线程。
- en: 'This new API has to be run with as many threads as the maximum number of requests
    we want to be able to process in batch:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新 API 必须以与我们希望能够处理的批处理请求最大数量相同的线程数运行：
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here are the very satisfying results I''ve got after a very quick fine-tuning
    of the timeout value:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我在对超时时间值进行快速微调后的非常满意的结果：
- en: <picture>![batching-gunicorn-flask-tensorflow-api-response-time](../Images/5141b3adf790bc580e4c443de1d5c6ef.png)</picture>
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![batching-gunicorn-flask-tensorflow-api-response-time](../Images/5141b3adf790bc580e4c443de1d5c6ef.png)</picture>
- en: As you can see, when simulating only 1 user, this new API set-up with a timeout
    of 500 ms increases response time by 500 ms (the timeout value) and is of course
    useless. But with 20 users, the average response time has been divided by 6.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，当仅模拟 1 个用户时，这个新 API 设置的超时时间为 500 毫秒，会使响应时间增加 500 毫秒（超时时间值），当然是无用的。但对于 20
    个用户，平均响应时间减少了 6 倍。
- en: Note that in order to get still better results, this batching technique can
    be used in parallel with the multi-workers one presented above.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为了获得更好的结果，这种批处理技术可以与上述的多工人技术并行使用。
- en: Didn't we reinvent the wheel?
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 难道我们没有重新发明轮子吗？
- en: As developers, it's a kind of guilty pleasure to code from scratch something
    that should very likely already be implemented by an existing tool and that's
    what we've just done all along this article.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作为开发人员，从头开始编写一些可能已经由现有工具实现的东西是一种隐秘的快乐，这也是我们在这篇文章中一直做的事情。
- en: A tool indeed exists for serving a TensorFlow model in an API. It has been developed
    by TensorFlow and it is called... [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 确实存在一个用于在 API 中服务 TensorFlow 模型的工具。它由 TensorFlow 开发，名为... [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)。
- en: Not only does it allow to start a prediction API for the model without writing
    any other code than the model itself, but it also provides several optimizations
    that you may find interesting, like parallelism and requests batching!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 它不仅允许为模型启动一个预测 API，而无需编写模型以外的任何其他代码，而且还提供了几种你可能会觉得有趣的优化，例如并行处理和请求批处理！
- en: 'To serve a model with TF Serving, the first thing you need is to export it
    to your disk (in the TensorFlow format, not the Keras one), which can be done
    with the following piece of code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 TF Serving 服务一个模型，你需要做的第一件事是将其导出到磁盘（使用 TensorFlow 格式，而不是 Keras 格式），可以使用以下代码实现：
- en: Then, one of the easiest ways to start a serving API is to use the TF Serving
    [docker image](https://www.tensorflow.org/tfx/serving/docker).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，启动一个服务 API 的最简单方法之一是使用 TF Serving [docker 镜像](https://www.tensorflow.org/tfx/serving/docker)。
- en: 'The command below will start a prediction API that you can test by requesting
    this endpoint: [http://localhost:8501/v1/models/my_model:predict](http://localhost:8501/v1/models/my_model:predict)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的命令将启动一个预测 API，你可以通过请求此端点来测试：[http://localhost:8501/v1/models/my_model:predict](http://localhost:8501/v1/models/my_model:predict)
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that it does not use the GPU by default. You can find in [this documentation](https://www.tensorflow.org/tfx/serving/docker#serving_with_docker_using_your_gpu)
    how to make it work.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，它默认不使用 GPU。你可以在 [此文档](https://www.tensorflow.org/tfx/serving/docker#serving_with_docker_using_your_gpu)
    中找到如何使其工作。
- en: 'What about batching? You will have to write such a batching config file:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 关于批处理呢？你需要编写这样的批处理配置文件：
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'It includes, in particular, the 2 parameters used in our custom implementation:
    max_batch_size and batch_timeout_micros parameter. The latter is what we called
    the timeout and has to be given in microseconds.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 特别包括了我们自定义实现中使用的2个参数：max_batch_size 和 batch_timeout_micros 参数。后者是我们称之为超时的参数，必须以微秒为单位给出。
- en: 'Finally, the command to run TF Serving with batching enabled is:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，运行启用了批处理的TF Serving的命令是：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'I tested it and thanks to some black magic, I got even better results than
    with my custom batching API:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我进行了测试，感谢一些黑魔法，我得到了比自定义批处理API更好的结果：
- en: <picture>![batching-tensorflow-serving-api-response-time](../Images/01ab4df8dea678bba57bcbd7fde2432f.png)</picture>
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: <picture>![batching-tensorflow-serving-api-response-time](../Images/01ab4df8dea678bba57bcbd7fde2432f.png)</picture>
- en: I hope that you will be able to use the different tips presented in this article
    to boost your own Machine Learning APIs. There are many tracks I did not explore,
    especially the optimizations on the model itself (rather than the API) like [Model
    Pruning](https://towardsdatascience.com/pruning-deep-neural-network-56cae1ec5505)
    or [Post-training Quantization](https://www.tensorflow.org/model_optimization/guide/quantization).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你能利用本文中提出的不同技巧来提升你自己的机器学习API。我没有探索的很多领域，尤其是模型本身的优化（而不是API），如[模型剪枝](https://towardsdatascience.com/pruning-deep-neural-network-56cae1ec5505)或[后训练量化](https://www.tensorflow.org/model_optimization/guide/quantization)。
- en: '**Bio: [Yannick Wolf](https://www.linkedin.com/in/yannick-wolff-457616124/)**
    is Lead Data Scientist at Sicara.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**简历：[Yannick Wolf](https://www.linkedin.com/in/yannick-wolff-457616124/)**
    是Sicara的首席数据科学家。'
- en: '[Original](https://www.sicara.ai/blog/optimize-response-time-api). Reposted
    with permission.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.sicara.ai/blog/optimize-response-time-api)。经许可转载。'
- en: '**Related:**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Software Interfaces for Machine Learning Deployment](/2020/03/software-interfaces-machine-learning-deployment.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习部署的软件接口](/2020/03/software-interfaces-machine-learning-deployment.html)'
- en: '[TensorFlow 2.0 Tutorial: Optimizing Training Time Performance](/2020/03/tensorflow-optimizing-training-time-performance.html)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow 2.0教程：优化训练时间性能](/2020/03/tensorflow-optimizing-training-time-performance.html)'
- en: '[How to Do Hyperparameter Tuning on Any Python Script in 3 Easy Steps](/2020/04/hyperparameter-tuning-python.html)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在3个简单步骤中对任何Python脚本进行超参数调优](/2020/04/hyperparameter-tuning-python.html)'
- en: More On This Topic
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Top 10 MLOps Tools to Optimize & Manage Machine Learning Lifecycle](https://www.kdnuggets.com/2022/10/top-10-mlops-tools-optimize-manage-machine-learning-lifecycle.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化和管理机器学习生命周期的十大MLOps工具](https://www.kdnuggets.com/2022/10/top-10-mlops-tools-optimize-manage-machine-learning-lifecycle.html)'
- en: '[Deploying Your Machine Learning Model to Production in the Cloud](https://www.kdnuggets.com/deploying-your-ml-model-to-production-in-the-cloud)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[将机器学习模型部署到云中的生产环境](https://www.kdnuggets.com/deploying-your-ml-model-to-production-in-the-cloud)'
- en: '[How to Optimize SQL Queries for Faster Data Retrieval](https://www.kdnuggets.com/2023/06/optimize-sql-queries-faster-data-retrieval.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何优化SQL查询以加快数据检索速度](https://www.kdnuggets.com/2023/06/optimize-sql-queries-faster-data-retrieval.html)'
- en: '[How To Optimize Dockerfile Instructions for Faster Build Times](https://www.kdnuggets.com/how-to-optimize-dockerfile-instructions-for-faster-build-times)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何优化Dockerfile指令以加快构建时间](https://www.kdnuggets.com/how-to-optimize-dockerfile-instructions-for-faster-build-times)'
- en: '[A Full End-to-End Deployment of a Machine Learning Algorithm into a…](https://www.kdnuggets.com/2021/12/deployment-machine-learning-algorithm-live-production-environment.html)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[将机器学习算法完整端到端地部署到……](https://www.kdnuggets.com/2021/12/deployment-machine-learning-algorithm-live-production-environment.html)'
- en: '[Operationalizing Machine Learning from PoC to Production](https://www.kdnuggets.com/2022/05/operationalizing-machine-learning-poc-production.html)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[将机器学习从概念验证到生产化的操作化](https://www.kdnuggets.com/2022/05/operationalizing-machine-learning-poc-production.html)'
