- en: Classifying Long Text Documents Using BERT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用BERT进行长文本文档分类
- en: 原文：[https://www.kdnuggets.com/2022/02/classifying-long-text-documents-bert.html](https://www.kdnuggets.com/2022/02/classifying-long-text-documents-bert.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/02/classifying-long-text-documents-bert.html](https://www.kdnuggets.com/2022/02/classifying-long-text-documents-bert.html)
- en: '**By Sinequa**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**由Sinequa提供**'
- en: What do we want to achieve?
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们想要实现什么目标？
- en: 'We want to classify texts into predefined categories which is a very common
    task in NLP. For many years, the classical approach for simple documents was to
    generate features using [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) and
    combine it with [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression).
    Formerly we used to rely on this stack at Sinequa for textual classification,
    and, spoiler alert, with the model presented here we have beaten our baseline
    from 5% to 30% for very noisy and long documents datasets. This former approach
    had two main issues: the feature sparsity that we tackled via compression techniques
    and the word-matching issue that we tamed leveraging Sinequa’s powerful linguistic
    capacities (mainly through our homegrown tokenizer).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想将文本分类到预定义的类别中，这是NLP中非常常见的任务。多年来，简单文档的经典方法是使用[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)生成特征，并将其与[逻辑回归](https://en.wikipedia.org/wiki/Logistic_regression)结合。以前我们在Sinequa用来进行文本分类时依赖于这一套方案，剧透一下，使用这里介绍的模型，我们将非常嘈杂和长文档数据集的基线从5%提高到了30%。这种旧方法有两个主要问题：特征稀疏性，我们通过压缩技术解决了这一问题，以及词匹配问题，我们通过利用Sinequa强大的语言学能力（主要通过我们自家开发的分词器）来克服。
- en: Later on, the pandora box of language models (pre-trained on humongous corpora
    in an unsupervised fashion and fine-tuned on downstream supervised tasks) was
    opened and TF-IDF based techniques were not state of the art anymore. Such language
    models could be [word2vec](https://arxiv.org/pdf/1301.3781.pdf) combined with
    LSTMs or CNNs, ELMo, and most importantly the Transformer (in 2017: [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，语言模型的潘多拉盒子被打开了（这些模型在海量语料库上进行无监督预训练，并在下游监督任务中进行微调），TF-IDF基础的技术不再是最先进的。这些语言模型可能是[word2vec](https://arxiv.org/pdf/1301.3781.pdf)与LSTM或CNN、ELMo，以及最重要的Transformer（2017年：[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)）。
- en: '[BERT](https://arxiv.org/pdf/1810.04805.pdf) is a Transformer based language
    model that has gained a lot of momentum in the last couple of years since it beat
    all NLP baselines by far and came as a natural choice to build our text classification.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[BERT](https://arxiv.org/pdf/1810.04805.pdf)是一个基于Transformer的语言模型，近年来获得了大量关注，因为它远远超越了所有NLP基线，并成为构建我们文本分类的自然选择。'
- en: What is the challenge then?
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 那么挑战是什么呢？
- en: 'Transformer based language models such as BERT are really good at understanding
    the semantic context (where bag-of-words techniques fail) because they were designed
    specifically for that purpose. As explained in the introduction, BERT outperforms
    all NLP baselines, but as we say in the scientific community, “no free lunch”.
    This extensive semantic comprehension of a model like BERT offers comes with a
    big caveat: it cannot deal with very long text sequences. Basically, this limitation
    is 512 tokens (a token being a word or a subword of the text) which represent
    more or less two or three Wikipedia paragraphs and we obviously don’t want to
    consider only such a small sub-part of a text to classify it.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Transformer的语言模型如BERT在理解语义上下文方面表现出色（这是词袋方法所无法做到的），因为它们专门为此目的而设计。如引言中所述，BERT在所有NLP基线测试中表现优异，但正如我们在科学界所说的，“没有免费的午餐”。像BERT这样的模型提供的广泛语义理解带来了一个大问题：它无法处理非常长的文本序列。基本上，这个限制是512个标记（标记是文本中的一个词或子词），这大致相当于两到三段维基百科内容，而我们显然不希望仅仅考虑如此小的文本子部分进行分类。
- en: To illustrate this, let’s consider the task of classifying comprehensive product
    reviews into positive or negative reviews. The first sentences or paragraphs may
    only contain a description of the product and it would likely require to go further
    down the review to understand whether the reviewer actually likes the product
    or not. If our model does not encompass the whole content, it might not be possible
    to make the right prediction. Therefore, one requirement for our model is to capture
    the context of a document while managing correctly long-time dependencies between
    the sentences at the beginning and the end of the document.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，考虑将全面的产品评论分类为正面或负面评论的任务。前几句话或段落可能只包含产品的描述，可能需要进一步阅读评论才能理解评论者是否真正喜欢这个产品。如果我们的模型不能涵盖全部内容，可能无法做出正确的预测。因此，我们的模型的一个要求是捕捉文档的上下文，同时正确管理文档开头和结尾之间的长期依赖关系。
- en: Technically speaking, the core limitation is the memory footprint that grows
    quadratically with the number of tokens along with the use of pre-trained models
    that come with a fixed size determined by Google (& al.). This is expected since
    each token is “attentive” [https://arxiv.org/pdf/1706.03762.pdf] to every other
    token and therefore requires a [N x N] attention matrix, with [N] the number of
    tokens. For example, BERT accepts a maximum of 512 tokens which hardly qualifies
    as long text. And going beyond 512 tokens rapidly reaches the limits of even modern
    GPUs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，核心限制是内存占用，它随着标记数量的增加而按平方增长，同时使用的预训练模型具有由 Google 等公司确定的固定大小。这是预期中的情况，因为每个标记都“注意”
    [https://arxiv.org/pdf/1706.03762.pdf] 到每个其他标记，因此需要一个 [N x N] 的注意力矩阵，其中 [N] 是标记的数量。例如，BERT
    接受最多 512 个标记，这很难称为长文本。超过 512 个标记很快就会达到甚至现代 GPU 的极限。
- en: Another problem that arises using Transformers in a production environment is
    the very slow inference due to the size of the models (110M parameters for BERT
    base) and, again, the quadratic cost. So, our goal is not only to find an architecture
    that fits into memory during the training but to find one that also responds reasonably
    fast during inference.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中使用 Transformers 另一个出现的问题是由于模型的大小（BERT base 具有 110M 参数）以及平方成本导致推理非常缓慢。因此，我们的目标不仅是找到一个在训练过程中适合内存的架构，还要找到一个在推理过程中也能合理快速响应的架构。
- en: 'The last challenge we address here is to build a model based on various feature
    types: long text of course, but also additional textual metadata (such as title,
    abstract …) and categories (location, authors …).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里面临的最后一个挑战是基于各种特征类型构建一个模型：当然包括长文本，还包括附加的文本元数据（如标题、摘要等）和类别（如位置、作者等）。
- en: So, how to deal with really long documents?
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 那么，如何处理非常长的文档呢？
- en: 'The main idea is to split the document into shorter sequences and feed these
    sequences into a BERT model. We obtain the CLS embedding for each sequence and
    merge the embeddings. There are a couple of possibilities to perform the merge,
    we experimented with:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的想法是将文档拆分成更短的序列，并将这些序列输入到 BERT 模型中。我们为每个序列获得 CLS 嵌入并合并这些嵌入。合并有几种可能性，我们尝试了：
- en: Convolutional Neural Networks (CNN)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）
- en: Long Short-Term Memory Networks (LSTM)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短期记忆网络（LSTM）
- en: Transformers (to aggregate Transformers, yes :) )
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers（用来聚合Transformers，没错 :)）
- en: Our experiments on different standard text classification corpora showed that
    using additional Transformer layers to merge the produced embeddings works best
    without introducing a large computational cost.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在不同标准文本分类语料库上的实验表明，使用额外的 Transformer 层来合并生成的嵌入效果最佳，而不会引入大量计算成本。
- en: Want the formal description, right?
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 想要正式描述，对吧？
- en: We consider a text classification task with *L* labels. For a document *D*,
    its tokens given by the [WordPiece tokenization](https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46) can
    be written *X =( x₁, …, xₙ) *with *N* the total number of token in *D*. Let K
    be the maximal sequence length (up to 512 for BERT). Let *I* be the number of
    sequences of *K *tokens or less in *D*, it is given by I=⌊ N/K ⌋.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑一个具有 *L* 标签的文本分类任务。对于一个文档 *D*，其由 [WordPiece 分词](https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46)
    提供的标记可以表示为 *X =( x₁, …, xₙ)*，其中 *N* 为 *D* 中标记的总数。设 K 为最大序列长度（对于 BERT 为 512）。设
    *I* 为 *D* 中 *K* 个标记或更少的序列数，它由 I=⌊ N/K ⌋ 给出。
- en: 'Note that if the last sequence in the document has a size lower to *K* it will
    be padded with 0 until the *Kᵗʰ* index. Then if *s*ⁱ with *i****∈ ****{1, ..,
    I},* is the *i*-th sequence with *K* elements in *D*, we have:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果文档中的最后一个序列的大小小于 *K*，它将用0填充，直到 *Kᵗʰ* 索引。如果 *s*ⁱ 其中 *i* ∈ {1, .., I} 是 *D*
    中第 *i* 个具有 *K* 元素的序列，我们有：
- en: '![Classifying Long Text Documents Using BERT](../Images/fe2509ceb48bea5689f16d7a8f8c8ee4.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![使用BERT对长文本文档进行分类](../Images/fe2509ceb48bea5689f16d7a8f8c8ee4.png)'
- en: We can note that
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以注意到
- en: '![Classifying Long Text Documents Using BERT](../Images/27b03622aad1a7ae0acb19ec0608d055.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![使用BERT对长文本文档进行分类](../Images/27b03622aad1a7ae0acb19ec0608d055.png)'
- en: BERT returns the CLS embedding but also an embedding per token.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 返回 CLS 嵌入以及每个 token 的嵌入。
- en: 'Let define the embeddings per token returned by BERT for the i-th sequence
    of the document such as:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义 BERT 为文档的第 *i* 个序列返回的每个 token 的嵌入，如下所示：
- en: '![Classifying Long Text Documents Using BERT](../Images/35c447f8e3bd57d3afd2a4c217f40271.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![使用BERT对长文本文档进行分类](../Images/35c447f8e3bd57d3afd2a4c217f40271.png)'
- en: where *CLS* is the embedding of the special token inserted in front of each
    text sequence fed to BERT, it is generally considered as an embedding summarizing
    the full sequence.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *CLS* 是插入到每个输入 BERT 的文本序列前的特殊 token 的嵌入，通常被认为是总结整个序列的嵌入。
- en: 'To combine the sequences, we only use *CLSᵢ* and do not use *y*. We use *t* transformers *T₁,
    …,Tₜ* to obtain the final vector to feed to the last dense layer of the network:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了结合这些序列，我们只使用 *CLSᵢ*，不使用 *y*。我们使用 *t* 个变换器 *T₁, …,Tₜ* 来获得最终向量，以便输入到网络的最后一个全连接层：
- en: '![Classifying Long Text Documents Using BERT](../Images/525bf66dd2e6ee6f0f01f037fd2fc077.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![使用BERT对长文本文档进行分类](../Images/525bf66dd2e6ee6f0f01f037fd2fc077.png)'
- en: where **∘** is the function composition operation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 **∘** 是函数复合操作。
- en: Given the last dense layer weights *W ∈ *ℝᴸˣᴴ where *H* is the hidden size of
    the transformer and bias *b* *∈ *ℝᴸ
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 给定最后一个全连接层的权重 *W ∈ ℝᴸˣᴴ*，其中 *H* 是变换器的隐藏大小，偏置 *b* ∈ ℝᴸ
- en: 'The probabilities ***P*** *∈ *ℝᴸ are given by:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 概率 ***P*** *∈ ℝᴸ* 由以下公式给出：
- en: '![Classifying Long Text Documents Using BERT](../Images/ca67617333e27a025e6f4a90cda4b273.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![使用BERT对长文本文档进行分类](../Images/ca67617333e27a025e6f4a90cda4b273.png)'
- en: Finally, applying *argmax *on the vector *P* returns the predicted label. For
    a summary of the above architecture, you can have a look at figure 1.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对向量 *P* 应用 *argmax* 返回预测的标签。有关上述架构的总结，您可以查看图1。
- en: The architecture above enables us to leverage BERT for the text classification
    task bypassing the maximum sequence length limitation of transformers while at
    the same time keeping the context over multiple sequences. Let’s see how to combine
    it with other types of features.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 上述架构使我们能够利用 BERT 进行文本分类任务，绕过变换器的最大序列长度限制，同时保持对多个序列的上下文。让我们看看如何将其与其他类型的特征结合。
- en: How to deal with metadata?
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何处理元数据？
- en: Oftentimes, a document comes with more than just its content. There can be metadata
    that we divide into two groups, textual metadata, and categorical metadata.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 文档通常不仅仅包含其内容，还可能有我们分为两个组的元数据：文本元数据和分类元数据。
- en: Textual Metadata
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本元数据
- en: By textual metadata, we mean short text that has (after tokenization) a relatively
    small number of tokens. This is required to fit entirely into our language model.
    A typical example of such metadata would be titles or abstracts.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 文本元数据是指短文本，它（在分词后）具有相对较少的 token 数量。这是为了能够完全适配到我们的语言模型中。此类元数据的典型示例包括标题或摘要。
- en: Given a document with *M* metadata annotation. Let
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个具有 *M* 个元数据注释的文档。让
- en: '![Classifying Long Text Documents Using BERT](../Images/f70c6576edd8fae6d57820c73e26627c.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![使用BERT对长文本文档进行分类](../Images/f70c6576edd8fae6d57820c73e26627c.png)'
- en: 'be the CLS embeddings produced by BERT for each metadata. The same technique
    as above is used to get the probability vector as:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 是 BERT 为每个元数据生成的 CLS 嵌入。使用与上述相同的技术来获取概率向量，如下所示：
- en: '![Classifying Long Text Documents Using BERT](../Images/8b53f277d7ed866c65bdc58c0c6c187e.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![使用BERT对长文本文档进行分类](../Images/8b53f277d7ed866c65bdc58c0c6c187e.png)'
- en: Categorical Metadata
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类元数据
- en: Categorical metadata can be a numerical or textual value that represents a category.
    Numerical values can be the number of pages whereas textual values can be the
    publisher name or a geo-location.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 分类元数据可以是表示类别的数值或文本值。数值可以是页数，而文本值可以是出版商名称或地理位置。
- en: A common way to deal with such features is to implement [the Wide and Deep architecture](https://export.arxiv.org/pdf/1606.07792).
    Our experiments showed that results yielded by the deep part of this network were
    sufficiently good and the wide part was not required.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 处理此类特征的一种常见方法是实现 [宽和深架构](https://export.arxiv.org/pdf/1606.07792)。我们的实验表明，这个网络的深度部分产生的结果已经足够好，而宽度部分是不必要的。
- en: We encode the categorical metadata in a single cross-category vector using one-hot
    encoding. This encoding is then passed into an embedding layer that learns a vector
    representation for each distinct category. The last step is to apply a pooling
    layer on the resulting embedding matrix.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 one-hot 编码将分类元数据编码为一个单一的跨类别向量。然后，将该编码传递到嵌入层，该层学习每个不同类别的向量表示。最后一步是对结果嵌入矩阵应用池化层。
- en: We considered max, average and min pooling and found that using average pooling
    worked best for our test corpora.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了最大池化、平均池化和最小池化，并发现使用平均池化对我们的测试语料库效果最好。
- en: How does the complete architecture look?
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完整的架构是怎样的？
- en: Hope you stuck around until now, the following figure will hopefully make things
    a lot clearer.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你坚持到现在，下面的图示将有助于进一步澄清问题。
- en: '![Classifying Long Text Documents Using BERT](../Images/cafa9638af84fb59586d0e154088a182.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![使用 BERT 对长文本进行分类](../Images/cafa9638af84fb59586d0e154088a182.png)'
- en: Figure 1
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1
- en: There are three sub-models, one for text, another for textual metadata, and
    the last one for categorical metadata. The output of the three sub-models is merely
    concatenated into a single vector before passing it through a dropout layer and
    finally into the last dense layer with a softmax activation for the classification.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个子模型，一个用于文本，另一个用于文本元数据，最后一个用于分类元数据。三个子模型的输出仅仅是串联成一个单一的向量，然后通过一个 dropout 层，最后进入一个具有
    softmax 激活的最后密集层进行分类。
- en: You probably have noticed that there are multiple BERT instances depicted in
    the architecture, not only for the text input but also for the textual metadata.
    As BERT comes with many parameters to train, we decided not to include a separate
    BERT model per sub-model, but instead share the weights of a single model in between
    the sub-models. Sharing weights certainly reduces the RAM used by the model (enabling
    training with larger batch-size, so accelerating training in a sense) but it does
    not change the inference-time since there will still be as many BERT executions
    no matter whether their weights are shared or not.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在架构中描绘了多个 BERT 实例，这不仅用于文本输入，还用于文本元数据。由于 BERT 需要训练许多参数，我们决定不为每个子模型包含一个单独的
    BERT 模型，而是共享一个模型的权重。共享权重确实减少了模型使用的 RAM（使得可以使用更大的批量大小进行训练，从而在某种程度上加速训练），但它并不会改变推理时间，因为无论权重是否共享，BERT
    执行的次数依然是一样的。
- en: What about inference time?
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理时间怎么样？
- en: By now, you must have guessed that including that many invocations of the BERT
    model do not come for free. And it is true that it is computationally expensive
    to run inference of such a model. However, there are a couple of tricks to improve
    inference times. In the following, we focus on CPU inference as this is very important
    in production environments.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在，你一定已经猜到包含如此多的 BERT 模型调用并不是免费的。确实，运行这样的模型推理计算代价昂贵。然而，有几个技巧可以提高推理时间。以下内容我们将重点讨论
    CPU 推理，因为这在生产环境中非常重要。
- en: 'A couple of notes for the conducted experiments:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于进行的实验，有几点说明：
- en: We consider a simplified model only containing a text feature.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们考虑了一个仅包含文本特征的简化模型。
- en: We limited the tokens that we used per document to 25,600 tokens which correspond
    roughly to around 130,000 characters if the document contains English text.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将每个文档使用的 tokens 限制为 25,600 个，这大致相当于 130,000 个字符（如果文档包含英文文本）。
- en: We perform the experiments with documents that have the above described maximum
    length. In practice, documents have varying sizes and as we use dynamic size tensors
    in our model, inference times are considerably faster for short documents. As
    a rule of thumb, when using a document that is half as long reduces the inference
    time by 50 %.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用具有上述最大长度的文档进行实验。实际上，文档的大小各不相同，由于我们在模型中使用了动态大小的张量，因此短文档的推理时间显著更快。作为一个经验法则，使用一个长度是原文档一半的文档，推理时间会减少
    50%。
- en: '![Classifying Long Text Documents Using BERT](../Images/0845164d19924ac923409789417a43cc.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![使用 BERT 对长文本进行分类](../Images/0845164d19924ac923409789417a43cc.png)'
- en: '![Classifying Long Text Documents Using BERT](../Images/4550d304174ee43ad464ad219c6fd843.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![使用 BERT 分类长文本文档](../Images/4550d304174ee43ad464ad219c6fd843.png)'
- en: '**References**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[https://arxiv.org/abs/2006.04152](https://arxiv.org/abs/2006.04152), [https://arxiv.org/pdf/2001.08950.pdf](https://arxiv.org/pdf/2001.08950.pdf)'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/2006.04152](https://arxiv.org/abs/2006.04152)， [https://arxiv.org/pdf/2001.08950.pdf](https://arxiv.org/pdf/2001.08950.pdf)'
- en: '[https://blog.tensorflow.org/2020/04/tfrt-new-tensorflow-runtime.html](https://blog.tensorflow.org/2020/04/tfrt-new-tensorflow-runtime.html)'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://blog.tensorflow.org/2020/04/tfrt-new-tensorflow-runtime.html](https://blog.tensorflow.org/2020/04/tfrt-new-tensorflow-runtime.html)'
- en: '[https://www.tensorflow.org/xla?hl=fr](https://www.tensorflow.org/xla?hl=fr)'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.tensorflow.org/xla?hl=fr](https://www.tensorflow.org/xla?hl=fr)'
- en: '[https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333](https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333)'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333](https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333)'
- en: What else is there to do?
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 还有什么可以做的？
- en: Linear Transformers
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性 Transformer
- en: 'Building a Transformer-like architecture that does not come with the quadratic
    complexity in time and memory is currently a very active field of research. There
    are a couple of candidates that are definitely worth trying once pre-trained models
    will be released:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一种类似于 Transformer 的架构，但没有时间和内存的二次复杂度，目前是一个非常活跃的研究领域。一旦预训练模型发布，有几个候选者绝对值得尝试：
- en: Linformer [[https://arxiv.org/pdf/2006.04768.pdf](https://arxiv.org/pdf/2006.04768.pdf)]
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linformer [[https://arxiv.org/pdf/2006.04768.pdf](https://arxiv.org/pdf/2006.04768.pdf)]
- en: BigBird [[https://arxiv.org/pdf/2007.14062.pdf](https://arxiv.org/pdf/2007.14062.pdf)]
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BigBird [[https://arxiv.org/pdf/2007.14062.pdf](https://arxiv.org/pdf/2007.14062.pdf)]
- en: Reformers [[https://arxiv.org/pdf/2001.04451.pdf](https://arxiv.org/pdf/2001.04451.pdf)]
    (only O(N log(N)) complexity)
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改革者 [[https://arxiv.org/pdf/2001.04451.pdf](https://arxiv.org/pdf/2001.04451.pdf)]（仅
    O(N log(N)) 复杂度）
- en: Performers [[https://arxiv.org/pdf/2009.14794.pdf](https://arxiv.org/pdf/2009.14794.pdf)]
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Performers [[https://arxiv.org/pdf/2009.14794.pdf](https://arxiv.org/pdf/2009.14794.pdf)]
- en: etc…
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等等…
- en: A preliminary test with the very promising [Longformer ](https://arxiv.org/pdf/2004.05150.pdf)model
    could not be executed successfully. We tried to train a LongFormer model using
    the TensorFlow implementation of [Hugging Face](https://huggingface.co/transformers/model_doc/longformer.html).
    However, it appears that the implementation is not yet memory-optimized as it
    was not possible to train it even on a large GPU with 48 GB of memory.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对非常有前景的 [Longformer](https://arxiv.org/pdf/2004.05150.pdf) 模型进行的初步测试未能成功执行。我们尝试使用
    [Hugging Face](https://huggingface.co/transformers/model_doc/longformer.html)
    的 TensorFlow 实现来训练 LongFormer 模型。然而，似乎该实现尚未进行内存优化，因为即使在具有 48 GB 内存的大型 GPU 上也无法训练。
- en: Inference time is the cornerstone of any ML project that needs to run in production,
    so we do plan to use such “linear transformers” in the future in addition to pruning
    and quantization.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 推理时间是任何需要在生产中运行的 ML 项目的基石，因此我们计划在未来使用这种“线性 Transformer”，除此之外还会进行剪枝和量化。
- en: Are we done yet?
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们完成了吗？
- en: Yes, thanks for sticking with us until the end. If you have questions or remarks
    about our model, feel free to comment. We would love to hear from you.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，感谢你坚持到最后。如果你对我们的模型有任何问题或评论，请随时留言。我们非常乐意听取你的意见。
- en: '[Original](https://sinequa.medium.com/classifying-long-textual-documents-up-to-25-000-tokens-using-bert-9d2dd55ca060).
    Reposted with permission.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://sinequa.medium.com/classifying-long-textual-documents-up-to-25-000-tokens-using-bert-9d2dd55ca060)。经许可转载。'
- en: '* * *'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你在 IT 领域的组织'
- en: '* * *'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Convert Text Documents to a TF-IDF Matrix with tfidfvectorizer](https://www.kdnuggets.com/2022/09/convert-text-documents-tfidf-matrix-tfidfvectorizer.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 tfidfvectorizer 将文本文档转换为 TF-IDF 矩阵](https://www.kdnuggets.com/2022/09/convert-text-documents-tfidf-matrix-tfidfvectorizer.html)'
- en: '[Converting Text Documents to Token Counts with CountVectorizer](https://www.kdnuggets.com/2022/10/converting-text-documents-token-counts-countvectorizer.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 CountVectorizer 将文本文档转换为词频计数](https://www.kdnuggets.com/2022/10/converting-text-documents-token-counts-countvectorizer.html)'
- en: '[GPT4All is the Local ChatGPT for your Documents and it is Free!](https://www.kdnuggets.com/2023/06/gpt4all-local-chatgpt-documents-free.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT4All 是您文档的本地 ChatGPT，且完全免费！](https://www.kdnuggets.com/2023/06/gpt4all-local-chatgpt-documents-free.html)'
- en: '[Extractive Summarization with LLM using BERT](https://www.kdnuggets.com/extractive-summarization-with-llm-using-bert)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 BERT 进行提取式摘要](https://www.kdnuggets.com/extractive-summarization-with-llm-using-bert)'
- en: '[How Long Does It Take to Learn Data Science Fundamentals?](https://www.kdnuggets.com/2022/03/long-take-learn-data-science-fundamentals.html)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学基础知识需要多长时间？](https://www.kdnuggets.com/2022/03/long-take-learn-data-science-fundamentals.html)'
- en: '[Fine-Tuning BERT for Tweets Classification with HuggingFace](https://www.kdnuggets.com/2022/01/finetuning-bert-tweets-classification-ft-hugging-face.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 HuggingFace 微调 BERT 进行推文分类](https://www.kdnuggets.com/2022/01/finetuning-bert-tweets-classification-ft-hugging-face.html)'
