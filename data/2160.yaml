- en: Some Kick Ass Prompt Engineering Techniques to Boost our LLM Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些提升LLM模型的强大提示工程技术
- en: 原文：[https://www.kdnuggets.com/some-kick-ass-prompt-engineering-techniques-to-boost-our-llm-models](https://www.kdnuggets.com/some-kick-ass-prompt-engineering-techniques-to-boost-our-llm-models)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/some-kick-ass-prompt-engineering-techniques-to-boost-our-llm-models](https://www.kdnuggets.com/some-kick-ass-prompt-engineering-techniques-to-boost-our-llm-models)
- en: '![Some Kick Ass Prompt Engineering Techniques to Boost our LLM Models](../Images/ce3a364e3caa086c1d829b53449f1791.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![一些提升LLM模型的强大提示工程技术](../Images/ce3a364e3caa086c1d829b53449f1791.png)'
- en: Image created with DALL-E3
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DALL-E3创建的图像
- en: Artificial Intelligence has been a complete revolution in the tech world.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能在科技领域带来了彻底的革命。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的IT需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Its ability to mimic human intelligence and perform tasks that were once considered
    solely human domains still amazes most of us.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 其模仿人类智慧并执行曾被认为仅为人类领域的任务的能力仍然让我们大多数人感到惊讶。
- en: However, no matter how good these late AI leap forwards have been, there’s always
    room for improvement.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，无论这些最近的AI进展有多么出色，仍然有改进的空间。
- en: And this is precisely where prompt engineering kicks in!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正是提示工程发挥作用的地方！
- en: Enter this field that can significantly enhance the productivity of AI models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 进入这个可以显著提升AI模型生产力的领域。
- en: Let’s discover it all together!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一起发现这一切吧！
- en: The Essence of Prompt Engineering
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示工程的本质
- en: Prompt engineering is a fast-growing domain within AI that focuses on improving
    the efficiency and effectiveness of language models. It’s all about crafting perfect
    prompts to guide AI models to produce our desired outputs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程是一个快速增长的领域，专注于提高语言模型的效率和有效性。其核心在于设计完美的提示，以指导AI模型产生我们期望的输出。
- en: Think of it as learning how to give better instructions to someone to ensure
    they understand and execute a task correctly.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 将其视为学习如何给出更好的指示，以确保对方理解并正确执行任务。
- en: Why Prompt Engineering Matters
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么提示工程很重要
- en: '**Enhanced Productivity:** By using high-quality prompts, AI models can generate
    more accurate and relevant responses. This means less time spent on corrections
    and more time leveraging AI’s capabilities.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高生产力：** 通过使用高质量的提示，AI模型可以生成更准确和相关的响应。这意味着减少了修正的时间，增加了利用AI能力的时间。'
- en: '**Cost Efficiency:** Training AI models is resource-intensive. Prompt engineering
    can reduce the need for retraining by optimizing model performance through better
    prompts.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本效率：** 训练AI模型是资源密集型的。提示工程可以通过更好的提示优化模型性能，从而减少重新训练的需求。'
- en: '**Versatility:** A well-crafted prompt can make AI models more versatile, allowing
    them to tackle a broader range of tasks and challenges.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性：** 一个精心设计的提示可以使AI模型更加多样化，使它们能够处理更广泛的任务和挑战。'
- en: Before diving into the most advanced techniques, let’s recall two of the most
    useful (and basic) prompt engineering techniques.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解最先进的技术之前，让我们回顾两种最有用（也最基础）的提示工程技术。
- en: A Glimpse into Basic Prompt Engineering Methods
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本提示工程方法的初步了解
- en: Sequential Thinking with “Let’s think step by step”
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用“让我们一步一步思考”进行顺序思考
- en: Today it is well-known that LLM models’ accuracy is significantly improved when
    adding the word sequence “Let’s think step by step”.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 今天大家都知道，添加“让我们一步一步思考”这个词序列，可以显著提高LLM模型的准确性。
- en: Why… you might ask?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么……你可能会问？
- en: Well, this is because we are forcing the model to break down any task into multiple
    steps, thus making sure the model has enough time to process each of them.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为我们强迫模型将任何任务分解为多个步骤，从而确保模型有足够的时间处理每一个步骤。
- en: 'For instance, I could challenge GPT3.5 with the following prompt:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我可以用以下提示挑战 GPT3.5：
- en: If John has 5 pears, then eats 2, buys 5 more, then gives 3 to his friend, how
    many pears does he have?
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果约翰有5个梨，然后吃了2个，买了5个，再给朋友3个，他现在有多少个梨？
- en: The model will give me an answer right away. However, if I add the final “Let’s
    think step by step”, I am forcing the model to generate a thinking process with
    multiple steps.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 模型会立刻给我答案。然而，如果我加上最后的“让我们一步一步来思考”，我就是在强迫模型生成一个多步骤的思维过程。
- en: Few-Shot Prompting
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 少样本提示
- en: While the Zero-shot prompting refers to asking the model to perform a task without
    providing any context or previous knowledge, the few-shot prompting technique
    implies that we present the LLM with a few examples of our desired output along
    with some specific question.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然零样本提示指的是在没有提供任何上下文或先前知识的情况下要求模型执行任务，但少样本提示技术则意味着我们向 LLM 提供一些我们期望的输出示例以及一些具体问题。
- en: For example, if we want to come up with a model that defines any term using
    a poetic tone, it might be quite hard to explain. Right?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，如果我们想创建一个用诗意的语调定义任何术语的模型，这可能会很难解释。对吧？
- en: However, we could use the following few-shot prompts to steer the model in the
    direction we want.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我们可以使用以下少样本提示来引导模型朝着我们想要的方向。
- en: Your task is to answer in a consistent style aligned with the following style.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你的任务是以一致的风格回答，符合以下风格。
- en: ''
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**<user>**: Teach me about resilience.'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**<user>**: 教我关于韧性的问题。'
- en: ''
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**<system>**: Resilience is like a tree that bends with the wind but never
    breaks.'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**<system>**: 韧性就像一棵随风摇摆却永不折断的树。'
- en: ''
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is the ability to bounce back from adversity and keep moving forward.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这是一种从逆境中反弹并不断前进的能力。
- en: ''
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**<user>**: Your input here.'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**<user>**: 你的输入在这里。'
- en: If you have not tried it out yet, you can go challenge GPT.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没试过，你可以去挑战 GPT。
- en: However, as I am pretty sure most of you already know these basic techniques,
    I will try to challenge you with some advanced techniques.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，由于我很确定你们中的大多数人已经了解这些基本技巧，我将尝试用一些高级技巧来挑战你们。
- en: Advanced Prompt Engineering techniques
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级提示工程技术
- en: 1\. Chain of Thought (CoT) Prompting
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 思维链 (CoT) 提示
- en: Introduced by [Google in 2022](https://arxiv.org/abs/2201.11903), this method
    involves instructing the model to undergo several reasoning stages before delivering
    the ultimate response.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由[谷歌于2022年](https://arxiv.org/abs/2201.11903)介绍，这种方法包括指示模型在给出最终回答之前经历几个推理阶段。
- en: Sounds familiar right? If so, you are totally right.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来很熟悉，对吧？如果是这样，你完全正确。
- en: It is like merging both Sequential Thinking and Few-Shot Prompting.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像将顺序思维和少样本提示合并在一起。
- en: How?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 怎么做？
- en: Essentially, CoT prompting directs the LLM to process information sequentially.
    This means we exemplify how to solve a first problem with multiple steps reasoning
    and then send to the model our real task, expecting it to emulate a comparable
    chain of thought when responding to the actual query we want it to solve.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，CoT 提示引导 LLM 顺序处理信息。这意味着我们展示如何通过多个步骤解决第一个问题，然后将实际任务发送给模型，期望它在回答实际查询时模仿类似的思维链。
- en: '![Some Kick Ass Prompt Engineering Techniques to Boost our LLM Models](../Images/2a7d5fcfa8df7cbef187c9ce7298f57a.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![一些强大的提示工程技术来提升我们的 LLM 模型](../Images/2a7d5fcfa8df7cbef187c9ce7298f57a.png)'
- en: Image taken from the [paper](https://arxiv.org/abs/2201.11903).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图片取自[论文](https://arxiv.org/abs/2201.11903)。
- en: So, following the very first example, we could provide a prompt with the first
    problem and a sequential explanation of how to solve it. Then, we can send our
    input and see how magic happens!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，按照第一个示例，我们可以提供一个包含第一个问题和顺序解释的提示。然后，我们可以发送我们的输入，看看魔法是如何发生的！
- en: '**<user>** If John has 5 pears, then eats 2, buys 5 more, then gives 3 to his
    friend, how many pears does he have? Let’s think step by step.'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**<user>** 如果约翰有5个梨，然后吃了2个，买了5个，再给朋友3个，他现在有多少个梨？让我们一步一步来思考。'
- en: ''
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**<agent>** Let’s go step by step. John starts with 5 pears. He eats 2 pears,
    so he has 5–2 = 3 pears left. Then, he buys 5 more pears. So, he now has 3 + 5
    = 8 pears. Finally, he gives 3 pears to his friend. Therefore, he has 8–3 = 5
    pears left. So, after all these steps, John has 5 pears.'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**<agent>** 让我们一步一步来。约翰开始时有5个梨。他吃了2个梨，所以剩下5–2 = 3个梨。然后，他又买了5个梨。所以，他现在有3 + 5
    = 8个梨。最后，他给了朋友3个梨。因此，他剩下8–3 = 5个梨。所以，经过所有这些步骤，约翰有5个梨。'
- en: ''
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**<user>** Your input here.'
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**<user>** 你的输入在这里。'
- en: However, most of you must be thinking… Do I have to think of a sequential way
    to solve any problem any time I want to ask something on ChatGPT?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你们大多数人可能会想… 每次我想在ChatGPT上问问题时，都必须想到一种逐步解决问题的方法吗？
- en: Well… you are not the first one! And this leads us to…
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯… 你不是第一个这样想的人！这就引出了…
- en: 2\. Automatic Chain-of-Thought (Auto-CoT)
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 自动化思维链（Auto-CoT）
- en: 'In 2022, [Zhang](https://arxiv.org/abs/2210.03493) and colleagues introduced
    a method to avoid this manual process. There are two main reasons to avoid any
    manual task:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在2022年，[张](https://arxiv.org/abs/2210.03493)和同事们引入了一种避免这种手动过程的方法。避免任何手动任务的主要原因有两个：
- en: It can be boring.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这可能会很无聊。
- en: It can yield bad results - for instance, when our mental process is wrong.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这可能会导致糟糕的结果——例如，当我们的思维过程错误时。
- en: They suggested using LLMs combined with the "Let's think step by step" prompt
    to sequentially produce reasoning chains for each demonstration.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 他们建议结合使用LLMs和“让我们一步步思考”提示来逐步生成每个演示的推理链。
- en: This means asking ChatGPT how to solve any problem sequentially and then using
    this very same example to train it how to solve any other problem.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着询问ChatGPT如何逐步解决任何问题，然后使用这个例子来训练它如何解决其他问题。
- en: 3\. Self-Consistency
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 自我一致性
- en: Self-consistency is another interesting prompting technique that aims to improve
    the chain of thought prompting for more complex reasoning problems.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 自我一致性是另一种有趣的提示技术，旨在改善更复杂推理问题的思维链提示。
- en: So… what’s the main difference?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所以… 主要区别是什么？
- en: 'The main idea under Self-Consistency is being aware that we can train the model
    with a wrong example. Just imagine I solve the previous problem with a wrong mental
    process:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 自我一致性的主要思想是意识到我们可以用错误的例子来训练模型。试想一下我用错误的思维过程解决了之前的问题：
- en: '**<User>** If John has 5 pears, then eats 2, buys 5 more, then gives 3 to his
    friend, how many pears does he have? Let’s think step by step.'
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**<User>** 如果约翰有5个梨子，然后吃掉2个，再买5个，然后给朋友3个，他还有多少个梨子？让我们一步步思考。'
- en: ''
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**<Agent>** Start with 5 pears. John eats 2 pears. Then, he gives 3 pears to
    his friend. These actions can be combined: 2 (eaten) + 3 (given) = 5 pears in
    total affected. Now, subtract the total pears affected from the initial 5 pears:
    5 (initial) - 5 (affected) = 0 pears left.'
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**<Agent>** 从5个梨子开始。约翰吃了2个梨子。然后，他给了朋友3个梨子。这些操作可以组合在一起：2（吃掉的）+ 3（给出的）= 总共5个受影响的梨子。现在，从最初的5个梨子中减去受影响的梨子：5（初始）
    - 5（受影响） = 剩下0个梨子。'
- en: Then, any other task I send to the model will be wrong.
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然后，我发送给模型的任何其他任务都会是错误的。
- en: This is why Self-Consistency involves sampling from various reasoning paths,
    each of them containing a chain of thought, and then letting the LLM choose the
    best and most consistent path to solve the problem.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么自我一致性涉及从各种推理路径中采样，每条路径包含一链思路，然后让LLM选择最佳和最一致的路径来解决问题。
- en: '![Some Kick Ass Prompt Engineering Techniques to Boost our LLM Models](../Images/07b661d8f983b82ca1f7f2e6938338de.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![一些提升我们LLM模型的绝佳提示工程技巧](../Images/07b661d8f983b82ca1f7f2e6938338de.png)'
- en: Image taken from the [paper](https://arxiv.org/abs/2203.11171?ref=mlq.ai)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图片摘自[论文](https://arxiv.org/abs/2203.11171?ref=mlq.ai)
- en: In this case, and following the very first example again, we can show the model
    different ways of solving the problem.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，并且再次按照最初的例子，我们可以向模型展示不同的解决问题的方法。
- en: '**<User>** If John has 5 pears, then eats 2, buys 5 more, then gives 3 to his
    friend, how many pears does he have?'
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**<User>** 如果约翰有5个梨子，然后吃掉2个，再买5个，然后给朋友3个，他还有多少个梨子？'
- en: ''
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**<Agent>** Start with 5 pears. John eats 2 pears, leaving him with 5–2 = 3
    pears. He buys 5 more pears, which brings the total to 3 + 5 = 8 pears. Finally,
    he gives 3 pears to his friend, so he has 8–3 = 5 pears left.'
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**<Agent>** 从5个梨子开始。约翰吃了2个梨子，剩下5–2 = 3个梨子。他再买了5个梨子，总数变为3 + 5 = 8个梨子。最后，他给了朋友3个梨子，所以他剩下8–3
    = 5个梨子。'
- en: ''
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**<User>** If John has 5 pears, then eats 2, buys 5 more, then gives 3 to his
    friend, how many pears does he have?'
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**<User>** 如果约翰有5个梨子，然后吃掉2个，再买5个，然后给朋友3个，他还有多少个梨子？'
- en: ''
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**<Agent>** Start with 5 pears. He then buys 5 more pears. John eats 2 pears
    now.  These actions can be combined: 2 (eaten) + 5 (bought) = 7 pears in total.
    Subtract the pear that Jon has eaten from the total amount of pears 7 (total amount)
    - 2 (eaten) = 5 pears left.'
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**<Agent>** 从5个梨子开始。然后他再买5个梨子。约翰现在吃了2个梨子。这些操作可以组合在一起：2（吃掉的）+ 5（买的）= 总共7个梨子。从总数中减去约翰吃掉的梨子，7（总数）
    - 2（吃掉的）= 剩下5个梨子。'
- en: ''
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**<User>** Your input here.'
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**<User>** 你的输入在这里。'
- en: And here comes the last technique.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的技巧来了。
- en: 4\. General Knowledge Prompting
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 一般知识提示
- en: A common practice of prompt engineering is augmenting a query with additional
    knowledge before sending the final API call to GPT-3 or GPT-4.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程的一个常见做法是，在发送最终的API调用到GPT-3或GPT-4之前，使用额外的知识来增强查询。
- en: According to [Jiacheng Liu and Co](https://arxiv.org/pdf/2110.08387.pdf?ref=mlq.ai),
    we can always add some knowledge to any request so the LLM knows better about
    the question.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[Jiacheng Liu和其他人](https://arxiv.org/pdf/2110.08387.pdf?ref=mlq.ai)的说法，我们可以在任何请求中添加一些知识，以便LLM更好地了解问题。
- en: '![Some Kick Ass Prompt Engineering Techniques to Boost our LLM Models](../Images/4189d478dcc00efd9c7334b8a686de91.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![一些顶级提示工程技巧来提升我们的LLM模型](../Images/4189d478dcc00efd9c7334b8a686de91.png)'
- en: Image taken from the [paper](https://arxiv.org/pdf/2110.08387.pdf?ref=mlq.ai).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图片取自[论文](https://arxiv.org/pdf/2110.08387.pdf?ref=mlq.ai)。
- en: So for instance, when asking ChatGPT if part of golf is trying to get a higher
    point total than others, it will validate us. But, the main goal of golf is quite
    the opposite. This is why we can add some previous knowledge telling it “The player
    with the lower score wins”.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当询问ChatGPT高尔夫的一部分是否是试图获得比其他人更高的分数时，它会验证我们。但高尔夫的主要目标正好相反。这就是为什么我们可以添加一些先前的知识告诉它“得分较低的玩家获胜”。
- en: '![Some Kick Ass Prompt Engineering Techniques to Boost our LLM Models](../Images/5a397bad9a0e8a5af2f46cb6b5923543.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![一些顶级提示工程技巧来提升我们的LLM模型](../Images/5a397bad9a0e8a5af2f46cb6b5923543.png)'
- en: So.. what’s the funny part if we are telling the model exactly the answer?
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果我们准确地告诉模型答案，有什么有趣的部分呢？
- en: In this case, this technique is used to improve the way LLM interacts with us.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，这种技术用于改善LLM与我们的互动方式。
- en: So rather than pulling supplementary context from an outside database, the paper's
    authors recommend having the LLM produce its own knowledge. This self-generated
    knowledge is then integrated into the prompt to bolster commonsense reasoning
    and give better outputs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，与其从外部数据库中提取补充背景，论文的作者建议让LLM生成自己的知识。然后将这些自生成的知识整合到提示中，以增强常识推理并提供更好的输出。
- en: So this is how LLMs can be improved without increasing its training dataset!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是如何在不增加训练数据集的情况下改进LLM！
- en: Concluding Thoughts
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Prompt engineering has emerged as a pivotal technique in enhancing the capabilities
    of LLM. By iterating and improving prompts, we can communicate in a more direct
    manner to AI models and thus obtain more accurate and contextually relevant outputs,
    saving both time and resources.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程已成为增强LLM能力的关键技术。通过迭代和改进提示，我们可以更直接地与AI模型沟通，从而获得更准确和上下文相关的输出，节省时间和资源。
- en: For tech enthusiasts, data scientists, and content creators alike, understanding
    and mastering prompt engineering can be a valuable asset in harnessing the full
    potential of AI.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于技术爱好者、数据科学家和内容创作者而言，理解和掌握提示工程可以在充分利用AI的潜力方面成为一种宝贵的资产。
- en: By combining carefully designed input prompts with these more advanced techniques,
    having the skill set of prompt engineering will undoubtedly give you an edge in
    the coming years.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将精心设计的输入提示与这些更先进的技术相结合，掌握提示工程的技能无疑将在未来几年为你提供优势。
- en: '**[](https://www.linkedin.com/in/josep-ferrer-sanchez/)**[Josep Ferrer](https://www.linkedin.com/in/josep-ferrer-sanchez)****
    is an analytics engineer from Barcelona. He graduated in physics engineering and
    is currently working in the data science field applied to human mobility. He is
    a part-time content creator focused on data science and technology. Josep writes
    on all things AI, covering the application of the ongoing explosion in the field.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**[](https://www.linkedin.com/in/josep-ferrer-sanchez/)**[Josep Ferrer](https://www.linkedin.com/in/josep-ferrer-sanchez)****
    是一位来自巴塞罗那的分析工程师。他毕业于物理工程专业，目前从事应用于人类流动的数据科学工作。他还是一位兼职内容创作者，专注于数据科学和技术。Josep 写作涉及所有AI相关的内容，涵盖了该领域正在爆炸性增长的应用。'
- en: More On This Topic
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Kick Ass Midjourney Prompts with Poe](https://www.kdnuggets.com/kick-ass-midjourney-prompts-with-poe)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[顶级Midjourney提示与Poe](https://www.kdnuggets.com/kick-ass-midjourney-prompts-with-poe)'
- en: '[Prompt Engineering 101: Mastering Effective LLM Communication](https://www.kdnuggets.com/prompt-engineering-101-mastering-effective-llm-communication)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提示工程101：掌握有效的LLM沟通](https://www.kdnuggets.com/prompt-engineering-101-mastering-effective-llm-communication)'
- en: '[Things Aren''t Always Normal: Some of the "Other" Distributions](https://www.kdnuggets.com/2023/01/things-arent-always-normal-distributions.html)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[事情不总是正常的：一些“其他”分布](https://www.kdnuggets.com/2023/01/things-arent-always-normal-distributions.html)'
- en: '[It''s alive! Build your first robots with Python and some cheap,…](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[它活了！使用 Python 和一些便宜的组件构建你的第一个机器人…](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
- en: '[I Used ChatGPT (Every Day) for 5 Months. Here Are Some Hidden Gems…](https://www.kdnuggets.com/2023/07/used-chatgpt-every-day-5-months-hidden-gems-change-life.html)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我每天使用 ChatGPT 达 5 个月。这里有一些隐藏的宝石…](https://www.kdnuggets.com/2023/07/used-chatgpt-every-day-5-months-hidden-gems-change-life.html)'
- en: '[RAG vs Finetuning: Which Is the Best Tool to Boost Your LLM Application?](https://www.kdnuggets.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[RAG 与微调：哪种工具最能提升你的LLM应用？](https://www.kdnuggets.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application)'
