- en: PyTorch Tips to Boost Your Productivity
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 提升生产力的技巧
- en: 原文：[https://www.kdnuggets.com/2023/08/pytorch-tips-boost-productivity.html](https://www.kdnuggets.com/2023/08/pytorch-tips-boost-productivity.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/08/pytorch-tips-boost-productivity.html](https://www.kdnuggets.com/2023/08/pytorch-tips-boost-productivity.html)
- en: '![PyTorch Tips to Boost Your Productivity](../Images/8c8e68e22a2d3366038e409b423555d4.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![PyTorch 提升生产力的技巧](../Images/8c8e68e22a2d3366038e409b423555d4.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Have you ever spent hours debugging a machine learning model but can’t seem
    to find a reason the accuracy does not improve? Have you ever felt everything
    should work perfectly but for some mysterious reason you are not getting exemplary
    results?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾经花了几个小时来调试一个机器学习模型，却无法找到准确率没有提高的原因？你是否觉得一切都应该完美无缺，但由于某种神秘原因，你却没有得到理想的结果？
- en: Well no more. Exploring PyTorch as a beginner can be daunting. In this article,
    you explore tried and tested workflows that will surely improve your results and
    boost your model’s performance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不再如此。对于初学者来说，探索 PyTorch 可能会令人望而却步。在这篇文章中，你将探索经过验证的工作流程，这些工作流程肯定会提高你的结果并提升你模型的性能。
- en: 1\. Overfit a Single Batch
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 对单个批次进行过拟合
- en: Ever trained a model for hours on a large dataset just to find the loss isn’t
    decreasing and the accuracy just flattens? Well, do a sanity check first.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 是否曾经在一个大型数据集上训练模型几个小时，却发现损失没有下降，准确率也没有改善？那么，首先做一个合理性检查。
- en: It can be time-consuming to train and evaluate on a large dataset, and it is
    easier to first debug models on a small subset of the data. Once we are sure the
    model is working, we can then easily scale training to the complete dataset.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型数据集上训练和评估可能是耗时的，首先在小数据子集上调试模型更容易。一旦我们确定模型正常工作，就可以轻松地将训练扩展到完整数据集上。
- en: Instead of training on the whole dataset, **always train on a single batch for
    a sanity check**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 不要在整个数据集上进行训练，**总是对单个批次进行训练以进行合理性检查**。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Consider the above code snippet. Assume we already have a training data loader
    and a model. Instead of iterating over the complete dataset, we can easily fetch
    the first batch of the dataset. We can then train on the single batch to check
    if the model can learn the patterns and variance within this small portion of
    the data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑上述代码片段。假设我们已经有了一个训练数据加载器和一个模型。与其遍历整个数据集，不如轻松获取数据集的第一个批次。然后我们可以对这个单独的批次进行训练，以检查模型是否能够学习这个小数据部分中的模式和变化。
- en: 'If the loss decreases to a very small value, we know the model can overfit
    this data and can be sure it is learning in a short time. We can then train this
    on the complete dataset by simply changing a single line as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果损失减少到非常小的值，我们知道模型可能会过拟合这些数据，并且可以确定它在短时间内已经学到了东西。然后，我们可以通过简单地修改一行代码在完整数据集上进行训练，如下所示：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If the model can overfit a single batch, it should be able to learn the patterns
    in the complete dataset. This overfitting batch method enables easier debugging.
    If the model can not even overfit a single batch, we can be sure there is a problem
    with the model implementation and not the dataset.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型能够对单个批次进行过拟合，它应该能够学习完整数据集中的模式。这种过拟合批次的方法使调试变得更加容易。如果模型甚至不能对单个批次进行过拟合，我们可以确定问题出在模型实现上，而不是数据集上。
- en: 2\. Normalize and Shuffle Data
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 数据标准化和洗牌
- en: '*For datasets where the sequence of data is not important, it is helpful to
    shuffle the data*. For example, for the image classification tasks, the model
    will fit the data better if it is fed images of different classes within a single
    batch. **Passing data in the same sequence, we risk the model learning the patterns
    based on the sequence of data passed, instead of learning the intrinsic variance
    within the data.** Therefore, it is better to pass shuffled data. For this, we
    can simply use the DataLoader object provided by PyTorch and set shuffle to True.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于数据顺序不重要的数据集，打乱数据是有帮助的*。例如，对于图像分类任务，如果模型在单个批次中接收到不同类别的图像，它会更好地拟合数据。**以相同的顺序传递数据，我们冒着模型根据数据传递的顺序学习模式的风险，而不是学习数据内部的固有方差。**
    因此，传递打乱后的数据更为合适。为此，我们可以简单地使用 PyTorch 提供的 DataLoader 对象，并将 shuffle 设置为 True。'
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Moreover, it is important to normalize data when using machine learning models.
    It is essential when there is a large variance in our data, and a particular parameter
    has higher values than all the other attributes in the dataset. This can cause
    one of the parameters to dominate all the others, resulting in lower accuracy.
    **We want all input parameters to fall within the same range, and it is better
    to have 0 mean and 1.0 variance.** For this, we have to transform our dataset.
    Knowing the mean and variance of the dataset, we can simply use the torchvision.transforms.Normalize
    function.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在使用机器学习模型时，规范化数据是非常重要的。尤其是在数据方差较大且某个特定参数的值高于数据集中其他所有属性时，这一点尤为关键。这可能导致某个参数主导其他所有参数，从而降低模型的准确性。**我们希望所有输入参数都在相同范围内，最好是均值为0，方差为1.0。**
    为此，我们必须转换数据集。了解数据集的均值和方差后，我们可以简单地使用 torchvision.transforms.Normalize 函数。
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can pass our per-channel mean and standard deviation in the transforms.Normalize
    function, and it will automatically convert the data having 0 mean and a standard
    deviation of 1.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 transforms.Normalize 函数中传递每通道的均值和标准差，它会自动将数据转换为均值为 0 和标准差为 1。
- en: 3\. Gradient Clipping
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 梯度裁剪
- en: Exploding gradient is a known problem in RNNs and LSTMs. However, it is not
    only limited to these architectures. Any model with deep layers can suffer from
    exploding gradients. Backpropagation on high gradients can lead to divergence
    instead of a gradual decrease in loss.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度爆炸是 RNN 和 LSTM 中的一个已知问题。然而，这个问题不仅限于这些架构。任何具有深层的模型都可能遭遇梯度爆炸。高梯度的反向传播可能导致发散，而不是损失的逐渐减少。
- en: Consider the below code snippet.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 参考下面的代码片段。
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To solve the exploding gradient problem, we use the gradient clipping technique
    that clips gradient values within a specified range. For example, if we use 1
    as our clipping or norm value as above, all gradients will be clipped in the [-1,
    1] range. If we have an exploding gradient value of 50, it will be clipped to
    1\. Thus, **gradient clipping resolves the exploding gradient problem allowing
    a slow optimization of the model toward convergence.**
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决梯度爆炸问题，我们使用梯度裁剪技术，将梯度值裁剪到指定范围内。例如，如果我们使用 1 作为裁剪值或规范值，如上所述，所有梯度将被裁剪到 [-1,
    1] 范围内。如果我们遇到 50 的梯度爆炸值，它将被裁剪到 1。**因此，梯度裁剪解决了梯度爆炸问题，允许模型向收敛方向进行缓慢优化。**
- en: 4\. Toggle Train / Eval Mode
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 切换训练 / 评估模式
- en: This single line of code will surely increase your model’s test accuracy. Almost
    always, a deep learning model will use dropout and normalization layers. These
    are only required for stable training and ensuring the model does not either overfit
    or diverge because of variance in data. Layers such as BatchNorm and Dropout offer
    regularization for model parameters during training. However, once trained they
    are not required. **Changing a model to evaluation mode disables layers only required
    for training and the complete model parameters are used for prediction.**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这一行代码将显著提高你的模型测试准确性。几乎所有深度学习模型都会使用 dropout 和归一化层。这些层仅在稳定训练和确保模型不会因数据方差而过拟合或发散时才需要。像
    BatchNorm 和 Dropout 这样的层在训练过程中提供模型参数的正则化。然而，一旦训练完成，这些层就不再需要。**将模型切换到评估模式可以禁用仅在训练中需要的层，从而使模型的所有参数都用于预测。**
- en: For a better understanding, consider this code snippet.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，请考虑以下代码片段。
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: When evaluating, we do not need to make any optimization of model parameters.
    We do not compute any gradients during validation steps. For a better evaluation,
    we can then omit the Dropout and other normalization layers. For example, it will
    enable all model parameters instead of only a subset of weights like in the Dropout
    layer. This will substantially increase the model’s accuracy as you will be able
    to use the complete model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估时，我们不需要对模型参数进行任何优化。在验证步骤中我们不会计算任何梯度。为了更好的评估，我们可以省略 Dropout 和其他归一化层。例如，这将启用所有模型参数，而不是像
    Dropout 层中那样仅启用部分权重。这将显著提高模型的准确性，因为你将能够使用完整的模型。
- en: 5\. Use Module and ModuleList
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 使用 Module 和 ModuleList
- en: 'PyTorch model usually inherits from the torch.nn.Module base class. As per
    the documentation:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 模型通常继承自 torch.nn.Module 基类。根据文档：
- en: '*Submodules assigned in this way will be registered and will have their parameters
    converted too when you call* [*to()*](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to)*,
    etc.*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*以这种方式分配的子模块将被注册，并且当你调用* [*to()*](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to)*，等等时，它们的参数也会被转换。*'
- en: What the module base class allows is registering each layer within the model.
    We can then use model.to() and similar functions such as model.train() and model.eval()
    and they will be applied to each layer within the model. Failing to do so, will
    not change the device or training mode for each layer contained within the model.
    You will have to do it manually. **The Module base class will automatically make
    the conversions for you once you use a function simply on the model object.**
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 模块基类允许在模型中注册每一层。然后我们可以使用 `model.to()` 以及类似的函数，如 `model.train()` 和 `model.eval()`，这些函数将应用到模型中的每一层。如果不这样做，将不会更改模型中每一层的设备或训练模式，你需要手动进行。**模块基类一旦在模型对象上使用函数，将自动为你进行转换。**
- en: Moreover, some models contain similar sequential layers that can be easily initialized
    using a for loop and contained within a list. This simplifies the code. However,
    it causes the same problem as above, as the modules within a simple Python List
    are not registered automatically within the model. **We should use a ModuleList
    for containing similar sequential layers within a model.**
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些模型包含类似的顺序层，可以使用 for 循环轻松初始化并放在列表中。这简化了代码。然而，这也会导致上述问题，因为简单的 Python 列表中的模块不会自动在模型中注册。**我们应该使用
    ModuleList 来包含模型中的类似顺序层。**
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The above code snippet shows the proper way of creating the model and sublayers
    with the model. Th use of Module and ModuleList helps avoid unexpected errors
    when training and evaluating the model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段展示了创建模型及其子层的正确方法。使用 Module 和 ModuleList 可以避免在训练和评估模型时出现意外错误。
- en: Conclusion
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The above mentioned methods are the best practices for the PyTorch machine learning
    framework. They are widely used and are recommended by the PyTorch documentation.
    Using such methods should be the primary way of a machine learning code flow,
    and will surely improve your results.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法是 PyTorch 机器学习框架的最佳实践。它们被广泛使用，并且得到了 PyTorch 文档的推荐。使用这些方法应该是机器学习代码流的主要方式，并且肯定会提高你的结果。
- en: '**[Muhammad Arham](https://www.linkedin.com/in/muhammad-arham-a5b1b1237/)**
    is a Deep Learning Engineer working in Computer Vision and Natural Language Processing.
    He has worked on the deployment and optimizations of several generative AI applications
    that reached the global top charts at Vyro.AI. He is interested in building and
    optimizing machine learning models for intelligent systems and believes in continual
    improvement.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**[穆罕默德·阿赫姆](https://www.linkedin.com/in/muhammad-arham-a5b1b1237/)** 是一位从事计算机视觉和自然语言处理的深度学习工程师。他曾在
    Vyro.AI 上工作，部署和优化了多个生成 AI 应用程序，这些应用程序在全球排行榜上名列前茅。他对构建和优化智能系统的机器学习模型感兴趣，并相信持续改进。'
- en: More On This Topic
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[From Zero to Hero: Create Your First ML Model with PyTorch](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从零到英雄：使用 PyTorch 创建你的第一个 ML 模型](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
- en: '[Maximize Your Productivity as a Data Scientist by Organizing](https://www.kdnuggets.com/2022/03/maximize-productivity-data-scientist-organizing.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过组织工作最大化你的数据科学生产力](https://www.kdnuggets.com/2022/03/maximize-productivity-data-scientist-organizing.html)'
- en: '[Top 6 Tools to Improve Your Productivity on Snowflake](https://www.kdnuggets.com/2023/08/top-6-tools-improve-productivity-snowflake.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提升你在 Snowflake 上生产力的 6 个顶级工具](https://www.kdnuggets.com/2023/08/top-6-tools-improve-productivity-snowflake.html)'
- en: '[6 ChatGPT Prompts to Enhance your Productivity at Work](https://www.kdnuggets.com/6-chatgpt-prompts-to-enhance-your-productivity-at-work)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6 个 ChatGPT 提示提升你在工作中的生产力](https://www.kdnuggets.com/6-chatgpt-prompts-to-enhance-your-productivity-at-work)'
- en: '[Boost your machine learning model performance!](https://www.kdnuggets.com/2023/04/manning-boost-machine-learning-model-performance.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提升你的机器学习模型性能！](https://www.kdnuggets.com/2023/04/manning-boost-machine-learning-model-performance.html)'
- en: '[5 ChatGPT Features to Boost your Daily Work](https://www.kdnuggets.com/2023/05/5-chatgpt-features-boost-daily-work.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5 个 ChatGPT 功能助力你的日常工作](https://www.kdnuggets.com/2023/05/5-chatgpt-features-boost-daily-work.html)'
