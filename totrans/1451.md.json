["```py\nimport pandas as pd \nimport numpy as np  \nimport scipy as sp  \nimport seaborn as sns\nsns.set_style('darkgrid') \nimport pickle       \nimport regex as re  \nimport gensimfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction import stop_words\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizerimport matplotlib.pyplot as plt \nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.naive_bayes import MultinomialNB  \nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropoutimport warnings\nwarnings.filterwarnings('ignore')%matplotlib inline\n```", "```py\n# Loads in the data into a Pandas data frame\nfcc_csv = pd.read_csv('./data/CSRIC_Best_Practices.csv')\nfcc_csv.head()\n```", "```py\n# Here is a function for basic exploratory data analysis:def eda(dataframe):\n    # Replace any blank spaces w/ a underscore.\n    dataframe.columns = dataframe.columns.str.replace(\" \", \"_\")\n    # Checks for the null values.\n    print(\"missing values{}\".format(dataframe.isnull().sum().sum()))\n    # Checks the data frame range size.\n    print(\"dataframe index: {}\".format(dataframe.index))\n    # Checks for data types of the columns within the data frame.\n    print(\"dataframe types: {}\".format(dataframe.dtypes))\n    # Checks the shape of the data frame.\n    print(\"dataframe shape: {}\".format(dataframe.shape))\n    # Gives us any statistical information of the data frame.\n    print(\"dataframe describe: {}\".format(dataframe.describe()))\n    # Gives us the duplicated data of the data frame. print(\"duplicates{}\".format(dataframe[dataframe.duplicated()].sum()))\n    # A for loop that does this for every single column & their \n    # values within our data frame giving us all unique values.\n    for item in dataframe:\n        print(item)\n        print(dataframe[item].nunique())# Let's apply this function to our entire data frame.\neda(fcc_csv)\n```", "```py\n# Here's a function to convert NaN's in the data set to 'None' for \n# string objects.\n# Just pass in the entire data frame.\ndef convert_str_nan(data):\n    return data.astype(object).replace(np.nan, 'None', inplace = True)convert_str_nan(fcc_csv)\n```", "```py\n# Let's rename the 'Priority_(1,2,3)' column so we can utilize it.\nfcc_csv.rename(columns = {\n    'Priority_(1,2,3)': 'Priorities'\n},\ninplace = True)# Let's view the values & how the correspond to the 'Priority' \n# column.\nfcc_csv['Priorities'].value_counts()# We notice that we have an unbalanced classification problem.\n# Let's group the \"Highly Important\" (2) & \"Critical\" (3) aspects \n# because that's where we can make recommendations.\n# Let's double check that it worked.\nfcc_csv['Priorities'] = [0 if i == 1 else 1 for i in fcc_csv['Priorities']]\nfcc_csv['Priorities'].value_counts()\n```", "```py\n# Let's view the largest negative correlated columns to our \n# \"Priorities\" column.\nlargest_neg_corr_list = fcc_csv.corr()[['Priorities']].sort_values('Priorities').head(5).T.columns\nlargest_neg_corr_list# Let's view the largest positive correlated columns to our \n# \"Priorities\" column.\nlargest_pos_corr_list = fcc_csv.corr()[['Priorities']].sort_values('Priorities').tail(5).T.columns.drop('Priorities')\nlargest_pos_corr_list\n```", "```py\n# Let's pass in every column that is categorical into our X.\n# These are the strongest & weakest correlated columns to our \n# \"Priorities\" variable. \nX = fcc_csv[['Network_Operator', 'Equipment_Supplier', 'Property_Manager', 'Service_Provider', 'wireline', 'wireless', 'satellite', 'Public_Safety']]\ny = fcc_csv['Priorities'] # Our y is what we want to predict.# We have to train/test split the data so we can model the data on \n# our training set & test it.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)# We need to transpose the trains so they contain the same amount of # rows.\nX = X.transpose()\n```", "```py\n# Instantiates the Naive Bayes classifier.\nmnb = MultinomialNB()\nparams = {'min_samples_split':[12, 25, 40]}# Grid searches our Naive Bayes.\nmnb_grid = {}\ngs_mnb = GridSearchCV(mnb, param_grid = mnb_grid, cv = 3)\ngs_mnb.fit(X_train, y_train)\ngs_mnb.score(X_train, y_train)# Scores the Naive Bayes.\ngs_mnb.score(X_test, y_test)\n```", "```py\n# Instantiates the random forest classifier.\nrf = RandomForestClassifier(n_estimators = 10)# Grid searches our random forest classifier.\ngs_rf = GridSearchCV(rf, param_grid = params, return_train_score = True, cv = 5)\ngs_rf.fit(X_train, y_train)\ngs_rf.score(X_train, y_train)# Our random forest test score.\ngs_rf.score(X_test, y_test)\n```", "```py\nscores_test = []\nscores_train = []\nn_estimators = []for n_est in range(30):\n    ada = AdaBoostClassifier(n_estimators = n_est + 1, random_state = 42)\n    ada.fit(X_train, y_train)\n    n_estimators.append(n_est + 1)\n    scores_test.append(ada.score(X_test, y_test))\n    scores_train.append(ada.score(X_train, y_train))# Our Ada Boost score on our train set.\nada.score(X_train, y_train)# Our Ada Boost score on our test set.\nada.score(X_test, y_test)\n```", "```py\nmodel_dropout = Sequential()n_input = X_train.shape[1]\nn_hidden = n_inputmodel_dropout.add(Dense(n_hidden, input_dim = n_input, activation = 'relu'))\nmodel_dropout.add(Dropout(0.5)) # refers to nodes in the first hidden layer\nmodel_dropout.add(Dense(1, activation = 'sigmoid'))model_dropout.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['acc'])history_dropout = model_dropout.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 100, batch_size = None)\n```", "```py\n# Let's clean the data using Regex.\n# Let's use regex to remove the words: service providers, equipment # suppliers, network operators, property managers, public safety\n# Let's also remove any mention of any URLs.fcc_csv['Description'] = fcc_csv.Description.map(lambda x: re.sub('\\s[\\/]?r\\/[^s]+', ' ', x))\nfcc_csv['Description'] = fcc_csv.Description.map(lambda x: re.sub('http[s]?:\\/\\/[^\\s]*', ' ', x))\nfcc_csv['Description'] = fcc_csv.Description.map(lambda x: re.sub('(service providers|equipment suppliers|network operators|property managers|public safety)[s]?', ' ', x,  flags = re.I))\n```", "```py\n# This is a text preprocessing function that gets our data ready for # modeling & creates new columns for the \n# description text in their tokenized, lemmatized & stemmed forms. \n# This allows for easy selection of \n# different forms of the text for use in vectorization & modeling.def preprocessed_columns(dataframe = fcc_csv, \n                        column = 'Description', \n                        new_lemma_column = 'lemmatized', \n                        new_stem_column = 'stemmed',\n                        new_token_column = 'tokenized',\n                        regular_expression = r'\\w+'): \n\n    tokenizer = RegexpTokenizer(regular_expression)     \n    lemmatizer = WordNetLemmatizer()                     \n    stemmer = PorterStemmer()                            \n\n    lemmatized = []                                      \n    stemmed = []                                         \n    tokenized = []\n\n    for i in dataframe[column]:                        \n        tokens = tokenizer.tokenize(i.lower())           \n        tokenized.append(tokens)        lemma = [lemmatizer.lemmatize(token) for token in tokens]     \n        lemmatized.append(lemma)                                              stems = [stemmer.stem(token) for token in tokens]            \n        stemmed.append(stems)                                         \n\n    dataframe[new_token_column] = [' '.join(i) for i in tokenized]    \n    dataframe[new_lemma_column] = [' '.join(i) for i in lemmatized]   \n    dataframe[new_stem_column] = [' '.join(i) for i in stemmed]   \n\n    return dataframe\n```", "```py\n# Instantiate a CountVectorizer removing english stopwords, ngram \n# range of unigrams & bigrams.cv = CountVectorizer(stop_words = 'english', ngram_range = (1,2), min_df = 25, max_df = .95)# Create a dataframe of our CV transformed tokenized words\ncv_df_token = pd.SparseDataFrame(cv.fit_transform(processed['tokenized']), columns = cv.get_feature_names())\ncv_df_token.fillna(0, inplace = True)cv_df_token.head()\n```", "```py\n# Split our data frame into really \"important\" & \"not important\" \n# columns.\n# We will use the \"really_important\" descriptions to determine \n# severity & to give recommendations/analysis.\nfcc_really_important = processed[processed['Priorities'] == 1]\nfcc_not_important = processed[processed['Priorities'] == 0]print(fcc_really_important.shape)\nprint(fcc_not_important.shape)\n```", "```py\nX_1 = processed['tokenized']# We're train test splitting 3 different columns.\n# These columns are the tokenized, lemmatized & stemmed from the \n# processed dataframe.\nX_1_train, X_1_test, y_train, y_test = train_test_split(X_1, y, test_size = 0.3, stratify = y, random_state = 42)\n```", "```py\npipe_cv = Pipeline([\n    ('cv', CountVectorizer()),\n    ('lr', LogisticRegression())\n])params = {\n    'lr__C':[0.6, 1, 1.2],\n    'lr__penalty':[\"l1\", \"l2\"],\n    'cv__max_features':[None, 750, 1000, 1250],\n    'cv__stop_words':['english', None],\n    'cv__ngram_range':[(1,1), (1,4)]\n}\n```", "```py\n# Our Logistic Regression Model.\ngs_lr_tokenized_cv = GridSearchCV(pipe_cv, param_grid = params, cv = 5)\ngs_lr_tokenized_cv.fit(X_1_train, y_train)\ngs_lr_tokenized_cv.score(X_1_train, y_train)gs_lr_tokenized_cv.score(X_1_test, y_test)\n```", "```py\ngs_lr_tokenized_cv.best_params_\n```", "```py\ncoefs = gs_lr_tokenized_cv.best_estimator_.steps[1][1].coef_\nwords = pd.DataFrame(zip(cv.get_feature_names(), np.exp(coefs[0])))\nwords = words.sort_values(1)\n```"]