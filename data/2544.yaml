- en: Mastering Clustering with a Segmentation Problem
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 掌握具有细分问题的聚类
- en: 原文：[https://www.kdnuggets.com/2021/08/mastering-clustering-segmentation-problem.html](https://www.kdnuggets.com/2021/08/mastering-clustering-segmentation-problem.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/08/mastering-clustering-segmentation-problem.html](https://www.kdnuggets.com/2021/08/mastering-clustering-segmentation-problem.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Indraneel Dutta Baruah](https://indraneeldb1993ds.medium.com/), AI Driven
    Solutions Developer**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Indraneel Dutta Baruah](https://indraneeldb1993ds.medium.com/) 提供，人工智能驱动的解决方案开发者**'
- en: '![](../Images/62dfc3ee20b01cdf165d6a7406f9fb4a.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62dfc3ee20b01cdf165d6a7406f9fb4a.png)'
- en: Photo by [Mel Poole](https://unsplash.com/@melipoole) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Mel Poole](https://unsplash.com/@melipoole) 提供，发布于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In the current age, the availability of granular data for a large pool of customers/products
    and technological capability to handle petabytes of data efficiently is growing
    rapidly. Due to this, it’s now possible to come up with very strategic and meaningful
    clusters for effective targeting. And identifying the target segments requires
    a robust segmentation exercise. In this blog, we will be discussing the most popular
    algorithms for unsupervised clustering algorithms and how to implement them in
    python.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前时代，针对大量客户/产品的详细数据的可用性和处理PB级数据的技术能力正在迅速增长。因此，现在可以提出非常有战略意义和有效的目标群体集群。识别目标细分需要一个强大的细分过程。在这篇博客中，我们将讨论最流行的无监督聚类算法及其在python中的实现方法。
- en: In this blog, we will be working with clickstream [data](https://archive.ics.uci.edu/ml/datasets/clickstream+data+for+online+shopping) from
    an online store offering clothing for pregnant women. It includes variables like
    product category, location of the photo on the webpage, country of origin of the
    IP address and product price in US dollars. It has data from April 2008 to August
    2008.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我们将使用来自在线商店的点击流 [数据](https://archive.ics.uci.edu/ml/datasets/clickstream+data+for+online+shopping)，该商店提供孕妇服装。数据包括产品类别、网页上照片的位置、IP地址的来源国家和以美元计的产品价格。数据时间范围从2008年4月到2008年8月。
- en: '*The first step is to prepare the data for segmentation. I encourage you to
    check out the article below for an in-depth explanation of different steps for
    preparing data for segmentation before proceeding further:*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*第一步是为细分准备数据。我建议你查看下面的文章，以获取有关数据准备的详细说明，然后再继续：*'
- en: '[*One Hot Encoding, Standardization, PCA: Data preparation for segmentation
    in python*](https://towardsdatascience.com/one-hot-encoding-standardization-pca-data-preparation-steps-for-segmentation-in-python-24d07671cf0b)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[*独热编码、标准化、PCA：Python中细分的数据准备步骤*](https://towardsdatascience.com/one-hot-encoding-standardization-pca-data-preparation-steps-for-segmentation-in-python-24d07671cf0b)'
- en: '*Selecting the optimal number of clusters is another key concept one should
    be aware of while dealing with a segmentation problem. It will be helpful if you
    read the article below for understanding a comprehensive list of popular metrics
    for selecting clusters:*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*选择最佳聚类数是处理细分问题时应了解的另一个关键概念。如果你阅读下面的文章，将有助于理解选择聚类的全面指标列表：*'
- en: '[*Cheatsheet for implementing 7 methods for selecting the optimal number of
    clusters in Python*](https://towardsdatascience.com/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[*在Python中实现选择最佳聚类数的7种方法的备忘单*](https://towardsdatascience.com/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad)'
- en: 'We will be talking about 4 categories of models in this blog:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我们将讨论4类模型：
- en: K-means
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K-means
- en: Agglomerative clustering
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 凝聚层次聚类
- en: Density-based spatial clustering (DBSCAN)
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于密度的空间聚类（DBSCAN）
- en: Gaussian Mixture Modelling (GMM)
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高斯混合模型（GMM）
- en: '**K-means**'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**K-means**'
- en: 'The K-means algorithm is an iterative process with three critical stages:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: K-means算法是一个迭代过程，包括三个关键阶段：
- en: '**1\. Pick initial cluster centroids**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 选择初始聚类中心**'
- en: The algorithm starts by picking initial k cluster centers which are known as
    centroids. Determining the optimal number of clusters i.e k as well as proper
    selection of the initial clusters is extremely important for the performance of
    the model. The number of clusters should always be dependent on the nature of
    the dataset while poor selection of the initial cluster can lead to the problem
    of local convergence. Thankfully, we have solutions for both.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法首先选择初始k个聚类中心，即质心。确定最佳聚类数k以及正确选择初始聚类对模型性能至关重要。聚类数应始终依赖于数据集的性质，而初始聚类选择不当可能会导致局部收敛问题。幸运的是，我们有解决这两个问题的方法。
- en: 'For further details on selecting the optimal number of clusters please refer
    to this detailed [blog](https://towardsdatascience.com/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad).
    For selection of initial clusters, we can either run multiple iterations of the
    model with various initializations to pick the most stable one or use the “k-means++”
    algorithm which has the following steps:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有关选择最佳聚类数的更多细节，请参阅这篇详细的[博客](https://towardsdatascience.com/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad)。对于初始聚类的选择，我们可以通过运行多个不同初始化的模型迭代来选择最稳定的聚类，或者使用“k-means++”算法，其步骤如下：
- en: '*Randomly select the first centroid from the dataset*'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*从数据集中随机选择第一个质心*'
- en: '*Compute distance of all points in the dataset from the selected centroid*'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*计算数据集中所有点与选择的质心之间的距离*'
- en: '*Pick a point as the new centroid that has maximum probability proportional
    to this distance*'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*选择具有最大概率的点作为新的质心，按距离的比例*'
- en: '*Repeat steps 2 and 3 until k centroids have been sampled*'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*重复步骤2和3，直到采样到k个质心*'
- en: The algorithm initializes the centroids to be distant from each other leading
    to more stable results than random initialization.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法将质心初始化为彼此远离，从而比随机初始化更稳定。
- en: '**2\. Cluster assignment**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**2. 聚类分配**'
- en: K-means then assigns the data points to the closest cluster centroids based
    on euclidean distance between the point and all centroids.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: K-means然后根据点到所有质心的欧几里得距离，将数据点分配到最近的聚类质心。
- en: '**3\. Move centroid**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**3. 移动质心**'
- en: The model finally calculates the average of all the points in a cluster and
    moves the centroid to that average location.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 模型最终计算聚类中所有点的平均值，并将质心移动到该平均位置。
- en: Step 2 and 3 are repeated until there is no change in the clusters or possibly
    some other stopping condition is met (like maximum number of iterations).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤2和3会重复，直到聚类没有变化或满足其他停止条件（如最大迭代次数）。
- en: 'For implementing the model in python we need to do specify the number of clusters
    first. We have used the elbow method, Gap Statistic, Silhouette score, Calinski
    Harabasz score and Davies Bouldin score. For each of these methods the optimal
    number of clusters are as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中实现模型时，我们需要首先指定聚类数。我们使用了肘部法、Gap Statistic、轮廓系数、Calinski Harabasz分数和Davies
    Bouldin分数。每种方法的最佳聚类数如下：
- en: 'Elbow method: 8'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '肘部法: 8'
- en: 'Gap statistic: 29'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Gap statistic: 29'
- en: 'Silhouette score: 4'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '轮廓系数: 4'
- en: 'Calinski Harabasz score: 2'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Calinski Harabasz分数: 2'
- en: 'Davies Bouldin score: 4'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Davies Bouldin分数: 4'
- en: As seen above, 2 out of 5 methods suggest that we should use 4 clusters. If
    each model suggests a different number of clusters we can either take an average
    or median. The codes for finding the optimal number of k can be found [here ](https://github.com/IDB-FOR-DATASCIENCE/Segmentation-Modelling.git)and
    further details on each method can be found in this [blog](https://towardsdatascience.com/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，5种方法中有2种建议使用4个聚类。如果每种模型建议不同数量的聚类，我们可以取其平均值或中位数。找到最佳k数的代码可以在[这里](https://github.com/IDB-FOR-DATASCIENCE/Segmentation-Modelling.git)找到，而每种方法的进一步细节可以在这篇[博客](https://towardsdatascience.com/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad)中找到。
- en: Once we have the optimal number of clusters, we can fit the model and get the
    performance of the model using Silhouette score, Calinski Harabasz score and Davies
    Bouldin score.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了最佳聚类数，就可以拟合模型，并使用轮廓系数、Calinski Harabasz分数和Davies Bouldin分数来评估模型的性能。
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/9bf021dc8b01897827e7f184d2b3bf7e.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9bf021dc8b01897827e7f184d2b3bf7e.png)'
- en: 'Fig 1: Cluster Validation Metrics for K-Means (Image by author)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '图1: K-Means聚类验证指标（作者提供的图像）'
- en: We can also check the relative size and distribution of the clusters using an
    inter-cluster distance map.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用聚类间距离图来检查聚类的相对大小和分布。
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/ce591a9f71ff69a8c6b217c8bfce364e.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce591a9f71ff69a8c6b217c8bfce364e.png)'
- en: 'Fig 2: Inter Cluster Distance Map: K-Means (Image by author)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：聚类间距离图：K-Means（作者提供的图片）
- en: As seen in the figure above, two clusters are quite large compared to the others
    and they seem to have decent separation between them. However, if two clusters
    overlap in the 2D space, it does not imply that they overlap in the original feature
    space. Further details on the model can be found [here](https://projecteuclid.org/euclid.bsmsp/1200512992).
    Finally, other variants of K-Means like Mini Batch K-means, K-Medoids will be
    discussed in a separate blog.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，两个聚类相较于其他聚类较大，并且它们之间似乎有良好的分隔。然而，如果两个聚类在二维空间中重叠，这并不意味着它们在原始特征空间中也重叠。有关模型的更多细节可以在[这里](https://projecteuclid.org/euclid.bsmsp/1200512992)找到。最后，像Mini
    Batch K-means、K-Medoids这样的K-Means变体将在另一篇博客中讨论。
- en: Agglomerative clustering
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚合聚类
- en: 'Agglomerative clustering is a general family of clustering algorithms that
    build nested clusters by merging data points successively. This hierarchy of clusters
    can be represented as a tree diagram known as dendrogram. The top of the tree
    is a single cluster with all data points while the bottom contains individual
    points. There are multiple options for linking data points in a successive manner:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合聚类是一种通用的聚类算法家族，通过连续合并数据点来构建嵌套的聚类。这种层级的聚类可以表示为称为树状图的树形图。树的顶端是一个包含所有数据点的单一聚类，而底部包含单个点。有多种选项可以以连续的方式链接数据点：
- en: '**Single linkage: **Itminimizes the distance between the closest observations
    of pairs of clusters'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单链式连接：** 它最小化聚类对之间最近观察值的距离'
- en: '**Complete or Maximum linkage: **Tries tominimize the maximum distance between
    observations of pairs of clusters'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整或最大链式连接：** 尝试最小化聚类对之间观察值的最大距离'
- en: '**Average linkage: **It minimizes the average of the distances between all
    observations of pairs of clusters'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均链式连接：** 它最小化所有聚类对之间观察值距离的平均值'
- en: '**Ward: **Similar to the k-means as it minimizes the sum of squared differences
    within all clusters but with a hierarchical approach. We will be using this option
    in our exercise.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ward：** 类似于k-means，因为它最小化所有聚类内的平方差之和，但采用层级方法。我们将在练习中使用这个选项。'
- en: The ideal option can be picked by checking which linkage method performs best
    based on cluster validation metrics (Silhouette score, Calinski Harabasz score
    and Davies Bouldin score). And similar to K-means, we will have to specify the
    number of clusters in this model and the dendrogram can help us do that.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的选项可以通过检查哪种连接方法在聚类验证指标（轮廓系数、Calinski Harabasz评分和Davies Bouldin评分）中表现最佳来选择。与K-means类似，我们还需要在此模型中指定聚类数，树状图可以帮助我们做到这一点。
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/7e54da967a2addfb47f66bb546d18f76.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e54da967a2addfb47f66bb546d18f76.png)'
- en: 'Fig 3: Dendrogram (Image by author)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：树状图（作者提供的图片）
- en: 'From figure 3, we can see that we can choose either 4 or 8 clusters. We also
    use the elbow method, Silhouette score and Calinski Harabasz score to find the
    optimal number of clusters and get the following results:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从图3中，我们可以选择4或8个聚类。我们还使用肘部法、轮廓系数和Calinski Harabasz评分来寻找最佳聚类数，得到以下结果：
- en: 'Elbow method: 10'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 肘部法：10
- en: 'Davies Bouldin score : 8'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Davies Bouldin评分：8
- en: 'Silhouette score: 3'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 轮廓系数：3
- en: 'Calinski Harabasz score: 2'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Calinski Harabasz评分：2
- en: We will go ahead with 8 as both the Davies Bouldin score and dendrogram suggest
    so. If the metrics give us different number of cluster we can either go ahead
    with the one suggested by the dendrogram (as it is based on this specific model)
    or take average/median of all the metrics. The codes for finding the optimal number
    of clusters can be found [here ](https://github.com/IDB-FOR-DATASCIENCE/Segmentation-Modelling.git)and
    further details on each method can be found in this [blog](https://towardsdatascience.com/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将选择8，因为Davies Bouldin评分和树状图都建议这样做。如果指标给出不同的聚类数，我们可以选择树状图建议的数量（因为它基于这个特定模型）或取所有指标的平均值/中位数。寻找最佳聚类数的代码可以在[这里](https://github.com/IDB-FOR-DATASCIENCE/Segmentation-Modelling.git)找到，关于每种方法的更多细节可以在这个[博客](https://towardsdatascience.com/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad)中找到。
- en: Similar to k means, we can fit the model with the optimal number of clusters
    as well as linkage type and test its performance using the three metrics used
    in K-means.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与 K-means 类似，我们可以通过优化的聚类数量和连接类型来拟合模型，并使用 K-means 中使用的三种度量标准来测试其性能。
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/64426e74f015697c664b606bccc02b83.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64426e74f015697c664b606bccc02b83.png)'
- en: 'Fig 4: Cluster Validation metrics: Agglomerative Clustering (Image by Author)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：聚类验证指标：层次聚类（作者提供的图像）
- en: Comparing figure 1 and 4, we can see that K-means outperforms agglomerative
    clustering based on all cluster validation metrics.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 比较图 1 和图 4，我们可以看到 K-means 在所有聚类验证指标上优于层次聚类。
- en: Density-based spatial clustering (DBSCAN)
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于密度的空间聚类 (DBSCAN)
- en: 'DBSCAN groups together points that are closely packed together while marking
    others as outliers which lie alone in low-density regions. There are two key parameters
    in the model needed to define ‘density’: minimum number of points required to
    form a dense region `min_samples` and distance to define a neighborhood `eps`.
    Higher `min_samples` or lower `eps` demands greater density to form a cluster.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 将紧密聚集在一起的点分组，而将那些孤立在低密度区域的点标记为离群点。模型中定义“密度”的两个关键参数是：形成密集区域所需的最小点数 `min_samples`
    和定义邻域的距离 `eps`。较高的 `min_samples` 或较低的 `eps` 需要更高的密度来形成一个簇。
- en: 'Based on these parameters, DBSCAN starts with an arbitrary point x and identifies
    points that are within neighbourhood of x based on `eps` and classifies x as one
    of the following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些参数，DBSCAN 从一个任意点 x 开始，根据 `eps` 识别 x 的邻域内的点，并将 x 分类为以下之一：
- en: '**Core point**: If the number of points in the neighbourhood is at least equal
    to the `min_samples` parameter then it called a core point and a cluster is formed
    around x.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**核心点**：如果邻域中的点数至少等于 `min_samples` 参数，则称其为核心点，并且围绕 x 形成一个簇。'
- en: '**Border point**: x is considered a border point if it is part of a cluster
    with a different core point but number of points in it’s neighbourhood is less
    than the `min_samples` parameter. Intuitively, these points are on the fringes
    of a cluster.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**边界点**：如果 x 是一个聚类的一部分，该聚类的核心点不同，但其邻域中的点数少于 `min_samples` 参数，则 x 被认为是边界点。直观上，这些点位于聚类的边缘。'
- en: '**Outlier or noise**: If x is not a core point and distance from any core sample
    is at least equal to or greater than`eps` , it is considered an outlier or noise.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**离群点或噪声**：如果 x 不是核心点，并且与任何核心样本的距离至少等于或大于 `eps`，则它被认为是离群点或噪声。'
- en: For tuning the parameters of the model, we first identify the optimal `eps` value
    by finding the distance among a point’s neighbors and plotting the minimum distance.
    This gives us the elbow curve to find density of the data points and optimal `eps` value
    can be found at the inflection point. We use the `NearestNeighbours` function
    to get the minimum distance and the `KneeLocator` function to identify the inflection
    point.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调整模型参数，我们首先通过查找点邻域之间的距离并绘制最小距离来识别最佳的 `eps` 值。这会给我们一个肘部曲线来找出数据点的密度，最佳的 `eps`
    值可以在拐点处找到。我们使用 `NearestNeighbours` 函数来获取最小距离，并使用 `KneeLocator` 函数来识别拐点。
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/81e88c69f8a685abf4e6ab76b5cddffb.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81e88c69f8a685abf4e6ab76b5cddffb.png)'
- en: 'Figure 5: Optimal value for eps (Image by Author)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：eps 的最佳值（作者提供的图像）
- en: 'As seen above, the optimal value for `eps` is 1.9335816413107338\. We use this
    value for the parameter going forward and try to find the optimal value of `min_samples` parameter
    based on Silhouette score, Calinski Harabasz score and Davies Bouldin score. For
    each of these methods the optimal number of clusters are as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，最佳的 `eps` 值为 1.9335816413107338。我们将使用这个值作为参数，并根据轮廓系数、Calinski Harabasz
    评分和 Davies Bouldin 评分来尝试找出最佳的 `min_samples` 参数值。对于这些方法，每种方法的最佳簇数量如下：
- en: 'Silhouette score: 18'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 轮廓系数：18
- en: 'Calinski Harabasz score: 29'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Calinski Harabasz 评分：29
- en: 'Davies Bouldin score: 2'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Davies Bouldin 评分：2
- en: The codes for finding the optimal number of `min_samples` can be found [here ](https://github.com/IDB-FOR-DATASCIENCE/Segmentation-Modelling.git)and
    further details on each method can be found in this [blog](https://towardsdatascience.com/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad).
    We go ahead with the median suggestion which is 18 by Silhouette score.In case
    we don’t have time to run a grid search over these metrics, one quick rule of
    thumb is to set `min_samples` parameter as twice the number of features.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 查找最优 `min_samples` 的代码可以在 [这里](https://github.com/IDB-FOR-DATASCIENCE/Segmentation-Modelling.git)
    找到，每种方法的进一步细节可以在这个 [博客](https://towardsdatascience.com/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad)
    中找到。我们选择了 Silhouette 评分的中位数建议，即 18。如果没有时间对这些指标进行网格搜索，一条快速的经验法则是将 `min_samples`
    参数设置为特征数的两倍。
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/09e270403b04b681ea3e8e4022a15c4d.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09e270403b04b681ea3e8e4022a15c4d.png)'
- en: 'Figure 6: Cluster Validation metrics: DBSCAN (Image by Author)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：聚类验证指标：DBSCAN（图片来源：作者）
- en: 'Comparing figure 1 and 6, we can see that DBSCAN performs better than K-means
    on Silhouette score. The model is described in the paper:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 比较图 1 和图 6，我们可以看到 DBSCAN 在 Silhouette 评分上优于 K-means。该模型在论文中进行了描述：
- en: '[A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases
    with Noise](https://www.osti.gov/biblio/421283), 1996.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[一种用于发现大型空间数据库中簇的基于密度的算法](https://www.osti.gov/biblio/421283)，1996 年。'
- en: In a separate blog, we will be discussing a more advanced version of DBSCAN
    called Hierarchical Density-Based Spatial Clustering (HDBSCAN).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一篇博客中，我们将讨论一种更高级的 DBSCAN 版本，称为层次密度聚类（HDBSCAN）。
- en: Gaussian Mixture Modelling (GMM)
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高斯混合建模（GMM）
- en: 'A Gaussian mixture model is a distance based probabilistic model that assumes
    all the data points are generated from a linear combination of multivariate Gaussian
    distributions with unknown parameters. Like K-means it takes into account centers
    of the latent Gaussian distributions but unlike K-means, the covariance structure
    of the distributions is also taken into account. The algorithm implements the
    expectation-maximization (EM) algorithm to iteratively find the distribution parameters
    that maximize a model quality measure called log likelihood. The key steps performed
    in this model are:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合模型是一种基于距离的概率模型，它假设所有数据点是从具有未知参数的多变量高斯分布的线性组合中生成的。与 K-means 一样，它考虑了潜在高斯分布的中心，但与
    K-means 不同的是，还考虑了分布的协方差结构。该算法实现了期望最大化 (EM) 算法，通过迭代找到使模型质量度量（称为对数似然）最大的分布参数。该模型执行的关键步骤包括：
- en: Initialize k gaussian distributions
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 k 个高斯分布
- en: Calculate probabilities of each point’s association with each of the distributions
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个点与每个分布的关联概率
- en: Recalculate distribution parameters based on each point’s probabilities associated
    with the the distributions
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据每个点与分布的关联概率重新计算分布参数
- en: Repeat process till log-likelihood is maximized
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复过程直到对数似然最大化
- en: 'There are 4 options for calculating covariances in GMM:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: GMM 中计算协方差有 4 种选项：
- en: '**Full: **Each distribution has its own general covariance matrix'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**完全：**每个分布具有自己的总体协方差矩阵'
- en: '**Tied: **All distributions share general covariance matrix'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**绑带：**所有分布共享总体协方差矩阵'
- en: '**Diag: **Each distribution has its own diagonal covariance matrix'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**对角线：**每个分布具有自己的对角协方差矩阵'
- en: '**Spherical: **Each distribution has its own single variance'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**球形：**每个分布具有自己的单一方差'
- en: 'Apart from selecting the covariance type, we need to select the optimal number
    of clusters in the model as well. We use BIC score, Silhouette score, Calinski
    Harabasz score and Davies Bouldin score for selecting both parameters using grid
    search. For each of these methods the optimal number of clusters are as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 除了选择协方差类型，我们还需要选择模型中的最佳簇数。我们使用 BIC 评分、Silhouette 评分、Calinski Harabasz 评分和 Davies
    Bouldin 评分，通过网格搜索来选择这两个参数。对于每种方法，最佳簇数如下：
- en: 'BIC Score: Covariance- ‘full’ and cluster number- 26'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BIC 评分：协方差-‘完全’和簇数-26
- en: 'Silhouette score: Covariance- ‘tied’ and cluster number- 2'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Silhouette 评分：协方差-‘绑带’和簇数-2
- en: 'Calinski Harabasz score: Covariance- ‘spherical’ and cluster number- 4'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Calinski Harabasz 评分：协方差-‘球形’和簇数-4
- en: 'Davies Bouldin score: Covariance- ‘full’ and cluster number- 8'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Davies Bouldin 评分：协方差-‘完全’和簇数-8
- en: The codes for finding the optimal parameter values can be found [here ](https://github.com/IDB-FOR-DATASCIENCE/Segmentation-Modelling.git)and
    further details on each method can be found in this [blog](https://towardsdatascience.com/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad).
    We chose covariance as “full” and the number of clusters as 26 based on the BIC
    score as it is based on this specific model. If we have similar configurations
    from multiple metrics, we can take average/median/mode of all the metrics. We
    can now fit the model and check model performance.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 查找最佳参数值的代码可以在[这里](https://github.com/IDB-FOR-DATASCIENCE/Segmentation-Modelling.git)找到，关于每种方法的更多细节可以在这个[博客](https://towardsdatascience.com/cheat-sheet-to-implementing-7-methods-for-selecting-optimal-number-of-clusters-in-python-898241e1d6ad)中找到。我们选择了“full”协方差和26个聚类数，是基于特定模型的BIC得分。如果我们从多个指标中获得相似的配置，我们可以取所有指标的平均值/中位数/众数。现在我们可以拟合模型并检查模型性能。
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/cf524dd0d37296f34ff67da98bda4d13.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf524dd0d37296f34ff67da98bda4d13.png)'
- en: 'Figure 7: Cluster Validation metrics: GMM (Image by Author)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 聚类验证指标：GMM（图像由作者提供）'
- en: Comparing figure 1 and 7, we can see that K-means outperforms GMM based on all
    cluster validation metrics. In a separate blog, we will be discussing a more advanced
    version of GMM called Variational Bayesian Gaussian Mixture.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 比较图1和图7，我们可以看到K-means在所有聚类验证指标上优于GMM。在另一个博客中，我们将讨论一种更先进的GMM版本，称为变分贝叶斯高斯混合模型。
- en: Conclusion
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: 'The aim of this blog is to help the readers understand how 4 popular clustering
    models work as well as their detailed implementation in python. As shown below,
    each model has its own pros and cons:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这个博客的目的是帮助读者理解4种流行的聚类模型的工作原理以及它们在python中的详细实现。如下面所示，每种模型都有其优缺点：
- en: '![](../Images/735adf0c7dcc5d605f72470873d97465.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/735adf0c7dcc5d605f72470873d97465.png)'
- en: 'Fig 8: Pros and Cons of clustering algorithms (Image by Author)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 聚类算法的优缺点（图像由作者提供）'
- en: Finally, it is important to understand that these models are just a means to
    find logical and easily understandable customer/product segments which can be
    targeted effectively. So in most practical cases, we will end up trying multiple
    models and creating customer/product profiles from each iteration till we find
    segments that make the most business sense.Thus, segmentation is both an art and
    science.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，重要的是要理解这些模型只是找到逻辑性和易于理解的客户/产品细分的一种手段，可以有效地针对这些细分市场。因此，在大多数实际情况下，我们最终会尝试多个模型，并从每次迭代中创建客户/产品档案，直到找到最具商业意义的细分市场。因此，细分既是一门艺术，也是一门科学。
- en: Do you have any questions or suggestions about this blog? Please feel free to
    drop in a note.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对这个博客有任何问题或建议吗？请随时留言。
- en: '**Thank you for reading!**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**感谢阅读！**'
- en: If you, like me, are passionate about AI, Data Science, or Economics, please
    feel free to add/follow me on [LinkedIn](http://www.linkedin.com/in/indraneel-dutta-baruah-ds), [Github](https://github.com/IDB-FOR-DATASCIENCE) and [Medium](https://medium.com/@indraneeldb1993ds).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你像我一样，对AI、数据科学或经济学充满热情，请随时在[LinkedIn](http://www.linkedin.com/in/indraneel-dutta-baruah-ds)、[Github](https://github.com/IDB-FOR-DATASCIENCE)和[Medium](https://medium.com/@indraneeldb1993ds)上添加/关注我。
- en: '**References**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'Ester, M, Kriegel, H P, Sander, J, and Xiaowei, Xu. *A density-based algorithm
    for discovering clusters in large spatial databases with noise*. United States:
    N. p., 1996\. Web.'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ester, M, Kriegel, H P, Sander, J, 和 Xiaowei, Xu. *一种基于密度的算法，用于发现具有噪声的大型空间数据库中的聚类*。美国：N.
    p., 1996。网络。
- en: 'MacQueen, J. Some methods for classification and analysis of multivariate observations.
    Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability,
    Volume 1: Statistics, 281–297, University of California Press, Berkeley, Calif.,
    1967. [https://projecteuclid.org/euclid.bsmsp/1200512992](https://projecteuclid.org/euclid.bsmsp/1200512992)'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MacQueen, J. 一些用于分类和分析多变量观察的方法。第五届伯克利统计与概率研讨会论文集，第1卷：统计学, 281–297，加州大学出版社，伯克利，加州，1967。
    [https://projecteuclid.org/euclid.bsmsp/1200512992](https://projecteuclid.org/euclid.bsmsp/1200512992)
- en: '[Scikit-learn: Machine Learning in Python](http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html),
    Pedregosa *et al.*, JMLR 12, pp. 2825–2830, 2011.'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Scikit-learn: Python中的机器学习](http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html)，Pedregosa
    *等人*，JMLR 12，第2825–2830页，2011年。'
- en: '**Bio: [Indraneel Dutta Baruah](https://indraneeldb1993ds.medium.com/)** is
    striving for excellence in solving business problems using AI!'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介: [Indraneel Dutta Baruah](https://indraneeldb1993ds.medium.com/)** 正在努力通过AI解决商业问题，追求卓越！'
- en: '[Original](https://towardsdatascience.com/k-means-dbscan-gmm-agglomerative-clustering-mastering-the-popular-models-in-a-segmentation-c891a3818e29).
    Reposted with permission.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/k-means-dbscan-gmm-agglomerative-clustering-mastering-the-popular-models-in-a-segmentation-c891a3818e29)。经许可转载。'
- en: '**Related:**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Centroid Initialization Methods for k-means Clustering](/2020/06/centroid-initialization-k-means-clustering.html)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[k-means聚类的质心初始化方法](/2020/06/centroid-initialization-k-means-clustering.html)'
- en: '[Key Data Science Algorithms Explained: From k-means to k-medoids clustering](/2020/12/algorithms-explained-k-means-k-medoids-clustering.html)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关键数据科学算法解析：从k-means到k-medoids聚类](/2020/12/algorithms-explained-k-means-k-medoids-clustering.html)'
- en: '[Customer Segmentation Using K Means Clustering](/2019/11/customer-segmentation-using-k-means-clustering.html)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用K均值聚类进行客户细分](/2019/11/customer-segmentation-using-k-means-clustering.html)'
- en: '* * *'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三推荐课程
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织IT需求'
- en: '* * *'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Clustering Unleashed: Understanding K-Means Clustering](https://www.kdnuggets.com/2023/07/clustering-unleashed-understanding-kmeans-clustering.html)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[聚类的释放：理解K均值聚类](https://www.kdnuggets.com/2023/07/clustering-unleashed-understanding-kmeans-clustering.html)'
- en: '[Centroid Initialization Methods for k-means Clustering](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[k-means聚类的质心初始化方法](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
- en: '[Introduction to Clustering in Python with PyCaret](https://www.kdnuggets.com/2021/12/introduction-clustering-python-pycaret.html)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python中PyCaret的聚类介绍](https://www.kdnuggets.com/2021/12/introduction-clustering-python-pycaret.html)'
- en: '[Choosing the Right Clustering Algorithm for Your Dataset](https://www.kdnuggets.com/2019/10/right-clustering-algorithm.html)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为你的数据集选择正确的聚类算法](https://www.kdnuggets.com/2019/10/right-clustering-algorithm.html)'
- en: '[DBSCAN Clustering Algorithm in Machine Learning](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的DBSCAN聚类算法](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html)'
- en: '[What is K-Means Clustering and How Does its Algorithm Work?](https://www.kdnuggets.com/2023/05/kmeans-clustering-algorithm-work.html)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是K均值聚类及其算法如何工作？](https://www.kdnuggets.com/2023/05/kmeans-clustering-algorithm-work.html)'
