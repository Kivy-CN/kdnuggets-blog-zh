- en: How to Use GPT for Generating Creative Content with Hugging Face Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/how-to-use-gpt-for-generating-creative-content-with-hugging-face-transformers](https://www.kdnuggets.com/how-to-use-gpt-for-generating-creative-content-with-hugging-face-transformers)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![How to Use GPT for Generating Creative Content with Hugging Face Transformers](../Images/7c2590f4db8e141e3008c7f98ba82fe5.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: '## Introduction'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: GPT, short for Generative Pre-trained Transformer, is a family of transformer-based
    language models. Known as an example of an early transformer-based model capable
    of generating coherent text, OpenAI's GPT-2 was one of the initial triumphs of
    its kind, and can be used as a tool for a variety of applications, including helping
    write content in a more creative way. The Hugging Face Transformers library is
    a library of pretrained models that simplifies working with these sophisticated
    language models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The generation of creative content could be valuable, for example, in the world
    of data science and machine learning, where it might be used in a variety of ways
    to spruce up dull reports, create synthetic data, or simply help to guide the
    telling of a more interesting story. This tutorial will guide you through using
    GPT-2 with the Hugging Face Transformers library to generate creative content.
    Note that we use the GPT-2 model here for its simplicity and manageable size,
    but swapping it out for another generative model will follow the same steps.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up the Environment
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before getting started, we need to set up our environment. This will involve
    installing and importing the necessary libraries and importing the required packages.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the necessary libraries:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the required packages:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can learn about Huging Face Auto Classes and AutoModels [here](https://huggingface.co/docs/transformers/model_doc/auto).
    Moving on.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Loading the Model and Tokenizer
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we will load the model and tokenizer in our script. The model in this
    case is GPT-2, while the tokenizer is responsible for converting text into a format
    that the model can understand.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that changing the model_name above can swap in different Hugging Face language
    models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Preparing Input Text for Generation
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to have our model generate text, we need to provide the model with
    an initial input, or prompt. This prompt will be tokenized by the tokenizer.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the `return_tensors='pt'` argument ensures that PyTorch tensors are
    returned.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Generating Creative Content
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the input text has been tokenized and prepared for input into the model,
    we can then use the model to generate creative content.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Customizing Generation with Advanced Settings
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For added creativity, we can adjust the temperature and use top-k sampling and
    top-p (nucleus) sampling.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Adjusting the temperature:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Using top-k sampling and top-p sampling:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Practical Examples of Creative Content Generation
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here are some practical examples of using GPT-2 to generate creative content.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Summary
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Experimenting with different parameters and settings can significantly impact
    the quality and creativity of the generated content. GPT, especially the newer
    versions of which we are all aware, has tremendous potential in creative fields,
    enabling data scientists to generate engaging narratives, synthetic data, and
    more. For further reading, consider exploring the Hugging Face documentation and
    other resources to deepen your understanding and expand your skills.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: By following this guide, you should now be able to harness the power of GPT-3
    and Hugging Face Transformers to generate creative content for various applications
    in data science and beyond.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'For additional information on these topics, check out the following resources:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[Hugging Face Transformers Documentation](https://huggingface.co/transformers/)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PyTorch Documentation](https://pytorch.org/docs/stable/index.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Generative Pre-trained Transformer (Wikipedia)](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](https://www.linkedin.com/in/mattmayo13/)****[Matthew Mayo](https://www.kdnuggets.com/wp-content/uploads/./profile-pic.jpg)****
    ([**@mattmayo13**](https://twitter.com/mattmayo13)) holds a master''s degree in
    computer science and a graduate diploma in data mining. As managing editor of
    [KDnuggets](https://www.kdnuggets.com/) & [Statology](https://www.statology.org/),
    and contributing editor at [Machine Learning Mastery](https://machinelearningmastery.com/),
    Matthew aims to make complex data science concepts accessible. His professional
    interests include natural language processing, language models, machine learning
    algorithms, and exploring emerging AI. He is driven by a mission to democratize
    knowledge in the data science community. Matthew has been coding since he was
    6 years old.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How to Fine-Tune BERT for Sentiment Analysis with Hugging Face Transformers](https://www.kdnuggets.com/how-to-fine-tune-bert-sentiment-analysis-hugging-face-transformers)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a Recommendation System with Hugging Face Transformers](https://www.kdnuggets.com/building-a-recommendation-system-with-hugging-face-transformers)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using Hugging Face Transformers for Emotion Detection in Text](https://www.kdnuggets.com/using-hugging-face-transformers-for-emotion-detection-in-text)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Build and Train a Transformer Model from Scratch with…](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何从头开始构建和训练一个 Transformer 模型…](https://www.kdnuggets.com/how-to-build-and-train-a-transformer-model-from-scratch-with-hugging-face-transformers)'
- en: '[How to Translate Languages with MarianMT and Hugging Face Transformers](https://www.kdnuggets.com/how-to-translate-languages-with-marianmt-and-hugging-face-transformers)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 MarianMT 和 Hugging Face Transformers 进行语言翻译](https://www.kdnuggets.com/how-to-translate-languages-with-marianmt-and-hugging-face-transformers)'
- en: '[Meet Gorilla: UC Berkeley and Microsoft’s API-Augmented LLM…](https://www.kdnuggets.com/2023/06/meet-gorilla-uc-berkeley-microsoft-apiaugmented-llm-outperforms-gpt4-chatgpt-claude.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[认识 Gorilla：加州大学伯克利分校和微软的 API 增强型 LLM…](https://www.kdnuggets.com/2023/06/meet-gorilla-uc-berkeley-microsoft-apiaugmented-llm-outperforms-gpt4-chatgpt-claude.html)'
