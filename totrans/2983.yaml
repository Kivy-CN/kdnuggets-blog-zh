- en: GANs Need Some Attention, Too
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GANs 也需要一些关注
- en: 原文：[https://www.kdnuggets.com/2019/03/gans-need-some-attention-too.html](https://www.kdnuggets.com/2019/03/gans-need-some-attention-too.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/03/gans-need-some-attention-too.html](https://www.kdnuggets.com/2019/03/gans-need-some-attention-too.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By Bilal Shahid (Edited by Taraneh Khazaei, Lindsay Brin)**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：Bilal Shahid（编辑：Taraneh Khazaei，Lindsay Brin）**'
- en: Self-Attention Generative Adversarial Networks (SAGAN; [Zhang et al., 2018](https://arxiv.org/pdf/1805.08318.pdf))
    are convolutional neural networks that use the self-attention paradigm to capture
    long-range spatial relationships in existing images to better synthesize new images.
    AISC recently presented and discussed this paper, led by Xiyang Chen. The details
    of the event can be found on the [AISC website](https://aisc.a-i.science/events/2018-06-11/),
    and the full presentation can be viewed on [YouTube](https://www.youtube.com/watch?v=FdeHlC4QiqA).
    Here, we present an overview of the paper and its main contributions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力生成对抗网络（SAGAN；[Zhang 等人，2018](https://arxiv.org/pdf/1805.08318.pdf)）是使用自注意力范式来捕捉现有图像中的长程空间关系以更好地合成新图像的卷积神经网络。AISC
    最近介绍并讨论了这篇论文，由 Xiyang Chen 主导。事件的详细信息可以在[AISC 网站](https://aisc.a-i.science/events/2018-06-11/)上找到，完整的演示可以在[YouTube](https://www.youtube.com/watch?v=FdeHlC4QiqA)上查看。在这里，我们提供了这篇论文及其主要贡献的概述。
- en: Challenges with Traditional GANs
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 传统 GAN 的挑战
- en: While excellent at generating fairly realistic images, traditional deep convolutional
    GANs are unable to capture long-range dependencies in images. These conventional
    GANs work well for images that do not contain a lot of structural and geometric
    information (e.g., images depicting oceans, skies, and fields). However, when
    there is a high rate of information variation in an image, conventional GANs tend
    to miss out on obtaining all of this variation, and thus fail to represent the
    global relationships faithfully. These non-local dependencies consistently appear
    in certain classes of images. For example, GANs can draw animal images with realistic
    fur, but often fail to draw separate feet.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然传统的深度卷积 GAN 在生成相当逼真的图像方面表现优异，但它们无法捕捉图像中的长程依赖。这些传统的 GAN 对于不包含大量结构和几何信息的图像（例如，描绘海洋、天空和田野的图像）表现良好。然而，当图像中的信息变化率很高时，传统的
    GAN 往往无法获取所有这些变化，从而未能忠实地表示全局关系。这些非局部依赖在某些类别的图像中始终出现。例如，GAN 可以绘制具有逼真毛发的动物图像，但通常无法绘制分开的脚部。
- en: '![Figure](../Images/4dcc98be4dcd3cddecb226515d6946d3.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/4dcc98be4dcd3cddecb226515d6946d3.png)'
- en: '***Output images from the previous state-of-the-art GAN (CGANs with Projections
    Discriminator; [Miyato et al., 2018](https://arxiv.org/abs/1802.05637))***'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '***来自之前的最先进 GAN（具有投影判别器的 CGAN；[Miyato 等人，2018](https://arxiv.org/abs/1802.05637)）的输出图像***'
- en: Given the limited representation capacity of the convolution operator (i.e.,
    the receptive field is local), conventional GANs can only capture long-range relationships
    after several convolutional layers. One approach to alleviating this problem is
    to Increase the size of the kernel, but this is statistically and computationally
    inefficient. Various attention and self-attention models have been formulated
    to capture and use structural patterns and non-local relationships. However, these
    models are generally not effective in striking a balance between computational
    efficiency and modeling long-range relationships.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于卷积运算符的表示能力有限（即感受野是局部的），传统的 GAN 只能在经过几层卷积之后才能捕捉长程关系。缓解此问题的一种方法是增加卷积核的大小，但这在统计和计算上都不高效。各种注意力和自注意力模型已被提出，以捕捉和利用结构模式和非局部关系。然而，这些模型通常无法在计算效率和建模长程关系之间取得平衡。
- en: Self-Attention for GANs
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GAN 的自注意力
- en: This functional gap is where SAGAN comes in. [Zhang et al. (2018)](https://arxiv.org/abs/1805.08318) propose
    a method wherein the GAN model is equipped with a tool to capture long-range,
    multi-level relationships in an image. This tool is the self-attention mechanism.
    Self-attention attempts to relate different portions of the input features to
    compute another representation of the input suitable for the task at hand. The
    idea of self-attention has been successfully applied in the areas of reading comprehension
    ([Cheng et al., 2016](https://arxiv.org/pdf/1601.06733.pdf)), textual entailment
    ([Parikh et al., 2016](https://arxiv.org/pdf/1606.01933.pdf)), and video processing
    ([X. Wang et al., 2017](https://arxiv.org/pdf/1711.07971.pdf)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个功能差距就是SAGAN的作用所在。[Zhang et al. (2018)](https://arxiv.org/abs/1805.08318) 提出了一种方法，其中GAN模型配备了一种工具来捕捉图像中的长程、多层次关系。这个工具就是自注意力机制。自注意力试图将输入特征的不同部分关联起来，以计算适合当前任务的输入表示。自注意力的思想已经成功应用于阅读理解（[Cheng
    et al., 2016](https://arxiv.org/pdf/1601.06733.pdf)）、文本蕴含（[Parikh et al., 2016](https://arxiv.org/pdf/1606.01933.pdf)）和视频处理（[X.
    Wang et al., 2017](https://arxiv.org/pdf/1711.07971.pdf)）等领域。
- en: Bringing self-attention to the image synthesis domain draws inspiration from
    “Non-local neural networks” ([X. Wang et al., 2017](https://arxiv.org/pdf/1711.07971.pdf))
    that use self-attention to capture spatial-temporal information in video sequences.
    Generally speaking, self-attention simply calculates the response at a single
    position as a weighted sum of the features at all positions. This mechanism allows
    the network to focus on areas of images that are widely separated yet have structural
    relevancy.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 将自注意力引入图像合成领域受到“非局部神经网络”（[X. Wang et al., 2017](https://arxiv.org/pdf/1711.07971.pdf)）的启发，该网络利用自注意力捕捉视频序列中的时空信息。一般而言，自注意力只是计算单个位置的响应作为所有位置特征的加权和。这个机制允许网络关注图像中结构上相关但空间上相隔较远的区域。
- en: '![Figure](../Images/a4ab4243f2515f6668c656ceb7f12da7.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/a4ab4243f2515f6668c656ceb7f12da7.png)'
- en: '***The proposed self-attention module in Self-Attention Generative Adversarial
    Networks ([Zhang et al., 2018](https://arxiv.org/abs/1805.08318))***'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '***《自注意力生成对抗网络中提出的自注意力模块》（[Zhang et al., 2018](https://arxiv.org/abs/1805.08318)）***'
- en: 'In SAGAN, the self-attention module works in conjunction with the convolution
    network and uses the key-value-query model ([Vaswani, et al., 2017](https://arxiv.org/abs/1706.03762)).
    This module takes the feature map, created by the convolutional neural network,
    and transforms it into three feature spaces. These feature spaces, called key
    f(x), value h(x), and query g(x), are created by passing the original feature
    map through three different 1x1 convolution maps. Key f(x) and query g(x) matrices
    are then multiplied. Next, the softmax operation is applied on each row of the
    multiplication result. The attention map generated from softmax identifies which
    areas of the image the network should attend to, as described in Equation (1)
    from Zhang et al. 2018:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在SAGAN中，自注意力模块与卷积网络协同工作，并使用键-值-查询模型（[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)）。该模块将卷积神经网络生成的特征图转换为三个特征空间。这些特征空间分别称为键
    f(x)、值 h(x) 和查询 g(x)，通过将原始特征图传递通过三个不同的1x1卷积图生成。然后，键 f(x) 和查询 g(x) 矩阵相乘。接下来，对乘法结果的每一行应用
    softmax 操作。由 softmax 生成的注意力图确定了网络应该关注图像的哪些区域，如公式（1）所述，来自 Zhang et al. 2018：
- en: '![](../Images/2be4597853e42395afd54d41f488cdbe.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2be4597853e42395afd54d41f488cdbe.png)'
- en: 'Therefore, the attention map is then multiplied with value h(x) to generate
    the self-attention feature map as follows (equation (2) from Zhang et al. 2018):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，注意力图会与值 h(x) 相乘，生成自注意力特征图，如下所示（公式（2）来自 Zhang et al. 2018）：
- en: '![](../Images/f31e7f12fd2d1aabab75c10c5e58f6fc.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f31e7f12fd2d1aabab75c10c5e58f6fc.png)'
- en: 'Finally, the output is calculated by adding the original input feature map
    to the scaled self-attention map. The scaling parameter ???? is initialized to
    0 at the beginning to cause the network to first focus on the local information.
    As the scaling parameter ???? gets updated during training, the network gradually
    learns to attend to the non-local areas of an image (Equation (3) from Zhang et
    al. 2018):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，输出是通过将原始输入特征图与缩放后的自注意力图相加来计算的。缩放参数 ???? 在开始时初始化为 0，以使网络首先关注局部信息。随着缩放参数 ????
    在训练过程中更新，网络逐渐学习关注图像的非局部区域（公式（3）来自 Zhang et al. 2018）：
- en: '![](../Images/376658f037439643d4b741aa2caa969e.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/376658f037439643d4b741aa2caa969e.png)'
- en: '![Figure](../Images/454076caa5eaf57d706de907c61b99f5.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/454076caa5eaf57d706de907c61b99f5.png)'
- en: '***Output images from Self-Attention Generative Adversarial Networks ([Zhang,
    et al., 2018](https://arxiv.org/abs/1805.08318))***'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '***自注意力生成对抗网络输出图像 ([Zhang, et al., 2018](https://arxiv.org/abs/1805.08318))***'
- en: Handling Instability in GAN Training
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理GAN训练中的不稳定性
- en: 'Another contribution made by the SAGAN paper is related to the well-known issue
    of the instability of GANs’ training. The paper proposes two techniques to handle
    this problem: Spectral normalization and Two Time-scale Update Rule (TTUR).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: SAGAN论文的另一个贡献与生成对抗网络（GANs）训练不稳定的问题有关。论文提出了两种处理此问题的技术：光谱归一化和双时间尺度更新规则（TTUR）。
- en: Generators are shown to perform better and have improved training dynamics when
    they are well-conditioned ([Odena, et al., 2018](https://arxiv.org/abs/1802.08768)).
    With SAGANs, generator conditioning is accomplished using [spectral normalization](https://christiancosgrove.com/blog/2018/01/04/spectral-normalization-explained.html).
    This approach was first introduced in ([Miyato et al. 2018](https://arxiv.org/pdf/1802.05957.pdf)),
    but for the discriminator network only, to address training dynamics that may
    result in the generator not learning the target distribution well. SAGAN employs
    spectral normalization in both the generator and the discriminator network, limiting
    the spectral norm of the weight matrices in both of the networks. This process
    is beneficial because it constrains the Lipschitz constant without requiring any
    hyper-parameter tuning, prevents the escalation of parameter magnitudes and unusual
    gradients, and allows for fewer discriminator updates compared to the generator.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当生成器经过良好条件化时，表现更佳且训练动态得到了改善 ([Odena, et al., 2018](https://arxiv.org/abs/1802.08768))。使用SAGAN时，生成器的条件化通过 [光谱归一化](https://christiancosgrove.com/blog/2018/01/04/spectral-normalization-explained.html)
    来实现。这种方法最初是在 ([Miyato et al. 2018](https://arxiv.org/pdf/1802.05957.pdf)) 中引入的，但仅针对判别器网络，以解决生成器未能很好地学习目标分布的问题。SAGAN在生成器和判别器网络中都采用光谱归一化，限制两个网络中权重矩阵的光谱范数。这一过程是有益的，因为它在不需要任何超参数调节的情况下限制了Lipschitz常数，防止了参数幅度和异常梯度的激增，并允许与生成器相比，判别器的更新次数减少。
- en: In addition to spectral normalization, the paper uses TTUR ([Heusel et al.,
    2018](https://arxiv.org/abs/1706.08500)) to address the problem of slow learning
    with regularized discriminators. Methods using regularized discriminators generally
    require multiple discriminator updates per generator update. To accelerate the
    learning speed, the generator and the discriminator are trained with different
    learning rates.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 除了光谱归一化，论文还使用了TTUR ([Heusel et al., 2018](https://arxiv.org/abs/1706.08500))
    来解决使用正则化判别器时学习缓慢的问题。使用正则化判别器的方法通常需要每次生成器更新时多次更新判别器。为了加快学习速度，生成器和判别器使用不同的学习率进行训练。
- en: Conclusion
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: SAGAN is a substantial improvement to the state of the art for image generation
    techniques. The effective integration of the self-attention technique enables
    the network to faithfully capture and relate long-range spatial information in
    a computationally efficient manner. The use of spectral normalization in both
    the discriminator and the generator network, along with TTUR, not only reduces
    the computational cost of training, but also improves the training stability.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: SAGAN是图像生成技术的一项重大进步。有效整合自注意力技术使网络能够以计算高效的方式真实捕捉和关联长距离空间信息。在判别器和生成器网络中使用光谱归一化，以及TTUR，不仅降低了训练的计算成本，还提高了训练稳定性。
- en: '[Original](https://aisc.a-i.science/blog/2019/self-attention-gan/). Reposted
    with permission.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://aisc.a-i.science/blog/2019/self-attention-gan/)。经授权转载。'
- en: '**Related:**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Adversarial Examples, Explained](/2018/10/adversarial-examples-explained.html)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[对抗样本解释](/2018/10/adversarial-examples-explained.html)'
- en: '[Generative Adversarial Networks – Paper Reading Road Map](/2018/10/generative-adversarial-networks-paper-reading-road-map.html)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[生成对抗网络 – 论文阅读路线图](/2018/10/generative-adversarial-networks-paper-reading-road-map.html)'
- en: '[What are Some “Advanced” AI and Machine Learning Online Courses?](/2019/02/some-advanced-ai-machine-learning-online-courses.html)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是一些“高级”人工智能和机器学习在线课程？](/2019/02/some-advanced-ai-machine-learning-online-courses.html)'
- en: '* * *'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在的组织的IT'
- en: '* * *'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Adding an attention mechanism to RNNs](https://www.kdnuggets.com/2022/03/packt-adding-attention-mechanism-rnns.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为RNN添加注意力机制](https://www.kdnuggets.com/2022/03/packt-adding-attention-mechanism-rnns.html)'
- en: '[Finally a Book on Attention!](https://www.kdnuggets.com/2022/11/mlm-finally-book-attention.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[终于有一本关于注意力机制的书！](https://www.kdnuggets.com/2022/11/mlm-finally-book-attention.html)'
- en: '[Things Aren''t Always Normal: Some of the "Other" Distributions](https://www.kdnuggets.com/2023/01/things-arent-always-normal-distributions.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[事情并不总是正常：一些“其他”分布](https://www.kdnuggets.com/2023/01/things-arent-always-normal-distributions.html)'
- en: '[I Used ChatGPT (Every Day) for 5 Months. Here Are Some Hidden Gems…](https://www.kdnuggets.com/2023/07/used-chatgpt-every-day-5-months-hidden-gems-change-life.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我使用ChatGPT（每天）5个月。这里有一些隐藏的宝藏…](https://www.kdnuggets.com/2023/07/used-chatgpt-every-day-5-months-hidden-gems-change-life.html)'
- en: '[Some Kick Ass Prompt Engineering Techniques to Boost our LLM Models](https://www.kdnuggets.com/some-kick-ass-prompt-engineering-techniques-to-boost-our-llm-models)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一些惊人的提示工程技术，以提升我们的LLM模型](https://www.kdnuggets.com/some-kick-ass-prompt-engineering-techniques-to-boost-our-llm-models)'
- en: '[It''s alive! Build your first robots with Python and some cheap,…](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[它活了！用Python和一些便宜的组件打造你的第一个机器人…](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
