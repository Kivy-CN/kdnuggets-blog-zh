- en: Labelling Data Using Snorkel
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Snorkel 标记数据
- en: 原文：[https://www.kdnuggets.com/2020/07/labelling-data-using-snorkel.html](https://www.kdnuggets.com/2020/07/labelling-data-using-snorkel.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/07/labelling-data-using-snorkel.html](https://www.kdnuggets.com/2020/07/labelling-data-using-snorkel.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Alister D’Costa](https://www.linkedin.com/in/alisterdcosta/), [Stefan
    Denkovski](https://www.linkedin.com/in/stefandenkovski/), [Michal Malyska](https://www.linkedin.com/in/malyskamichal/),
    [Sally Moon](https://www.linkedin.com/in/sallysaeyoungmoon/), [Brandon Rufino](https://www.linkedin.com/in/brandon-rufino/),
    [NLP4H](https://nlp4h.com/authors/)**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Alister D’Costa](https://www.linkedin.com/in/alisterdcosta/)、[Stefan Denkovski](https://www.linkedin.com/in/stefandenkovski/)、[Michal
    Malyska](https://www.linkedin.com/in/malyskamichal/)、[Sally Moon](https://www.linkedin.com/in/sallysaeyoungmoon/)、[Brandon
    Rufino](https://www.linkedin.com/in/brandon-rufino/)、[NLP4H](https://nlp4h.com/authors/)**'
- en: '![Image for post](../Images/e16079ec47d75638df56b6c91eb5db14.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![Image for post](../Images/e16079ec47d75638df56b6c91eb5db14.png)'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速入门网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的信息技术'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In this tutorial, we will walk through the process of using Snorkel to generate
    labels for an unlabelled dataset. We will provide you examples of basic Snorkel
    components by guiding you through a real clinical application of Snorkel. Specifically,
    we will use Snorkel to try to boost our results in predicting [Multiple Sclerosis
    (MS) severity scores](https://www.mstrust.org.uk/a-z/expanded-disability-status-scale-edss).
    Enjoy!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将深入介绍如何使用 Snorkel 为未标记的数据集生成标签。我们将通过引导您完成 Snorkel 的实际临床应用，提供基本的 Snorkel
    组件示例。具体而言，我们将使用 Snorkel 尝试提升预测 [多发性硬化症 (MS) 严重程度评分](https://www.mstrust.org.uk/a-z/expanded-disability-status-scale-edss)
    的结果。请享用！
- en: '*Check out the *[*Snorkel Intro Tutorial*](https://www.snorkel.org/use-cases/01-spam-tutorial)* for
    a walk through on spam labelling. For more examples of high-performance in real-world
    uses of Snorkel, see *[*Snorkel’s publication list*](https://www.snorkel.org/resources/)*.*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*查看 *[*Snorkel 介绍教程*](https://www.snorkel.org/use-cases/01-spam-tutorial)* 以了解垃圾邮件标记的流程。有关
    Snorkel 在实际应用中的高性能示例，请参见 *[*Snorkel 的出版列表*](https://www.snorkel.org/resources/)*。*'
- en: '*Check out our other work focused on NLP for MS severity classification *[*here*](https://medium.com/@nlp4health/ms-bert-using-neurological-examination-notes-for-multiple-sclerosis-severity-classification-f75f13600d3e)*.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*查看我们其他聚焦于 NLP 的 MS 严重程度分类工作 *[*这里*](https://medium.com/@nlp4health/ms-bert-using-neurological-examination-notes-for-multiple-sclerosis-severity-classification-f75f13600d3e)*。*'
- en: What is Snorkel?
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是 Snorkel？
- en: 'Snorkel is a system that facilitates the process of building and managing training
    datasets without manual labelling. The first component of a Snorkel pipeline includes
    labelling functions, which are designed to be weak heuristic functions that predict
    a label given unlabelled data. The labelling functions that we developed for MS
    severity score labelling were the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Snorkel 是一个帮助构建和管理训练数据集的系统，无需手动标记。Snorkel 流水线的第一个组件包括标记功能，这些功能设计为弱启发式函数，用于预测给定未标记数据的标签。我们为
    MS 严重程度评分标记开发的标记功能如下：
- en: Multiple key-word searches (using regular expressions) within the text. For
    example, in finding a severity score we searched for the phrase in numeric format
    and in roman numeral format.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在文本中进行多个关键字搜索（使用正则表达式）。例如，在查找严重程度评分时，我们搜索了数字格式和罗马数字格式的短语。
- en: Common baselines such as logistic regression, linear discriminant analysis,
    and support vector machines which were trained using term frequency-inverse document
    frequency (or tf-idf for short) features.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见基准如逻辑回归、线性判别分析和支持向量机，这些模型使用词频-逆文档频率（简称 tf-idf）特征进行训练。
- en: Word2Vec Convolutional Neural Network (CNN).
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2Vec 卷积神经网络 (CNN)。
- en: Our MS-BERT classifier described [in this blog post](https://medium.com/@nlp4health/ms-bert-using-neurological-examination-notes-for-multiple-sclerosis-severity-classification-f75f13600d3e).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的 MS-BERT 分类器在[这篇博客文章](https://medium.com/@nlp4health/ms-bert-using-neurological-examination-notes-for-multiple-sclerosis-severity-classification-f75f13600d3e)中有所描述。
- en: The second component of the Snorkel pipeline is a generative model that outputs
    a single confidence weighted training label per data point given predictions from
    all the labelling functions. It does this by learning to estimate the accuracy
    and correlations of the labelling functions based on their agreements and disagreements.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Snorkel 管道的第二个组成部分是一个生成模型，该模型根据所有标注函数的预测输出每个数据点的单个置信度加权训练标签。它通过学习根据标注函数的一致性和分歧来估计标注函数的准确性和相关性。
- en: Snorkel Tutorial
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Snorkel 教程
- en: To reiterate, in this article we demonstrate label generation for MS severity
    scores. A common measurement of MS severity is EDSS or the Expanded Disability
    Status Scale. This is a scale that increases from 0 to 10 depending on the severity
    of MS symptoms. We will refer to EDSS in general as the MS severity score but
    for our keen readers we thought we would provide this information. This score
    is further described [here](https://www.mstrust.org.uk/a-z/expanded-disability-status-scale-edss).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 重申一下，在本文中我们演示了 MS 严重程度评分的标签生成。MS 严重程度的常见测量指标是 EDSS 或扩展残疾状态量表。这是一种从 0 到 10 的量表，取决于
    MS 症状的严重程度。我们通常将 EDSS 称为 MS 严重程度评分，但为了让我们的读者了解更多信息，我们提供了这些信息。有关此评分的更多描述，请参见[这里](https://www.mstrust.org.uk/a-z/expanded-disability-status-scale-edss)。
- en: 'Step 0: Acquire a Dataset'
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 0 步：获取数据集
- en: In our task, we worked with a dataset compiled by a leading MS research hospital,
    containing over *70,000* MS consult notes for about 5000 patients. Of the 70,000
    notes only *16,000* are manually labeled by an expert for MS severity. This means
    that their are approximately *54,000* unlabelled notes. As you may or not be aware,
    having a larger dataset to train models generally lead to better model performance.
    Hence, we used Snorkel to generate what we call ‘silver’ labels for our *54,000* unlabelled
    notes. The *16,000* ‘gold’ labelled notes were used to train our classifiers before
    creating their respective labelling function.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的任务中，我们使用了由一家领先的 MS 研究医院编制的数据集，其中包含超过*70,000*份 MS 诊疗记录，涉及约 5000 名患者。在这 70,000
    份记录中，只有*16,000*份是由专家手动标记的 MS 严重程度。这意味着大约有*54,000*份记录未被标记。正如您可能知道的那样，训练模型时拥有更大的数据集通常会导致更好的模型性能。因此，我们使用
    Snorkel 为我们的*54,000*份未标记记录生成了所谓的“银”标签。*16,000*份“金”标签记录在创建各自的标注函数之前用于训练我们的分类器。
- en: 'Step 1: Installing Snorkel'
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 1 步：安装 Snorkel
- en: 'To install Snorkel to your project, you can run the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 Snorkel 安装到您的项目中，您可以运行以下命令：
- en: 'Step 2: Adding the Labelling Functions'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 2 步：添加标注函数
- en: Setting up
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置
- en: 'Labelling functions allow you to define weak heuristics and rules that predict
    a label given unlabelled data. These heuristics can be derived from expert knowledge
    or other labelling models. In the case of MS severity score prediction, our labelling
    functions included: key-word search functions derived from clinicians, baseline
    models trained to predict MS severity scores (tf-idf, word2vec cnn, etc.), and
    our MS-BERT classifier.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 标注函数允许您定义弱启发式规则和规则，以便在给定未标记数据的情况下预测标签。这些启发式规则可以来源于专家知识或其他标注模型。在 MS 严重程度评分预测的情况下，我们的标注函数包括：来源于临床医生的关键词搜索函数、训练以预测
    MS 严重程度评分的基线模型（tf-idf、word2vec cnn 等）以及我们的 MS-BERT 分类器。
- en: As you will see below, you mark labelling functions by adding “@labeling_function()”
    above the function. For each labelling function, a single row of a dataframe containing
    unlabelled data (i.e. one observation/sample) is passed in. Each labelling function
    applies heuristics or models to obtain a prediction for each row. If the prediction
    is not found, the function abstains (i.e. returns -1).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，您通过在函数上方添加“@labeling_function()”来标记标注函数。对于每个标注函数，一行包含未标记数据（即一个观测/样本）的数据框将被传入。每个标注函数应用启发式规则或模型以获取每行的预测。如果未找到预测，则该函数会回避（即返回
    -1）。
- en: When all labelling functions have been defined, you can make use of the “PandasLFApplier”
    to obtain a matrix of predictions given all labelling functions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当所有标注函数都定义好之后，您可以利用“PandasLFApplier”来获取给定所有标注函数的预测矩阵。
- en: Upon running the following code, you will obtain a (N X num_lfs) L_predictions
    matrix, where N is number of observations in ‘df_unlabelled’ and ‘num_lfs’ is
    the number of labelling functions defined in ‘lfs’.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下代码后，你将获得一个 (N X num_lfs) L_predictions 矩阵，其中 N 是“df_unlabelled”中的观察数，“num_lfs”是“lfs”中定义的标注函数数量。
- en: 'Labelling Function Example #1: Key-Word Search'
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '标注函数示例 #1：关键字搜索'
- en: Below shows an example of a key-word search (using regular expressions) used
    to extract MS severity scores recorded in decimal form. The regular expression
    functions are applied to attempt to search for the MS severity score recorded
    in decimal form. If found, the function returns the score in the appropriate output
    format. Else, the function abstains (i.e. returns -1) to indicate that the score
    is not found.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下面展示了一个使用正则表达式的关键字搜索示例，用于提取以十进制形式记录的 MS 严重性分数。正则表达式函数用于尝试搜索以十进制形式记录的 MS 严重性分数。如果找到，函数将返回适当格式的分数。否则，函数会弃权（即返回
    -1），表示没有找到分数。
- en: 'Labelling Function Example #2: Trained Classifier'
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '标注函数示例 #2：训练分类器'
- en: Above we see an example using a key-word search. To integrate a trained classifier,
    you must perform one extra step. That is, you must train and export your model
    before creating your labelling function. Here is an example of how we trained
    a logistic regression that was built on top of tf-idf features.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 上面我们看到一个使用关键字搜索的示例。要整合训练好的分类器，你必须执行一个额外的步骤。也就是说，你必须在创建标注函数之前训练和导出你的模型。这里是一个基于
    tf-idf 特征训练逻辑回归的示例。
- en: 'With the model trained, implementing a labelling function is as simple as this:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练好后，实现标注函数就简单如斯：
- en: 'Step 3(a): Using Snorkel’s Majority Vote'
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 3(a) 步：使用 Snorkel 的多数投票
- en: Some would say the simpliest function Snorkel uses to generate a label is ‘Majority
    Vote’. Majority Vote, as the name implies, makes a prediction based on the most
    voted for class.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有人说 Snorkel 用于生成标签的最简单函数是“多数投票”。如其名所示，多数投票基于最多票数的类别进行预测。
- en: To implement Majority Vote you must specify the ‘cardinality’ (i.e. number of
    classes).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现多数投票，你必须指定“基数”（即类别数）。
- en: 'Step 3(b): Using Snorkel’s Label Model'
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 3(b) 步：使用 Snorkel 的标签模型
- en: To take advantage of Snorkel’s full functionality, we used the ‘Label Model’
    to generate a single confidence-weighted label given a matrix of predictions obtained
    from all the labelling functions (i.e. L_unlabelled). The Label Model predicts
    by learning to estimate the accuracy and correlations of the labelling functions
    based on their agreements and disagreements.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用 Snorkel 的功能，我们使用“标签模型”来生成一个基于所有标注函数获得的预测矩阵（即 L_unlabelled）的单一置信度加权标签。标签模型通过学习估计标注函数的准确性和相关性来进行预测，基于它们的同意和分歧。
- en: You can define a Label Model and specify ‘cardinality’. After you fit the Label
    Model with L_unlabelled, it will generate single predictions for the unlabelled
    data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以定义一个标签模型并指定“基数”。在用 L_unlabelled 训练标签模型后，它将为未标记的数据生成单一的预测。
- en: 'Step 4: Evaluation Tools'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 4 步：评估工具
- en: LF Analysis — Coverage, Overlaps, Conflicts
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LF 分析 — 覆盖度、重叠度、冲突
- en: To better understand how your labelling functions are functioning, you can make
    use of Snorkel’s LFAnalysis. The LF analysis reports the polarity, coverage, overlap,
    and conflicts of each labelling function.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解你的标注函数的功能，你可以利用 Snorkel 的 LFAnalysis。LF 分析报告了每个标注函数的极性、覆盖度、重叠度和冲突情况。
- en: 'The definition of these terms are as follows and you can refer to the [Snorkel
    documentation](https://snorkel.readthedocs.io/en/v0.9.1/packages/_autosummary/labeling/snorkel.labeling.LFAnalysis.html#snorkel.labeling.LFAnalysis.lf_polarities) for
    more information:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这些术语的定义如下，你可以参考 [Snorkel 文档](https://snorkel.readthedocs.io/en/v0.9.1/packages/_autosummary/labeling/snorkel.labeling.LFAnalysis.html#snorkel.labeling.LFAnalysis.lf_polarities)
    获取更多信息：
- en: 'Polarity: Infer the polarities of each LF based on evidence in a label matrix.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极性：根据标签矩阵中的证据推断每个 LF 的极性。
- en: 'Coverage: Computes the fraction of data points with at least one label.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 覆盖度：计算具有至少一个标签的数据点的比例。
- en: 'Overlap: Computes the fraction of data points with at least two (non-abstain)
    labels.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重叠度：计算具有至少两个（非弃权）标签的数据点的比例。
- en: 'Conflicts: Computes the fraction of data points each labelling function disagrees
    with at least one other labelling function.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冲突：计算每个标注函数与至少一个其他标注函数不一致的数据点比例。
- en: LFAnalysis will provide an analysis of how your labelling functions performed
    relative to each other.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: LFAnalysis 将提供你的标注函数相互之间的表现分析。
- en: ‘get_label_buckets’
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ‘get_label_buckets’
- en: Snorkel provides some more evaluation tools to help you understand the quality
    of your labelling functions. In particular, ‘get_label_buckets’ is a handy way
    to combine labels and make comparisons. For more information, read the [Snorkel
    documentation](https://snorkel.readthedocs.io/en/v0.9.1/packages/_autosummary/analysis/snorkel.analysis.get_label_buckets.html).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Snorkel 提供了一些额外的评估工具，帮助你了解标注函数的质量。特别是，‘get_label_buckets’ 是一个方便的方式来组合标签并进行比较。有关更多信息，请阅读 [Snorkel
    文档](https://snorkel.readthedocs.io/en/v0.9.1/packages/_autosummary/analysis/snorkel.analysis.get_label_buckets.html)。
- en: The following code allows you to compare the true labels (y_gold) and predicted
    labels (y_preds) to view data points that were correctly or incorrectly labelled
    by Snorkel. This will allow you to pin-point which data points are difficult to
    correctly label, so that you can fine-tune your labelling functions to cover these
    edge cases.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码允许你比较真实标签（`y_gold`）和预测标签（`y_preds`），以查看 Snorkel 正确或错误标记的数据点。这将帮助你精确找出哪些数据点难以正确标记，以便你可以调整标注函数来覆盖这些边缘案例。
- en: Note, for this analysis we went back and created a L_train matrix which contains
    our labelling function predictions for our ‘gold’ labelled dataset.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了进行此分析，我们回过头来创建了一个包含我们‘黄金’标注数据集的标注函数预测的 L_train 矩阵。
- en: Alternatively, you can use ‘get_label_buckets’ to make comparisons between labelling
    functions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你可以使用 ‘get_label_buckets’ 来比较标注函数之间的差异。
- en: The following code allows you to compare the label predictions in L_unlabelled
    to observe how different labelling functions label datapoints differently.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码允许你比较 L_unlabelled 中的标签预测，以观察不同的标注函数如何以不同方式标记数据点。
- en: 'Step 5: Deployment'
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第 5 步：部署
- en: Choosing the Best Labelling Model to Label Unlabelled Data
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择最佳标注模型来标记未标记的数据
- en: Following the procedure outlined above, we developed various labelling functions
    based on key-word searches, baseline models, and our MS-BERT classifier. We experimented
    with various ensembles of labelling functions and used Snorkel’s Label Model to
    obtain predictions for a held-out labelled dataset. This allowed us to determine
    which ensemble of labelling functions would be best to label our unlabelled dataset.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 按照上述程序，我们基于关键词搜索、基准模型和我们的 MS-BERT 分类器开发了各种标注函数。我们尝试了各种标注函数的集成，并使用 Snorkel 的
    Label Model 来获取持出标注数据集的预测。这使我们能够确定哪种标注函数集成最适合标注我们的未标记数据集。
- en: As shown in the table below, we observed that the MS-BERT classifier (MSBC)
    alone outperformed all ensembles that contain itself by at least 0.02 on Macro-F1\.
    The addition of weaker heuristics and classifiers consistently decreased the ensemble’s
    performance. Furthermore, we observed that the amount of conflict for the MS-BERT
    classifier increased as weaker classifiers and heuristics were added to the ensemble.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如下表所示，我们观察到仅使用 MS-BERT 分类器（MSBC）就比包含自身的所有集成模型在 Macro-F1 上高出至少 0.02。加入较弱的启发式方法和分类器会持续降低集成模型的表现。此外，我们观察到，随着较弱的分类器和启发式方法被加入到集成模型中，MS-BERT
    分类器的冲突量增加。
- en: '![Figure](../Images/bcd40b9154f9ad113f7b3c41b440b185.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/bcd40b9154f9ad113f7b3c41b440b185.png)'
- en: Note, Rule Based (RB) refers to our key-word searches. LDA refers to linear
    discriminant analysis. TFIDFs refer to all models built on top of tf-idf features
    (i.e. logistic regression, linear discriminant analysis, and support vector machines).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，基于规则的（RB）指的是我们的关键词搜索。LDA 指的是线性判别分析。TFIDFs 指的是所有基于 tf-idf 特征构建的模型（即逻辑回归、线性判别分析和支持向量机）。
- en: To understand our findings, we have to remind ourselves that Snorkel’s label
    model learns to predict the accuracy and correlations of the labelling functions
    based on agreements and disagreements amongst each other. Therefore in the presence
    of a strong labelling function, such as our MS-BERT classifier, the addition of
    weaker labelling functions introduces more disagreements with the strong labelling
    functions and therefore decreases performance. From these findings, we learned
    that Snorkel may be more suited for situations where you *only* have weak heuristics
    and rules. However, if you already have a strong labelling function, developing
    a Snorkel ensemble with weaker heuristics may compromise performance.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解我们的发现，我们必须提醒自己，Snorkel 的标签模型通过各标注函数之间的一致性和不一致性来预测标注函数的准确性和相关性。因此，在存在强标注函数的情况下，例如我们的
    MS-BERT 分类器，添加较弱的标注函数会引入更多与强标注函数的不一致，从而降低性能。从这些发现中，我们了解到 Snorkel 可能更适合仅拥有弱启发式和规则的情况。然而，如果你已经拥有强标注函数，开发具有较弱启发式的
    Snorkel 集成可能会妨碍性能。
- en: Therefore, the MS-BERT classifier alone was chosen to label our unlabelled dataset.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，MS-BERT 分类器被选中用于标注我们的未标记数据集。
- en: Semi-Supervised Labelling Results
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 半监督标注结果
- en: 'The MS-BERT classifier was used to obtain ‘silver’ labels for our unlabelled
    dataset. These ‘silver’ labels were combined with our ‘gold’ labels to obtain
    a silver+gold dataset. To infer the quality of the silver labels, new MS-BERT
    classifiers were developed: 1) MS-BERT+ (trained on silver+gold labelled data);
    and 2) MS-BERT-silver (trained on silver labelled data). These classifiers were
    evaluated on a held-out test dataset that was previously used to evaluate our
    original MS-BERT classifier (trained on gold labelled data). MS-BERT+ achieved
    a Macro-F1 of 0.86238 and a Micro-F1 of 0.92569, and MS-BERT-silver achieved a
    Macro-F1 of 0.82922 and a Micro-F1 of 0.91442\. Although their performance was
    slightly lower that our original MS-BERT classifier (Macro-F1 of 0.88296, Micro-F1
    of 0.94177), they still outperformed the previous best baseline models for MS
    severity prediction. The strong results of MS-BERT-silver helps show the effectiveness
    of using our MS-BERT classifier as a labelling function. It demonstrates potential
    to reduce tedious hours required by a professional to read through a patient’s
    consult note and manually generate MS severity scores.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: MS-BERT 分类器用于为我们的未标记数据集获取“银色”标签。这些“银色”标签与我们的“金色”标签结合，形成银色+金色数据集。为了推断银色标签的质量，开发了新的
    MS-BERT 分类器：1) MS-BERT+（在银色+金色标记数据上训练）；2) MS-BERT-silver（在银色标记数据上训练）。这些分类器在之前用于评估我们原始
    MS-BERT 分类器（在金色标记数据上训练）的保留测试数据集上进行了评估。MS-BERT+ 实现了 0.86238 的 Macro-F1 和 0.92569
    的 Micro-F1，而 MS-BERT-silver 实现了 0.82922 的 Macro-F1 和 0.91442 的 Micro-F1。虽然它们的性能略低于我们原始的
    MS-BERT 分类器（Macro-F1 为 0.88296，Micro-F1 为 0.94177），但它们仍超越了以前的 MS 严重性预测基准模型。MS-BERT-silver
    的强大结果有助于展示使用我们 MS-BERT 分类器作为标注函数的有效性。它展示了减少专业人员阅读患者咨询记录并手动生成 MS 严重性评分所需的繁琐时间的潜力。
- en: Thank You!
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 谢谢！
- en: Thanks for reading everyone! If you have any questions please do not hesitate
    to contact us at nlp4health (at gmail dot) com. :)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢大家的阅读！如果您有任何问题，请随时通过 nlp4health (at gmail dot) com 联系我们。:)
- en: Acknowledgements
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 致谢
- en: We would like to thank the researchers and staff at the Data Science and Advanced
    Analytics (DSAA) department, St. Michael’s Hospital, for providing consistent
    support and guidance throughout this project. We would also like to thank Dr.
    Marzyeh Ghassemi, and Taylor Killan for providing us the opportunity to work on
    this exciting project. Lastly, we would like to thank Dr. Tony Antoniou and Dr.
    Jiwon Oh from the MS clinic at St. Michael’s Hospital for their support on the
    neurological examination notes.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢圣迈克尔医院数据科学与高级分析（DSAA）部门的研究人员和工作人员，在整个项目中提供了一贯的支持和指导。我们还要感谢 Marzyeh Ghassemi
    博士和 Taylor Killan，为我们提供了参与这一激动人心项目的机会。最后，我们要感谢圣迈克尔医院 MS 门诊的 Tony Antoniou 博士和
    Jiwon Oh 博士，感谢他们对神经检查记录的支持。
- en: '*Originally published at *[*https://nlp4h.com*](https://nlp4h.com/blog/snorkel_tutorial/)*.*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*最初发布于*[*https://nlp4h.com*](https://nlp4h.com/blog/snorkel_tutorial/)*.*'
- en: '**Bio: [The authors](https://nlp4h.com/authors/)** are a group of graduate
    students at University of Toronto working on NLP for healthcare.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[The authors](https://nlp4h.com/authors/)** 是多伦多大学的研究生，专注于医疗保健中的自然语言处理。'
- en: '[Original](https://medium.com/swlh/a-snorkel-tutorial-using-clinical-notes-to-predict-multiple-sclerosis-severity-scores-e3863801630f).
    Reposted with permission.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/swlh/a-snorkel-tutorial-using-clinical-notes-to-predict-multiple-sclerosis-severity-scores-e3863801630f)。已获许可转载。'
- en: '**Related:**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Hand labeling is the past. The future is #NoLabel AI](/2020/02/hand-labeling-past-future-nolabel-ai.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[手动标注已经是过去。未来是#NoLabel AI](/2020/02/hand-labeling-past-future-nolabel-ai.html)'
- en: '[From Languages to Information: Another Great NLP Course from Stanford](/2020/06/languages-information-another-great-nlp-course-stanford.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从语言到信息：斯坦福大学的另一个优秀NLP课程](/2020/06/languages-information-another-great-nlp-course-stanford.html)'
- en: '[The Unreasonable Progress of Deep Neural Networks in Natural Language Processing
    (NLP)](/2020/06/unreasonable-progress-deep-neural-networks-nlp.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度神经网络在自然语言处理（NLP）中的不合理进步](/2020/06/unreasonable-progress-deep-neural-networks-nlp.html)'
- en: More On This Topic
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关话题
- en: '[How to Determine the Best Fitting Data Distribution Using Python](https://www.kdnuggets.com/2021/09/determine-best-fitting-data-distribution-python.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用Python确定最佳拟合数据分布](https://www.kdnuggets.com/2021/09/determine-best-fitting-data-distribution-python.html)'
- en: '[Detecting Data Drift for Ensuring Production ML Model Quality Using Eurybia](https://www.kdnuggets.com/2022/07/detecting-data-drift-ensuring-production-ml-model-quality-eurybia.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Eurybia检测数据漂移以确保生产机器学习模型质量](https://www.kdnuggets.com/2022/07/detecting-data-drift-ensuring-production-ml-model-quality-eurybia.html)'
- en: '[4 Ways Hackers Are Using Data Science to Steal Billions](https://www.kdnuggets.com/2022/02/4-ways-hackers-data-science-steal-billions.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[黑客如何利用数据科学窃取数十亿美元的4种方式](https://www.kdnuggets.com/2022/02/4-ways-hackers-data-science-steal-billions.html)'
- en: '[Using Data Science to Make Clean Energy More Equitable](https://www.kdnuggets.com/2022/03/data-science-make-clean-energy-equitable.html)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用数据科学使清洁能源更加公平](https://www.kdnuggets.com/2022/03/data-science-make-clean-energy-equitable.html)'
- en: '[How to Ace Data Science Assessment Test by Using Automatic EDA Tools](https://www.kdnuggets.com/2022/04/ace-data-science-assessment-test-automatic-eda-tools.html)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何通过使用自动EDA工具来应对数据科学评估测试](https://www.kdnuggets.com/2022/04/ace-data-science-assessment-test-automatic-eda-tools.html)'
- en: '[Introduction to Data Visualization Using Matplotlib](https://www.kdnuggets.com/2022/12/introduction-data-visualization-matplotlib.html)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Matplotlib进行数据可视化入门](https://www.kdnuggets.com/2022/12/introduction-data-visualization-matplotlib.html)'
