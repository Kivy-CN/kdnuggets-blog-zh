# 数据分析的 Python……真的那么简单吗？！？！

> 原文：[https://www.kdnuggets.com/2020/04/python-data-analysis-really-that-simple.html](https://www.kdnuggets.com/2020/04/python-data-analysis-really-that-simple.html)

[评论](#comments)

**由 [Ferenc Bodon 博士](https://www.linkedin.com/in/ferencbodon/)，Kx 的数据工程师和云解决方案架构师**

![图示](../Images/b8a0052d9e1d377c6bae71923bcd8ad5.png)

由 CineArt 设计并制作

[Python](https://www.python.org/) 是一种易于学习、高效且拥有大量活跃社区支持的流行编程语言。它是一种通用语言，拥有针对各种领域的专用库，包括 Web 开发、脚本编写、数据科学和 DevOps。

其主要的数据分析库 [Pandas](https://pandas.pydata.org/) 在数据科学家和数据工程师中获得了广泛的欢迎。它遵循 Python 的原则，因此似乎容易学习、阅读，并允许快速开发……至少基于教科书的例子。但如果我们离开教科书例子的安全和方便的世界，会发生什么？Pandas 仍然是一个易于使用的工具来查询 **表格数据** 吗？与其他专业工具如 [R](https://www.r-project.org/) 和 [kdb+](https://code.kx.com/q/learn/) 相比，它的表现如何？

在本文中，我将以一个超越最简单用例的例子为例，通过对 **多个列的聚合** 来演示。我的用例复杂性大约是 5 级中的第 2 级。任何分析数据表的人都会遇到这个问题，可能在第二周就会遇到。为了对比，我还将涵盖其他旨在进行数据分析的流行工具。

+   首先，问题可以通过 [ANSI SQL](https://en.wikipedia.org/wiki/SQL) 解决，因此所有传统的 RDBM 系统如 [PostgreSQL](https://www.postgresql.org/)、[MySQL](https://www.linkedin.com/redir/general-malware-page?url=https%3A%2F%2Fwww%2emysql%2ecom%2F) 等都可以参与游戏。在实验中，我将使用 [BigQuery](https://cloud.google.com/bigquery/)，这是 Google 提供的无服务器、高度可扩展的数据仓库解决方案。

+   [R](https://www.r-project.org/)编程语言是为统计分析设计的。它通过其类[data.frame](https://www.google.com/search?q=r+data.frame&oq=r+data.fr&aqs=chrome.0.0j69i57j0l4j69i60j69i61.6225j1j7&sourceid=chrome&ie=UTF-8)本地支持表格。由于核心函数[aggregate](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/aggregate)的限制，使用多个聚合功能相当不便。R社区开发了库[plyr](https://cran.r-project.org/web/packages/plyr/index.html)以简化data.frame的使用。包[plyr](https://cran.r-project.org/web/packages/plyr/index.html)已被淘汰，包[dplyr](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8)被引入，承诺提供改进的API和更快的执行速度。包[dplyr](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8)是[tidyverse](https://www.tidyverse.org/)集合的一部分，专为专业数据科学工作设计。它提供了一个抽象的查询层，并将查询与数据存储解耦，无论是data.frame还是支持ANSI SQL的外部数据库。包[data.table](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html)是[dplyr](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8)的替代方案，以其速度和简洁的语法而著名。data.table也可以使用dplyr语法进行查询。

+   在[Q/Kdb+](https://code.kx.com/v2/learn/q-for-all/)编程语言中，表格也是一等公民，速度是该语言的主要设计理念。Kdb+自2004年诞生以来就利用多核处理器，并[使用Map-Reduce](https://code.kx.com/q4m3/14_Introduction_to_Kdb+/#1437-map-reduce)处理数据，如果数据在磁盘上是[分区的](https://code.kx.com/q4m3/14_Introduction_to_Kdb+/#143-partitioned-tables)。从4.0版本（2020年3月发布）开始，大多数原语（如sum、avg、dev）使用从属线程并行执行，即使表格在内存中。生产力是另一个设计考虑因素——任何不有助于理解的冗余编程元素（甚至一个括号）都被视为视觉噪声。Kdb+是任何数据分析工具的有力竞争者。

我将考虑各种解决方案的优雅性、简洁性和速度。同时，我会研究如何**调整性能**以及通过采用**并行计算**来利用多核处理器或计算机集群。

### 问题

![示例输入表](../Images/90809ed59bbfbe9914c3071e0b9e2e8d.png)

我们得到一个简单的表格，包含四列，一列名为**bucket**，另外三列为**qty**、**risk**和**weight**。为简便起见，假设数值列包含整数。

我们希望查看每个**bucket**的情况。

+   元素的数量，作为列NR

+   **qty**和 **risk**的总和和平均值，作为列 TOTAL_QTY/TOTAL_RISK 和 AVG_QTY/AVG_RISK

+   **qty**和 **risk**的加权平均值，作为列 W_AVG_QTY 和 W_AVG_RISK。权重在列 **weight**中提供。

为了得到解决方案，我不会使用任何已废弃的方法，例如 [通过嵌套字典重命名聚合](https://github.com/pandas-dev/pandas/issues/18366)。让我们分别解决每个任务。

### 每个桶中的元素数量

计算每个桶中的元素数量看起来不太吸引人，需要大量打字。

![未提供该图像的替代文本](../Images/09dbc34b1ce0a1f854605837fbbecd46.png)

字面量 **bucket** 需要三次，并且你需要使用 5 个括号/圆括号 [????](https://en.wikipedia.org/wiki/%F0%9F%98%90)。

R 的解决方案看起来更有吸引力。

![未提供该图像的替代文本](../Images/d8dd9c42ea18835850c82d7ec1698196.png)

dplyr 和 data.table 的库开发者也对词汇重复感到厌恶。他们分别引入了内置变量 **n()** 和 **.N**，它们保存当前组中的观察数量。这简化了表达式，我们可以省去一对圆括号。

![未提供该图像的替代文本](../Images/d0a435e726edac8b2dfa565a41abb199.png)

ANSI SQL 表达式简单易懂。

![未提供该图像的替代文本](../Images/cf1334e8735319fcab68085a13ebf857.png)

通过在 GROUP BY 子句中使用列索引可以避免字面量 bucket 的重复。个人认为这不是推荐的设计，因为表达式不具自我文档性且不够健壮。事实上， [Apache 已废弃使用数字](https://issues.apache.org/jira/browse/DRILL-942) 在 GROUP BY 子句中。

kdb+ 表达式更优雅。它不需要括号、引号或任何词汇重复。

![未提供该图像的替代文本](../Images/18dd6a40ac5e9e05bc79f239bb8bdf60.png)

SQL 形成了数据分析的基础，因此可能每个人都理解 ANSI SQL 和 kdb+ 解决方案。R 和 kdb+ 的开发者一致认为“GROUP BY”太冗长，一个简单的“by”字面量已经足够表达。

请注意，除了 Pandas 外，没有其他语言在这个简单表达式中使用任何引号。Pandas 中的查询需要四对引号 [????](https://en.wikipedia.org/wiki/%F0%9F%98%90) 来包裹列名。在 R、SQL 和 kdb+ 中，你可以像引用变量一样引用列。R 中的 .() 符号 - 它是 list() 的别名 - 提供了这种便利特性。

### 多列的聚合

使用 Pandas 计算单列的总和和平均值，以及计算多列的总和都非常简单。

![未提供该图像的替代文本](../Images/6ec1c299d8ec3781122391df1160c25f.png)

![未提供该图像的替代文本](../Images/bdb598a90547d3cec86cc0947080d21e.png)

![未提供该图像的替代文本](../Images/a236c220eb5a01618c1178fc6015d892.png)

![未提供此图像的替代文本](../Images/abc52c2336cdd49ae81ecc4a7f89f146.png)

如果你尝试将两种方法结合在一起，代码会变得复杂，因为这会导致列名冲突。需要引入[多级列](https://pandas.pydata.org/pandas-docs/stable/advanced.html)和函数[map](https://docs.python.org/3/library/functions.html#map)。

![未提供此图像的替代文本](../Images/1152478e711c7acec5ae6ff74ebb8b27.png)

![未提供此图像的替代文本](../Images/b579576cf182169ab5bfa0e1bd7b5bf3.png)

SQL、R和kdb+的等价物不需要引入任何新概念。新的聚合操作仅通过逗号分隔。你可以使用关键字[sum](https://code.kx.com/q/ref/arith-integer/#sum)和[avg](https://code.kx.com/v2/ref/stats-aggregates/#avg-average)/[mean](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/mean)来分别获取和平均值。

![未提供此图像的替代文本](../Images/2f244c5f53b988918700736159dfec04.png)

观察kdb+表达式的简洁性；它不需要括号或方括号。

![未提供此图像的替代文本](../Images/a229bab4db85c16e330ea10bcdd452b3.png)

### 加权平均

加权平均由[Pandas](http://www.numpy.org/)依赖的[Numpy](http://www.numpy.org/)库支持。不幸的是，它不能像`np.sum`那样直接使用。你需要将其包裹在一个[lambda 表达式](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions)中，引入一个局部变量，使用[apply](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html)代替[agg](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.agg.html)，并从系列创建一个数据框。

![未提供此图像的替代文本](../Images/ddb0578e6a3329675ac8fec347b94763.png)

![未提供此图像的替代文本](../Images/31ddf49336f61983417332e95ee3c09a.png)

标准SQL及其Google扩展BigQuery都没有提供获取加权平均的内置函数。你需要回忆定义并手动实现它。

![未提供此图像的替代文本](../Images/9451c257a36f1647822fc087988a3722.png)

再次强调，R和Q/Kdb+的解决方案不需要引入任何新概念。加权平均函数在本地得到支持，并接受两个列名作为参数。

![未提供此图像的替代文本](../Images/4d22bb3a4180e3b8d37f45f5225f8945.png)

在kdb+中，你可以使用中缀表示法来获得更自然的语法——只需将其读作"**w** 加权平均 **x**"。

![未提供此图像的替代文本](../Images/44af1a3b6d157860e89282b6d5e55796.png)

### 一条语句搞定

让我们把所有部分结合在一起。我们创建了多个数据框，因此需要将它们合并

![未提供此图像的替代文本](../Images/385ca65b6d18111fbafd145f8b680d10.png)

![未提供此图像的替代文本](../Images/adb4ef52dba992faf73efce55d01ae7f.png)

注意，第一个 [join](https://docs.python.org/3/library/stdtypes.html#str.join) 表达式与其他表达式无关。它从字符串列表中创建一个字符串，而其他表达式执行 [left joins](https://en.wikipedia.org/wiki/Join_(SQL))。

要获得最终结果，我们需要三个表达式和一个临时变量**res**。如果我们花更多时间搜索论坛，可以发现这种复杂性部分归因于函数 [agg](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.agg.html) 中的嵌套字典的弃用。此外，我们可能会发现使用仅有的函数 [apply](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html) 的一种替代方法，没有连接。不幸的是，这种解决方案返回所有浮点型的数值列，因此需要显式地转换整数列。

![未提供该图像的替代文本](../Images/43c61ad539c87048ef81ac96b602cc49.png)

这个解决方案需要创建一个临时函数，这个函数可能在你的源代码中不会再次使用。我们可以将所有语句压缩成一个单一的无状态解决方案，但这会导致难以阅读和维护的嵌套代码。此外，这种第二种方法在中等大小的表格上速度较慢。

Pandas [2019 年 7 月 18 日的发布](https://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.25.0.html) 支持通过 [named aggregator](http://pandas-docs.github.io/pandas-docs-travis/user_guide/groupby.html#aggregation) 进行分组聚合。它提供了比前述基于 apply 的解决方案更优雅的语法，并且不需要类型转换。此外，开发人员可能已经认识到使用引号造成的麻烦。不幸的是，权重平均数不被支持，因为聚合中只能使用单列。为了完整性，我们提供了新的适当语法，忽略了加权平均计算。很高兴看到输出列名不再需要引号。

![未提供该图像的替代文本](../Images/629c97d73d3196e3a291e0f8889848d6.png)

相比之下，SQL 早在 30 年前就已经提供了优雅的解决方案。

![未提供该图像的替代文本](../Images/9b1f4d0545f1bb90da296f0b3f167ab9.png)

让我们看看 R 如何使用 data.table 解决这个任务

![未提供该图像的替代文本](../Images/97325ee6d91274a11ecbbea83638169c.png)

以及 kdb+ 中的解决方案的表现

![未提供该图像的替代文本](../Images/50880fb4fe953406cda883a5245f61e9.png)

看起来 kdb+ 提供了**最直观、最简单且最易读**的解决方案。它是无状态的，不需要括号/方括号和临时变量（或函数）的创建。

### 那性能怎么样呢？

实验在 Windows 和 Linux 上进行，使用的是稳定的最新二进制文件和库。查询执行了百次，测试 Jupyter notebooks 可在 [Github](https://github.com/BodonFerenc/PythonIsThisReallySimple) 上获取。数据是随机生成的。桶字段是大小为二的字符串，字段 **qty** 和 **risk** 由 64 位整数表示。在 R 中使用库 [bit64](https://cran.r-project.org/web/packages/bit64/index.html) 获取 64 位整数。下表总结了以毫秒为单位的执行时间。坐标轴采用对数刻度。将两个 Python 解决方案（版本 3.6.8，Pandas 0.25.3）与三个 R 库（版本 3.6.0）以及两个 kdb+ 版本进行比较。

![没有提供此图像的替代文本](../Images/50507e7d70a7e844b6ebc76ee774bdac.png)

kdb+ 解决方案不仅比 Pandas 更优雅，而且速度也快了一个数量级。对于这个特定的查询，R 的 data.table 1.12.6 版本比 kdb+ 3.6 慢三倍。kdb+ 4.0 对于十亿行的表格比 kdb+ 3.6 快五倍。dplyr 0.8.3 版本的包比 plyr 1.8.5 版本慢两个数量级 ????。

Pandas 在处理 10 亿行的输入表时达到了内存限制。所有其他解决方案都能够处理这种规模的数据而不会耗尽内存。

让我们看看如何减少执行时间。

### 性能优化

列桶包含字符串。如果领域大小较小且有很多重复项，则建议使用**类别**值而不是字符串。类别类似于枚举，由整数表示，因此它们消耗的内存更少，比较速度更快。你可以通过以下方法在 Pandas 中将字符串转换为类别

![没有提供此图像的替代文本](../Images/73accb51f6d0fb3b5bc5ef29919a4f3a.png)

但在构建表时创建类别列会更节省内存。我们使用函数 [product](https://docs.python.org/2/library/itertools.html#itertools.product) 通过采用 [笛卡尔积](https://en.wikipedia.org/wiki/Cartesian_product) 来生成长度为二的字符串全集。在下面的代码片段中，我们省略了创建其他列的语法。*N* 存储要插入的行数。

![没有提供此图像的替代文本](../Images/c5b89afd5ab91b225ef844c1d90ffb9b.png)

在 R 中，类别被称为**因素**。类 data.frame 会自动将字符串转换为因素（使用 "stringAsFactors = FALSE" 可以避免这种情况），但在 data.table 中，字符串会被保留，原因正当。

![没有提供此图像的替代文本](../Images/5b272ddf500676bc805aa50daf5d8217.png)

BigQuery 没有类别或因素的概念。相反，它应用各种 [编码和压缩技术](http://db.csail.mit.edu/projects/cstore/abadisigmod06.pdf) 来实现最佳性能。要生成随机字符串，你可以使用 Unicode [代码点](https://en.wikipedia.org/wiki/Code_point)、函数 RAND 和 [CODE_POINTS_TO_STRING](https://cloud.google.com/bigquery/docs/reference/standard-sql/string_functions#code_points_to_string) 以及一些转换。小写字母 "a" 的代码点是 97——你可以使用函数 [TO_CODE_POINTS](https://cloud.google.com/bigquery/docs/reference/standard-sql/string_functions#to_code_points) 找出这一点。

![未提供此图像的替代文本](../Images/ea82a723615b346ff150438ac487e9e7.png)

你可以使用一个 [用户定义的函数](https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions) 来避免代码重复。

![未提供此图像的替代文本](../Images/871bafeb6392a6867c6ebdd44de59c04.png)

作为对比，在 kdb+ 中，相同的操作看起来是这样的。

![未提供此图像的替代文本](../Images/59f019d6bf657f2bec2628a52651a4ab.png)

构造 [N?M](https://code.kx.com/q/ref/deal/) 会根据 M 的类型生成 N 个随机值。如果 M 是整数、浮点数、日期或布尔值，则返回一个随机整数、浮点数、日期或布尔值向量。如果 M 是列表，则随机选择列表元素。如果 N 是负整数，则结果不会包含任何重复项。在 kdb+ 中，许多操作符以类似方式被重载。

在 kdb+ 中，枚举被称为**符号**，并被开发人员广泛使用。该语言强烈支持符号。你可以使用它们而无需事先定义可能的值，kdb+ 会为你维护映射。

根据我的测量，通过类别/符号的优化在 Pandas 和 kdb+ 中可以将运行时间减少**两倍**。R 的 data.table 显示出不同的特性。使用因素而不是字符串对性能没有影响。这是由于**全局字符串池**的内置字符串优化。虽然，因素被存储为 32 位整数，而字符串需要 64 位指向池元素的指针，但这种差异对执行时间的影响很小。

如果我们使用占用空间更少的类型，可以进一步提高性能。例如，如果列 **qty** 可以适配到 2 字节，那么我们可以在 Pandas 中使用 [int16](https://numpy.org/devdocs/user/basics.types.html)，在 kdb+ 中使用 [short](https://code.kx.com/q4m3/2_Basic_Data_Types_Atoms/#212-short-and-int)。这会减少内存操作，而内存操作通常是数据分析的瓶颈。R 不支持 2 字节整数，因此我们使用了其默认的 4 字节整数类型。

32 位整数（R 中的默认值）可以带来 5-6 倍的执行时间改进。只有在你真正需要大基数时才使用 64 位整数。这对 dplyr 包尤其适用。

在计算聚合时，我们需要跟踪由于分组而形成的桶。如果表按组列排序，则由于组已在RAM中连续收集，聚合会更快。执行时间在Pandas中降至约三分之一，在R中降至一半，在kdb+中降至五分之一。以下是具有类型优化的大小为10亿的排序表的执行时间。

![没有提供替代文本](../Images/f76b3f1c536fe46463888ba67835d7b9.png)

### 并行化

所有现代计算机都有多个CPU核心。Pandas默认在单核上操作。我们需要做什么才能使计算并行并利用多个核心？

Python库 [Dask](https://dask.org/) 和 [Ray](https://github.com/ray-project/ray) 是运行并行计算的两个最著名的库。库 [Modin](https://modin.readthedocs.io/en/latest/) 是这些引擎的Pandas数据框的封装器。关于这一点的“媒体”报道很大声，声称通过替换一行代码，你可以在普通笔记本电脑上获得显著的查询性能提升。

![没有提供替代文本](../Images/cbd9819fe215b1dfa1a2eebe966e7882.png)

到

![没有提供替代文本](../Images/3ca36daed1406cf034705e2564d9f129.png)

这可能对于少量的教科书示例来说是对的，但在现实生活中并不适用。在我的简单实验中，我遇到了Ray和Dask的几个问题。让我一个个描述这些问题，从Ray开始。

首先，类别类型不被支持。其次，函数 [apply](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html#pandas.DataFrame.apply) 和 [agg](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html#pandas.DataFrame.agg) 的行为与相应的Pandas函数不同。函数 [agg](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html#pandas.DataFrame.agg) 中使用多个聚合与分组操作不被支持，因此操作会回退到Pandas操作。函数 [apply](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html#pandas.DataFrame.apply) 只处理返回标量的lambda函数。这禁用了第二种更优雅的Pandas解决方案。此外，apply返回的是Modin数据框而不是系列。你需要转置结果并将列索引（0）重命名为有意义的列名，例如。

![没有提供替代文本](../Images/2921d63f8270367cfdd8f64cebfde02c.png)

最终，代码运行明显比Pandas等效版本要慢，并且在处理1亿行时崩溃。Pandas和其他工具能够轻松处理10亿行数据。

转移到库时，Dask 也有一些麻烦。首先，如果桶的类型是分类类型，它不处理加权平均数。我们失去了一个重要的性能提升技术。其次，你需要提供类型提示以抑制警告。这意味着又一次的列名重复。在更优雅的基于应用的解决方案中，你需要四次输入输出列名（如 TOTAL_QTY）☹️。因此，看来转移到 Dask 并不像通过简单的 [compute](https://docs.dask.org/en/latest/dataframe.html) 语句来触发计算那样简单。

![未提供此图像的替代文本](../Images/c43adaa24c846a34ec6614a449924c16.png)

**kdb+ 中的并行化是自动的** 对于磁盘上的分区表和版本 4.0 的内存表。你不会遇到任何类型问题——一切运行顺畅。你只需通过 [命令行参数 -s](https://code.kx.com/q/basics/syscmds/#s-number-of-slaves) 以多进程模式启动 kdb+。内置的 [map-reduce](https://code.kx.com/q4m3/14_Introduction_to_Kdb+/#1437-map-reduce) 分解将计算分散到多个核心，涵盖了大多数操作，包括 [sum](https://code.kx.com/q/ref/sum/)， [avg](https://code.kx.com/q/ref/avg/)， [count](https://code.kx.com/q/ref/count/)， [wavg](https://code.kx.com/q/ref/avg/)， [cor](https://code.kx.com/q/ref/cor/) 等。你也可以通过手动分区表并使用函数 [peach](https://code.kx.com/v2/ref/each/) 获得性能提升，该函数并行执行。我们只需更改

![未提供此图像的替代文本](../Images/b008380e3dddeb72d3431f2b32531649.png)

到

![未提供此图像的替代文本](../Images/9644443ab45161925cc211f122377808.png)

这个简单的代码修改使得在 16 核心的机器上使用 kdb+ 版本 3.6 的执行速度快了近一个数量级。由于版本 4.0 已经采用了并行计算，手动并行化没有任何价值。如果你追求最佳性能，那么使用 kdb+ 4.0 的代码会比 3.6 更简单。

**R data.table 默认也使用多个线程** 并在后台并行执行查询。你可以通过函数 [setDTthreads](https://www.rdocumentation.org/packages/data.table/versions/1.12.8/topics/setDTthreads) 检查和设置 data.table 使用的线程数量。

让我们比较所有语言的最优版本的执行时间。对于 SQL，我们评估了 BigQuery，因为它被认为是处理巨大数据集的最快 SQL 实现，得益于其大规模并行化。

![未提供此图像的替代文本](../Images/ae75206d32368a5a479ea33b41000b14.png)

kdb+ 在这一类别中再次获胜，BigQuery 是亚军。R data.table 的速度是 Pandas 的两倍。

### 超过 10 亿行

我们的经验中最大表格包含十亿行。超过这个数量，表格无法适配到内存中，因此Pandas无能为力。Dask和Ray设计用于并行处理和计算机集群，相比其他竞争者表现较差。**对于BigQuery，表格的大小几乎没有关系。** 如果我们将行数从10亿增加到100亿或1000亿，执行时间几乎不会增加。

在kdb+中，数据可以持久化到磁盘，因此可以处理TB级的数据。查询将保持不变，kdb+会自动应用map-reduce并利用多核处理器。此外，如果数据被分段且段位于具有独立IO通道的不同存储上，则IO操作将并行执行。这些低级优化使得基于kdb+的解决方案能够优雅地扩展。

### 使用kdb+的分布式计算

作为最终的简单练习，让我们探讨如何将计算分散到多个kdb+进程中，利用我们的机器集群并水平分割样本表。实现类似Ray/Dask/Spark的分布式计算在kdb+中有多困难？

函数peach使用外部的kdb+从属进程，而不是从属线程，如果变量[.z.pd](https://code.kx.com/v2/ref/dotz/#zpd-peach-handles)存储连接到从属kdb+进程的连接句柄。

![没有提供此图像的替代文本](../Images/7076b41e633ca03c787d89834e9c2625.png)

我们可以按桶值分割表**t**

![没有提供此图像的替代文本](../Images/9e2947cfcc84890d63991af6544b3986.png)

最后，我们可以分发选择语句并合并结果。函数[raze](https://code.kx.com/q/ref/raze/)类似于Pandas的[concat](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html)函数。从一个表格列表中，它通过连接生成一个大表。

![没有提供此图像的替代文本](../Images/db8042fb22f14074923e27a320a5be16.png)

做得好！我们用四行代码实现了“kdb-spark”。

### 代码简洁性

我收集了一些关于代码本身的统计数据。虽然简短的代码不一定意味着干净的代码，但对于这个特定的例子，这些指标与简洁性很好地相关。

![没有提供此图像的替代文本](../Images/68d2942f4fa51ce053a89dab058e893c.png)

### 结论

我的观察结果汇总在下面的表格中。表情符号????表示优秀，✔️表示良好，☹️表示表现令人失望。

![没有提供此图像的替代文本](../Images/2b59ea9e74f33cc6be88ae78a1e435b8.png)

Python通常是学生学习的第一个编程语言。它简单、性能良好且学习曲线较小。它的库Pandas是引导新手进入数据分析世界的自然步骤。Pandas也常用于专业环境和更复杂的数据分析。Pandas在简单的教科书练习中看起来很诱人，但在我们的简单实际应用中却不方便使用。

更多的关注点在于通过集群计算扩展Pandas并支持大型表。Ray、Dask和Modin处于早期阶段，有很多限制。在我们的使用案例中，它们只是增加了语法复杂性，并实际上降低了性能。

R在每个方面都优于Pandas，包括简单性、优雅性和性能。它有几个内置的优化，比如固有的多线程。字符串表示的优化效果很好，使开发者可以专注于分析，而不是琐碎的技术细节。R数据表被[移植到Python](https://github.com/h2oai/datatable)也就不足为奇了。也许数据表包将来会取代Pandas？

kdb+将数据分析提升到新的水平。它被设计为极致的生产力工具。在我们的使用案例中，它在简单性、优雅性和性能方面明显胜出。难怪资本市场前20名组织中有20个选择了kdb+作为主要的数据分析工具。在这个行业中，数据分析驱动收入，工具在极端条件下进行测试。

BigQuery在处理超过100亿行的数据时表现出色，前提是你没有一群计算机。如果你需要分析巨大的表格，并且对运行时间非常敏感，那么BigQuery为你做得很好。

### 相关工作

[DB ops benchmark](https://h2oai.github.io/db-benchmark/) 最初由[R数据表](https://twitter.com/MattDowle)的创建者[Matt Dowle](https://twitter.com/MattDowle)发起。除了Pandas、data.table、dask dplyr，他们还在各种查询和不同参数下测试了[Apache Spark](https://spark.apache.org/)、[ClickHouse](https://clickhouse.yandex/)和[Julia数据框](https://juliadata.github.io/DataFrames.jl/stable/)。任何人都可以并排查看查询，甚至下载测试环境以进行自定义硬件的实验。

[Mark Litwintschik](https://www.linkedin.com/in/marklitwintschik/) 使用了NYC出租车驾驶数据集和[33种数据库系统](https://tech.marksblogg.com/benchmarks.html)上的四个查询。他提供了测试环境的详细描述、所用设置以及一些有价值的个人备注。这是一项彻底的工作，展示了Mark在大数据平台上的杰出知识。他的观察与我们的实验一致，kdb+在非GPU数据库解决方案中表现最快。

STAC-M3基准测试最初由几家全球最大银行和交易公司于[2010年开发](https://stacresearch.com/system/files/central/STAC-M3_Overview.pdf)。它旨在精确测量新兴硬件和软件创新如何提高时间序列分析的性能。在STAC-M3开发之后，kdb+迅速成为运行测试的硬件供应商的首选数据库平台，因为它设定了其他软件供应商无法超越的性能标准。此外，Google也承认STAC-M3作为时间序列分析的行业标准基准。他们使用kdb+来[展示](https://cloud.google.com/blog/products/compute/can-cloud-instances-perform-better-than-bare-metal-latest-stac-m3-benchmarks-say-yes)将数据和工作负载从本地迁移到GCP不会妥协性能。

### 致谢

我想感谢[Péter Györök](https://github.com/gyorokpeter)、[Péter Simon Vargha](https://www.linkedin.com/in/varghaps/)和[Gergely Daróczi](https://www.linkedin.com/in/daroczig/)提供的富有洞见的建议。

**简历：[Ferenc Bodon博士](https://www.linkedin.com/in/ferencbodon/)** 是一位经验丰富的数据工程师、软件开发者、多语言程序员、拥有数据挖掘和统计学学术背景的软件架构师。具有长期思维，始终致力于寻找高质量、稳健且可扩展的解决方案，并允许快速开发。相信软件质量，并且在找到“完美解决方案”之前无法放松。

[原文](https://www.linkedin.com/pulse/python-data-analysis-really-simple-ferenc-bodon-ph-d-/)。经许可转载。

**相关：**

+   [在Python中加速数据分析的10个简单技巧](/2019/07/10-simple-hacks-speed-data-analysis-python.html)

+   [你将永远需要的最后一份SQL数据分析指南](/2019/10/last-sql-guide-data-analysis-ever-need.html)

+   [柏林租金冻结：我能在网上找到多少非法的高价报价？](/2020/03/berlin-rent-freeze-illegal-overpriced-offers.html)

* * *

## 我们的三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速开启网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升您的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持您的组织在IT领域

* * *

### 更多相关主题

+   [每个数据科学家都应该知道的三个R库（即使你使用Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)

+   [是什么让 Python 成为初创公司的理想编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)

+   [停止学习数据科学以寻找目的，并寻找目的来…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [学习数据科学统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)

+   [成功数据科学家的 5 个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)

+   [一个 90 亿美元的人工智能失败案例，分析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)
