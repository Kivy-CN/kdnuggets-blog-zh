- en: 'Serving ML Models in Production: Common Patterns'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在生产环境中服务ML模型：常见模式
- en: 原文：[https://www.kdnuggets.com/2021/10/serving-ml-models-production-common-patterns.html](https://www.kdnuggets.com/2021/10/serving-ml-models-production-common-patterns.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/10/serving-ml-models-production-common-patterns.html](https://www.kdnuggets.com/2021/10/serving-ml-models-production-common-patterns.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Simon Mo](https://www.anyscale.com/blog?author=simon-mo), [Edward Oakes](https://www.anyscale.com/blog?author=edward-oakes)
    and [Michael Galarnyk](https://www.anyscale.com/blog?author=michael-galarnyk)**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[Simon Mo](https://www.anyscale.com/blog?author=simon-mo)、[Edward Oakes](https://www.anyscale.com/blog?author=edward-oakes)
    和 [Michael Galarnyk](https://www.anyscale.com/blog?author=michael-galarnyk)**'
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在IT方面'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This post is based on Simon Mo’s “Patterns of Machine Learning in Production” [talk](https://www.youtube.com/watch?v=mM4hJLelzSw) from
    Ray Summit 2021.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文基于Simon Mo在Ray Summit 2021上的“生产中的机器学习模式”[演讲](https://www.youtube.com/watch?v=mM4hJLelzSw)。
- en: 'Over the past couple years, we''ve listened to ML practitioners across many
    different industries to learn and improve the tooling around ML production use
    cases. Through this, we''ve seen 4 common patterns of machine learning in production:
    pipeline, ensemble, business logic, and online learning. In the ML serving space,
    implementing these patterns typically involves a tradeoff between ease of development
    and production readiness. [Ray Serve](https://docs.ray.io/en/latest/serve/index.html) was
    built to support these patterns by being both easy to develop and production ready.
    It is a scalable and programmable serving framework built on top of [Ray](https://www.ray.io/) to
    help you scale your microservices and ML models in production.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，我们听取了来自不同领域的ML从业者的意见，以改进ML生产使用案例的工具。通过这次经历，我们观察到生产中的机器学习有四种常见模式：管道、集成、业务逻辑和在线学习。在ML服务领域，实施这些模式通常涉及开发简易性和生产就绪性的权衡。[Ray
    Serve](https://docs.ray.io/en/latest/serve/index.html) 旨在支持这些模式，通过易于开发和生产就绪的特性。它是一个可扩展和可编程的服务框架，建立在[Ray](https://www.ray.io/)之上，帮助你在生产环境中扩展微服务和ML模型。
- en: 'This post goes over:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论了：
- en: What is Ray Serve
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是Ray Serve
- en: Where Ray Serve fits in the ML Serving Space
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ray Serve在ML服务领域中的位置
- en: Some common patterns of ML in production
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产中一些常见的ML模式
- en: How to implement these patterns using Ray Serve
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用Ray Serve实现这些模式
- en: What is Ray Serve?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是Ray Serve？
- en: '![Ray Ecosystem Serving ML Models in Production: Common Patterns](../Images/4bf023c751a82dcc888c87fd498e4ab9.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![Ray生态系统服务ML模型：常见模式](../Images/4bf023c751a82dcc888c87fd498e4ab9.png)'
- en: Ray Serve is built on top of the Ray distributed computing platform, allowing
    it to easily scale to many machines, both in your datacenter and in the cloud.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Serve建立在Ray分布式计算平台之上，能够轻松扩展到许多机器，无论是在你的数据中心还是在云端。
- en: 'Ray Serve is an easy-to-use scalable model serving library built on Ray. Some
    advantages of the library include:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Serve是一个易于使用的可扩展模型服务库，建立在Ray之上。该库的一些优点包括：
- en: 'Scalability: Horizontally scale across hundreds of processes or machines, while
    keeping the overhead in single-digit milliseconds'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展性：横向扩展到数百个进程或机器，同时保持开销在个位数毫秒内
- en: 'Multi-model composition: Easily compose multiple models, mix model serving
    with business logic, and independently scale components, without complex microservices.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模型组合：轻松组合多个模型，将模型服务与业务逻辑混合，并独立扩展组件，无需复杂的微服务。
- en: '[Batching](https://docs.ray.io/en/latest/serve/tutorials/batch.html): Native
    support for batching requests to better utilize hardware and improve throughput.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[批处理](https://docs.ray.io/en/latest/serve/tutorials/batch.html)：原生支持请求批处理，以更好地利用硬件并提高吞吐量。'
- en: '[FastAPI Integration](https://docs.ray.io/en/latest/serve/http-servehandle.html#serve-fastapi-http): Scale
    an existing FastAPI server easily or define an HTTP interface for your model using
    its simple, elegant API.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[FastAPI 集成](https://docs.ray.io/en/latest/serve/http-servehandle.html#serve-fastapi-http):
    通过其简单、优雅的 API，轻松扩展现有的 FastAPI 服务器或为你的模型定义 HTTP 接口。'
- en: 'Framework-agnostic: Use a single toolkit to serve everything from deep learning
    models built with frameworks like [PyTorch](https://docs.ray.io/en/master/serve/tutorials/pytorch.html), [Tensorflow
    and Keras](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html), to [Scikit-Learn
    models](https://docs.ray.io/en/latest/serve/tutorials/sklearn.html), to [arbitrary
    Python business logic](https://www.anyscale.com/blog/how-ikigai-labs-serves-interactive-ai-workflows-at-scale-using-ray-serve).'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 框架无关：使用单一工具包来服务从 [PyTorch](https://docs.ray.io/en/master/serve/tutorials/pytorch.html)、[Tensorflow
    和 Keras](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html) 构建的深度学习模型，到
    [Scikit-Learn 模型](https://docs.ray.io/en/latest/serve/tutorials/sklearn.html)，再到
    [任意 Python 业务逻辑](https://www.anyscale.com/blog/how-ikigai-labs-serves-interactive-ai-workflows-at-scale-using-ray-serve)。
- en: You can get started with Ray Serve by checking out the [Ray Serve Quickstart.](https://docs.ray.io/en/latest/serve/index.html)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过查看 [Ray Serve 快速入门](https://docs.ray.io/en/latest/serve/index.html) 来开始使用
    Ray Serve。
- en: Where Ray Serve fits in the ML Serving Space
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ray Serve 在 ML 服务领域中的定位
- en: '![WhereRayServeFitsIn](../Images/b4eed5d580a6125b7245a30168ed04ba.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![Ray Serve 的定位](../Images/b4eed5d580a6125b7245a30168ed04ba.png)'
- en: The image above shows that In the ML serving space, there is typically a tradeoff
    between ease of development and production readiness.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示，在 ML 服务领域，开发的便利性与生产准备性的平衡通常是一个权衡。
- en: '**Web Frameworks**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**Web 框架**'
- en: To deploy a ML service, people typically start with the simplest systems out
    of the box like Flask or FastAPI. However, even though they can deliver a single
    prediction well and work well in proofs of concept, they cannot achieve high performance
    and scaling up is often costly.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署 ML 服务，人们通常从像 Flask 或 FastAPI 这样的开箱即用的简单系统开始。然而，即便它们可以很好地提供单次预测并在概念验证中表现良好，它们通常无法实现高性能，并且扩展往往成本高昂。
- en: '**Custom Tooling**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**自定义工具**'
- en: If web frameworks fail, teams typically transition to some sort of custom tooling
    by gluing together several tools to make the system ready for production. However,
    these custom toolings are typically hard to develop, deploy, and manage.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Web 框架失败，团队通常会过渡到某种自定义工具，通过将几个工具粘合在一起来使系统准备好投入生产。然而，这些自定义工具通常难以开发、部署和管理。
- en: '**Specialized Systems**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**专用系统**'
- en: There is a group of specialized systems for deploying and managing ML models
    in production. While these systems are great at managing and serving ML models,
    they often have less flexibility than web frameworks and often have a high learning
    curve.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有一类专门用于在生产环境中部署和管理 ML 模型的系统。虽然这些系统在管理和服务 ML 模型方面表现出色，但它们通常不如 Web 框架灵活，并且学习曲线较高。
- en: '**Ray Serve**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**Ray Serve**'
- en: Ray Serve is a web framework specialized for ML model serving. It aspires to
    be easy to use, easy to deploy, and production ready.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Serve 是一个专门用于 ML 模型服务的 Web 框架。它致力于易于使用、易于部署，并且具备生产就绪性。
- en: What makes Ray Serve Different?
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Ray Serve 有何不同？
- en: '![Many Tools Run 1 Model Well](../Images/4b40ac8b5d230e62b6a366aa3861b83a.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![许多工具运行 1 个模型很好](../Images/4b40ac8b5d230e62b6a366aa3861b83a.png)'
- en: 'There are so many tools for training and serving one model. These tools help
    you run and deploy one model very well. The problem is that machine learning in
    real life is usually not that simple. In a production setting, you can encounter
    problems like:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和服务一个模型的工具有很多。这些工具能够很好地运行和部署单个模型。问题在于，现实中的机器学习通常不那么简单。在生产环境中，你可能会遇到以下问题：
- en: Wrangling with infrastructure to scale beyond one copy of a model.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理基础设施以超越一个模型的副本进行扩展。
- en: Having to work through complex YAML configuration files, learn custom tooling,
    and develop MLOps expertise.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要处理复杂的 YAML 配置文件，学习自定义工具，并发展 MLOps 专业知识。
- en: Hit scalability or performance issues, unable to deliver business SLA objectives.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遇到可扩展性或性能问题，无法满足业务 SLA 目标。
- en: Many tools are very costly and can often lead to underutilization of resources.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多工具成本高昂，且经常导致资源利用不充分。
- en: Scaling out a single model is hard enough. For many ML in production use cases,
    we observed that complex workloads require composing many different models together.
    Ray Serve is natively built for this kind of use case involving many models spanning
    multiple nodes. You can check out [this part of the talk](https://youtu.be/mM4hJLelzSw?t=651) where
    we go in depth about Ray Serve’s architectural components.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展单一模型本身就足够困难。对于许多生产环境中的机器学习案例，我们观察到复杂的工作负载需要将许多不同的模型组合在一起。Ray Serve 本身就是为了这种涉及多个节点的多个模型的用例而构建的。你可以查看 [this
    part of the talk](https://youtu.be/mM4hJLelzSw?t=651)，其中我们深入探讨了Ray Serve的架构组件。
- en: Patterns of ML Models in Production
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生产环境中的机器学习模型模式
- en: '![13PatternsMLProduction](../Images/5373302cc6c845b5e4c850313e950697.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![13PatternsMLProduction](../Images/5373302cc6c845b5e4c850313e950697.png)'
- en: 'A significant portion of ML applications in production follow 4 model patterns:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 生产环境中许多机器学习应用遵循四种模型模式：
- en: pipeline
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道
- en: ensemble
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成
- en: business logic
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 业务逻辑
- en: online learning
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线学习
- en: This section will describe each of these patterns, show how they are used, go
    over how existing tools typically implement them, and show how Ray Serve can solve
    these challenges.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述这些模式中的每一种，展示它们的使用方式，讲解现有工具如何实现它们，并展示Ray Serve如何解决这些挑战。
- en: Pipeline Pattern
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管道模式
- en: '![Figure](../Images/54548db95bd65dc4919d3abfdede19df.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/54548db95bd65dc4919d3abfdede19df.png)'
- en: A typical computer vision pipeline
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的计算机视觉管道
- en: 'The image above shows a typical computer vision pipeline that uses multiple
    deep learning models to caption the object in the picture. This pipeline consists
    of the following steps:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了一个典型的计算机视觉管道，该管道使用多个深度学习模型对图片中的物体进行标注。这个管道包括以下步骤：
- en: 1) The raw image goes through common preprocessing like image decoding, augmentation
    and clipping.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 原始图像经过常见的预处理，如图像解码、增强和裁剪。
- en: 2) A detection classifier model is used to identify the bounding box and the
    category. It's a cat.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 使用检测分类模型来识别边界框和类别。它是一只猫。
- en: 3) The image is passed into a keypoint detection model to identify the posture
    of the object. For the cat image, the model could identify key points like paws,
    neck, and head.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 图像被传递到一个关键点检测模型，以识别物体的姿势。对于猫的图像，模型可以识别出像爪子、脖子和头部这样的关键点。
- en: 4) Lastly, an NLP synthesis model generates a category of what the picture shows.
    In this case, a standing cat.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 最后，一个NLP合成模型生成图片显示的类别。在这种情况下，是一只站立的猫。
- en: A typical pipeline rarely consists of just one model. To tackle real-life issues,
    ML applications often use many different models to perform even simple tasks.
    In general, pipelines break a specific task into many steps, where each step is
    conquered by a machine learning algorithm or some procedure. Let’s now go over
    a couple pipelines you might already be familiar with.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的管道很少仅由一个模型组成。为了应对现实问题，机器学习应用通常使用许多不同的模型来执行即使是简单的任务。通常，管道将特定任务分解为多个步骤，每个步骤由机器学习算法或某些过程完成。现在，让我们回顾几个你可能已经熟悉的管道。
- en: '**Scikit-Learn Pipeline**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scikit-Learn 管道**'
- en: '`Pipeline([(‘scaler’, StandardScaler()), (‘svc’, SVC())])`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pipeline([(‘scaler’, StandardScaler()), (‘svc’, SVC())])`'
- en: '[scikit-learn’s pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) can
    be used to combine multiple “models” and “processing objects” together.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[scikit-learn 的管道](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) 可以用来将多个“模型”和“处理对象”组合在一起。'
- en: '**Recommendation Systems**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**推荐系统**'
- en: '`[EmbeddingLookup(), FeatureInteraction(), NearestNeighbors(), Ranking()]`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`[EmbeddingLookup(), FeatureInteraction(), NearestNeighbors(), Ranking()]`'
- en: There are common pipeline patterns in recommendation systems. Item and video
    recommendations like those that you might see at [Amazon](https://www.amazon.science/publications/two-decades-of-recommender-systems-at-amazon-com) and [YouTube](https://research.google/pubs/pub45530/),
    respectively,  typically go through multiple stages like embedding lookup, feature
    interaction, nearest neighbor models, and ranking models.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统中存在常见的管道模式。像你在 [Amazon](https://www.amazon.science/publications/two-decades-of-recommender-systems-at-amazon-com) 和 [YouTube](https://research.google/pubs/pub45530/) 上看到的物品和视频推荐，通常会经过多个阶段，如嵌入查找、特征交互、最近邻模型和排名模型。
- en: '**Common Preprocessing**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**常见的预处理**'
- en: '`[HeavyWeightMLMegaModel(), DecisionTree()/BoostingModel()]`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`[HeavyWeightMLMegaModel(), DecisionTree()/BoostingModel()]`'
- en: There are some very common use cases where some massive ML models are used to
    take care of common processing for text or images. For example, at Facebook, groups
    of ML researchers at [FAIR](https://ai.facebook.com/) create state of the art
    heavyweight models for vision and text. Then different product groups create downstream
    models to tackle their business use case (e.g. [suicide prevention](https://ai.facebook.com/blog/under-the-hood-suicide-prevention-tools-powered-by-ai/))
    by implementing smaller models using random forest. The shared common preprocessing
    step oftentimes are materialized into a [feature store pipeline](https://www.tecton.ai/blog/what-is-a-feature-store/).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些非常常见的用例，其中一些大型 ML 模型用于处理文本或图像的常见处理任务。例如，在 Facebook，[FAIR](https://ai.facebook.com/)
    的一组 ML 研究人员创建了用于视觉和文本的最先进的重型模型。然后不同的产品组创建下游模型以解决他们的业务用例（例如，[自杀预防](https://ai.facebook.com/blog/under-the-hood-suicide-prevention-tools-powered-by-ai/)），通过使用随机森林实现较小的模型。共享的常见预处理步骤通常会被实现为
    [特征存储管道](https://www.tecton.ai/blog/what-is-a-feature-store/)。
- en: General Pipeline Implementation Options
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一般管道实现选项
- en: '![17PipelineImplementation](../Images/79bbb459ea0883039df1a9aef463ae8a.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![17PipelineImplementation](../Images/79bbb459ea0883039df1a9aef463ae8a.png)'
- en: Before Ray Serve, implementing pipelines generally meant you had to choose between
    wrapping your models in a web server or using many specialized microservices.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Ray Serve 之前，实现管道通常意味着你必须在将模型封装在 web 服务器中或使用许多专用微服务之间做出选择。
- en: 'In general, there are two approaches to implement a pipeline: wrap your models
    in a web server or use many specialized microservices.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，实现管道有两种方法：将模型封装在 web 服务器中或使用许多专用微服务。
- en: '**Wrap Models in a Web Server**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**将模型封装在 web 服务器中**'
- en: The left side of the image above shows models that get run in a for loop during
    the web handling path. Whenever a request comes in, models get loaded (they can
    also be cached) and run through the pipeline. While this is simple and easy to
    implement, a major flaw is that this is hard to scale and not performant because
    each request gets handled sequentially.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 上图的左侧展示了在 web 处理路径中以 for 循环方式运行的模型。每当有请求进来时，模型会被加载（它们也可以被缓存）并通过管道运行。虽然这种方法简单易实现，但一个主要缺陷是它难以扩展，并且由于每个请求都是按顺序处理的，因此性能不佳。
- en: '**Many Specialized Microservices**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**许多专用微服务**'
- en: The right side of the image above shows many specialized microservices where
    you essentially build and deploy one microservice per model. These microservices
    can be native ML platforms, [Kubeflow](https://www.kubeflow.org/), or even hosted
    services like AWS [SageMaker](https://aws.amazon.com/sagemaker/). However, as
    the number of models grow, the complexity and operational cost drastically increases.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 上图的右侧展示了许多专用微服务，其中你基本上为每个模型构建和部署一个微服务。这些微服务可以是本地的 ML 平台、[Kubeflow](https://www.kubeflow.org/)，甚至是像
    AWS [SageMaker](https://aws.amazon.com/sagemaker/) 这样的托管服务。然而，随着模型数量的增加，复杂性和运营成本也会急剧上升。
- en: Implementing Pipelines in Ray Serve
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Ray Serve 中实现管道
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Pseudocode showing how Ray Serve allows deployments to call other deployments
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 伪代码显示了 Ray Serve 如何允许部署调用其他部署
- en: In Ray Serve, you can directly call other deployments within your deployment. 
    In this code above, there are three deployments. `Featurizer` and `Predictor`
    are just regular deployments containing the models. The `Orchestrator` receives
    the web input, passes it to the featurizer process via the featurizer handle,
    and then passes the computed feature to the predictor process. The interface is
    just Python and you don’t need to learn any new framework or domain-specific language.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Ray Serve 中，你可以在你的部署中直接调用其他部署。在上面的代码中，有三个部署。`Featurizer` 和 `Predictor` 只是包含模型的常规部署。`Orchestrator`
    接收 web 输入，通过特征提取器句柄将其传递给特征提取器进程，然后将计算出的特征传递给预测器进程。接口只是 Python，你不需要学习任何新的框架或领域特定语言。
- en: Ray Serve achieves this with a mechanism called ServeHandle which gives you
    a similar flexibility to embed everything in the web server, without sacrificing
    performance or scalability. It allows you to directly call other deployments that
    live in other processes on other nodes. This allows you to scale out each deployment
    individually and load balance calls across the replicas.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Serve 通过一种叫做 ServeHandle 的机制实现这一点，它使你能够将所有内容嵌入 web 服务器，而不牺牲性能或可扩展性。它允许你直接调用其他在不同节点上运行的进程中的部署。这允许你单独扩展每个部署，并在副本之间进行负载均衡。
- en: If you would like to get a deeper understanding of how this works, [check out
    this section of Simon Mo’s talk](https://youtu.be/mM4hJLelzSw?t=650) to learn
    about Ray Serve’s architecture. If you would like an example of a computer vision
    pipeline in production, [check out how Robovision used 5 ML models for vehicle
    detection](https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures#:~:text=A%20Tool%20for%20High%20Performance%20ML%20Systems).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解其工作原理， [查看Simon Mo的演讲部分](https://youtu.be/mM4hJLelzSw?t=650) 了解Ray Serve的架构。如果你想了解生产中的计算机视觉管道示例，[查看Robovision如何使用5个机器学习模型进行车辆检测](https://www.anyscale.com/blog/the-third-generation-of-production-ml-architectures#:~:text=A%20Tool%20for%20High%20Performance%20ML%20Systems)。
- en: Ensemble Pattern
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成模式
- en: '![Ensemble Pattern](../Images/16434d4cd2a985d8583f19809d92c256.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![集成模式](../Images/16434d4cd2a985d8583f19809d92c256.png)'
- en: In a lot of production use cases, a pipeline is appropriate. However, one limitation
    of pipelines is that there can often be many upstream models for a given downstream
    model. This is where ensembles are useful.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多生产使用案例中，管道是合适的。然而，管道的一个限制是，对于给定的下游模型，通常会有很多上游模型。这就是集成模型有用的地方。
- en: '![Figure](../Images/3322debd6651907f43438df2050ec8ac.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/3322debd6651907f43438df2050ec8ac.png)'
- en: Ensemble Use Cases
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 集成使用案例
- en: Ensemble patterns involve mixing output from one or more models. They are also
    called model stacking in some cases. Below are three use cases of ensemble patterns.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模式涉及将一个或多个模型的输出混合。在某些情况下，它们也被称为模型堆叠。以下是集成模式的三种使用案例。
- en: '**Model Update**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型更新**'
- en: New models are developed and trained over time. This means there will always
    be new versions of the model in production. The question becomes, how do you make
    sure the new models are valid and performant in live online traffic scenarios?
    One way to do this is by putting some portion of the traffic through the new model.
    You still select the output from the known good model, but you are also collecting
    live output from the newer version of the models in order to validate it.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 新模型会随着时间的推移进行开发和训练。这意味着在生产环境中总会有模型的新版本。问题是，如何确保新模型在实时在线流量场景中有效且性能优越？一种方法是将部分流量通过新模型。你仍然从已知良好的模型中选择输出，但你也在收集新版本模型的实时输出以进行验证。
- en: '**Aggregation**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚合**'
- en: The most widely known use case is for aggregation. For regression models, outputs
    from multiple models are averaged. For classification models, the output will
    be a voted version of multiple models’ output. For example, if two models vote
    for cat and one model votes for dog, then the aggregated output will be cat. Aggregation
    helps combat inaccuracy in individual models and generally makes the output more
    accurate and “safer”.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最广为人知的使用案例是聚合。对于回归模型，多个模型的输出会被平均。对于分类模型，输出将是多个模型输出的投票版本。例如，如果两个模型投票给猫，一个模型投票给狗，则聚合输出将是猫。聚合有助于减少个别模型的错误，并通常使输出更准确和“安全”。
- en: '**Dynamic Selection**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态选择**'
- en: Another use case for ensemble models is to dynamically perform model selection
    given input attributes. For example, if the input contains a cat, model A will
    be used because it is specialized for cats.  If the input contains a dog, model
    B will be used because it is specialized for dogs. Note that this dynamic selection
    doesn’t necessarily mean the pipeline itself has to be static. It could also be
    selecting models given user feedback.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型的另一个使用案例是根据输入属性动态执行模型选择。例如，如果输入包含猫，模型A将被使用，因为它专门用于猫。如果输入包含狗，则会使用模型B，因为它专门用于狗。请注意，这种动态选择不一定意味着管道本身必须是静态的。它也可以根据用户反馈选择模型。
- en: General Ensemble Implementation Options
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一般集成实现选项
- en: '![General Ensemble Implementation](../Images/1749b4e53b9e59fc2b20a77e30ca3589.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![一般集成实现](../Images/1749b4e53b9e59fc2b20a77e30ca3589.png)'
- en: Before Ray Serve, implementing ensembles generally meant you had to choose between
    wrapping your models in a web server or using many specialized microservices.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在Ray Serve之前，实现集成通常意味着你必须在将模型包装在网络服务器中或使用许多专门的微服务之间做出选择。
- en: Ensemble implementations suffer the same sort of issues as pipelines. It is
    simple to wrap models in a web server, but it is not performant. When you use
    specialized microservices, you end up having a lot of operational overhead as
    the number of microservices scale with the number of models.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法面临与管道相同的问题。将模型包装在网络服务器中很简单，但性能不佳。当使用专门的微服务时，随着微服务数量与模型数量的增长，你会遇到大量的操作开销。
- en: '![Figure](../Images/cef1f239a6ce6e2c866380f7578463f6.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/cef1f239a6ce6e2c866380f7578463f6.png)'
- en: Ensemble example from the [2020 Anyscale demo](https://youtu.be/8GTd8Y_JGTQ)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[2020 Anyscale 演示](https://youtu.be/8GTd8Y_JGTQ)的集成示例
- en: With Ray Serve, the kind of pattern is incredibly simple. You can look at the [2020
    Anyscale demo](https://youtu.be/8GTd8Y_JGTQ) to see how to utilize Ray Serve’s
    handle mechanism to perform dynamic model selection.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Ray Serve，这种模式非常简单。你可以查看[2020 Anyscale 演示](https://youtu.be/8GTd8Y_JGTQ)，了解如何利用
    Ray Serve 的处理机制执行动态模型选择。
- en: Another example of using Ray Serve for ensembling is Wildlife Studios combining
    output of many classifiers for a single prediction. You can check out how they
    were able to [serve in-game offers 3x faster with Ray Serve](https://www.anyscale.com/blog/wildlife-studios-serves-in-game-offers-3x-faster-at-1-10th-the-cost-with-ray).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Ray Serve 进行集成的另一个例子是 Wildlife Studios 将多个分类器的输出合并为一个预测。你可以查看他们如何通过[Ray Serve
    以 3 倍的速度提供游戏内优惠](https://www.anyscale.com/blog/wildlife-studios-serves-in-game-offers-3x-faster-at-1-10th-the-cost-with-ray)。
- en: Business Logic Pattern
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 业务逻辑模式
- en: 'Productionizing machine learning will always involve business logic. No models
    can stand-alone and serve requests by themselves. Business logic patterns involve
    everything that’s involved in a common ML task that is not ML model inference.
    This includes:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器学习投入生产总是涉及业务逻辑。没有模型能够独立运行并自行处理请求。业务逻辑模式涉及在常见的机器学习任务中，除了机器学习模型推理之外的一切内容。这包括：
- en: Database lookups for relational records
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于关系记录的数据库查找
- en: Web API calls for external services
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Web API 调用外部服务
- en: Feature store lookup for pre-compute feature vectors
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于预计算特征向量的特征存储查找
- en: Feature transformations like data validation, encoding, and decoding.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征转换，如数据验证、编码和解码。
- en: General Business Logic Implementation Options
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一般业务逻辑实现选项
- en: '![General Business Logic Implementation Options](../Images/0563870ba1f93676e64c2abf3153cea3.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![一般业务逻辑实现选项](../Images/0563870ba1f93676e64c2abf3153cea3.png)'
- en: 'The pseudocode for the web handler above does the following things:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 上述 Web 处理器的伪代码完成了以下任务：
- en: It loads the model (let’s say from S3)
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载模型（比如从 S3）
- en: Validates the input from the database
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证来自数据库的输入
- en: Looks up some pre-computed features from the feature store.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从特征存储中查找一些预计算的特征。
- en: Only after the web handler completes these business logic steps are the inputs
    passed through to ML models. The problem is that the requirements of model inference
    and business logic lead to the server being *both network bounded and compute
    bounded*. This is due to the model loading step, database lookup, and feature
    store lookups being network bounded and I/O heavy as well as the model inference
    being compute bound and memory hungry. The combination of these factors lead to
    an inefficient utilization of resources. Scaling will be expensive.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在 Web 处理器完成这些业务逻辑步骤后，输入才会传递到机器学习模型中。问题在于模型推理和业务逻辑的要求导致服务器*既受网络限制又受计算限制*。这是因为模型加载步骤、数据库查找和特征存储查找受网络限制且
    I/O 负荷重，而模型推理受计算限制且内存需求大。这些因素的组合导致资源利用效率低下。扩展将非常昂贵。
- en: '![Figure](../Images/0f770ce2d44b93326cd3be5d2f05d86d.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/0f770ce2d44b93326cd3be5d2f05d86d.png)'
- en: Web handler approach (left) and microservices approach (right)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Web 处理器方法（左）和微服务方法（右）
- en: A common way to increase utilization is to split models out into model servers
    or microservices.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 增加利用率的一个常见方法是将模型拆分到模型服务器或微服务中。
- en: The web app is purely network bounded while the model servers are compute bounded.
    However, a common problem is the interface between the two. If you put too much
    business logic into the model server, then the model servers become a mix of network
    bounded and compute bounded calls.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Web 应用完全是网络限制，而模型服务器则是计算限制。然而，一个常见的问题是两者之间的接口。如果你在模型服务器中放入过多的业务逻辑，那么模型服务器就会变成网络限制和计算限制调用的混合体。
- en: If you let the model servers be pure model servers, then you have the “**tensor-in,
    tensor-out**” interface problem. The input types for model servers are typically
    very constrained to just tensors or some alternate form of it. This makes it hard
    to keep the pre-processing, post-processing, and business logic in sync with the
    model itself.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你让模型服务器仅作为纯模型服务器，那么你会遇到“**张量输入，张量输出**”接口问题。模型服务器的输入类型通常仅限于张量或某种替代形式。这使得将预处理、后处理和业务逻辑与模型本身保持同步变得困难。
- en: It becomes hard to reason about the interaction between the processing logic
    and the model itself because during training, the processing logic and models
    are tightly coupled, but when serving, they are split across two servers and two
    implementations.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，由于处理逻辑和模型紧密耦合，因此很难推理处理逻辑与模型之间的交互，但在服务时，它们分布在两个服务器和两个实现中。
- en: Neither the web handler approach nor the microservices approach is satisfactory.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是 Web 处理程序方法还是微服务方法，都不令人满意。
- en: Implementing Business Logic in Ray Serve
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Ray Serve 中实现业务逻辑
- en: '![Figure](../Images/d89c6e5d4fea7fa9ed2e67228e990351.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/d89c6e5d4fea7fa9ed2e67228e990351.png)'
- en: Business Logic in Ray Serve
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Serve 中的业务逻辑
- en: With Ray Serve, you just have to make some simple changes to the old web server
    to alleviate the issues described above. Instead of loading the model directly,
    you can retrieve a ServeHandle that wraps the model, and offload the computation
    to another deployment. All the data types are preserved and there is no need to
    write “tensor-in, tensor-out” API calls--you can just pass in regular Python types.
    Additionally, the model deployment class can stay in the same file, and be deployed
    together with the prediction handler. This makes it easy to understand and debug
    the code. `model.remote` looks like just a function and you can easily trace it
    to the model deployment class.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Ray Serve，你只需对旧的 Web 服务器做一些简单的更改，就能缓解上述问题。你可以检索一个包装了模型的 ServeHandle，而不是直接加载模型，并将计算任务转移到另一个部署中。所有数据类型都得以保留，不需要编写“tensor-in,
    tensor-out” API 调用——你可以直接传入常规 Python 类型。此外，模型部署类可以保留在同一个文件中，并与预测处理程序一起部署。这使得理解和调试代码变得更加容易。`model.remote`
    看起来就像一个函数，你可以轻松追踪到模型部署类。
- en: In this way, Ray Serve helps you split up the business logic and inference into
    two separation components, one I/O heavy and the other compute heavy. This allows
    you to scale each piece individually, without losing the ease of deployment. Additionally,
    because `model.remote` is just a function call, it’s a lot easier to test and
    debug than separate external services.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，Ray Serve 帮助你将业务逻辑和推断分离成两个独立的组件，一个 I/O 密集型，另一个计算密集型。这使得你可以单独扩展每个部分，而不会丧失部署的便利性。此外，由于
    `model.remote` 只是一个函数调用，它比独立的外部服务更容易测试和调试。
- en: Ray Serve FastAPI Integration
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Ray Serve FastAPI 集成
- en: '![Figure](../Images/8595ad1b26ebf6e48945fe49e0c55bab.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/8595ad1b26ebf6e48945fe49e0c55bab.png)'
- en: 'Ray Serve: Ingress with FastAPI'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 'Ray Serve: 使用 FastAPI 进行 Ingress'
- en: An important part of implementing business logic and other patterns is authentication
    and input validation. [Ray Serve natively integrates with FastAPI](https://docs.ray.io/en/latest/serve/http-servehandle.html#serve-fastapi-http),
    which is a type safe and ergonomic web framework. [FastAPI](https://fastapi.tiangolo.com/) has
    features like automatic dependency injection, type checking and validation, and
    OpenAPI doc generation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 实现业务逻辑和其他模式的重要部分是身份验证和输入验证。[Ray Serve 本地集成 FastAPI](https://docs.ray.io/en/latest/serve/http-servehandle.html#serve-fastapi-http)，这是一种类型安全且符合人体工程学的
    Web 框架。[FastAPI](https://fastapi.tiangolo.com/) 具有自动依赖注入、类型检查和验证以及 OpenAPI 文档生成等功能。
- en: With Ray Serve, you can directly pass the FastAPI app object into it with `@serve.ingress`.
    This decorator makes sure that all existing FastAPI routes still work and that
    you can attach new routes with the deployment class so states like loaded models,
    and networked database connections can easily be managed. Architecturally, we
    just made sure that your FastAPI app is correctly embedded into the replica actor
    and the FastAPI app can scale out across many Ray nodes.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Ray Serve，你可以直接将 FastAPI 应用对象传递给 `@serve.ingress`。这个装饰器确保所有现有的 FastAPI 路由仍然有效，并且你可以通过部署类附加新的路由，这样加载的模型和网络数据库连接等状态就能轻松管理。从架构上讲，我们只是确保你的
    FastAPI 应用正确嵌入到副本演员中，并且 FastAPI 应用可以在多个 Ray 节点上扩展。
- en: Online Learning
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在线学习
- en: Online learning is an emerging pattern that’s become more and more widely used.
    It refers to a model running in production that is constantly being updated, trained,
    validated and deployed. Below are three use cases of online learning patterns.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习是一种新兴的模式，已变得越来越广泛使用。它指的是一个在生产环境中运行的模型，该模型不断更新、训练、验证和部署。以下是在线学习模式的三个使用案例。
- en: '**Dynamically Learn Model Weights**'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态学习模型权重**'
- en: There are use cases for dynamically learning model weights online. As users
    interact with your services, these updated model weights can contribute to a personalized
    model for each user or group.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些用例涉及在线动态学习模型权重。当用户与您的服务交互时，这些更新的模型权重可以有助于为每个用户或群体提供个性化的模型。
- en: '![Figure](../Images/f7f04df0e61fb914656ea0ffd641fc7f.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/f7f04df0e61fb914656ea0ffd641fc7f.png)'
- en: '[Online Learning example at Ant Group](https://www.anyscale.com/blog/online-resource-allocation-with-ray-at-ant-group) (image
    courtesy of Ant Group)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[蚂蚁集团在线学习示例](https://www.anyscale.com/blog/online-resource-allocation-with-ray-at-ant-group)（图片由蚂蚁集团提供）'
- en: One case study of online learning consists of an online resource allocation
    business solution at Ant Group. The model is trained from offline data, then combined
    with real time streaming data source, and then served live traffic. One thing
    to note is that online learning systems are drastically more complex than their
    static serving counterparts. In this case, putting models in the web server, or
    even splitting it up into multiple microservices, would not help with the implementation.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习的一个案例研究涉及蚂蚁集团的在线资源分配业务解决方案。该模型从离线数据中训练，然后与实时流数据源结合，并且服务于实时流量。需要注意的是，在线学习系统比其静态服务对手复杂得多。在这种情况下，将模型放在网络服务器上，甚至拆分成多个微服务，都不会有助于实现。
- en: '**Dynamically Learn Parameters to Orchestrate Models**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态学习参数以协调模型**'
- en: There are also use cases for learning parameters to orchestrate or compose models,
    for example, [learning which model a user prefers](https://dl.acm.org/doi/fullHtml/10.1145/3383313.3411550).
    This manifests often in model selection scenarios or contextual bandit algorithms.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些用例涉及学习参数以协调或组合模型，例如，[学习用户偏好的模型](https://dl.acm.org/doi/fullHtml/10.1145/3383313.3411550)。这通常体现在模型选择场景或上下文赌博算法中。
- en: '**Reinforcement Learning**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**'
- en: Reinforcement learning is the branch of machine learning that trains agents
    to interact with the environment. The environment can be the physical world or
    a simulated environment. You can learn about reinforcement learning [here](https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google) and
    see how you can deploy a RL model using Ray Serve [here](https://docs.ray.io/en/latest/serve/tutorials/rllib.html#serve-rllib-tutorial).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是机器学习的一个分支，训练代理与环境进行互动。环境可以是物理世界或模拟环境。您可以在[这里](https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google)了解强化学习，并在[这里](https://docs.ray.io/en/latest/serve/tutorials/rllib.html#serve-rllib-tutorial)查看如何使用
    Ray Serve 部署 RL 模型。
- en: Conclusion
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: '![Figure](../Images/2f8011f22aa2376f8e75a951ce194e89.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/2f8011f22aa2376f8e75a951ce194e89.png)'
- en: Ray Serve is easy to develop and production ready.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Serve 易于开发且准备好投入生产。
- en: This post went over 4 main patterns of machine learning in production, how Ray
    Serve can help you natively scale and work with complex architectures, and how
    ML in production often means many models in production. Ray Serve is built with
    all of this in mind on top of the distributed runtime Ray. If you’re interested
    in learning more about Ray, you can check out the [documentation](https://ray.io/),
    join us on [Discourse](https://discuss.ray.io/), and check out the [whitepaper](https://tinyurl.com/ray-white-paper)!
    If you're interested in working with us to make it easier to leverage Ray, we're [hiring](https://jobs.lever.co/anyscale)!
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了生产环境中机器学习的4种主要模式，Ray Serve 如何原生扩展并与复杂架构协同工作，以及生产中的机器学习通常意味着多个模型的生产。Ray
    Serve 在分布式运行时 Ray 之上构建，考虑了所有这些因素。如果您有兴趣了解更多关于 Ray 的信息，您可以查看[文档](https://ray.io/)、加入我们的[讨论](https://discuss.ray.io/)以及查看[白皮书](https://tinyurl.com/ray-white-paper)!
    如果您有兴趣与我们合作以简化 Ray 的使用，我们正在[招聘](https://jobs.lever.co/anyscale)!
- en: '[Original](https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns).
    Reposted with permission.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns)。转载已获许可。'
- en: '**Related:**'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Getting Started with Distributed Machine Learning with PyTorch and Ray](/2021/03/getting-started-distributed-machine-learning-pytorch-ray.html)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 PyTorch 和 Ray 开始分布式机器学习](/2021/03/getting-started-distributed-machine-learning-pytorch-ray.html)'
- en: '[How to Speed up Scikit-Learn Model Training](/2021/02/speed-up-scikit-learn-model-training.html)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何加速 Scikit-Learn 模型训练](/2021/02/speed-up-scikit-learn-model-training.html)'
- en: '[How to Build a Data Science Portfolio](/2018/07/build-data-science-portfolio.html)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何构建数据科学投资组合](/2018/07/build-data-science-portfolio.html)'
- en: More On This Topic
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Top 7 Model Deployment and Serving Tools](https://www.kdnuggets.com/top-7-model-deployment-and-serving-tools)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[顶级 7 大模型部署和服务工具](https://www.kdnuggets.com/top-7-model-deployment-and-serving-tools)'
- en: '[Prioritizing Data Science Models for Production](https://www.kdnuggets.com/2022/04/prioritizing-data-science-models-production.html)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优先考虑数据科学模型的生产](https://www.kdnuggets.com/2022/04/prioritizing-data-science-models-production.html)'
- en: '[Feature Store Summit 2023: Practical Strategies for Deploying ML…](https://www.kdnuggets.com/2023/09/hopsworks-feature-store-summit-2023-practical-strategies-deploying-ml-models-production-environments)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2023 年特性存储峰会：机器学习模型生产环境部署的实用策略](https://www.kdnuggets.com/2023/09/hopsworks-feature-store-summit-2023-practical-strategies-deploying-ml-models-production-environments)'
- en: '[Design Patterns in Machine Learning for MLOps](https://www.kdnuggets.com/2022/02/design-patterns-machine-learning-mlops.html)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的设计模式与 MLOps](https://www.kdnuggets.com/2022/02/design-patterns-machine-learning-mlops.html)'
- en: '[Unveiling Hidden Patterns: An Introduction to Hierarchical Clustering](https://www.kdnuggets.com/unveiling-hidden-patterns-an-introduction-to-hierarchical-clustering)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[揭示隐藏模式：层次聚类简介](https://www.kdnuggets.com/unveiling-hidden-patterns-an-introduction-to-hierarchical-clustering)'
- en: '[A Full End-to-End Deployment of a Machine Learning Algorithm into a…](https://www.kdnuggets.com/2021/12/deployment-machine-learning-algorithm-live-production-environment.html)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[将机器学习算法完整端到端部署到生产环境中](https://www.kdnuggets.com/2021/12/deployment-machine-learning-algorithm-live-production-environment.html)'
