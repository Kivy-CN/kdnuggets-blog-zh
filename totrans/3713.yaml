- en: 'Approaches to Text Summarization: An Overview'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html](https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Approaches to Text Summarization: An Overview](../Images/0501fd9d406e6cac4cee064e52464f23.png)'
  prefs: []
  type: TYPE_IMG
- en: Image By Editor
  prefs: []
  type: TYPE_NORMAL
- en: 'The *bona fide* semantic understanding of human language text, exhibited by
    its effective summarization, may well be the holy grail of natural language processing
    (NLP). That statement isn''t as hyperbolic as it sounds: as true human language
    understanding definitely *is* the holy grail of NLP, and genuine effective summarization
    of said human language would necessarily entail true understanding, transitivity
    would back me up on this.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately — or perhaps not, depending on your outlook — honest to goodness
    "understanding" of human language is not something we can currently count on for
    text summarization. However, the show must go on, and there currently exist an
    array of actual techniques for summarizing text, some of which stretch back decades.
    These techniques take different approaches to reach the same goal, and can be
    classified into a fairly narrow set of categories for pursuing their shared goal.
  prefs: []
  type: TYPE_NORMAL
- en: This article will present the main approaches to text summarization currently
    employed, as well as discuss some of their characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Automated Text Summarization Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To be clear, when we say "automated text summarization," we are talking about
    employing machines to perform the summarization of a document or document using
    some form of heuristics or statistical methods. A summary in this case is a shortened
    piece of text which accurately captures and conveys the most important and relevant
    information contained in the document or documents we want to be summarized. As
    hinted at above, there are a number of these different tried and truly automated
    text summarization techniques that are currently in use.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few ways of going about classifying automated text summarization
    techniques, as can be seen in Figure 1\. This article will explore these techniques
    from the point of view of summarization *output type*. In this regard, there are
    2 categories of techniques: extractive and abstraction.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Approaches to Text Summarization: An Overview](../Images/40d151b32f9a8e5e1fc8740658fb6576.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 1.** Automated text summarization approaches (source: [Kushal Chauhan,
    Jutana](https://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1),
    modified).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extractive text summarization** methods function by identifying the important
    sentences or excerpts from the text and reproducing them verbatim as part of the
    summary. No new text is generated; only existing text is used in the summarization
    process.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Abstractive text summarization** methods employ more powerful natural language
    processing techniques to interpret text and generate new summary text, as opposed
    to selecting the most representative existing excerpts to perform the summarization.'
  prefs: []
  type: TYPE_NORMAL
- en: While both are valid approaches to text summarization, it should not be difficult
    to convince you that abstractive techniques are far more difficult to implement.
    In fact, the majority of summarization processes today are extraction-based. This
    doesn't mean that abstractive methods should be discounted or ignored; on the
    contrary, research into their implementation — and true semantic understanding
    of human language in general — is a worthy pursuit, and much work is needed before
    we can confidently say that we have gained a true foothold in this endeavor.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, the rest of this article will focus on the specifics of extractive
    text summarization, and its differing implementation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Extractive Summarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Extractive summarization techniques vary, yet they all share the same basic
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Construct an intermediate representation of the input text (text to be summarized)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Score the sentences based on the constructed intermediate representation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a summary consisting of the top *k* most important sentences
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tasks 2 and 3 are straightforward enough; in *sentence scoring*, we want to
    determine how well each sentence relays important aspects of the text being summarized,
    while *sentence selection* is performed using some specific optimization approach.
    Algorithms for each of these 2 steps can vary, but they are conceptually quite
    simple: assign a score to each sentence using some metric, and then select from
    the best-scored sentences via some well-defined sentence selection method.'
  prefs: []
  type: TYPE_NORMAL
- en: The first task, intermediate representation, could use further elaboration.
  prefs: []
  type: TYPE_NORMAL
- en: Intermediate Representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some sense needs to be made of natural language prior to its sentence scoring
    and selection, and creating some intermediate representation of each sentence
    serves this purpose. The 2 major categories of intermediate representation, *topic
    representation* and *indicator representation*, are briefly defined below, as
    are their sub-categories.
  prefs: []
  type: TYPE_NORMAL
- en: '**Topic Representation** - transformation of the text with a focus on text
    topic identification; major sub-categories of this approach are:'
  prefs: []
  type: TYPE_NORMAL
- en: frequency-driven approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: topic word approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[latent semantic analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)
    (LSA)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian topic models — [latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)
    (LDA), for example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 2 most popular word frequency approaches are word probability and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).
  prefs: []
  type: TYPE_NORMAL
- en: 'In topic words approaches, there are 2 ways to compute a sentence''s importance:
    by the number of topic signatures it contains (the number of topics the sentence
    discusses), or by the proportion of topics the sentence contains versus the number
    of topics contained in the text. As such, the first of these tends to reward longer
    sentences, while the second measures topic word density.'
  prefs: []
  type: TYPE_NORMAL
- en: Explanations of latent semantic analysis and Bayesian topic model approaches,
    such as LDA, are beyond the scope of this article, but can be read about in the
    links above.
  prefs: []
  type: TYPE_NORMAL
- en: '![Approaches to Text Summarization: An Overview](../Images/d10abecad603535ec8a190e7755a7789.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 2.** Constructing bag of words feature vectors (source: [Dipanjan
    Sarkar](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41)).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Indicator Representation** - transformation of each sentence in the text
    into a list of features of importance; possible features include:'
  prefs: []
  type: TYPE_NORMAL
- en: sentence length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sentence position
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: whether sentence contains a particular word (see Figure 2 for an example of
    such a feature extraction method, [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: whether sentence contains a particular phrase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using a set of features to represent and rank the text data can be performed
    using one of 2 overarching indicator representation methods: graph methods and
    machine learning methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using **graph representations**:'
  prefs: []
  type: TYPE_NORMAL
- en: we find that sub-graphs end up representing topics covered in the text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we are able to isolate important sentences in the text, given that these are
    the ones which would be connected to a greater number of other sentences (if you
    consider sentences as vertices and sentence similarity represented by edges)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we do not need to consider language-specific processing, and the same methods
    can be applied to a variety of languages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we can often find that the semantic information gained via graph-exposed sentence
    similarity enhances summarization performance beyond more simple frequency approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With **machine learning representations**:'
  prefs: []
  type: TYPE_NORMAL
- en: the summarization problem is modeled as a classification problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we require labeled training data to build a classifier to classify sentences
    as summary or non-summary sentences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: to combat the labeled data conundrum, alternatives such as semi-supervised learning
    show promise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we find that certain methods which assume dependency between sentences often
    outperform other techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text summarization is an exciting sub-discipline of natural language processing.
    While a variety of approaches to extractive summarization are in use and are being
    researched daily, an understanding of the basis of the concepts above should allow
    you to have some understanding of how any of these operate, at least at a 30,000
    foot level. You should also be able to pick up recent papers or read recent implementation
    blog posts with some confidence you have the basic level of understanding necessary
    for such an undertaking.
  prefs: []
  type: TYPE_NORMAL
- en: 'Much of this information owes a debt of gratitude to the paper, [Text Summarization
    Techniques: A Brief Survey](https://arxiv.org/abs/1707.02268), by Mehdi Allahyari,
    Seyedamin Pouriyeh, Mehdi Assefi, Saeid Safaei, Elizabeth D. Trippe, Juan B. Gutierrez,
    and Krys Kochut.'
  prefs: []
  type: TYPE_NORMAL
- en: 'References & further reading:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Text Summarization Techniques: A Brief Survey](https://arxiv.org/abs/1707.02268),
    Mehdi Allahyari, Seyedamin Pouriyeh, Mehdi Assefi, Saeid Safaei, Elizabeth D.
    Trippe, Juan B. Gutierrez, Krys Kochut, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unsupervised Text Summarization using Sentence Embeddings](https://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1),
    Kushal Chauhan, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, this article focused on extractive summarization, but you can find more
    about abstractive summarization in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Abstractive and Extractive Text Summarization using Document Context Vector
    and Recurrent Neural Networks](https://arxiv.org/abs/1807.08000), Chandra Khatri,
    Gyanit Singh, Nish Parikh, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Neural Abstractive Text Summarization with Sequence-to-Sequence Models](https://arxiv.org/abs/1812.02303),
    Tian Shi, Yaser Keneshloo, Naren Ramakrishnan, Chandan K. Reddy, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Matthew Mayo](https://www.linkedin.com/in/mattmayo13/)** ([**@mattmayo13**](https://twitter.com/mattmayo13))
    is a Data Scientist and the Editor-in-Chief of KDnuggets, the seminal online Data
    Science and Machine Learning resource. His interests lie in natural language processing,
    algorithm design and optimization, unsupervised learning, neural networks, and
    automated approaches to machine learning. Matthew holds a Master''s degree in
    computer science and a graduate diploma in data mining. He can be reached at editor1
    at kdnuggets[dot]com.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Data Labeling for Machine Learning: Market Overview, Approaches, and Tools](https://www.kdnuggets.com/2021/12/data-labeling-ml-overview-and-tools.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting Started with Automated Text Summarization](https://www.kdnuggets.com/2019/11/getting-started-automated-text-summarization.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Text Summarization Development: A Python Tutorial with GPT-3.5](https://www.kdnuggets.com/2023/04/text-summarization-development-python-tutorial-gpt35.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Summarization with GPT-3](https://www.kdnuggets.com/2022/04/packt-summarization-gpt3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unlocking GPT-4 Summarization with Chain of Density Prompting](https://www.kdnuggets.com/unlocking-gpt-4-summarization-with-chain-of-density-prompting)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Extractive Summarization with LLM using BERT](https://www.kdnuggets.com/extractive-summarization-with-llm-using-bert)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
