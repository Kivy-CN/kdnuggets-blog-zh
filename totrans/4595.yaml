- en: 4 Tips for Advanced Feature Engineering and Preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/08/4-tips-advanced-feature-engineering-preprocessing.html](https://www.kdnuggets.com/2019/08/4-tips-advanced-feature-engineering-preprocessing.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/), Data
    Scientist, I/O Psychologist & Clinical Psychologist**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/0c884aa05ac480729f2e3cb199284005.png)[https://www.analyticsvidhya.com/blog/2018/11/data-engineer-comprehensive-list-resources-get-started/data-engineer/](https://www.analyticsvidhya.com/blog/2018/11/data-engineer-comprehensive-list-resources-get-started/data-engineer/)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Arguably, two of the most important steps in developing a machine learning model
    is [**feature engineering**](https://www.kdnuggets.com/2018/12/feature-engineering-explained.html)
    and ** preprocessing**. Feature engineering consists of the creation of features
    whereas preprocessing involves cleaning the data.
  prefs: []
  type: TYPE_NORMAL
- en: Torture the data, and it will confess to anything*. — Ronald Coase*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We often spend a significant amount of time refining the data into something
    useful for modeling purposes. In order to make this work more efficient, I would
    like to share 4 Tips and Tricks that could help you with engineering and preprocessing
    those features.
  prefs: []
  type: TYPE_NORMAL
- en: I should note that, how cliche it might be, **domain knowledge** might be one
    of the most important things to have when engineering features. It might help
    you in preventing under- and overfitting by having a better understanding of the
    features that you use.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the notebook with analyses [here](https://github.com/MaartenGr/feature-engineering).
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Resampling Imbalanced Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In practice, you will encounter imbalanced data more often than not. This does
    not necessarily have to be a problem if your target only has a slight imbalance.
    You could then resolve it by using proper validation measures for the data such
    as *Balanced Accuracy*, *Precision-Recall Curves *or *F1-score*.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, this is not always the case and your target variable might be
    highly imbalanced (e.g., 10:1). Instead, you can oversample the minority target
    in order to introduce balance using a technique called **SMOTE**.
  prefs: []
  type: TYPE_NORMAL
- en: SMOTE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[SMOTE](https://jair.org/index.php/jair/article/view/10302)stands for *Synthetic
    Minority Oversampling Technique* and is an oversampling technique used to increase
    the samples in a minority class.'
  prefs: []
  type: TYPE_NORMAL
- en: It generates new samples by looking at the *feature space* of the target and
    detecting nearest neighbors. Then, it simply selects similar samples and changes
    a column at a time randomly within the *feature space* of the neighboring samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The module to implement SMOTE can be found within the [imbalanced-learn](https://imbalanced-learn.readthedocs.io/en/stable/index.html) package.
    You can simply import the package and apply a fit_transform:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/5c104f6abf4bc7a85327c2c00b51c6ba.png)Original data (LEFT)
    versus oversampled data (RIGHT).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see the model successfully oversampled the target variable. There
    are several strategies that you can take when oversampling using SMOTE:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**''minority''**`: resample only the minority class;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**''not minority''**`: resample all classes but the minority class;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**''not majority''**`: resample all classes but the majority class;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**''all''**`: resample all classes;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When `**dict**`, the keys correspond to the targeted classes. The values correspond
    to the desired number of samples for each targeted class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I chose to use a dictionary to specify the extent to which I wanted to oversample
    my data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Additional tip 1**: If you have categorical variables in your dataset SMOTE
    is likely to create values for those variables that cannot happen. For example,
    if you have a variable called isMale, which could only take 0 or 1, then SMOTE
    might create 0.365 as a value.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead, you can use SMOTENC which takes into account the nature of categorical
    variables. This version is also available in the [imbalanced-learn](https://imbalanced-learn.readthedocs.io/en/stable/index.html)package.
  prefs: []
  type: TYPE_NORMAL
- en: '**Additional tip 2: **Make sure to oversample after creating the train/test
    split so that you only oversample the train data. You typically do not want to
    test your model on synthetic data.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Creating New Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To improve the quality and predictive power of our models, new features from
    existing variables are often created. We can create some interaction (e.g., multiply
    or divide) between each pair of variables hoping to find an interesting new feature.
    This, however, is a lengthy process and requires a significant amount of coding.
    Fortunately, this can be automated using **Deep Feature Synthesis**.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Feature Synthesis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep feature synthesis (DFS) is an algorithm which enables you to quickly create
    new variables with varying depth. For example, you can multiply pairs of columns
    but you can also choose to first multiply Column A with Column B and then add
    Column C.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let me introduce the data I will be using for the example. I have chosen
    to use [HR analytics data](https://www.kaggle.com/lnvardanyan/hr-analytics) since
    the features are easily interpretable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb2ae660de54d3ae23210372dce00ae9.png)'
  prefs: []
  type: TYPE_IMG
- en: Simply based on our intuition we could identify `**average_monthly_hours**` divided
    by `**number_project**` as an interesting new variable. However, there might be
    much more relationships that we miss out on if we only follow intuition.
  prefs: []
  type: TYPE_NORMAL
- en: 'The [package](https://github.com/Featuretools/featuretools) does require an
    understanding of their use of Entities. However, if you use a single table you
    can simply follow the code below:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to create an `**entity**` from which relationships can be
    created with other tables if necessary. Next, we can simply run `**ft.dfs**` in
    order to create new variables. We specify how variables are created with the parameter `**trans_primitives**`.We
    chose to either add numeric variables together or multiply.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/28f211d4ba3627362890151f8d5e3091.png)The output of DFS
    if verbose = True'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the image above we have created an additional 668 features
    using only a few lines of code. A few examples of the features that were created:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**last_evaluation**`multiplied with `**satisfaction_level**`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**left**`multiplied with `**promotion_last_5years**`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**average_monthly_hours**`multiplied with `**satisfaction_level**`plus `**time_spend_company**`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Additional tip 1: **Note that the implementation here is relatively basic.
    The great thing about DFS is that it can create new variables from aggregations
    between tables (e.g., facts and dimensions). See this [link](https://docs.featuretools.com/loading_data/using_entitysets.html) for
    an example.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Additional tip 2: **Run `**ft.list_primitives()**`in order to see the full
    list of aggregation that you can do. It even handles timestamps, null values,
    and long/lat information.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Handling Missing Values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As always, there is no one best way of dealing with missing values. Depending
    on your data it might be sufficient to simply fill them with the mean or mode
    of certain groups. However, there are advanced techniques that use known parts
    of the data to **impute** the missing values.
  prefs: []
  type: TYPE_NORMAL
- en: One such method is called **IterativeImputer** a new package in Scikit-Learn
    which is based on the popular R algorithm for imputing missing variables, MICE.
  prefs: []
  type: TYPE_NORMAL
- en: Iterative Imputer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although python is a great language for developing machine learning models,
    there are still quite a few methods that work better in R. An example is the well-establish
    imputation packages in R: missForest, mi, mice, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: The **Iterative Imputer** is developed by Scikit-Learn and models each feature
    with missing values as a function of other features. It uses that as an estimate
    for imputation. At each step, a feature is selected as output `y` and all other
    features are treated as inputs `X`. A regressor is then fitted on `X` and `y` and
    used to predict the missing values of `y`. This is done for each feature and repeated
    for several imputation rounds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us take a look at an example. The data that I use is the well known Titanic
    dataset. In this dataset, the column `Age` has missing values that we would like
    to fill. The code, as always, is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: The great thing about this method is that it allows you to use an estimator
    of your choosing. I used a RandomForestRegressor to mimic the behavior of the
    frequently used missForest in R.
  prefs: []
  type: TYPE_NORMAL
- en: '**Additional tip 1: **If you have sufficient data, then it might be an attractive
    option to simply delete samples with missing data. However, keep in mind that
    it could create bias in your data. Perhaps the missing data follows a pattern
    that you miss out on.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Additional tip 2**: The Iterative Imputer allows for different estimators
    to be used. After some testing, I found out that you can even use **Catboost** as
    an estimator! Unfortunately, LightGBM and XGBoost do not work since their random
    state names differ.'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Outlier Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Outliers are difficult to detect without a good understanding of the data. If
    you know the data well you can more easily specify the thresholds at which your
    data still makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e4afc742b3d0d845a43120ebd4893c4.png)'
  prefs: []
  type: TYPE_IMG
- en: At times this is not possible since a perfect understanding of the data is difficult
    to achieve. Instead, you can make use of Outlier Detection algorithms such as
    the popular **Isolation Forest**.
  prefs: []
  type: TYPE_NORMAL
- en: Isolation Forest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the Isolation Forest Algorithm, the keyword is *Isolation*. In essence, the
    algorithm checks how easily a sample can be isolated. This results in an isolation
    number which is calculated by the number of splits, in a Random Decision Tree,
    needed to isolate a sample. This isolation number is then averaged over all trees.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/6efe7cbcdaeaa7a5a1081b1ad8670850.png)Isolation Forest Procedure.
    Retrieved from: [https://donghwa-kim.github.io/iforest.html](https://donghwa-kim.github.io/iforest.html)'
  prefs: []
  type: TYPE_IMG
- en: If the algorithm only needs to do a few splits to find a sample, then it is
    more likely to be an outlier. The splits themselves are also randomly partitioned
    such that it produces shorter paths for anomalies. Thus, when the isolation number
    across all trees is low, then that sample is highly likely to be an anomaly.
  prefs: []
  type: TYPE_NORMAL
- en: 'To show an example I again used the credit card dataset that we used before:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3eb0d3d1bc57941cedf601ee45bb9219.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Additional Tip 1**: There is an [extended](https://github.com/sahandha/eif) version
    of the Isolation Forest available that improves on some shortcomings. However,
    there have been [mixed](https://towardsdatascience.com/outlier-detection-with-extended-isolation-forest-1e248a3fe97b) reviews.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hopefully, the tips and tricks in this article will help you engineer and preprocess
    those features. Notebook with code can be found [here](https://github.com/MaartenGr/feature-engineering/blob/master/Engineering%20Tips.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Any feedback and comments are, as always, greatly appreciated!
  prefs: []
  type: TYPE_NORMAL
- en: This is the fourth post as part of an (at least) monthly series by [Emset](https://www.emset.net/?source=post_page---------------------------) in
    which we show new and exciting methods for applying and developing Machine Learning
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Maarten Grootendorst](https://www.linkedin.com/in/mgrootendorst/)**
    is a Data Scientist, I/O Psychologist & Clinical Psychologist. He is passionate
    about anything AI-related! Get in touch: [https://www.linkedin.com/in/mgrootendorst/''](https://www.linkedin.com/in/mgrootendorst/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/4-tips-for-advanced-feature-engineering-and-preprocessing-ec11575c09ea).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[9 Tips For Training Lightning-Fast Neural Networks In Pytorch](/2019/08/9-tips-training-lightning-fast-neural-networks-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Tips for Dealing With Small Data](/2019/07/7-tips-dealing-small-data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Steps to Mastering Data Preparation for Machine Learning with Python — 2019
    Edition](/2019/06/7-steps-mastering-data-preparation-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Feature Store Summit 2022: A free conference on Feature Engineering](https://www.kdnuggets.com/2022/10/hopsworks-feature-store-summit-2022-free-conference-feature-engineering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Advanced Feature Selection Techniques for Machine Learning Models](https://www.kdnuggets.com/2023/06/advanced-feature-selection-techniques-machine-learning-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a Tractable, Feature Engineering Pipeline for Multivariate…](https://www.kdnuggets.com/2022/03/building-tractable-feature-engineering-pipeline-multivariate-time-series.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using RAPIDS cuDF to Leverage GPU in Feature Engineering](https://www.kdnuggets.com/2023/06/rapids-cudf-leverage-gpu-feature-engineering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Practical Approach To Feature Engineering In Machine Learning](https://www.kdnuggets.com/2023/07/practical-approach-feature-engineering-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Feature Engineering for Beginners](https://www.kdnuggets.com/feature-engineering-for-beginners)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
