# 人工智能中的偏见：入门

> 原文：[https://www.kdnuggets.com/2020/06/bias-ai-primer.html](https://www.kdnuggets.com/2020/06/bias-ai-primer.html)

[评论](#comments)

人工智能中的偏见现在和以前一样重要；它一直是一个重要的话题，但随着时间的推移，它似乎获得了更多的关注，这也是它应得的关注。人工智能偏见及其相关的伦理、包容性和多样性概念不再是相关课程和文本中的附带话题，而是如斯坦福的 CS224n: [自然语言处理与深度学习](/2020/05/best-nlp-deep-learning-course-free.html) 这样的课程中的核心和早期话题，还有即将出版的 Fast.ai 的书籍《[Deep Learning for Coders with fastai and PyTorch](/2020/06/fastai-book-free-ebook.html)》仅举两个具体的例子。

除了越来越多的从业者在日常工作中逐渐关注和包含人工智能偏见和伦理问题，许多研究人员今天也在做出有针对性且非常有意识的影响。[玛格丽特·米切尔](http://m-mitchell.com/) 是从事这一领域的研究人员之一。米切尔（[@mmitchell_ai](https://twitter.com/mmitchell_ai)）是谷歌研究与机器智能组的高级研究科学家。根据她的 [网站](http://m-mitchell.com/)，她的工作内容如下：

> * * *
> 
> ## 我们的前三个课程推荐
> ## 
> ![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。
> 
> ![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能
> 
> ![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的 IT
> 
> * * *
> 
> 我的研究通常涉及视觉-语言和基础语言生成，专注于如何将人工智能发展到积极的目标。这包括帮助计算机根据它们可以处理的内容进行沟通的研究，以及从人工智能的最新技术中创建辅助和临床技术的项目。

尽管玛格丽特的工作显然超越了基础，她确实在斯坦福 CS224n: 自然语言处理与深度学习的 [2019年冬季课程](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/) 上进行了关于人工智能偏见的话题介绍，相关幻灯片可在课程网站上获得。这些幻灯片（及演讲）的标题为 **[人工智能的视觉和语言中的偏见](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture19-bias.pdf)**，是对那些对人工智能偏见和伦理感兴趣但缺乏入门点的人们的绝佳资源。

![Figure](../Images/42f68e7509eca52bef16da16150e573b.png)

**图 1.** 公平性与包容性的评估：混淆矩阵（来自 Margaret Mitchell 的 [人工智能视觉与语言中的偏见幻灯片](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture19-bias.pdf)）。

讲座是一次自成一体的自然语言处理与深度学习课程（尽管涵盖了语言、视觉以及更多的“通用” AI），可以在一个场合中轻松消化，无论是通过幻灯片本身还是配套的讲座视频（见下文）。

首先，幻灯片涵盖了（毫不意外地）多种特定的偏见及其如何影响 AI 系统的不同方面，包括人类报告偏见、数据中的偏见、解释偏见、算法偏见等。还涉及了一些密切相关的概念，如原型理论的影响、公平性与包容性、反馈回路以及不公正的 AI 结果等。此外，还讨论了诸如从面部图像预测犯罪等有争议（甚至更糟糕）的项目，以及它们的缺陷和偏见。

讲座中强调了人类在 AI 偏见中的角色。AI 绝非一个孤立的、自成体系的技术，它与开发、训练、整理数据、操作及获取洞察的人员直接相关，反映了这些构建和互动的人的特质。正如 Mitchell 所说，“[我们要影响 AI 的发展方式]”，并给出了具体的例子说明如何做到这一点。

![图](../Images/54810b601491dd74cad9c3fda41f1715.png)

**图 2.** 数据中的人类偏见以及数据收集和注释中的人类偏见示例（来自 Margaret Mitchell 的 [人工智能视觉与语言中的偏见幻灯片](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture19-bias.pdf)）。

如前所述，幻灯片的内容也在 Mitchell 的课堂讲座视频中涵盖，这段视频也免费向所有人开放（见下文），你可以在一个小时的讲座中听到作者对幻灯片内容的详细解释。

AI 偏见是一个广泛的研究领域。然而，更重要的是，虽然它可以被视为一个值得深入探讨的独立实体，但非专家在总体 AI 相关背景中也需要考虑偏见。简而言之，人工智能中的偏见是所有人，包括利益相关者和非利益相关者、技术人员和非技术人员，从研究人员到工程师、从实践者到产品设计师等都需要关注的问题。

每个人都应尽力将 AI 建设得尽可能无偏见。因此，深入了解 AI 偏见的相关问题是每个人的必修课，而 Margaret Mitchell 的幻灯片和讲座是一个很好的起点。

**相关**：

+   [谷歌开源TFCO以帮助构建公平的机器学习模型](2020/03/google-open-sources-tfco-fair-machine-learning-models.html)

+   [最佳深度学习NLP课程免费](/2020/05/best-nlp-deep-learning-course-free.html)

+   [将伦理应用于人工智能的5种方法](/2019/12/5-ways-apply-ethics-ai.html)

### 更多相关内容

+   [3分钟理解偏差-方差权衡](https://www.kdnuggets.com/2020/09/understanding-bias-variance-trade-off-3-minutes.html)

+   [处理推荐和搜索中的位置偏差](https://www.kdnuggets.com/2023/03/dealing-position-bias-recommendations-search.html)

+   [偏差-方差权衡](https://www.kdnuggets.com/2022/08/biasvariance-tradeoff.html)

+   [统计学入门：Statology入门](https://www.kdnuggets.com/introduction-to-statistics-statology-primer)

+   [概率：Statology入门](https://www.kdnuggets.com/probability-statology-primer)

+   [数据描述：Statology入门](https://www.kdnuggets.com/describing-data-statology-primer)
