- en: WTF is Regularization and What is it For?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/wtf-is-regularization-and-what-is-it-for](https://www.kdnuggets.com/wtf-is-regularization-and-what-is-it-for)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![WTF is Regularization and What is it For?](../Images/d39360463487f8003bd583da1523f5d9.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: “An ounce of prevention is worth a pound of cure" goes the old saying, reminding
    us that it's easier to stop something from happening in the first place than to
    repair the damage after it has happened.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: In the era of artificial intelligence (AI), this proverb underscores the importance
    of avoiding potential pitfalls, such as overfitting, through techniques like regularization.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will discover regularization by starting with its fundamental
    principles to its application using Sci-kit Learn(Machine Learning) and Tensorflow(Deep
    Learning) and witness its transformative power with real-world datasets by comparing
    these results. Let’s start!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: What is regularization?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regularization is a critical concept in machine learning and deep learning that
    aims to prevent models from overfitting.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting happens when a model learns the training data too well. The situation
    shows your model is too good to be true.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see what overfitting looks like.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![WTF is Regularization and What is it For?](../Images/00ccaf0f05fa34b1cd5866ec402cda5c.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: Regularization techniques adjust the learning process to simplify the model,
    ensuring it performs well on training data and generalizes well to new data. We
    will explore two well-known ways of doing this.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Type of Regularization
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In machine learning, regularization is often applied to linear models, such
    as linear and logistic regression. In this context, the most common forms of regularization
    are:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization (Lasso regression)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L2 regularization (Ridge regression)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lasso Regularization** encourages the model to use only the most essential
    features by allowing some coefficient values to be exactly zero, which can be
    particularly useful for feature selection.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/8c5e1b738e7a4baa7bd1b0ad96b01bb5.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: '![WTF is Regularization and What is it For?](../Images/f00c7b9e37dfd3f73eae01feef99b5a1.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: On the other hand, **Ridge regularization** discourages significant coefficients
    by penalizing the square of their values.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/921975c606833ee14811111d51b36028.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: '![WTF is Regularization and What is it For?](../Images/4a1d226d0650c5dd32dc770b8353f0e9.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: In short, they calculated differently.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Let’s apply these to the cardiac patient data to see its power In deep learning
    and machine learning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'The Power of Regularization: Analyzing Cardiac Patient Data'
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will apply regularization to analyze cardiac patient data to see the
    power of regularization. You can reach the dataset from [here](https://www.kaggle.com/datasets/arezaei81/heartcsv?resource=download).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: To apply machine learning, we will use Scikit-learn; to apply deep learning,
    we will use TensorFlow. Let's start!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Regularization in Machine Learning
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scikit-learn is one of the most popular [Python libraries](https://www.stratascratch.com/blog/top-18-python-libraries-a-data-scientist-should-know/?utm_source=blog&utm_medium=click&utm_campaign=kdn+regularization)
    for machine learning that provides simple and efficient data analysis and modeling
    tools.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: It includes implementations of various regularization techniques, particularly
    for linear models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Here, we'll explore how to apply L1 (Lasso) and L2 (Ridge) regularization.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: In the following code, we will train logistic regression using Ridge(L2) and
    Lasso regularization (L1) techniques. At the end, we will see the detailed report.
    Let’s see the code.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here is the output.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![WTF is Regularization and What is it For?](../Images/e9a18c84b34123f686942ea4dc3a6e64.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: Let’s evaluate the result.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: L1 Regularization
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At C=0.001, accuracy is notably low (48%). This shows that the model is underfitting.
    It shows too much regularization.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As C increases to 0.01, accuracy remains unchanged for L1, suggesting that the
    model still suffers from underfitting or the regularization is too strong.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At C=0.1, accuracy improves significantly to 87%, showing that reducing the
    regularization strength allows the model to learn better from the data.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L2 Regularization
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Across the board, L2 regularization performs consistently well, with accuracy
    at 87% for C=0.001 and slightly higher at 89% for C=0.01, then stabilizing at
    87% for C=0.1.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: This suggests that L2 regularization is generally more forgiving and effective
    for this dataset in logistic regression models, potentially due to its nature.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Regularization in Deep Learning
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Several regularization techniques are used in deep learning, including L1 (Lasso)
    and L2 (Ridge) regularization, dropout, and early stopping.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: In this one, to repeat what we did in the machine learning example before, we
    will apply L1 and L2 regularization. Let’s define a list of L1 and L2 regularization
    values this time.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Then, for all of these values, we will train and evaluate our deep learning
    model, and at the end, we will assess the results.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see the code.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here is the output.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![WTF is Regularization and What is it For?](../Images/aed9cbfb94be24b5c3de86e5d681d49d.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: The deep learning model performances vary more widely across different combinations
    of L1 and L2 regularization values.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: The best performance is observed at L1=0.01 and L2=0.001, with an accuracy of
    88.5%, which indicates a balanced regularization that prevents overfitting while
    allowing the model to capture the underlying patterns in the data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳性能出现在L1=0.01和L2=0.001时，准确率为88.5%，这表明平衡的正则化防止了过拟合，同时允许模型捕捉数据中的潜在模式。
- en: Higher regularization values, especially at L1=0.1 or L2=0.1, drastically reduce
    model accuracy to 52.5%, suggesting that too much regularization severely limits
    the model's learning capacity.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 较高的正则化值，特别是在L1=0.1或L2=0.1时，会大幅降低模型准确性到52.5%，这表明过度正则化严重限制了模型的学习能力。
- en: Machine Learning & Deep Learning in Regularization
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习与深度学习中的正则化
- en: Let’s compare the results between Machine Learning and Deep Learning.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较一下机器学习和深度学习之间的结果。
- en: '**Effectiveness of Regularization**: Both in machine learning and deep learning
    contexts, appropriate regularization helps mitigate overfitting, but excessive
    regularization leads to underfitting. The optimal regularization strength varies,
    with deep learning models potentially requiring a more nuanced balance due to
    their higher complexity.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化的有效性**：在机器学习和深度学习的背景下，适当的正则化有助于减轻过拟合，但过度正则化会导致欠拟合。最佳正则化强度因情况而异，深度学习模型可能由于其更高的复杂性需要更细致的平衡。'
- en: '**Performance:** The best-performing machine learning model (L2 with C=0.01,
    89% accuracy) and the best-performing deep learning model (L1=0.01, L2=0.001,
    88.5% accuracy) achieve comparable accuracies, demonstrating that both approaches
    can be effectively regularized to achieve high performance on this dataset.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**性能：** 表现最佳的机器学习模型（L2与C=0.01，89%准确率）和表现最佳的深度学习模型（L1=0.01，L2=0.001，88.5%准确率）达到了类似的准确性，证明了两种方法都可以有效正则化，以在该数据集上实现高性能。'
- en: '**Regularization Strategy:** L2 regularization appears to be more effective
    and less sensitive to the choice of C in logistic regression models, while a combination
    of L1 and L2 regularization provides the best result in deep learning, offering
    a balance between feature selection and weight penalization.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化策略：** L2正则化在逻辑回归模型中似乎更有效且对C的选择不那么敏感，而L1和L2正则化的组合在深度学习中提供了最佳结果，提供了特征选择和权重惩罚之间的平衡。'
- en: The choice and strength of regularization should be carefully tuned to balance
    learning complexity with the risk of overfitting or underfitting.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化的选择和强度应谨慎调整，以平衡学习复杂性与过拟合或欠拟合的风险。
- en: Conclusion
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Throughout this exploration, we've demystified regularization, showing its role
    in preventing overfitting and ensuring our models generalize well to unseen data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一探索过程中，我们揭示了正则化的神秘面纱，展示了它在防止过拟合和确保我们的模型能良好泛化到未见数据中的作用。
- en: Applying regularization techniques will bring you closer to proficiency in machine
    learning and deep learning, solidifying your data scientist toolset.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 应用正则化技术将使你更接近机器学习和深度学习的精通，巩固你的数据科学家工具包。
- en: Go into the data projects and try regularizing your data in different scenarios,
    such as [Delivery Duration Prediction](https://platform.stratascratch.com/data-projects/delivery-duration-prediction?utm_source=blog&utm_medium=click&utm_campaign=kdn+regularization).
    We used both Machine Learning and Deep Learning models in this data project. However,
    in the end, we also mentioned that there might be room for improvement. So why
    don’t you try regularization over there and see if it helps?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 进入数据项目，尝试在不同场景中对数据进行正则化，例如[交付时间预测](https://platform.stratascratch.com/data-projects/delivery-duration-prediction?utm_source=blog&utm_medium=click&utm_campaign=kdn+regularization)。我们在这个数据项目中使用了机器学习和深度学习模型。然而，最后我们也提到可能还有改进的空间。那么，为什么不在那里尝试正则化，看看是否有帮助呢？
- en: '[](https://twitter.com/StrataScratch)****[Nate Rosidi](https://twitter.com/StrataScratch)****
    is a data scientist and in product strategy. He''s also an adjunct professor teaching
    analytics, and is the founder of StrataScratch, a platform helping data scientists
    prepare for their interviews with real interview questions from top companies.
    Nate writes on the latest trends in the career market, gives interview advice,
    shares data science projects, and covers everything SQL.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://twitter.com/StrataScratch)****[内特·罗斯迪](https://twitter.com/StrataScratch)****是一位数据科学家和产品策略专家。他还是一名兼职教授，教授分析学，并且是StrataScratch的创始人，该平台帮助数据科学家通过顶级公司真实面试问题准备面试。内特撰写关于职业市场的最新趋势，提供面试建议，分享数据科学项目，并涵盖所有SQL内容。'
- en: More On This Topic
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题
- en: '[WTF is the Difference Between GBM and XGBoost?](https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GBM 和 XGBoost 之间的区别是什么？](https://www.kdnuggets.com/wtf-is-the-difference-between-gbm-and-xgboost)'
- en: '[WTF is a Tensor?!?](https://www.kdnuggets.com/2018/05/wtf-tensor.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是张量？！?](https://www.kdnuggets.com/2018/05/wtf-tensor.html)'
- en: '[The Difference Between L1 and L2 Regularization](https://www.kdnuggets.com/2022/08/difference-l1-l2-regularization.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[L1 和 L2 正则化的区别](https://www.kdnuggets.com/2022/08/difference-l1-l2-regularization.html)'
- en: '[Top AI and Data Science Tools and Techniques for 2022 and Beyond](https://www.kdnuggets.com/2022/03/nvidia-0317-top-ai-data-science-tools-techniques-2022-beyond.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2022 年及以后最顶尖的 AI 和数据科学工具与技术](https://www.kdnuggets.com/2022/03/nvidia-0317-top-ai-data-science-tools-techniques-2022-beyond.html)'
- en: '[How to Select Rows and Columns in Pandas Using [ ], .loc, iloc, .at…](https://www.kdnuggets.com/2019/06/select-rows-columns-pandas.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 [ ]、.loc、iloc、.at 等在 Pandas 中选择行和列](https://www.kdnuggets.com/2019/06/select-rows-columns-pandas.html)'
- en: '[Explainable Forecasting and Nowcasting with State-of-the-art Deep…](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[最先进的深度学习解释性预测和现在预测](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
