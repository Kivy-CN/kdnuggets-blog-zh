- en: 'Topic Modeling Approaches: Top2Vec vs BERTopic'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/01/topic-modeling-approaches-top2vec-bertopic.html](https://www.kdnuggets.com/2023/01/topic-modeling-approaches-top2vec-bertopic.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Topic Modeling Approaches: Top2Vec vs BERTopic](../Images/668abce1b24f95ea2750b95c939a3722.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Mikechie Esparagoza](https://www.pexels.com/photo/photo-of-assorted-letter-board-quote-hanged-on-wall-1742370/)
  prefs: []
  type: TYPE_NORMAL
- en: Every day, we are dealing most of the time with unlabeled text and supervised
    learning algorithms cannot be used at all to extract information from the data.
    A subfield of natural language can reveal the underlying structure in large amounts
    of text. This discipline is called Topic Modeling, that is specialized in extracting
    topics from text.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In this context, conventional approaches, like Latent Dirichlet Allocation and
    Non-Negative Matrix Factorization, demonstrated to not capture well the relationships
    between words since they are based on bag-of-word.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, we are going to focus on two promising approaches, Top2Vec
    and BERTopic, that address these drawbacks by exploiting pre-trained language
    models to generate topics. Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Top2Vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Top2Vec is a model capable of detecting automatically topics from the text by
    using pre-trained word vectors and creating meaningful embedded topics, documents
    and word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this approach, the procedure to extract topics can be split into different
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Create Semantic Embedding**: jointly embedded document and word vectors are
    created. The idea is that similar documents should be closer in the embedding
    space, while dissimilar documents should be distant between them.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Reduce the dimensionality of the document embedding**: The application of
    the dimensionality reduction approach is important to preserve most of the variability
    of the embedding of documents while reducing the high dimensional space. Moreover,
    it allows to identification of dense areas, in which each point represents a document
    vector. UMAP is the typical dimensionality reduction approach chosen in this step
    because it’s able to preserve the local and global structure of the high-dimensional
    data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Identify clusters of documents**: HDBScan, a density-based clustering approach,
    is applied to find dense areas of similar documents. Each document is assigned
    as noise if it’s not in a dense cluster, or a label if it belongs to a dense area.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Calculate centroids in the original embedding space**: The centroid is computed
    by considering the high dimensional space, instead of the reduced embedding space.
    The classic strategy consists in calculating the arithmetic mean of all the document
    vectors belonging to a dense area, obtained in the previous step with HDBSCAN.
    In this way, a topic vector is generated for each cluster.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Find words for each topic vector**: the nearest word vectors to the document
    vector are semantically the most representative.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example of Top2Vec
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial, we are going to analyze the negative reviews of McDonald's
    from a dataset available on [data.world](https://data.world/crowdflower/mcdonalds-review-sentiment).
    Identifying the topics from these reviews can be valuable for the multinational
    to improve the products and the organisation of this fast food chain in the USA
    locations provided by the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![Topic Modeling Approaches: Top2Vec vs BERTopic](../Images/5679d764b1c931d55b32169dcd6791b3.png)'
  prefs: []
  type: TYPE_IMG
- en: In a single line of code, we are going to perform all the steps of the top2vec
    explained previously.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The main arguments of Top2Vec are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'docs_bad: is a list of strings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'universal-sentence-encoder: is the chosen pre-trained embedding model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'deep-learn: is a parameter that determines the quality of the produced document
    vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The most
  prefs: []
  type: TYPE_NORMAL
- en: '![Topic Modeling Approaches: Top2Vec vs BERTopic](../Images/342e839d6dad2a34bf4cf34cdd41b104.png)![Topic
    Modeling Approaches: Top2Vec vs BERTopic](../Images/28b15cea4b0bc64984e1b42d9173a0d6.png)![Topic
    Modeling Approaches: Top2Vec vs BERTopic](../Images/d79a930ee5541b0a66f0479e165d19f5.png)'
  prefs: []
  type: TYPE_IMG
- en: From the word clouds, we can deduce that the topic 0 is about general complaints
    about the service in McDonald, like “slow service”, “horrible service” and “order
    wrong”, while the topic 1 and 2 refer respectively to breakfast food (McMuffin,
    biscuit, egg) and coffee (iced coffee and cup coffee).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we try to search documents using two keywords, wrong and slow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: BERTopic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*"BERTopic is a topic modeling technique that leverages transformers and c-TF-IDF
    to create dense clusters allowing for easily interpretable topics whilst keeping
    important words in the topic descriptions."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As the name suggests, BERTopic utilises powerful transformer models to identify
    the topics present in the text. Another characteristic of this topic modeling
    algorithm is the use of a variant of TF-IDF, called class-based variation of TF-IDF.
  prefs: []
  type: TYPE_NORMAL
- en: Like Top2Vec, it doesn’t need to know the number of topics, but it automatically
    extracts the topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, similarly to Top2Vec, it is an algorithm that involves different
    phases. The first three steps are the same: creation of embedding documents, dimensionality
    reduction with UMAP and clustering with HDBScan.'
  prefs: []
  type: TYPE_NORMAL
- en: The successive phases begin to diverge from Top2Vec. After finding the dense
    areas with HDBSCAN, each topic is tokenized into a bag-of-words representation,
    which takes into account if the word appears in the document or not. After the
    documents belonging to a cluster are considered a unique document and TF-IDF is
    applied. So, for each topic, we identify the most relevant words, that should
    have the highest c-TF-IDF.
  prefs: []
  type: TYPE_NORMAL
- en: Example of BERTopic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We repeat the analysis on the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to extract the topics from the reviews using BERTopic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Topic Modeling Approaches: Top2Vec vs BERTopic](../Images/3f49f3d932e79c03c92cc4f49904f3af.png)'
  prefs: []
  type: TYPE_IMG
- en: The table returned by the model provides information about the 14 topics extracted.
    Topic corresponds to the topic identifier, except for all the outliers that are
    ignored that are labeled as -1.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are going to pass to the most interesting part regarding the visualization
    of our topics into interactive graphs, such as the visualization of the most relevant
    terms for each topic, the intertopic distance map, the two-dimensional representation
    of the embedding space and the topic hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin to show the bar charts for the top ten topics. For each topic, we
    can observe the most important words, sorted in decreasing order based on the
    c-TF-IDF score. The more a word is relevant, the more the score is higher.
  prefs: []
  type: TYPE_NORMAL
- en: The first topic contains generic words, like location and food, topic 1 order
    and wait, topic 2 worst and service, topic 3 place and dirty, ad so on.
  prefs: []
  type: TYPE_NORMAL
- en: After visualizing the bar charts, it’s time to take a look at the intertopic
    distance map. We reduce the dimensionality of c-TF-IDF score into a two-dimensional
    space to visualize the topics in a plot. At the bottom, there is a slider that
    allows selecting the topic that will be coloured in red. We can notice that the
    topics are grouped in two different clusters, one with generic thematics like
    food, chicken and location, and one with different negative aspects, such as worst
    service, dirty, place and cold.
  prefs: []
  type: TYPE_NORMAL
- en: The next graph permits to see the relationship between the reviews and the topics.
    In particular, it can be useful to understand why a review is assigned to a specific
    topic and is aligned with the most relevant words found. For example, we can focus
    on the red cluster, corresponding to topic 2 with some words about the worst service.
    The documents within this dense area seem quite negative, like “Terrible customer
    service and even worse food”.
  prefs: []
  type: TYPE_NORMAL
- en: Differences between TopVec and BERTopic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At first sight, these approaches have many aspects in common, like finding automatically
    the number of topics, no necessity of pre-processing in most of cases, the application
    of UMAP to reduce the dimensionality of document embeddings and, then, HDBSCAN
    is used for modelling these reduced document embeddings, but they are fundamentally
    different when looking at the way they assign the topics to the documents.
  prefs: []
  type: TYPE_NORMAL
- en: Top2Vec creates topic representations by finding words located close to a cluster’s
    centroid.
  prefs: []
  type: TYPE_NORMAL
- en: Differently from Top2Vec, BERTopic doesn’t take into account the cluster’s centroid,
    but it considered all the documents in a cluster as a unique document and extracts
    topic representations using a class-based variation of TF-IDF.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Top2Vec** | **BERTopic** |'
  prefs: []
  type: TYPE_TB
- en: '| The strategy to extract topics based on cluster’s centroids.  | The strategy
    to extract topics based on c-TF-IDF. |'
  prefs: []
  type: TYPE_TB
- en: '| It doesn’t support Dynamic Topic Modeling. | It supports Dynamic Topic Modeling.
    |'
  prefs: []
  type: TYPE_TB
- en: '| It builds word clouds for each topic and provides searching tools for topics,
    documents and words. | It allows for building Interactive visualization plots,
    allowing to interpretation of the extracted topics. |'
  prefs: []
  type: TYPE_TB
- en: Endnotes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Topic Modeling is a growing field of Natural Language Processing and there
    are numerous possible applications, like reviews, audio and social media posts.
    As it has been shown, this article provides an overviews of Topi2Vec and BERTopic,
    that are two promising approaches, that can help you to identify topics with few
    lines of code and interpret the results through data visualizations. If you have
    questions about these techniques or you have other suggestions about other approaches
    to detect topics, write it in the comments.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Eugenia Anello](https://www.linkedin.com/in/eugenia-anello/)** is currently
    a research fellow at the Department of Information Engineering of the University
    of Padova, Italy. Her research project is focused on Continual Learning combined
    with Anomaly Detection.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Approaches to Text Summarization: An Overview](https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Labeling for Machine Learning: Market Overview, Approaches, and Tools](https://www.kdnuggets.com/2021/12/data-labeling-ml-overview-and-tools.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A new book that will revolutionize the way your organization…](https://www.kdnuggets.com/2022/02/manning-new-book-revolutionize-way-organization-approaches-data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AI-Generated Sports Highlights: Different Approaches](https://www.kdnuggets.com/2022/03/aigenerated-sports-highlights-different-approaches.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning’s Sweet Spot: Pure Approaches in NLP and Document Analysis](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3 Approaches to Data Imputation](https://www.kdnuggets.com/2022/12/3-approaches-data-imputation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
