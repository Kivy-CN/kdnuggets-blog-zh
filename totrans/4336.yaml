- en: 10 Underappreciated Python Packages for Machine Learning Practitioners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/01/10-underappreciated-python-packages-machine-learning-practitioners.html](https://www.kdnuggets.com/2021/01/10-underappreciated-python-packages-machine-learning-practitioners.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Vinay Uday Prabhu](https://vinayprabhu.github.io/), Chief Scientist at
    UnifyID Inc.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/581c9aff740dcd71f1784a23ed38d159.png)'
  prefs: []
  type: TYPE_IMG
- en: Collage of all the PyPi packages listed here
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'TL-DR: Resources curated:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ????????:[Github Repo](https://github.com/vinayprabhu/Favorite_PyPi_2020) with
    all the images, code and figures
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ???????? : [Collage in pdf form with clickable links](https://github.com/vinayprabhu/Favorite_PyPi_2020/blob/main/PyPi_2020_collage.pdf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ????????: [Colab notebook](https://github.com/vinayprabhu/Favorite_PyPi_2020/blob/main/Colab_Pypi_Top10.ipynb)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ????????:[ HTML Document](https://github.com/vinayprabhu/Favorite_PyPi_2020/blob/main/Top_Pypi_2020.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ????????:[Notebook in PDF format](https://github.com/vinayprabhu/Favorite_PyPi_2020/blob/main/Top_Pypi_2020.pdf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*“The power of Open Source is the power of the people. The people rule”: ****Philippe
    Kahn***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ever since my doctoral studies that mostly entailed performing statistical analysis
    in **R** ( and admittedly **Octave/MATLAB**), I have strongly embraced the emergence
    of Python as the lingua franca amongst machine learners / data scientists / *insert
    latest profession-buzzword here*.
  prefs: []
  type: TYPE_NORMAL
- en: My daily workflow involves *quickly* *reacting* to the vagaries of messy real-world
    data, in all it’s naive-assumption-shattering glory. One major difference between
    graduate school and industry to me is the conquest of the inner-ego that goads
    you to implement algorithms from scratch. Once past the *white-boarding/hypothesis
    building phase *I quickly parse through the [PyPi repository](https://pypi.org/) to
    check if any of the constituent modules have already been authored. This is typically
    followed by a
  prefs: []
  type: TYPE_NORMAL
- en: '**>>* pip install *PACKAGE_NAME****ritual and voila, I find myself standing
    on the shoulders of the open-source giants whose careful work I am now harnessing
    to scale the [DIKW pyramid](https://en.wikipedia.org/wiki/DIKW_pyramid).'
  prefs: []
  type: TYPE_NORMAL
- en: I authored this blogpost to *acknowledge, celebrate and yes, publicize*, some
    amazing and *under-appreciated* PyPi packages that I used this past year; ones
    that I strongly feel deserve more recognition and love from our community. This
    is also my humble ode to the open-source scholars’ sweat equity that oft gets
    buried inside the *pip install *command.
  prefs: []
  type: TYPE_NORMAL
- en: Caveat on sub-domain bias: *This particular post is focused on machine learning
    pipelines entailing**** neural networks/deep learning****. I plan to author similarly
    focused blogposts on specialized topics such as time-series analysis and human-kinematics
    analysis in the near future.*✌️
  prefs: []
  type: TYPE_NORMAL
- en: 'What follows below are basic introductions into the 10 PyPi packages spanning:'
  prefs: []
  type: TYPE_NORMAL
- en: a) **Neural network architecture specification and training**: *NSL-tf*, *Kymatio* and *LARQ*
  prefs: []
  type: TYPE_NORMAL
- en: b) **Post training calibration and performance benchmarking**: *NetCal, PyEER *and* Baycomp.*
  prefs: []
  type: TYPE_NORMAL
- en: c) **Pre real-world deployment stress-testing**:* PyOD, HyPPO* and *Gradio*
    d) **Documentation / dissemination**: *Jupyter_to_medium*
  prefs: []
  type: TYPE_NORMAL
- en: '0: Pip install the above mentioned packages :)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A) Neural network architecture specification and training: *NSL-tf*, *Kymatio* and *LARQ*
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '1: [Neural Structured Learning- Tensorflow](https://www.tensorflow.org/neural_structured_learning):'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the heart of most off-the-shelf classification algorithms in machine learning
    lies the ***i.i.d fallacy***. Simply put, the algorithm design rests on the assumption
    that the samples in the training set (as well as the test-set) are independent
    and identically distributed. In reality however, this rarely holds true and there
    exist correlations between the samples that can be harnessed to achieve better
    accuracy and explainability as well. In a wide array of application scenarios
    (See Fig-1), these correlations are captured by an underlying graph (G(V,E)) that
    can either be co-mined or statistically inferred. For example, if you are performing,
    say, sentiment detection of textual-tweets, the underlying follower-following
    social graph provides vital cues that models the *social* *context *in which the
    tweet was authored. This social neighborhood information can then be harnessed
    to perform network-aided classification that can be crucial in guarding against
    text-only shortcomings such as sarcasm mis-detection and hashtag-hijacking.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/2944877f0d45bad5f74fd27be3bb672f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig-1: Examples of online information graphs'
  prefs: []
  type: TYPE_NORMAL
- en: My [PhD thesis](https://kilthub.cmu.edu/articles/thesis/Network_Aided_Classification_and_Detection_of_Data/7430012/1) titled
    “Network Aided Classification and Detection of Data” literally explored the science
    and *algorithmics* of this graph-enhanced machine learning and it was so heartening
    to see Tensorflow release the [Neural structured learning](https://www.tensorflow.org/neural_structured_learning) framework
    along with a series of well crafted tutorials ( youtube [playlist](https://www.youtube.com/watch?v=N_IS3x5wFNI&list=PLS6Lwe0CFTqbS8WxxPmil0mCjAHZ0rD1x&ab_channel=TensorFlow) )
    along with an easy-to-follow [NSL Example colab-notebook](https://colab.research.google.com/drive/1yidXh-kM6fMi5c0yEXonvG4GFdcDO0-d#scrollTo=gRfU8T3BTYep&line=2&uniqifier=1).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example cell below, we train a NSL-enhanced neural network for the standard
    MNIST dataset in an adversarial setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '2: Kymatio: Wavelet scattering in Python'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here’s one of the best (or worst?) kept secrets in ML: ***A lot of the easy
    datasets (read the x-mnist family / cats-v-dogs / Hot-Dog classification) require
    NO backprop/SGD training histrionics***.
  prefs: []
  type: TYPE_NORMAL
- en: The classes are separable enough and the architecture-induced discriminative
    capacity is high enough that careful initialization using [Grassmannian codebooks](https://arxiv.org/pdf/1911.07418.pdf) or
    wavelet filters followed by ‘last-layer’ hyper-plane learning (using standard
    regression techniques) should suffice to obtain a high-accuracy classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this regard, [Kymatio](https://www.kymat.io/) has played a Caesar-esque
    role in the wavelet filters world uniting all the previous siloed projects such
    as `ScatNet`, `scattering.m`, `PyScatWave`, `WaveletScattering.jl`, and `PyScatHarm `into
    one easy to use monolithic portable framework that seamlessly works across six
    frontend–backend pairs: NumPy (CPU), scikit-learn (CPU), pure PyTorch (CPU and
    GPU), PyTorch+scikit-cuda (GPU), TensorFlow (CPU and GPU), and Keras (CPU and
    GPU).'
  prefs: []
  type: TYPE_NORMAL
- en: In the example cell below, we use the in-built Scattering2D class to train another
    MNIST-neural network that attains 92.84% accuracy in 15 epochs. This package is
    wonderfully well documented with a plethora of interesting examples such as [Classification
    of spoken digit recordings](https://www.kymat.io/gallery_1d/plot_classif_torch.html#sphx-glr-gallery-1d-plot-classif-torch-py) using
    1D scattering transforms and[ 3D scattering quantum chemistry regression](https://www.kymat.io/gallery_3d/scattering3d_qm7_torch.html#sphx-glr-gallery-3d-scattering3d-qm7-torch-py).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[3: LARQ](https://docs.larq.dev/zoo/tutorials/)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'I met the LARQ developers last December during NEURIPS-2019 in Vancouver where
    they unveiled their new open-source Python library for training Binarized Neural
    Networks (BNNs) alongside the poster of their paper titled [*Latent Weights Do
    Not Exist: Rethinking Binarized*](https://papers.nips.cc/paper/2019/file/9ca8c9b0996bbf05ae7753d34667a6fd-Paper.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Neural Network Optimization. While there seems to be a lot of interest towards
    model compression for resource constrained on-device deployment ([Here’s 42 of
    ‘em](https://awesomeopensource.com/projects/model-compression)!), training fast-and-frugal
    Binary Neural Networks from scratch seems to be an option that many seem to discount
    at the outset.
  prefs: []
  type: TYPE_NORMAL
- en: The LARQ package should help change things on that front given the ease of use,
    fast inference (Convolution operations turn into xor/bit-shifts with binarized
    weights), brilliant documentation and plentiful architecture examples that one
    can then hack away by means of a full-fledged model [zoo](https://docs.larq.dev/zoo/).
    This year, I have personally published work on [style-transfer](https://matthewmcateer.me/posts/bnn-nst/) and
    a [40 kB BiPedalNet model](https://pml4dc.github.io/iclr2020/papers/PML4DC2020_32.pdf) using
    LARQ and it’s always a breeze to work with this toolkit. Besides the [Zoo](https://docs.larq.dev/zoo/),
    the package is also accompanied by a highly optimized [Compute Engine](https://docs.larq.dev/compute-engine/) that* currently
    supports various mobile platforms, has been benchmarked on a Pixel 1 phone & Raspberry
    Pi and also provides a collection of hand-optimized TensorFlow Lite custom operators
    for supported instruction sets, developed in inline assembly or in C++ using compiler
    intrinsics.*
  prefs: []
  type: TYPE_NORMAL
- en: In the example code cell below, we train a 13.19 KB BNN that hits 98.31 % on
    the MNIST dataset in 6 epochs and also demonstrate how easy it is to pull one
    of the SOTA pre-trained *QuickNet* models from the LARQ-zoo and run inference.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![png](../Images/a6780fad5a800441cfe7fcb83d032f80.png)'
  prefs: []
  type: TYPE_IMG
- en: 'B) Post training calibration and performance benchmarking: NetCal, PyEER and
    BayComp'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we’ll look at packages that are useful in the post-training
    pre-deployment scenario where the practitioner’s goals are to calibrate the outputs
    a of a pre-trained model and rigorously benchmark the performance of multiple
    models ripe for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: '[1: Netcal](https://pypi.org/project/netcal/):'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Figure](../Images/32e6d40a03a71407982fa90300c831bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig-2: Scaling v/s Binning v/s Scaling-Binning. ‘B’ denotes the # of distinct
    probabilities the model'
  prefs: []
  type: TYPE_NORMAL
- en: outputs. Source: [https://arxiv.org/pdf/1909.10155.pdf](https://arxiv.org/pdf/1909.10155.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'Often times, I have seen ML practitioners buy into this false equivalence between
    the output *softmax* values and probabilities. They are anything but that! Their
    co-inhabitance of the (0,1] space allows them to masquerade as probabilities but
    the ‘raw’ softmax values are, well, ‘[uncalibrated](https://arxiv.org/pdf/1706.04599.pdf)’
    put nicely. Hence, post-training calibration is a rapidly growing body of work
    in deep learning, and the techniques proposed herewith largely falls into 3 categories
    (See Fig-2):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Binning (Ex: Histogram Binning, Isotonic Regression, Bayesian Binning into
    Quantiles (BBQ), Ensemble of Near Isotonic Regression (ENIR))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scaling (Ex: Logistic Calibration/Platt Scaling, Temperature Scaling , Beta
    Calibration)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hybrid scaling-binning](https://arxiv.org/pdf/1909.10155.pdf) (Python library: [https://pypi.org/project/uncertainty-calibration](https://pypi.org/project/uncertainty-calibration))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With regards to all the above stated Binning and Scaling techniques, the implementations
    with extremely well authored documentation is available in the NetCal. The package
    also included primitives for generating Reliability Diagrams and estimating calibration
    error metrics such as Expected /Max/Average Calibration Errors as well.
  prefs: []
  type: TYPE_NORMAL
- en: In the cell below, we see use the obtained softmax values on the MNIST test-set
    (from the NSL trained model above) to demonstrate the usage of the Temperature
    Scaling calibration and Reliability-Diagram generation routines.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![png](../Images/d5cc7402f1bff615fa0059bf41206c80.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![png](../Images/69889862580274b1cdc38edc66102162.png)'
  prefs: []
  type: TYPE_IMG
- en: '![png](../Images/daa5d781ae7379488957133f289260c2.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![png](../Images/af4846a01cfe622db1c26cb4d13f6a83.png)'
  prefs: []
  type: TYPE_IMG
- en: '[2: Baycomp](https://baycomp.readthedocs.io/): So you think you have a better
    classifier?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the under-rated conundrums that both ML practitioners and in some ways,
    research paper reviewers, grapple with, is rigorously ascertaining the predictive
    supremacy of one classifier model over the other(s). Model-Olympics platforms
    like [Papers with code](https://paperswithcode.com/) further promulgate this model-ranking
    fallacy by erroneously centering the top-1 accuracy metric (See Fig below) as
    the deciding measure.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/1a89fad6cb754f68e51ad5bae2408900.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: [https://paperswithcode.com/sota/image-classification-on-inaturalist-2018](https://paperswithcode.com/sota/image-classification-on-inaturalist-2018)
  prefs: []
  type: TYPE_NORMAL
- en: So, given two classifications models with similar engineering overheads to deploy,
    how do you choose one over the other? Typically, we have a standard benchmarking
    dataset (or a set of datasets) that serve as the testing ground for classifier-wars.
    After obtaining the ‘*raw accuracy metrics over this dataset-space*’ a statistical
    minded machine learner might be inclined to use tools from the frequentist null
    hypothesis significance testing (NHST) framework to establish which classifier
    is ‘better’. However, as stated [here](https://www.jmlr.org/papers/volume18/16-305/16-305.pdf),
    “*Many scientific fields however realized the shortcomings of frequentist reasoning
    and in the most radical cases even banned its use in publications”.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Baycomp emerges in this contextproviding a ***Bayesian framework for comparison
    of classifiers***. The library helps compute three probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: 'P_left : The probability that the first classifier has higher accuracy scores
    than the second.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'P_rope: The probability that differences are within the region of practical
    equivalence (rope)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'P_right: The probability that the second classifier has higher scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **r**egion **o**f **p**ractical **e**quivalence (rope) is specified by the
    machine learner who is well versed with what could be safely assumed to be *equivalent* in
    the domain of deployment.
  prefs: []
  type: TYPE_NORMAL
- en: In the example cell below, we consider both, a synthetic example entailing two
    closely competitive classifiers as well as the two classifiers we just trained
    using the NSL-TF and LARQ-BNN frameworks on the MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![png](../Images/2d6d7dda04dc122e5edcebdba65f2f69.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![png](../Images/c2a73cc8cfd3d6f173d37301cacecd95.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Figure](../Images/91a01fb33e6680632e0490584b6e874d.png)'
  prefs: []
  type: TYPE_IMG
- en: Using baycomp to compare classifiers +NSL v/s BNN classifier comparison on the
    MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: '***Important caveat***: *There is a related but distinct conversation surrounding
    the very culture of predictive accuracy veneration in ML. This*[* predicitivism
    v/s accommodation debate*](https://plato.stanford.edu/entries/prediction-accommodation/#:~:text=The%20view%20that%20predictions%20are,when%20predicted%20than%20when%20accommodated.)* in
    science has been evolving since the days of John Herschel and William Whewell
    in the 1800s.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[3: PyEER](https://pypi.org/project/pyeer/)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Figure](../Images/fe59eb7b9ce52d039f05b4232f0ce982.png)'
  prefs: []
  type: TYPE_IMG
- en: The wide repertoire of methods available in PyEER
  prefs: []
  type: TYPE_NORMAL
- en: Another way of comparing two classifiers, especially in the context of solving
    the binary authentication problem (Not *surveillance *but* Authentication*) is
    by plotting the comparative detection error tradeoff (DET) and Receiver operating
    characteristic (ROC) graphs. PyEER is an absolute tour-de-force in this regard
    as it serves as a one-stop-shop for not just plotting the relevant graphs but
    also auto-generating metrics-reports and estimating EER-optimal-thresholds. In
    the example cell below, we compare the Angle-Based Outlier Detector (ABOD)and
    the KNN inlier-outlier detector binary classifiers that’ll be introduced in the
    forthcoming section on pre-deployment Out-of-Distribution detection techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![png](../Images/96d46c201082e58e04fc232d1fa08fb2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'C: Pre real-world deployment stress-testing: PyOD, HyPPO and Gradio'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Figure](../Images/17ec9df55fe6dd212daf2484e5a6f557.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The landscape of OOD susceptibility: Access the SVG [here](https://matthew-mcateer.github.io/oodles-of-oods/)'
  prefs: []
  type: TYPE_NORMAL
- en: Vulnerability to Out-of-distribution (OOD) samples resulting in confident mispredictions
    is currently one of the most serious roadblocks that haunts the transition of
    ML ideas from pretty little papers to real-world deployment where the inputs have
    no guarantees to emanate from the proverbial *training manifold*. In a joint project
    with [Matthew McAteer](https://matthew-mcateer.github.io/oodles-of-oods/), I’ve
    created a landscape of susceptibility (See figure above) that should empower machine
    learners to cover the wide spectrum of specific vulnerability-vectors with regards
    to their models.
  prefs: []
  type: TYPE_NORMAL
- en: While there is no silver bullet (and there perhaps will never be — See [this](https://arxiv.org/pdf/1802.08686.pdf) & [this](https://arxiv.org/abs/1809.02104)),
    it’d be hard to argue against incorporating OOD-model regularization and OOD-detection
    modules into your pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: With regards to OOD-detection, I felt that there were 3 recent efforts that
    have gone under-adopted by the ML community.
  prefs: []
  type: TYPE_NORMAL
- en: The first two, *PyOD* and *HyPPO*, would be useful for pre-filtering inputs
    before performing inference and the third, Gradio, is an amazing tool for human-in-the-loop
    white-hat stress testing and complementary to efforts such as [Dynabench](https://dynabench.org/) by
    FAIR.
  prefs: []
  type: TYPE_NORMAL
- en: '[1: PYOD](https://pyod.readthedocs.io/en/latest/)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Figure](../Images/f1cb5578bfaf5d746d6dc6a33256602b.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: [https://www.jmlr.org/papers/volume20/19-011/19-011.pdf](https://www.jmlr.org/papers/volume20/19-011/19-011.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: PyOD is arguably the most comprehensive and scalable Outlier Detection Python
    toolkit out there that includes implementation of more than 30 detection algorithms!
  prefs: []
  type: TYPE_NORMAL
- en: It is somewhat rare for a student-maintained PyPi package to incorporate software
    engineering best practices that ensures that model classes implemented are covered
    by unit testing with cross platform continuous integration, code coverage and
    code maintainability checks. This combined with a a clean unified API, detailed
    documentation and just-in-time (JIT) compiled execution makes it an absolute breeze
    to both learn about the different techniques and use it in practice. The efforts
    invested by the authors towards careful parallelization has resulted in extremely
    fast and scalable outlier detection code that is also seamlessly compatible across *Python
    2 and 3 across major operating systems (Windows, Linux and MacOS)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example cell below, we train and visualize the results of two inlier-outlier
    detector binary classifiers on a synthetic dataset: the Angle-Based Outlier Detector
    (ABOD)and the KNN outlier detector.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s visualize the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure](../Images/b1e03b0842cef601d3e1bd7dce0867b8.png)'
  prefs: []
  type: TYPE_IMG
- en: ABOD v/s KNN for outlier detection
  prefs: []
  type: TYPE_NORMAL
- en: '[2: Hyppo](https://hyppo.neurodata.io/index.html):'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is somewhat bewildering to witness this collective amnesia on part of the
    Deep Learning community that keeps treating OOD susceptibility as a uniquely ‘deep
    neural networks’ shortcoming that somehow merits a deep-learning-solution whilst
    completing ignoring the cache of approaches and solutions already explored by
    the statistics community.
  prefs: []
  type: TYPE_NORMAL
- en: One could argue that OOD-detection by it’s very definition falls under the ambit
    of the multivariate hypothesis testing framework, and hence it is frustrating
    to see deep learning OOD papers not even benchmark the results obtained by their
    shiny new deep-approaches with what could be possible legacy hypothesis testing
    algorithms. With this setting, we now introduce HYPPO.
  prefs: []
  type: TYPE_NORMAL
- en: HYPPO (**HYP**othesis Testing in **P**yth**O**n, pronounced “Hippo”) is arguably
    the most comprehensive open-source software package for multivariate hypothesis
    testing produced by the [NEURODATA](https://neurodata.io/) community. In the figure
    below, we see the landscape of modules implemnetd in this package that spans synthetic
    data generation (with 20 dependency structures!), Independence Tests, K-sample
    Tests as well as Time-Series Tests.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/a278d6ff3db81a5ad235291b15bde662.png)'
  prefs: []
  type: TYPE_IMG
- en: Landscape of algorithms implemented in Hyppo
  prefs: []
  type: TYPE_NORMAL
- en: In the example cell below, we see the K-Sample-Distance Correlation(Or “Dcorr”)
    test being used to hypothesis test between the in-and-out distributed data generated
    by the generate_data() module in PyOD above. In a deep-learning setting, we could
    deploy these tests both at the input layer level or in the feature-embedding space
    to guesstimate if the output softmax values are even worthy of being processed
    further down the inference pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[3: Gradio](https://gradio.app/ml_examples):'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Figure](../Images/f63344218fef2c36bef52b79235e381e.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradio’s saliency cropping algorithm: [http://saliency-model.gradiohub.com/](http://saliency-model.gradiohub.com/)
  prefs: []
  type: TYPE_NORMAL
- en: Having a nice GUI to interact with the model you have just trained has thus
    far required a fair amount of JavaScript-front-end gimmickry or the Heroku-Flask
    route that can take focus away from the algorithmics.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to Gradio, one can can quickly fire up a gui with <10 lines of Python
    with pre-built input modules that cover textual input, image-inputs with an awesome
    Toast-UI image-editor and a sketchpad to boot as well!
  prefs: []
  type: TYPE_NORMAL
- en: This past year, I have heavily used Gradio in my workflow, using it to investigate
    why Twitter’s saliency cropping algorithm yields such racist results (See Figure
    to the left) to why Onions were triggering NSFW filters on [facebook](https://www.bbc.com/news/54467384) (See
    tweet below) .
  prefs: []
  type: TYPE_NORMAL
- en: 'The [NSFW-Onion](https://github.com/vinayprabhu/Crimes_of_Vision_Datasets/blob/master/Notebooks/Notebook_5b_Onion_Gradio_NSFW.ipynb) fiasco.
    Colab notebook: [https://github.com/vinayprabhu/Crimes_of_Vision_Datasets/blob/master/Notebooks/Notebook_5b_Onion_Gradio_NSFW.ipynb](https://github.com/vinayprabhu/Crimes_of_Vision_Datasets/blob/master/Notebooks/Notebook_5b_Onion_Gradio_NSFW.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: In the example cell below, we demonstrate two simple examples of using Gradio
    to fire-up UIs to stress-test the MNIST classification BNN model we just trained
    above with a sketchpad input and to demonstrate the ease of using the InceptionV3
    model to classify images. The Gradio team has also rapidly added explainaibility
    and embeddings-visualization tools, and implemented SOTA [blind super resolution](https://gradio.app/g/dawoodkhan82/dan) and [Real-Time
    High-Resolution Background Matting](https://gradio.app/g/BackgroundMattingV2) UIs
    as well!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure](../Images/9b5d730fae41a829217dfe3fbcc4ab6c.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradio MNIST classification with a sketchpad input![Figure](../Images/ead2537d8637878a263a7c0101d030a7.png)
  prefs: []
  type: TYPE_NORMAL
- en: Gradio Image classification (InceptionV3) with an image input
  prefs: []
  type: TYPE_NORMAL
- en: 'D) Documentation / dissemination:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '1) Jupyter_to_medium:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Figure](../Images/45bb652f1b909dd4c89ef395ee1ae867.png)'
  prefs: []
  type: TYPE_IMG
- en: Example image. Source: [https://www.dexplo.org/jupyter_to_medium/](https://www.dexplo.org/jupyter_to_medium/)
  prefs: []
  type: TYPE_NORMAL
- en: Video tutorial of the *Jupyter_to_medium package*
  prefs: []
  type: TYPE_NORMAL
- en: Last but not the least, I used the *Jupyter_to_medium* PyPi package to author
    this very blogpost from it’s [source notebook](https://github.com/vinayprabhu/Favorite_PyPi_2020)!
    As many of you might have experienced, converting your jupyter/colab notebook
    into a readable blogpost involves painful copy-pasting antics, code snapshotting
    and plug-in gimmicks. All of this is a thing of past after the release of this
    game-changing package.
  prefs: []
  type: TYPE_NORMAL
- en: 'The procedure is super-simple: pip install, finish the notebook, choose File
    →’Deploy as’, insert integration token from medium, and perform final edits/prettification
    if necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: On the concluding note, I’d like to thank the incredible researchers and engineers
    who created these wonderful PyPi packages . In the forthcoming blogpost(s), I
    plan to cover packages pertaining to specific topics such as time-series analysis
    and dimensionality reduction. Here’s a recap picture that summarizes the packages
    explored above.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/b0ff25585442b78ff440f70a0a846a2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Recap of the PyPi landscape covered in this blogpost
  prefs: []
  type: TYPE_NORMAL
- en: Hope some of you will find this blogpost useful in your ML adventures. Good
    luck and wish y’all a happy and productive 2021 ????!
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to leave feedback regarding the content /errata/ broken links. You
    may connect with me via [Linkedin](https://www.linkedin.com/in/vinay-prabhu-84619785/) or [Twitter](https://twitter.com/vinayprabhu) as
    well ????
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Vinay Uday Prabhu](https://vinayprabhu.github.io/)** is Chief Scientist
    at UnifyID Inc.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/most-useful-machine-learning-pypi-packages-of-2020-a0ec6678ce22).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Data Science as a Product – Why Is It So Hard?](/2020/12/data-science-product-hard.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Generating Beautiful Neural Network Visualizations](/2020/12/generating-beautiful-neural-network-visualizations.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fast and Intuitive Statistical Modeling with Pomegranate](/2020/12/fast-intuitive-statistical-modeling-pomegranate.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
