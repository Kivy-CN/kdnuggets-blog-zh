- en: Tuning Hyperparameters in Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/tuning-hyperparameters-in-neural-networks](https://www.kdnuggets.com/tuning-hyperparameters-in-neural-networks)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Tuning Hyperparameters in Neural Networks](../Images/b9de72ea3c51ae95da6057db4e0a857f.png)'
  prefs: []
  type: TYPE_IMG
- en: Hyperparameters determine how well your neural network learns and processes
    information. Model parameters are learned during training. Unlike these parameters,
    hyperparameters must be set before the training process starts. In this article,
    we will describe the techniques for optimizing the hyperparameters in the models.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters In Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Learning Rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The learning rate tells the model how much to change based on its errors. If
    the learning rate is high, the model learns quickly but might make mistakes. If
    the learning rate is low, the model learns slowly but more carefully. This leads
    to less errors and better accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Tuning Hyperparameters in Neural Networks](../Images/2b104f721c61169c260d51c2f1f83c47.png)Source:
    https://www.jeremyjordan.me/nn-learning-rate/'
  prefs: []
  type: TYPE_NORMAL
- en: There are ways of adjusting the learning rate to achieve the best results possible.
    This involves adjusting the learning rate at predefined intervals during training.
    Furthermore, optimizers like the Adam enables a self-tuning of the learning rate
    according to the execution of the training.
  prefs: []
  type: TYPE_NORMAL
- en: Batch Size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Batch size is the number of training samples a model undergoes at a given time.
    A large batch size basically means that the model goes through more samples before
    the parameter update. It can lead to more stable learning but requires more memory.
    A smaller batch size on the other hand updates the model more frequently. In this
    case, learning can be faster but it has more variation in each update.
  prefs: []
  type: TYPE_NORMAL
- en: The value of the batch size affects memory and processing time for learning.
  prefs: []
  type: TYPE_NORMAL
- en: Number of Epochs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Epochs refers to the number of times a model goes through the entire dataset
    during training. An epoch includes several cycles where all the data batches are
    shown to the model, it learns from it, and optimizes its parameters. More epochs
    are better in learning the model but if not well observed they can result in overfitting.
    Deciding the correct number of epochs is necessary to achieve a good accuracy.
    Techniques like early stopping are commonly used to find this balance.
  prefs: []
  type: TYPE_NORMAL
- en: Activation Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Activation functions decide whether a neuron should be activated or not. This
    leads to non-linearity in the model. This non-linearity is beneficial especially
    while trying to model complex interactions in the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Tuning Hyperparameters in Neural Networks](../Images/bda31e9774872cf755f405bf06b7dd96.png)Source:
    https://www.researchgate.net/publication/354971308/figure/fig1/AS:1080246367457377@1634562212739/Curves-of-the-Sigmoid-Tanh-and-ReLu-activation-functions.jpg'
  prefs: []
  type: TYPE_NORMAL
- en: Common activation functions include ReLU, Sigmoid and Tanh. ReLU makes the training
    of neural networks faster since it permits only the positive activations in neurons.
    Sigmoid is used for assigning probabilities since it outputs a value between 0
    and 1\. Tanh is advantageous especially when one does not want to use the whole
    scale which ranges from 0 to ± infinity. The selection of a right activation function
    requires careful consideration since it dictates whether the network shall be
    able to make a good prediction or not.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dropout is a technique which is used to avoid overfitting of the model. It randomly
    deactivates or "drops out" some neurons by setting their outputs to zero during
    each training iteration. This process prevents neurons from relying too heavily
    on specific inputs, features, or other neurons. By discarding the result of specific
    neurons, dropout helps the network to focus on essential features in the process
    of training. Dropout is mostly implemented during training while it is disabled
    in the inference phase.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Tuning Techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Manual Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This method involves trial and error of values for parameters that determine
    how the learning process of a machine learning model is done. These settings are
    adjusted one at a time to observe how it influences the model’s performance. Let's
    try to change the settings manually to get better accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Manual search is simple because you do not require any complicated algorithms
    to manually set parameters for testing. However, it has several disadvantages
    as compared to other methods. It can take a lot of time and it may not find the
    very best settings efficiently than the automated methods
  prefs: []
  type: TYPE_NORMAL
- en: Grid Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Grid search tests many different combinations of hyperparameters to find the
    best ones. It trains the model on part of the data. After that, it checks how
    well it does with another part. Let's implement grid search using GridSearchCV
    to find the best model .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Grid search is much faster than manual search. However, it is computationally
    expensive because it takes time to check every possible combination.
  prefs: []
  type: TYPE_NORMAL
- en: Random Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This technique randomly selects combinations of hyperparameters to find the
    most efficient model. For each random combination, it trains the model and checks
    how well it performs. In this way, it can quickly arrive at good settings that
    cause the model to perform better. We can implement random search using RandomizedSearchCV
    to achieve the best model on the training data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Random search is normally better than the grid search since only a few number
    of hyperparameters are checked to get suitable hyperparameters settings. Nonetheless,
    it might not search the correct combination of hyperparameters particularly when
    the working hyperparameters space is large.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've covered some of the basic hyperparameter tuning techniques. Advanced techniques
    include Bayesian Optimization, Genetic Algorithms and Hyperband.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Jayita Gulati](https://www.linkedin.com/in/jayitagulati1998/)** is a machine
    learning enthusiast and technical writer driven by her passion for building machine
    learning models. She holds a Master''s degree in Computer Science from the University
    of Liverpool.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Tuning Random Forest Hyperparameters](https://www.kdnuggets.com/2022/08/tuning-random-forest-hyperparameters.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Tuning XGBoost Hyperparameters](https://www.kdnuggets.com/2022/08/tuning-xgboost-hyperparameters.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3 Reasons Why You Should Use Linear Regression Models Instead of…](https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Neural Networks Don''t Lead Us Towards AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Explainable Forecasting and Nowcasting with State-of-the-art Deep…](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
