- en: 'XGBoost: A Concise Technical Overview'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'XGBoost: 简明技术概述'
- en: 原文：[https://www.kdnuggets.com/2017/10/xgboost-concise-technical-overview.html](https://www.kdnuggets.com/2017/10/xgboost-concise-technical-overview.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/10/xgboost-concise-technical-overview.html](https://www.kdnuggets.com/2017/10/xgboost-concise-technical-overview.html)
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍
- en: '"Our single XGBoost model can get to the top three! Our final model just averaged
    XGBoost models with different random seeds."'
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们的单一XGBoost模型可以进入前三名！我们的最终模型只是对不同随机种子的XGBoost模型进行平均。”
- en: ''
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- [Dmitrii Tsybulevskii & Stanislav Semenov](https://blog.kaggle.com/2016/08/24/avito-duplicate-ads-detection-winners-interview-1st-place-team-devil-team-stanislav-dmitrii/),
    winners of Avito Duplicate Ads Detection Kaggle competition.'
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- [Dmitrii Tsybulevskii & Stanislav Semenov](https://blog.kaggle.com/2016/08/24/avito-duplicate-ads-detection-winners-interview-1st-place-team-devil-team-stanislav-dmitrii/)，Avito重复广告检测Kaggle比赛的获胜者。'
- en: With entire [blogs](https://blog.kaggle.com/tag/xgboost/) dedicated to how the
    sole application of XGBoost can propel one’s ranking on Kaggle competitions, it
    is time we delved deeper into the concepts of XGBoost.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着整个[博客](https://blog.kaggle.com/tag/xgboost/)专注于如何仅仅使用XGBoost就能提升Kaggle比赛中的排名，现在是时候深入探讨XGBoost的概念了。
- en: Bagging algorithms control for high variance in a model. However, boosting algorithms
    are considered more effective as they deal with both bias as well as variance
    (the bias-variance trade-off).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging算法控制模型的高方差。然而，提升算法被认为更有效，因为它们同时处理偏差和方差（偏差-方差权衡）。
- en: 'XGBoost is an implementation of Gradient Boosting Machines (GBM) and is used
    for supervised learning. XGBoost is an open-sourced machine learning library available
    in Python, R, Julia, Java, C++, Scala. The features that standout are:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost是梯度提升机器（GBM）的实现，用于监督学习。XGBoost是一个开源的机器学习库，支持Python、R、Julia、Java、C++、Scala。其突出的特点有：
- en: Speed
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 速度
- en: Awareness of sparse data
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对稀疏数据的认识
- en: Implementation on single, distributed systems and out-of-core computation
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单机、分布式系统及外部计算的实现
- en: Parallelization
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 并行化
- en: Working
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作原理
- en: In order to understand XGBoost, we must first understand Gradient Descent and
    Gradient Boosting.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解XGBoost，我们必须首先理解梯度下降和梯度提升。
- en: '**a) Gradient Descent:**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**a) 梯度下降：**'
- en: A cost function measures how close the predicted values are, to the corresponding
    actual values. Ideally, we want as little difference as possible between the predicted
    values and the actual values. Thus, we want the cost function to be minimized.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数衡量预测值与实际值的接近程度。理想情况下，我们希望预测值和实际值之间的差异尽可能小。因此，我们希望将成本函数最小化。
- en: The weights associated with a trained model, cause it to predict values that
    are close to the actual values. Thus, the better the weights associated with the
    model, the more accurate are the predicted values and the lower is the cost function.
    With more records in the training set, the weights are learned and then updated.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练模型相关的权重使得模型预测的值接近实际值。因此，与模型相关的权重越好，预测值就越准确，成本函数也就越低。随着训练集记录的增加，权重被学习并更新。
- en: Gradient Descent is an iterative optimization algorithm. It is a method to minimize
    a function having several variables. Thus, Gradient Descent can be used to minimize
    the cost function. It first runs the model with initial weights, then seeks to
    minimize the cost function by updating the weights over several iterations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一个迭代优化算法。它是一种用于最小化具有多个变量的函数的方法。因此，梯度下降可以用来最小化成本函数。它首先用初始权重运行模型，然后通过在多个迭代中更新权重来最小化成本函数。
- en: '**b) Gradient Boosting:**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**b) 梯度提升：**'
- en: 'Boosting: An ensemble of weak learners is built, where the misclassified records
    are given greater weight (‘boosted’) to correctly predict them in later models.
    These weak learners are later combined to produce a single strong learner. There
    are many Boosting algorithms such as AdaBoost, Gradient Boosting and XGBoost.
    The latter two are tree-based models. Figure 1 depicts a Tree Ensemble Model.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 提升：构建一个由弱学习器组成的集成，其中被误分类的记录被赋予更大的权重（‘提升’），以便在后续模型中正确预测它们。这些弱学习器随后被组合成一个强学习器。有许多提升算法，如AdaBoost、梯度提升和XGBoost。后两者是基于树的模型。图1展示了一个树集成模型。
- en: '![](../Images/35b6f55e2b5953ff94ee568f744b4efb.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35b6f55e2b5953ff94ee568f744b4efb.png)'
- en: 'Figure 1: Tree Ensemble Model to predict whether a given user likes computer
    games or not. +2, +0.1, -1, +0.9, -0.9 are the prediction scores in each leaf.
    The final prediction for a given user is the sum of predictions from each tree.
    [Source](http://xgboost.readthedocs.io/en/latest/model.html)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：树集成模型预测给定用户是否喜欢计算机游戏。+2，+0.1，-1，+0.9，-0.9是每个叶子中的预测分数。给定用户的最终预测是每棵树的预测总和。[来源](http://xgboost.readthedocs.io/en/latest/model.html)
- en: Gradient Boosting carries the principle of Gradient Descent and Boosting to
    supervised learning. Gradient Boosted Models (GBM’s) are trees built sequentially,
    in series. In GBM’s, we take the weighted sum of multiple models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升将梯度下降和提升的原则应用于监督学习。梯度提升模型（GBM）是按顺序、串行构建的树。在GBM中，我们取多个模型的加权和。
- en: Each new model uses **Gradient Descent** optimization to update/ make corrections
    to the weights to be learned by the model to reach a local minima of the cost
    function.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个新模型使用**梯度下降**优化来更新/修正模型需要学习的权重，以达到成本函数的局部最小值。
- en: The vector of weights assigned to each model is not derived from the misclassifications
    of the previous model and the resulting increased weights assigned to misclassifications,
    but is derived from the weights optimized by Gradient Descent to minimize the
    cost function. The result of Gradient Descent is the same function of the model
    as the beginning, just with better parameters.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分配给每个模型的权重向量不是通过前一个模型的误分类及其导致的权重增加得出的，而是通过梯度下降优化的权重，以最小化成本函数。梯度下降的结果是模型的相同函数，只是具有更好的参数。
- en: Gradient **Boosting** adds a new function to the existing function in each step
    to predict the output. The result of Gradient Boosting is an altogether different
    function from the beginning, because the result is the addition of multiple functions.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度**提升**在每一步中向现有函数添加一个新函数以预测输出。梯度提升的结果是一个完全不同的函数，因为结果是多个函数的加和。
- en: '**c) XGBoost:**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**c) XGBoost：**'
- en: XGBoost was built to push the limit of computational resources for boosted trees.
    XGBoost is an implementation of GBM, with major improvements. GBM’s build trees
    sequentially, but XGBoost is parallelized. This makes XGBoost faster.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost的设计旨在推动提升树计算资源的极限。XGBoost是GBM的一个实现，具有显著改进。GBM按顺序构建树，而XGBoost则是并行化的，这使得XGBoost更快。
- en: 'Features of XGBoost:'
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XGBoost的特点：
- en: XGBoost is scalable in distributed as well as memory-limited settings. This
    scalability is due to several algorithmic optimizations.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost在分布式和内存有限的设置中都具有可扩展性。这种可扩展性归因于多个算法优化。
- en: '**1\. Split finding algorithms: approximate algorithm:**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**1. 分裂查找算法：近似算法：**'
- en: To find the best split over a continuous feature, data needs to be sorted and
    fit entirely into memory. This may be a problem in case of large datasets.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到连续特征的最佳分裂，需要将数据排序并完全加载到内存中。如果数据集很大，这可能成为一个问题。
- en: An approximate algorithm is used for this. Candidate split points are proposed
    based on the percentiles of feature distribution. The continuous features are
    binned into buckets that are split based on the candidate split points. The best
    solution for candidate split points is chosen from the aggregated statistics on
    the buckets.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为此使用了一种近似算法。基于特征分布的百分位数提出候选分裂点。连续特征被分箱为基于候选分裂点的桶。通过对桶的汇总统计选择候选分裂点的最佳解决方案。
- en: '**2\. Column block for parallel learning:**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**2. 并行学习的列块：**'
- en: Sorting the data is the most time-consuming aspect of tree learning. To reduce
    sorting costs, data is stored in in-memory units called ‘blocks’. Each block has
    data columns sorted by the corresponding feature value. This computation needs
    to be done only once before training and can be reused later.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 数据排序是树学习中最耗时的方面。为了减少排序成本，数据存储在称为“块”的内存单位中。每个块的数据显示列按相应的特征值排序。此计算仅需在训练前完成一次，并可在之后重复使用。
- en: Sorting of blocks can be done independently and can be divided between parallel
    threads of the CPU. The split finding can be parallelized as the collection of
    statistics for each column is done in parallel.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 块的排序可以独立完成，并可以在CPU的并行线程之间分配。分裂查找可以并行化，因为对每一列的统计收集是并行进行的。
- en: '**3\. Weighted quantile sketch for approximate tree learning:**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**3. 用于近似树学习的加权分位数草图：**'
- en: To propose candidate split points among weighted datasets, the Weighted Quantile
    Sketch algorithm is used. It carries out merge and prune operations on quantile
    summaries over the data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在加权数据集中提出候选分割点，使用了加权分位数草图算法。它对数据的分位数摘要执行合并和修剪操作。
- en: '**4\. Sparsity-aware algorithm:**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 稀疏感知算法：**'
- en: Input may be sparse due to reasons such as one-hot encoding, missing values
    and zero entries. XGBoost is aware of the sparsity pattern in the data and visits
    only the default direction (non-missing entries) in each node.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 输入可能由于独热编码、缺失值和零值条目等原因而稀疏。XGBoost 了解数据中的稀疏模式，并仅访问每个节点中的默认方向（非缺失条目）。
- en: '**5\. Cache-aware access:**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**5\. 缓存感知访问：**'
- en: To prevent cache miss during split finding and ensure parallelization, choose
    2^16 examples per block.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止在分割查找过程中出现缓存未命中并确保并行化，每个块选择 2^16 个示例。
- en: '**6\. Out-of-core computation:**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**6\. 核外计算：**'
- en: For data that does not fit into main memory, divide the data into multiple blocks,
    and store each block on the disk. Compress each block by columns and decompress
    on the fly by an independent thread while disk reading.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不能容纳在主内存中的数据，将数据划分为多个块，并将每个块存储在磁盘上。按列压缩每个块，并在磁盘读取时由独立线程即时解压缩。
- en: '**7\. Regularized Learning Objective:**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**7\. 正则化学习目标：**'
- en: 'To measure the performance of a model given a certain set of parameters, we
    need to define an objective function. An objective function must always contain
    two parts: training loss and regularization. The regularization term penalizes
    the complexity of the model.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要衡量给定一组参数下模型的性能，我们需要定义目标函数。目标函数必须始终包含两个部分：训练损失和正则化。正则化项对模型的复杂性进行惩罚。
- en: Obj(Θ)=L(θ)+ Ω(Θ)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Obj(Θ)=L(θ)+ Ω(Θ)
- en: where Ω is the regularization term which most algorithms forget to include in
    the objective function. However, XGBoost includes regularization, thus controlling
    the complexity of the model and preventing overfitting.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Ω 是正则化项，大多数算法在目标函数中忘记包括该项。然而，XGBoost 包含正则化，从而控制模型的复杂性并防止过拟合。
- en: The above 6 features maybe individually present in some algorithms, but XGBoost
    combines these techniques to make an end-to-end system that provides scalability
    and effective resource utilization.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 上述 6 个特性可能在某些算法中单独存在，但 XGBoost 结合了这些技术，形成一个端到端的系统，提供可扩展性和有效的资源利用。
- en: 'Python and R codes for implementation:'
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python 和 R 的实现代码：
- en: These links provide a good start to implement XGBoost using [Python](/2017/03/simple-xgboost-tutorial-iris-dataset.html)
    and [R.](https://rpubs.com/mharris/multiclass_xgboost)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这些链接为使用 [Python](/2017/03/simple-xgboost-tutorial-iris-dataset.html) 和 [R](https://rpubs.com/mharris/multiclass_xgboost)
    实现 XGBoost 提供了良好的起点。
- en: The following XGBoost code in Python works on the [Pima Indians Diabetes dataset](https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes).
    It predicts whether diabetes will occur or not in patients of Pima Indian heritage.
    This code is inspired from tutorials by [Jason Brownlee](https://machinelearningmastery.com/start-here/).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 Python 代码在 [Pima 印第安人糖尿病数据集](https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes)
    上运行。它预测 Pima 印第安人遗传背景的患者是否会发生糖尿病。该代码受到 [Jason Brownlee](https://machinelearningmastery.com/start-here/)
    教程的启发。
- en: Conclusion
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: You can refer to [this paper](https://arxiv.org/pdf/1603.02754.pdf), written
    by the developers of XGBoost, to learn of its detailed working. You can also find
    the project on [Github](https://github.com/dmlc/xgboost) and view tutorials and
    usecases [here](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考 [这篇论文](https://arxiv.org/pdf/1603.02754.pdf)，这是 XGBoost 开发者撰写的，以了解其详细工作原理。你还可以在
    [Github](https://github.com/dmlc/xgboost) 上找到该项目，并查看 [这里](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions)
    的教程和用例。
- en: XGBoost however, should not be used as a silver bullet. Best results can be
    obtained in conjunction with great data exploration and feature engineering.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，XGBoost 不应被视为万灵药。最佳结果可以通过与出色的数据探索和特征工程相结合获得。
- en: '**Related:**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[XGBoost, a top Machine Learning method on Kaggle, Explained](/2017/10/xgboost-top-machine-learning-method-kaggle-explained.html?preview=true)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost，Kaggle 上的顶级机器学习方法解析](/2017/10/xgboost-top-machine-learning-method-kaggle-explained.html?preview=true)'
- en: '[XGBoost, Implementing the Winningest Kaggle algortithm in Spark and Flink](/2016/03/xgboost-implementing-winningest-kaggle-algorithm-spark-flink.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost，将 Kaggle 上获胜的算法在 Spark 和 Flink 中实现](/2016/03/xgboost-implementing-winningest-kaggle-algorithm-spark-flink.html)'
- en: '[A simple XGBoost tutorial using the Iris dataset](/2017/03/simple-xgboost-tutorial-iris-dataset.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用鸢尾花数据集的简单 XGBoost 教程](/2017/03/simple-xgboost-tutorial-iris-dataset.html)'
- en: '* * *'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 工作'
- en: '* * *'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并通过寻找目标来…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一个90亿美元的AI失败案例分析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功的数据科学家的五个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么让 Python 成为初创公司的理想编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每位数据科学家都应了解的三个 R 语言库（即使你使用 Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
