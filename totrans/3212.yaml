- en: 'XGBoost: A Concise Technical Overview'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/10/xgboost-concise-technical-overview.html](https://www.kdnuggets.com/2017/10/xgboost-concise-technical-overview.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '"Our single XGBoost model can get to the top three! Our final model just averaged
    XGBoost models with different random seeds."'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- [Dmitrii Tsybulevskii & Stanislav Semenov](https://blog.kaggle.com/2016/08/24/avito-duplicate-ads-detection-winners-interview-1st-place-team-devil-team-stanislav-dmitrii/),
    winners of Avito Duplicate Ads Detection Kaggle competition.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With entire [blogs](https://blog.kaggle.com/tag/xgboost/) dedicated to how the
    sole application of XGBoost can propel one’s ranking on Kaggle competitions, it
    is time we delved deeper into the concepts of XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging algorithms control for high variance in a model. However, boosting algorithms
    are considered more effective as they deal with both bias as well as variance
    (the bias-variance trade-off).
  prefs: []
  type: TYPE_NORMAL
- en: 'XGBoost is an implementation of Gradient Boosting Machines (GBM) and is used
    for supervised learning. XGBoost is an open-sourced machine learning library available
    in Python, R, Julia, Java, C++, Scala. The features that standout are:'
  prefs: []
  type: TYPE_NORMAL
- en: Speed
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Awareness of sparse data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementation on single, distributed systems and out-of-core computation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parallelization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Working
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to understand XGBoost, we must first understand Gradient Descent and
    Gradient Boosting.
  prefs: []
  type: TYPE_NORMAL
- en: '**a) Gradient Descent:**'
  prefs: []
  type: TYPE_NORMAL
- en: A cost function measures how close the predicted values are, to the corresponding
    actual values. Ideally, we want as little difference as possible between the predicted
    values and the actual values. Thus, we want the cost function to be minimized.
  prefs: []
  type: TYPE_NORMAL
- en: The weights associated with a trained model, cause it to predict values that
    are close to the actual values. Thus, the better the weights associated with the
    model, the more accurate are the predicted values and the lower is the cost function.
    With more records in the training set, the weights are learned and then updated.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent is an iterative optimization algorithm. It is a method to minimize
    a function having several variables. Thus, Gradient Descent can be used to minimize
    the cost function. It first runs the model with initial weights, then seeks to
    minimize the cost function by updating the weights over several iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '**b) Gradient Boosting:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Boosting: An ensemble of weak learners is built, where the misclassified records
    are given greater weight (‘boosted’) to correctly predict them in later models.
    These weak learners are later combined to produce a single strong learner. There
    are many Boosting algorithms such as AdaBoost, Gradient Boosting and XGBoost.
    The latter two are tree-based models. Figure 1 depicts a Tree Ensemble Model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/35b6f55e2b5953ff94ee568f744b4efb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Tree Ensemble Model to predict whether a given user likes computer
    games or not. +2, +0.1, -1, +0.9, -0.9 are the prediction scores in each leaf.
    The final prediction for a given user is the sum of predictions from each tree.
    [Source](http://xgboost.readthedocs.io/en/latest/model.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Boosting carries the principle of Gradient Descent and Boosting to
    supervised learning. Gradient Boosted Models (GBM’s) are trees built sequentially,
    in series. In GBM’s, we take the weighted sum of multiple models.
  prefs: []
  type: TYPE_NORMAL
- en: Each new model uses **Gradient Descent** optimization to update/ make corrections
    to the weights to be learned by the model to reach a local minima of the cost
    function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The vector of weights assigned to each model is not derived from the misclassifications
    of the previous model and the resulting increased weights assigned to misclassifications,
    but is derived from the weights optimized by Gradient Descent to minimize the
    cost function. The result of Gradient Descent is the same function of the model
    as the beginning, just with better parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient **Boosting** adds a new function to the existing function in each step
    to predict the output. The result of Gradient Boosting is an altogether different
    function from the beginning, because the result is the addition of multiple functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**c) XGBoost:**'
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost was built to push the limit of computational resources for boosted trees.
    XGBoost is an implementation of GBM, with major improvements. GBM’s build trees
    sequentially, but XGBoost is parallelized. This makes XGBoost faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Features of XGBoost:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: XGBoost is scalable in distributed as well as memory-limited settings. This
    scalability is due to several algorithmic optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Split finding algorithms: approximate algorithm:**'
  prefs: []
  type: TYPE_NORMAL
- en: To find the best split over a continuous feature, data needs to be sorted and
    fit entirely into memory. This may be a problem in case of large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: An approximate algorithm is used for this. Candidate split points are proposed
    based on the percentiles of feature distribution. The continuous features are
    binned into buckets that are split based on the candidate split points. The best
    solution for candidate split points is chosen from the aggregated statistics on
    the buckets.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Column block for parallel learning:**'
  prefs: []
  type: TYPE_NORMAL
- en: Sorting the data is the most time-consuming aspect of tree learning. To reduce
    sorting costs, data is stored in in-memory units called ‘blocks’. Each block has
    data columns sorted by the corresponding feature value. This computation needs
    to be done only once before training and can be reused later.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting of blocks can be done independently and can be divided between parallel
    threads of the CPU. The split finding can be parallelized as the collection of
    statistics for each column is done in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Weighted quantile sketch for approximate tree learning:**'
  prefs: []
  type: TYPE_NORMAL
- en: To propose candidate split points among weighted datasets, the Weighted Quantile
    Sketch algorithm is used. It carries out merge and prune operations on quantile
    summaries over the data.
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Sparsity-aware algorithm:**'
  prefs: []
  type: TYPE_NORMAL
- en: Input may be sparse due to reasons such as one-hot encoding, missing values
    and zero entries. XGBoost is aware of the sparsity pattern in the data and visits
    only the default direction (non-missing entries) in each node.
  prefs: []
  type: TYPE_NORMAL
- en: '**5\. Cache-aware access:**'
  prefs: []
  type: TYPE_NORMAL
- en: To prevent cache miss during split finding and ensure parallelization, choose
    2^16 examples per block.
  prefs: []
  type: TYPE_NORMAL
- en: '**6\. Out-of-core computation:**'
  prefs: []
  type: TYPE_NORMAL
- en: For data that does not fit into main memory, divide the data into multiple blocks,
    and store each block on the disk. Compress each block by columns and decompress
    on the fly by an independent thread while disk reading.
  prefs: []
  type: TYPE_NORMAL
- en: '**7\. Regularized Learning Objective:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'To measure the performance of a model given a certain set of parameters, we
    need to define an objective function. An objective function must always contain
    two parts: training loss and regularization. The regularization term penalizes
    the complexity of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Obj(Θ)=L(θ)+ Ω(Θ)
  prefs: []
  type: TYPE_NORMAL
- en: where Ω is the regularization term which most algorithms forget to include in
    the objective function. However, XGBoost includes regularization, thus controlling
    the complexity of the model and preventing overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: The above 6 features maybe individually present in some algorithms, but XGBoost
    combines these techniques to make an end-to-end system that provides scalability
    and effective resource utilization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Python and R codes for implementation:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These links provide a good start to implement XGBoost using [Python](/2017/03/simple-xgboost-tutorial-iris-dataset.html)
    and [R.](https://rpubs.com/mharris/multiclass_xgboost)
  prefs: []
  type: TYPE_NORMAL
- en: The following XGBoost code in Python works on the [Pima Indians Diabetes dataset](https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes).
    It predicts whether diabetes will occur or not in patients of Pima Indian heritage.
    This code is inspired from tutorials by [Jason Brownlee](https://machinelearningmastery.com/start-here/).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can refer to [this paper](https://arxiv.org/pdf/1603.02754.pdf), written
    by the developers of XGBoost, to learn of its detailed working. You can also find
    the project on [Github](https://github.com/dmlc/xgboost) and view tutorials and
    usecases [here](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions).
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost however, should not be used as a silver bullet. Best results can be
    obtained in conjunction with great data exploration and feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[XGBoost, a top Machine Learning method on Kaggle, Explained](/2017/10/xgboost-top-machine-learning-method-kaggle-explained.html?preview=true)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[XGBoost, Implementing the Winningest Kaggle algortithm in Spark and Flink](/2016/03/xgboost-implementing-winningest-kaggle-algorithm-spark-flink.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A simple XGBoost tutorial using the Iris dataset](/2017/03/simple-xgboost-tutorial-iris-dataset.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
