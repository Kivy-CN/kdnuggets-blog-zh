- en: What is Transfer Learning?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是迁移学习？
- en: 原文：[https://www.kdnuggets.com/2022/01/transfer-learning.html](https://www.kdnuggets.com/2022/01/transfer-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/01/transfer-learning.html](https://www.kdnuggets.com/2022/01/transfer-learning.html)
- en: '![What is Transfer Learning?](../Images/7a6be2553ec0be9a7ea8f0a49778587a.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![什么是迁移学习？](../Images/7a6be2553ec0be9a7ea8f0a49778587a.png)'
- en: Image by [qimono on Pixabary](https://pixabay.com/users/qimono-1962238/)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [qimono on Pixabary](https://pixabay.com/users/qimono-1962238/) 提供
- en: Transfer Learning is a machine learning method where the application of knowledge
    obtained from a model used in one task, can be reused as a foundation point for
    another task.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是一种机器学习方法，其中一个任务中获得的模型知识可以作为另一个任务的基础点进行重用。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析水平'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Machine learning algorithms use historical data as their input to make predictions
    and produce new output values. They are typically designed to conduct isolated
    tasks. A source task is a task from which knowledge is transferred to a target
    task. A target task is when improved learning occurs due to the transfer of knowledge
    from a source task.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法使用历史数据作为输入来进行预测并产生新的输出值。它们通常被设计用于执行孤立的任务。源任务是指将知识转移到目标任务的任务。目标任务是指由于从源任务转移知识而发生的改进学习。
- en: During transfer learning, the knowledge leveraged and rapid progress from a
    source task is used to improve the learning and development to a new target task.
    The application of knowledge is using the source task’s attributes and characteristics,
    which will be applied and mapped onto the target task.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习过程中，利用从源任务中获得的知识和快速进展来改善对新目标任务的学习和发展。知识的应用是使用源任务的属性和特征，这些属性和特征将被应用并映射到目标任务上。
- en: However, if the transfer method results in a decrease in the performance of
    the new target task, it is called a negative transfer. One of the major challenges
    when working with transfer learning methods is being able to provide and ensure
    the positive transfer between related tasks, whilst avoiding the negative transfer
    between less related tasks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果转移方法导致新目标任务性能下降，则称为负迁移。在使用迁移学习方法时，主要挑战之一是能够提供并确保相关任务之间的正迁移，同时避免与不相关任务之间的负迁移。
- en: The What, When, and How of Transfer Learning
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移学习的什么、何时和如何
- en: What do we transfer? To understand which parts of the learned knowledge to transfer,
    we need to figure out which portions of knowledge best reflect both the source
    and target. Overall, improving the performance and accuracy of the target task.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迁移什么？要理解转移哪些学习到的知识，我们需要弄清楚哪些知识部分最能反映源任务和目标任务。总体而言，提高目标任务的性能和准确性。
- en: When do we transfer? Understanding when to transfer is important, as we don’t
    want to be transferring knowledge which could, in turn, make matters worse, leading
    to negative transfer. Our goal is to improve the performance of the target task,
    not make it worse.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 何时进行迁移？了解何时进行迁移非常重要，因为我们不希望转移的知识可能使情况变得更糟，导致负迁移。我们的目标是提高目标任务的性能，而不是使其变得更差。
- en: How do we transfer? Now we have a better idea of what we want to transfer and
    when we can then move on to working with different techniques to transfer the
    knowledge efficiently. We will speak more about this later on in the article.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何进行迁移？现在我们对想要迁移的内容和何时迁移有了更清楚的了解，我们可以继续研究不同的技术，以高效地转移知识。我们将在文章后面进一步讨论。
- en: 'Before we dive into the methodology behind transfer learning, it is good to
    know the different forms of transfer learning. We will go through three different
    types of transfer learning scenarios, based on relationships between the source
    task and target task. Below is an overview of the different types of transfer
    learning:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨迁移学习背后的方法论之前，了解迁移学习的不同形式是很有帮助的。我们将讨论三种不同类型的迁移学习场景，这些场景是根据源任务和目标任务之间的关系来区分的。以下是不同类型的迁移学习概述：
- en: Different Types of Transfer Learning
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移学习的不同类型
- en: '**Inductive Transfer Learning**: In this type of transfer learning, the source
    and target task are the same, however, they are still different from one another.
    The model will use inductive biases from the source task to help improve the performance
    of the target task. The source task may or may not contain labeled data, further
    leading onto the model using multitask learning and self-taught learning.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**归纳迁移学习**：在这种迁移学习类型中，源任务和目标任务相同，但它们之间仍然存在差异。模型将利用源任务中的归纳偏差来帮助提高目标任务的性能。源任务可能包含或不包含标记数据，这进一步导致模型使用多任务学习和自学学习。'
- en: '**Unsupervised Transfer Learning**: I assume you know what unsupervised learning
    is, however, if you don’t, it is when an algorithm is subjected to being able
    to identify patterns in datasets that have not been labeled or classified. In
    this case, the source and target are similar, however, the task is different,
    where both data is unlabelled in both source and target. Techniques such as dimensionality
    reduction and clustering are well known in unsupervised learning.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督迁移学习**：我假设你知道什么是无监督学习，不过，如果你不知道，无监督学习是指算法能够识别未标记或未分类的数据集中的模式。在这种情况下，源任务和目标任务是相似的，但任务是不同的，源任务和目标任务中的数据都是未标记的。降维和聚类等技术在无监督学习中是非常著名的。'
- en: '**Transductive Transfer Learning**: In this last type of transfer learning,
    the source and target tasks share similarities, however, the domains are different.
    The source domain contains a lot of labeled data, whereas there is an absence
    of labeled data in the target domain, further leading onto the model using domain
    adaptation.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**转导迁移学习**：在这种最后的迁移学习类型中，源任务和目标任务有相似之处，但领域是不同的。源领域包含大量的标记数据，而目标领域则缺乏标记数据，这进一步导致模型使用领域适应技术。'
- en: Transfer Learning vs. Fine-tuning
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移学习与微调
- en: Fine-tuning is an optional step in transfer learning and is primarily incorporated
    to improve the performance of the model. The difference between Transfer learning
    and Fine-tuning is all in the name.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是迁移学习中的一个可选步骤，主要用于提高模型的性能。迁移学习和微调之间的区别完全体现在名称上。
- en: Transfer learning is built on adopting features learned from one task and “transferring”
    the leveraged knowledge onto a new task. Transfer learning is usually used on
    tasks where the dataset is too small, to train a full-scale model from scratch.
    Fine-tuning is built on making “fine” adjustments to a process in order to obtain
    the desired output to further improve performance. The parameters of a trained
    model during fine-tuning, are adjusted and tailored precisely and specifically,
    whilst trying to validate the model to achieve the desired outputs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习基于从一个任务中学习到的特征，并将这些知识“转移”到新的任务上。迁移学习通常用于数据集过小，无法从头训练一个完整模型的任务。微调则是对过程进行“微小”的调整，以获得所需的输出，从而进一步提高性能。在微调过程中，经过训练的模型的参数会被精确和特定地调整，同时尝试验证模型以实现所需的输出。
- en: Why Use Transfer Learning?
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么使用迁移学习？
- en: 'Reasons to use transfer learning:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用迁移学习的原因：
- en: '**Not needing a lot of data** - Gaining access to data is always a hindrance
    due to its lack of availability. Working with insufficient amounts of data can
    result in low performance. This is where transfer learning shines as the machine
    learning model can be built with a small training dataset, due to it being pre-trained.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**无需大量数据** - 获取数据总是一个障碍，因为数据的可用性不足。使用不足量的数据可能导致性能较低。这正是迁移学习的优势所在，因为机器学习模型可以通过少量的训练数据集进行构建，因为它已经经过预训练。'
- en: '**Saving training time** - Machine learning models are difficult to train and
    can take up a lot of time, leading to inefficiency. It requires a long period
    of time to train a deep neural network from scratch on a complex task, so using
    a pre-trained model saves time on building a new one.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**节省训练时间** - 机器学习模型训练起来很困难且耗时，导致效率低下。训练一个深度神经网络从头开始处理复杂任务需要很长时间，因此使用预训练模型可以节省建立新模型的时间。'
- en: Transfer Learning Pros
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转移学习的优点
- en: '**Better base**: Using a pre-trained model in transfer learning offers you
    a better foundation and starting point, allowing you to perform some tasks without
    even training.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**更好的基础**：在转移学习中使用预训练模型为你提供了更好的基础和起点，使你能够在不进行训练的情况下完成一些任务。'
- en: '**Higher learning rate**: Due to the model already having been trained on a
    similar task beforehand, the model has a higher learning rate.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**更高的学习率**：由于模型已经在类似任务上进行过训练，因此具有更高的学习率。'
- en: '**Higher accuracy rate**: With a better base and higher learning rate, the
    model works at a higher performance, producing more accuracy outputs.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**更高的准确率**：凭借更好的基础和更高的学习率，模型在更高的性能下运行，产生更准确的输出。'
- en: When Does Transfer Learning *Not* Work?
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转移学习*不*有效时？
- en: Transfer learning should be avoided when the weights trained from your source
    task are different from your target task. For example, if your previous network
    was trained for classifying cats and dogs and your new network is trying to detect
    shoes and socks, there is going to be a problem as the weights transferred from
    your source to the target task will not be able to give you the best of results.
    Therefore, initialising the network with pre-trained weights that correspond with
    similar outputs to the one you are expecting is better than using weights with
    no correlation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当源任务的训练权重与目标任务不同的时候，应避免使用转移学习。例如，如果你之前的网络是用来分类猫和狗的，而你新的网络试图检测鞋子和袜子，那么会出现问题，因为从源任务到目标任务转移的权重无法提供最佳结果。因此，初始化一个与期望输出相似的预训练权重的网络比使用没有相关性的权重更好。
- en: Removing layers from a pre-trained model will cause issues with the architecture
    of the model. If you remove the first layers, your model will have a low learning
    rate as it has to juggle working with low-level features. Removing layers reduces
    the number of parameters that can be trained, which can result in overfitting.
    Being able to use the correct amount of layers is vital in reducing overfitting,
    however, this is also a timely process.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练模型中移除层会对模型的架构造成问题。如果你移除前几层，模型的学习率会很低，因为它必须处理低级特征。移除层会减少可训练的参数数量，这可能导致过拟合。使用正确数量的层对于减少过拟合至关重要，但这也是一个耗时的过程。
- en: Transfer Learning Cons
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转移学习的缺点
- en: '**Negative transfer learning**: As I mentioned above, negative transfer learning
    is when a previous learning method obstructs the new task. This only occurs if
    the source and target are not similar enough, causing the first round of training
    to be too far off. Algorithms don''t have to always agree with what we deem as
    similar, making it difficult to understand the fundamentals and standards of what
    type of training is sufficient.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**负迁移学习**：如上所述，负迁移学习是指以前的学习方法阻碍了新任务。这仅在源任务和目标任务不够相似时发生，导致第一次训练偏差过大。算法不一定总是与我们认为的相似一致，这使得理解什么类型的训练是足够的基础和标准变得困难。'
- en: Transfer Learning in 6 Steps
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转移学习的6个步骤
- en: Let’s dive into a better understanding of how transfer learning is implemented
    and the steps taken. There are 6 general steps taken in transfer learning and
    we will go through each of them.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解转移学习的实现方式及其步骤。转移学习一般包括6个步骤，我们将逐一介绍这些步骤。
- en: 'Select Source Task: The first step is selecting a pre-trained model that holds
    an abundance of data, having a relationship between the input and output data
    with your chosen Target task.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择源任务：第一步是选择一个拥有大量数据的预训练模型，该模型的输入和输出数据与您选择的目标任务有关系。
- en: 'Create a Base Model: Instantiate a base model with pre-trained weights. Pre-trained
    weights can be accessed through architectures such as Xception. This is developing
    your source model, so that it is better than the naive model we started with,
    ensuring some increase in learning rate.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建基础模型：实例化一个带有预训练权重的基础模型。预训练权重可以通过像Xception这样的架构获取。这是在开发你的源模型，使其比我们最初开始时的简单模型更好，确保学习率有所提升。
- en: 'Freeze Layers: To reduce initialising the weights again, freezing the layers
    from the pre-trained model is necessary. It will redeem the knowledge already
    learned and save you from training the model from scratch.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结层：为了避免再次初始化权重，冻结预训练模型的层是必要的。这将保留已经学到的知识，避免从头开始训练模型。
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Add new Trainable Layers: Adding new trainable layers on top of the frozen
    layer, will convert old features into predictions on a new dataset.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加新的可训练层：在冻结层的顶部添加新的可训练层，将把旧特征转化为新数据集上的预测。
- en: 'Train the New Layers: The pre-trained model contains the final output layer
    already. The likelihood that the current output on the pre-trained model and the
    output you want from your model will be different, is high. Therefore, you have
    to train the model with a new output layer. Therefore, adding new dense layers
    and the final dense layer in correspondence to your expected model, will improve
    the learning rate and produce outputs of your desire.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练新层：预训练模型已经包含了最终的输出层。预训练模型的当前输出和你期望的模型输出之间存在较大差异的可能性很高。因此，你需要用新的输出层来训练模型。因此，添加新的密集层和最终密集层以符合你期望的模型，将提升学习率并产生你期望的输出。
- en: 'Fine-tuning: You can improve the performance of your model by fine-tuning,
    which is done by unfreezing all or parts of the base models and then retraining
    the model with a very low learning rate. It is critical to use a low learning
    rate at this stage, as the model you are training is much larger than it was initially
    in the first round, along with it being a small dataset. As a result, you are
    at risk of overfitting if you apply large weight updates, therefore you want to
    fine-tune in an incremental way. Recompile the model as you have changed the model’s
    behavior and then retrain the model again, monitoring any overfitting feedback.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微调：你可以通过微调来提升模型的性能，这包括解冻基础模型的全部或部分内容，然后用非常低的学习率重新训练模型。在这个阶段使用低学习率至关重要，因为你正在训练的模型比最初的第一轮大得多，并且数据集也很小。因此，如果应用较大的权重更新，你有过拟合的风险，因此你需要以增量的方式进行微调。由于你改变了模型的行为，需要重新编译模型，然后再次训练模型，监控任何过拟合的反馈。
- en: I hope this article has given you a good introduction and understanding of Transfer
    Learning. Stay tuned, my next article will be me implementing Transfer Learning
    for Image Recognition and Natural Language Processing.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章为你提供了迁移学习的良好介绍和理解。敬请期待，我的下一篇文章将会介绍我如何实施迁移学习用于图像识别和自然语言处理。
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and freelance Technical writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** 是一名数据科学家和自由职业技术作家。她特别关注提供数据科学职业建议或教程以及数据科学相关理论知识。她还希望探索人工智能如何有助于人类寿命的不同方式。作为一个热衷学习者，她寻求拓宽自己的技术知识和写作技能，同时帮助指导他人。'
- en: More On This Topic
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[TensorFlow for Computer Vision - Transfer Learning Made Easy](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow 用于计算机视觉 - 轻松实现迁移学习](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)'
- en: '[Transfer Learning for Image Recognition and Natural Language Processing](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[图像识别和自然语言处理的迁移学习](https://www.kdnuggets.com/2022/01/transfer-learning-image-recognition-natural-language-processing.html)'
- en: '[A Practical Guide to Transfer Learning using PyTorch](https://www.kdnuggets.com/2023/06/practical-guide-transfer-learning-pytorch.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 PyTorch 进行迁移学习的实用指南](https://www.kdnuggets.com/2023/06/practical-guide-transfer-learning-pytorch.html)'
- en: '[Exploring the Potential of Transfer Learning in Small Data Scenarios](https://www.kdnuggets.com/exploring-the-potential-of-transfer-learning-in-small-data-scenarios)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[探索迁移学习在小数据场景中的潜力](https://www.kdnuggets.com/exploring-the-potential-of-transfer-learning-in-small-data-scenarios)'
- en: '[Using Transfer Learning to Boost Model Performance](https://www.kdnuggets.com/using-transfer-learning-to-boost-model-performance)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用迁移学习提升模型性能](https://www.kdnuggets.com/using-transfer-learning-to-boost-model-performance)'
- en: '[A Solid Plan for Learning Data Science, Machine Learning, and Deep Learning](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学、机器学习和深度学习的稳固计划](https://www.kdnuggets.com/2023/01/mwiti-solid-plan-learning-data-science-machine-learning-deep-learning.html)'
