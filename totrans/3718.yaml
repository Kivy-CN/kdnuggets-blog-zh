- en: The ABCs of NLP, From A to Z
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP的ABC，从A到Z
- en: 原文：[https://www.kdnuggets.com/2022/10/abcs-nlp-a-to-z.html](https://www.kdnuggets.com/2022/10/abcs-nlp-a-to-z.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/10/abcs-nlp-a-to-z.html](https://www.kdnuggets.com/2022/10/abcs-nlp-a-to-z.html)
- en: '![The ABCs of NLP](../Images/a95abd83df49823b923f73ff8556bcff.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![NLP的ABC](../Images/a95abd83df49823b923f73ff8556bcff.png)'
- en: Photo by [Towfiqu barbhuiya](https://unsplash.com/@towfiqu999999?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Towfiqu barbhuiya](https://unsplash.com/@towfiqu999999?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: There is no shortage of text data available today. Vast amounts of text are
    created each and every day, with this data ranging from fully structured to semi-structured
    to fully unstructured.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有大量的文本数据可供使用。每天都会生成大量的文本，这些数据从完全结构化到半结构化再到完全非结构化。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行 IT 管理'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: What can we do with this text? Well, quite a bit, actually; depending on exactly
    what your objectives are, there are 2 intricately related yet differentiated umbrellas
    of tasks which can be exploited in order to leverage the availability of all of
    this data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能用这些文本做些什么呢？实际上，可以做很多事情；根据你的具体目标，有两个错综复杂但有所区别的任务领域可以利用这些数据。
- en: Natural Languguage Processing vs. Text Mining
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理与文本挖掘
- en: Let's start with some definitions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些定义开始。
- en: '**Natural language processing (NLP)** concerns itself with the interaction
    between natural human languages and computing devices. NLP is a major aspect of
    computational linguistics, and also falls within the realms of computer science
    and artificial intelligence.**Text mining** exists in a similar realm as NLP,
    in that it is concerned with identifying interesting, non-trivial patterns in
    textual data.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理 (NLP)** 关注自然语言与计算设备之间的互动。NLP 是计算语言学的一个主要方面，也属于计算机科学和人工智能的领域。**文本挖掘**
    与 NLP 在某种程度上相似，因为它也关注于识别文本数据中有趣的、非平凡的模式。'
- en: As you may be able to tell from the above, the exact boundaries of these 2 concepts
    are not well-defined and agreed-upon, and bleed into one another to varying degrees,
    depending on the practitioners and researchers with whom one would discuss such
    matters. I find it easiest to differentiate by degree of insight. If raw text
    is data, then text mining can extract **information**, while NLP extracts *knowledge*
    (see the Pyramid of Understanding below). Syntax versus semantics, if you will.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从上述内容中可能能看出的，这两个概念的确切界限并不明确且达成共识，并且根据讨论这些问题的从业者和研究人员的不同程度有所重叠。我发现通过洞察的程度来区分最为容易。如果原始文本是数据，那么文本挖掘可以提取**信息**，而
    NLP 提取的是*知识*（见下图的理解金字塔）。可以说是语法与语义的区别。
- en: '![The Pyramid of Understanding](../Images/fcba6856f9d93514b15b4a585e1fb548.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![理解金字塔](../Images/fcba6856f9d93514b15b4a585e1fb548.png)'
- en: 'The Pyramid of Understanding: data, information, knowledge'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 理解金字塔：数据，信息，知识
- en: Various other explanations as to the precise relationship between text mining
    and NLP exist, and you are free to find something that works better for you. We
    aren't really as concerned with the **exact** definitions — absolute or relative
    — as much as we are with the intuitive recognition that the concepts are related
    with some overlap, yet still distinct.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本挖掘与 NLP 之间的精确关系，还有各种其他解释，你可以找到更适合你的解释。我们并不特别关心**确切**的定义——无论是绝对的还是相对的——而更关注的是直观地认识到这些概念是相关的，有一定重叠，但仍然有所区别。
- en: 'The point we will move forward with: although NLP and text mining are not the
    same thing, they are closely related, deal with the same raw data type, and have
    some crossover in their uses. For the remainder of our discussion, we will conveniently
    refer to these 2 concepts as NLP. Importantly, much of the preprocessing of data
    for the tasks falling under these 2 umbrellas is identical.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续讨论的要点是：尽管 NLP 和文本挖掘不是同一回事，但它们密切相关，处理相同的原始数据类型，并且在使用上有一些交集。在接下来的讨论中，我们将方便地将这两个概念统称为
    NLP。重要的是，这两个领域的任务在数据预处理方面有很多相似之处。
- en: 'Regardless of the exact NLP task you are looking to perform, maintaining text
    meaning and avoiding ambiguity is paramount. As such, attempts to avoid ambiguity
    is a large part of the text preprocessing process. We want to preserve intended
    meaning while eliminating noise. In order to do so, the following is required:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你要执行的 NLP 任务是什么，保持文本意义并避免歧义都是至关重要的。因此，避免歧义的尝试是文本预处理过程的重要组成部分。我们希望在消除噪声的同时保留意图表达。为此，需要做以下几点：
- en: Knowledge about language
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言知识
- en: Knowledge about the world
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 世界知识
- en: A way to combine knowledge sources
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合知识来源的方法
- en: If this were easy, we likely wouldn't be talking about it. What are some reasons
    why processing text is difficult?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这很简单，我们可能就不会讨论它了。处理文本的困难有哪些原因呢？
- en: Why is Processing Text Difficult?
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么处理文本很困难？
- en: '![Why is processing text difficult?](../Images/5a4c88a2115dd4c217aabde148806d05.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![为什么处理文本很困难？](../Images/5a4c88a2115dd4c217aabde148806d05.png)'
- en: 'Source: CS124 Stanford (https://web.stanford.edu/class/cs124/)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：CS124 斯坦福大学 (https://web.stanford.edu/class/cs124/)
- en: 'You may have noticed something curious about the above: "non-standard English."
    While certainly not the intention of the figure — we are taking it fully out of
    context here — for a variety of reasons, much of NLP research has historically
    taken place within the confines of the English language. So we can add to the
    question of "Why is processing text difficult?" the additional layers of trouble
    which non-English languages — and especially languages with smaller numbers of
    speakers, or even [endangered languages](https://en.wikipedia.org/wiki/Endangered_language)
    — must go through in order to be processed and have insights drawn from.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到上面提到的一个有趣的点：“非标准英语。”虽然这显然不是图表的意图——我们在这里完全脱离了上下文——但出于各种原因，很多 NLP 研究历史上都发生在英语语言的范围内。因此，我们可以在“为什么处理文本很困难？”的问题上增加额外的困难层面，这些困难是非英语语言，特别是说话人数较少的语言，甚至是[濒危语言](https://en.wikipedia.org/wiki/Endangered_language)在处理和提取洞见时必须经历的。
- en: '![The ABCs of NLP](../Images/4e246faf25596cb69da1df0fd63fba91.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![NLP的基础](../Images/4e246faf25596cb69da1df0fd63fba91.png)'
- en: '[Image by rawpixel.com](https://www.freepik.com/free-vector/illustration-language-concept_2825850.htm#query=language&position=0&from_view=search&track=sph)
    on Freepik'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[由 rawpixel.com 提供的图片](https://www.freepik.com/free-vector/illustration-language-concept_2825850.htm#query=language&position=0&from_view=search&track=sph)
    在 Freepik 上'
- en: Just being cognizant of the fact that NLP ≠ English when thinking and speaking
    about NLP is one small way in which this bias can be stemmed.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在思考和讨论自然语言处理（NLP）时，意识到 NLP ≠ 英语是减少这种偏见的一种小方法。
- en: Textual Data Science Task Framework
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本数据科学任务框架
- en: Can we craft a sufficiently general framework for approaching textual data science
    tasks? It turns out that processing text is very similar to other non-text processing
    tasks, and so we can look to the [KDD Process](https://www.kdnuggets.com/2016/03/data-science-process-rediscovered.html/2)
    for inspiration.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否为处理文本数据科学任务制定一个足够通用的框架？事实证明，处理文本与其他非文本处理任务非常相似，因此我们可以借鉴[KDD 过程](https://www.kdnuggets.com/2016/03/data-science-process-rediscovered.html/2)的灵感。
- en: We can say that these are the main steps of a generic text-based task, one which
    falls under text mining or NLP.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说这些是通用文本任务的主要步骤，这些任务属于文本挖掘或 NLP。
- en: '**1\. Data Collection or Assembly**'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1\. 数据收集或组装**'
- en: Obtain or build corpus, which could be anything from emails, to the entire set
    of English Wikipedia articles, to our company's financial reports, to the complete
    works of Shakespeare, to something different altogether
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取或构建语料库，这可以是从电子邮件到整个英语维基百科文章集，到公司的财务报告，到莎士比亚的完整作品，或者完全不同的东西
- en: '**2\. Data Preprocessing**'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2\. 数据预处理**'
- en: Perform the preparation tasks on the raw text corpus in anticipation of text
    mining or NLP task
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在原始文本语料库上执行准备任务，以便进行文本挖掘或 NLP 任务
- en: Data preprocessing consists of a number of steps, any number of which may or
    not apply to a given task, but generally fall under the broad categories of tokenization,
    normalization, and substitution
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理包含多个步骤，其中的任何一个步骤可能适用于或不适用于特定任务，但通常属于分词、规范化和替换这几个广泛类别
- en: '**3\. Data Exploration & Visualization**'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3\. 数据探索与可视化**'
- en: Regardless of what our data is -- text or not -- exploring and visualizing it
    is an essential step in gaining insight
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无论我们的数据是什么——是文本还是其他——探索和可视化数据是获取洞察的一个重要步骤
- en: Common tasks may include visualizing word counts and distributions, generating
    wordclouds, and performing distance measures
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见任务可能包括可视化词频和分布，生成词云，以及执行距离度量
- en: '**4\. Model Building**'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4\. 模型构建**'
- en: This is where our bread and butter text mining or NLP task takes place (includes
    training and testing)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是我们主要进行文本挖掘或自然语言处理任务的地方（包括训练和测试）
- en: Also includes feature selection & engineering when applicable
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在适用时还包括特征选择与工程
- en: 'Language models: Finite state machines, Markov models, vector space modeling
    of word meanings'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型：有限状态机，马尔可夫模型，词义的向量空间建模
- en: 'Machine learning classifiers: Naive bayes, logistic regression, decision trees,
    Support Vector Machines, neural networks'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习分类器：朴素贝叶斯，逻辑回归，决策树，支持向量机，神经网络
- en: 'Sequence models: Hidden Markov models, recursive neural networks (RNNs), Long
    short term memory neural networks (LSTMs)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列模型：隐马尔可夫模型，递归神经网络（RNNs），长短期记忆神经网络（LSTMs）
- en: '**5\. Model Evaluation**'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**5\. 模型评估**'
- en: Did the model perform as expected?
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型表现是否如预期？
- en: Metrics will vary dependent on what type of text mining or NLP task
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标会根据文本挖掘或自然语言处理任务的类型有所不同
- en: 'Even thinking outside of the box with chatbots (an NLP task) or generative
    models: some form of evaluation is necessary'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使是思考聊天机器人（一个自然语言处理任务）或生成模型：也需要某种形式的评估
- en: '![The ABCs of NLP](../Images/15b55f9e2420ef63c99915f836f88aa9.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![自然语言处理的ABC](../Images/15b55f9e2420ef63c99915f836f88aa9.png)'
- en: A simple textual data task framework
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的文本数据任务框架
- en: 'Clearly, any framework focused on the preprocessing of textual data would have
    to be synonymous with step number 2\. Expanding upon this step, specifically,
    we had the following to say about what this step would likely entail:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，任何关注文本数据预处理的框架都必须与步骤2\. 同义。具体来说，我们对这个步骤可能涉及的内容有以下说明：
- en: Perform the preparation tasks on the raw text corpus in anticipation of text
    mining or NLP task
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对原始文本语料库执行准备任务，以便进行文本挖掘或自然语言处理任务
- en: Data preprocessing consists of a number of steps, any number of which may or
    not apply to a given task, but generally fall under the broad categories of tokenization,
    normalization, and substitution
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理包含多个步骤，其中的任何一个步骤可能适用于或不适用于特定任务，但通常属于分词、规范化和替换这几个广泛类别
- en: More generally, we are interested in taking some predetermined body of text
    and performing upon it some basic analysis and transformations, in order to be
    left with artefacts which will be much more useful for performing some further,
    more meaningful analytic task afterward. This further task would be our core text
    mining or natural language processing work.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更一般来说，我们感兴趣的是对某些预定的文本进行基本的分析和转换，以便得到更加有用的成果，这些成果将有助于进行后续更有意义的分析任务。这个后续任务就是我们的核心文本挖掘或自然语言处理工作。
- en: 'So, as mentioned above, it seems as though there are 3 main components of text
    preprocessing:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如上所述，文本预处理似乎有3个主要组件：
- en: tokenization
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词
- en: normalization
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规范化
- en: subsitution
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 替换
- en: As we lay out a framework for approaching preprocessing, we should keep these
    high-level concepts in mind.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在制定预处理框架时，我们应当牢记这些高层次的概念。
- en: Text Preprocessing Framework
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本预处理框架
- en: We will introduce this framework conceptually, independent of tools. We will
    then followup with a practical implementation of these steps next time, in order
    to see how they would be carried out in the Python ecosystem.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从概念上介绍这个框架，独立于工具。接下来我们将跟进这些步骤的实际实施，以查看它们在Python生态系统中的执行方式。
- en: The sequence of these tasks is not necessarily as follows, and there can be
    some iteration upon them as well.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些任务的顺序不一定是固定的，也可能会有一些迭代。
- en: 1\. Noise Removal
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 噪声去除
- en: Noise removal performs some of the substitution tasks of the framework. Noise
    removal is a much more task-specific section of the framework than are the following
    steps.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声去除执行了框架中的一些替代任务。噪声去除是框架中一个更具任务特定的部分，而不是后续步骤。
- en: Keep in mind again that we are not dealing with a linear process, the steps
    of which must exclusively be applied in a specified order. Noise removal, therefore,
    can occur before or after the previously-outlined sections, or at some point between).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 再次记住，我们处理的过程不是线性的，步骤并非必须按特定顺序应用。因此，噪声去除可以在之前提到的部分之前、之后或之间的某个点进行。
- en: '**Corpus** (literally Latin for body) refers to a collection of texts. Such
    collections may be formed of a single language of texts, or can span multiple
    languages; there are numerous reasons for which multilingual corpora (the plural
    of corpus) may be useful. Corpora may also consist of themed texts (historical,
    Biblical, etc.). Corpora are generally solely used for statistical linguistic
    analysis and hypothesis testing.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**语料库**（拉丁语字面意思为“主体”）指的是文本的集合。这些集合可以是单一语言的文本，也可以跨越多种语言；多语言语料库（语料库的复数形式）有许多用途。语料库也可以包括主题文本（历史的、圣经的等）。语料库通常仅用于统计语言分析和假设检验。'
- en: How about something more concrete. Let's assume we obtained a corpus from the
    world wide web, and that it is housed in a raw web format. We can, then, assume
    that there is a high chance our text could be wrapped in HTML or XML tags. While
    this accounting for metadata can take place as part of the text collection or
    assembly process (step 1 of our textual data task framework), it depends on how
    the data was acquired and assembled. Sometimes we have control of this data collection
    and assembly process, and so our corpus may already have been de-noised during
    collection.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 怎么样更具体一点？假设我们从万维网获得了一个语料库，并且它是以原始网页格式存放的。那么我们可以假设我们的文本很有可能被HTML或XML标签包裹。虽然这种处理元数据的工作可以作为文本收集或组装过程的一部分（我们文本数据任务框架的第1步），但这取决于数据的获取和组装方式。有时我们可以控制这些数据的收集和组装过程，因此我们的语料库可能在收集过程中已经去噪了。
- en: But this is not always the case. If the corpus you happen to be using is noisy,
    you have to deal with it. Recall that analytics tasks are often talked about as
    being 80% data preparation!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 但情况并非总是如此。如果你所使用的语料库是嘈杂的，你必须处理这些问题。请记住，分析任务通常被认为有80%是数据准备！
- en: The good thing is that pattern matching can be your friend here, as can existing
    software tools built to deal with just such pattern matching tasks.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，模式匹配可以在这里成为你的朋友，现有的软件工具也可以处理这种模式匹配任务。
- en: remove text file headers, footers
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除文本文件的头部和尾部
- en: remove HTML, XML, etc. markup and metadata
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除HTML、XML等标记和元数据
- en: extract valuable data from other formats, such as JSON, or from within databases
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从其他格式（如JSON）或数据库中提取有价值的数据
- en: if you fear regular expressions, this could potentially be the part of text
    preprocessing in which your worst fears are realized
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你对正则表达式感到恐惧，这可能会是你最担心的文本预处理部分。
- en: '**Regular expressions**, often abbreviated *regexp* or *regexp*, are a tried
    and true method of concisely describing patterns of text. A regular expression
    is represented as a special text string itself, and is meant for developing search
    patterns on selections of text. Regular expressions can be thought of as an expanded
    set of rules beyond the wildcard characters of **?** and *****. Though often cited
    as frustrating to learn, regular expressions are incredibly powerful text searching
    tools.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则表达式**，通常缩写为*regexp*或*regexp*，是一种简洁描述文本模式的可靠方法。正则表达式本身作为一个特殊的文本字符串，用于在文本选择上开发搜索模式。正则表达式可以被看作是超越**?**和*****通配符的扩展规则集。尽管学习起来常被认为令人沮丧，但正则表达式是非常强大的文本搜索工具。'
- en: As you can imagine, the boundary between noise removal and data collection and
    assembly is a fuzzy one, and as such some noise removal must absolutely take place
    before other preprocessing steps. For example, any text required from a JSON structure
    would obviously need to be removed prior to tokenization.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所想，噪声去除和数据收集与组装之间的界限是模糊的，因此某些噪声去除必须在其他预处理步骤之前进行。例如，任何从JSON结构中提取的文本显然需要在分词之前去除。
- en: 2\. Normalization
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 标准化
- en: Before further processing, text needs to be normalized.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步处理之前，文本需要标准化。
- en: '**Normalization** generally refers to a series of related tasks meant to put
    all text on a level playing field: converting all text to the same case (upper
    or lower), removing punctuation, expanding contractions, converting numbers to
    their word equivalents, and so on. Normalization puts all words on equal footing,
    and allows processing to proceed uniformly.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**规范化** 通常指一系列相关任务，旨在将所有文本置于同一水平：将所有文本转换为相同的大小写（大写或小写）、删除标点符号、扩展缩写、将数字转换为其单词等价物，等等。规范化使所有单词处于同等地位，并允许处理统一进行。'
- en: 'Normalizing text can mean performing a number of tasks, but for our framework
    we will approach normalization in 3 distinct steps: (1) stemming, (2) lemmatization,
    and (3) everything else.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 规范化文本可能意味着执行多个任务，但对于我们的框架，我们将以三个不同的步骤来处理规范化：（1）词干提取，（2）词形还原，（3）其他所有操作。
- en: '**Stemming** is the process of eliminating affixes (suffixed, prefixes, infixes,
    circumfixes) from a word in order to obtain a word stem.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**词干提取** 是去除单词的词缀（后缀、前缀、插入词缀、环缀）以获得单词词干的过程。'
- en: '`running → run` **Lemmatization** is related to stemming, differing in that
    lemmatization is able to capture canonical forms based on a word''s lemma.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`running → run` **词形还原** 与词干提取相关，但不同的是词形还原能够捕捉基于单词词形的规范形式。'
- en: 'For example, stemming the word "better" would fail to return its citation form
    (another word for lemma); however, lemmatization would result in the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，词干提取单词 "better" 不能返回其引文形式（词形的另一种说法）；然而，词形还原将产生以下结果：
- en: '`better → good`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`better → good`'
- en: It should be easy to see why the implementation of a stemmer would be the less
    difficult feat of the two.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 应该很容易理解为什么实现一个词干提取器会比另一个任务简单。
- en: Everything else
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他所有操作
- en: A clever catch-all, right? Stemming and lemmatization are major parts of a text
    preprocessing endeavor, and as such they need to be treated with the respect they
    deserve. These aren't simple text manipulation; they rely on detailed and nuanced
    understanding of grammatical rules and norms.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一个巧妙的通用方法，对吧？词干提取和词形还原是文本预处理工作的重要部分，因此需要给予应有的尊重。这不仅仅是简单的文本操作；它们依赖于对语法规则和规范的详细和细致的理解。
- en: 'There are, however, numerous other steps that can be taken to help put all
    text on equal footing, many of which involve the comparatively simple ideas of
    substitution or removal. They are, however, no less important to the overall process.
    These include:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有许多其他步骤可以帮助将所有文本置于同等地位，其中许多涉及相对简单的替换或删除概念。然而，这些步骤对于整体过程同样重要。包括：
- en: set all characters to lowercase
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有字符设置为小写
- en: remove numbers (or convert numbers to textual representations)
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除数字（或将数字转换为文本表示）
- en: remove punctuation (generally part of tokenization, but still worth keeping
    in mind at this stage, even as confirmation)
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除标点符号（通常是标记化的一部分，但在这个阶段仍然值得记住，即使是为了确认）
- en: strip white space (also generally part of tokenization)
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去除空白（也通常是标记化的一部分）
- en: remove default stop words (general English stop words)
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除默认停用词（一般英语停用词）
- en: 'Stop words are those words which are filtered out before further processing
    of text, since these words contribute little to overall meaning, given that they
    are generally the most common words in a language. For instance, "the," "and,"
    and "a," while all required words in a particular passage, don''t generally contribute
    greatly to one''s understanding of content. As a simple example, the following
    panagram is just as legible if the stop words are removed:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词是指在进一步处理文本之前被过滤掉的那些词，因为这些词对整体意义贡献较少，因为它们通常是语言中最常见的词。例如，“the”、“and”和“a”，虽然在特定段落中都是必需的词，但通常不会对内容理解有很大贡献。作为一个简单的例子，以下的全句在去掉停用词后依然可以读懂：
- en: '`~~The~~ quick brown fox jumps over ~~the~~ lazy dog.`'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`~~The~~ quick brown fox jumps over ~~the~~ lazy dog.`'
- en: remove given (task-specific) stop words
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除给定的（特定任务的）停用词
- en: remove sparse terms (not always necessary or helpful, though!)
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除稀疏词（虽然不总是必要或有帮助！）
- en: A this point, it should be clear that text preprocessing relies heavily on pre-built
    dictionaries, databases, and rules. You will be relieved to find that when we
    undertake a practical text preprocessing task in the Python ecosystem in our next
    article that these pre-built support tools are readily available for our use;
    there is no need to be inventing our own wheels.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，应清楚文本预处理严重依赖于预构建的词典、数据库和规则。你会感到宽慰的是，当我们在下一篇文章中进行实际的 Python 文本预处理任务时，这些预构建的支持工具随时可用；不需要重新发明轮子。
- en: 3\. Tokenization
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 分词
- en: '**Tokenization** is, generally, an early step in the NLP process, a step which
    splits longer strings of text into smaller pieces, or tokens. Larger chunks of
    text can be tokenized into sentences, sentences can be tokenized into words, etc.
    Further processing is generally performed after a piece of text has been appropriately
    tokenized.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**分词**通常是NLP过程中的早期步骤，这一步将较长的文本字符串分割成更小的片段或标记。较大的文本块可以被分词为句子，句子可以被分词为单词，等等。通常在文本经过适当分词后，会进行进一步处理。'
- en: Tokenization is also referred to as text segmentation or lexical analysis. Sometimes
    **segmentation** is used to refer to the breakdown of a large chunk of text into
    pieces larger than words (e.g. paragraphs or sentences), while tokenization is
    reserved for the breakdown process which results exclusively in words.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 分词也称为文本分割或词汇分析。有时**分割**用于指将较大的文本块分解成比单词更大的片段（例如段落或句子），而分词专指分解过程仅产生单词的情况。
- en: This may sound like a straightforward process, but it is anything but. How are
    sentences identified within larger bodies of text? Off the top of your head you
    probably say "sentence-ending punctuation," and may even, just for a second, think
    that such a statement is unambiguous.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来可能是一个简单的过程，但实际上并非如此。如何在较大的文本中识别句子？你可能会马上说“句子结束的标点符号”，甚至可能会认为这样的说法毫无歧义。
- en: 'Sure, this sentence is easily identified with some basic segmentation rules:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个句子可以通过一些基本的分割规则轻松识别：
- en: '`The quick brown fox jumps over the lazy dog.`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`The quick brown fox jumps over the lazy dog.`'
- en: 'But what about this one:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 那这个呢：
- en: '`Dr. Ford did not ask Col. Mustard the name of Mr. Smith''s dog.`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dr. Ford did not ask Col. Mustard the name of Mr. Smith''s dog.`'
- en: 'Or this one:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 或者这个：
- en: '`"What is all the fuss about?" asked Mr. Peters.`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`"What is all the fuss about?" asked Mr. Peters.`'
- en: And that's just sentences. What about words? Easy, right? Right?
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是句子。那么单词呢？很简单，对吧？对吗？
- en: '`This full-time student isn''t living in on-campus housing, and she''s not
    wanting to visit Hawai''i.`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`This full-time student isn''t living in on-campus housing, and she''s not
    wanting to visit Hawai''i.`'
- en: It should be intuitive that there are varying strategies not only for identifying
    segment boundaries, but also what to do when boundaries are reached. For example,
    we might employ a segmentation strategy which (correctly) identifies a particular
    boundary between word tokens as the apostrophe in the word *she's* (a strategy
    tokenizing on whitespace alone would not be sufficient to recognize this). But
    we could then choose between competing strategies such as keeping the punctuation
    with one part of the word, or discarding it altogether. One of these approaches
    just seems correct, and does not seem to pose a real problem. But just think of
    all the other special cases in just the English language we would have to take
    into account.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 应当直观地理解，不仅有多种策略用于识别分隔边界，还有在达到边界时的处理方法。例如，我们可能采用一种分割策略（正确地）将单词标记之间的特定边界识别为单词*she's*中的撇号（单纯依赖空格分词的策略无法识别这一点）。但我们随后可能会在保留标点与单词一部分或完全丢弃标点之间做出选择。这些方法中的一种似乎是正确的，也没有明显问题。但请考虑英语语言中的所有其他特殊情况。
- en: '**Consideration**: when we segment text chunks into sentences, should we preserve
    sentence-ending delimiters? Are we interested in remembering where sentences ended?'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**考虑**：当我们将文本块分割成句子时，我们是否应该保留句子结束的分隔符？我们是否有兴趣记住句子的结束位置？'
- en: What NLP Tasks Exist?
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存在什么NLP任务？
- en: We have covered some text (pre)processing steps useful for NLP tasks, but what
    about the tasks themselves?
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了一些对NLP任务有用的文本（预）处理步骤，但任务本身呢？
- en: There are no hard lines between these task types; however, many are fairly well-defined
    at this point. A given macro NLP task may include a variety of sub-tasks.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这些任务类型之间没有明确的界限；然而，目前许多任务相对较为明确。一个宏观的NLP任务可能包括多种子任务。
- en: We first outlined the main approaches, since the technologies are often focused
    on for beginners, but it's good to have a concrete idea of what types of NLP tasks
    there are. Below are the main categories of NLP tasks.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先概述了主要的方法，因为这些技术通常是初学者关注的重点，但了解NLP任务的具体类型是很有帮助的。以下是NLP任务的主要类别。
- en: 1\. Text Classification Tasks
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 文本分类任务
- en: 'Representation: bag of words, n-grams, one-hot encoding (sparse matrix) - these
    methods do not preserve word order'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示：词袋模型、n-gram、一热编码（稀疏矩阵）——这些方法不保留单词顺序
- en: 'Goal: predict tags, categories, sentiment'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标：预测标签、类别、情感
- en: 'Application: filtering spam emails, classifying documents based on dominant
    content'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用：过滤垃圾邮件，根据主要内容分类文档
- en: '**Bag of words** is a particular representation model used to simplify the
    contents of a selection of text. The bag of words model omits grammar and word
    order, but is interested in the number of occurrences of words within the text.
    The ultimate representation of the text selection is that of a bag of words (**bag**
    referring to the set theory concept of [multisets](https://en.wikipedia.org/wiki/Multiset),
    which differ from simple sets).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**词袋** 是一种特定的表示模型，用于简化文本选择的内容。词袋模型省略了语法和词序，但关注于文本中词的出现次数。文本选择的最终表示是词袋 (**bag**
    指的是 [多重集](https://en.wikipedia.org/wiki/Multiset) 的集合论概念，与简单集合不同)。'
- en: 'Actual storage mechanisms for the bag of words representation can vary, but
    the following is a simple example using a dictionary for intuitiveness. Sample
    text:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋表示的实际存储机制可能有所不同，但以下是一个使用词典的简单示例。示例文本：
- en: '`"Well, well, well," said John.`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`"Well, well, well," said John.`'
- en: '"There, there," said James. "There, there."'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: “那里，那里，”詹姆斯说。“那里，那里。”
- en: 'The resulting bag of words representation as a dictionary:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的词袋表示作为词典：
- en: '[PRE0] **n-grams** is another representation model for simplifying text selection
    contents. As opposed to the orderless representation of bag of words, n-grams
    modeling is interested in preserving contiguous sequences of *N* items from the
    text selection.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE0] **n-grams** 是一种简化文本选择内容的表示模型。与无序的词袋表示不同，n-grams 建模关注于保留文本选择中连续的 *N*
    项序列。'
- en: 'An example of trigram (3-gram) model of the second sentence of the above example
    ("There, there," said James. "There, there.") appears as a list representation
    below:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例第二句的三元组 (3-gram) 模型示例如下所示：
- en: '[PRE1] Prior to the wide-spread use of neural networks in NLP — in what we
    will refer to as "traditional" NLP — vectorization of text often occurred via
    **one-hot encoding** (note that this persists as a useful encoding practice for
    a number of exercises, and has not fallen out of fashion due to the use of neural
    networks). For one-hot encoding, each word, or token, in a text corresponds to
    a vector element.![one-hot encoding](../Images/5ef1df6b072c42628ebebb1bed002594.png)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1] 在神经网络在 NLP 中广泛使用之前——我们称之为“传统”NLP——文本的向量化通常通过 **one-hot 编码** 进行（请注意，这仍然是一种有用的编码实践，尽管神经网络的使用并没有使其过时）。对于
    one-hot 编码，文本中的每个词或标记对应一个向量元素。![one-hot 编码](../Images/5ef1df6b072c42628ebebb1bed002594.png)'
- en: 'Source: [Adrian Colyer](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[Adrian Colyer](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)
- en: We could consider the image above, for example, as a small excerpt of a vector
    representing the sentence "The queen entered the room." Note that only the element
    for "queen" has been activated, while those for "king," "man," etc. have not.
    You can imagine how differently the one-hot vector representation of the sentence
    "The king was once a man, but is now a child" would appear in the same vector
    element section pictured above.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以将上面的图像视为表示句子“The queen entered the room.”的向量的小摘录。注意只有“queen”的元素被激活，而“king”、“man”等元素没有。你可以想象，如果句子“The
    king was once a man, but is now a child”的 one-hot 向量在上面的同一向量元素部分中会显得多么不同。
- en: 2\. Word Sequence Tasks
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 词序列任务
- en: 'Representation: sequences (preserves word order)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示：序列（保留词序）
- en: 'Goal: language modeling - predict next/previous word(s), text generation'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标：语言建模 - 预测下一个/前一个词，文本生成
- en: 'Application: translation, chatbots, sequence tagging (predict POS tags for
    each word in sequence), named entity recognition'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用：翻译、聊天机器人、序列标注（预测序列中每个词的词性标签）、命名实体识别
- en: '**Language modeling** is the process of building a statistical language model
    which is meant to provide an estimate of a natural language. For a sequence of
    input words, the model would assign a probability to the entire sequence, which
    contributes to the estimated likelihood of various possible sequences. This can
    be especially useful for NLP applications which generate or predict text.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言建模** 是建立统计语言模型的过程，旨在提供自然语言的估计。对于一系列输入词，模型将为整个序列分配一个概率，这有助于估计各种可能序列的可能性。这对于生成或预测文本的
    NLP 应用尤其有用。'
- en: 3\. Text Meaning Tasks
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 文本意义任务
- en: 'Representation: word vectors, the mapping of words to vectors (*n*-dimensional
    numeric vectors) aka embeddings'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示：单词向量，将单词映射到向量（*n*维数字向量），也称为嵌入
- en: 'Goal: how do we represent meaning?'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标：我们如何表示意义？
- en: 'Application: finding similar words (similar vectors), sentence embeddings (as
    opposed to word embeddings), topic modeling, search, question answering'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用：查找相似单词（相似向量）、句子嵌入（与单词嵌入相对）、主题建模、搜索、问答
- en: '**Dense embedding vectors** aka word embeddings result in the representation
    of core features embedded into an embedding space of size *d* dimensions. We can
    compress, if you will, the number of dimensions used to represent 20,000 unique
    words down to, perhaps, 50 or 100 dimensions. In this approach, each feature no
    longer has its own dimension, and is instead mapped to a vector.![Dense embedding
    vectors](../Images/8913c8af571d99ba5f4f14898a5f3a8e.png)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**密集嵌入向量**也称为单词嵌入，将核心特征嵌入到大小为*d*维度的嵌入空间中。如果需要，我们可以将表示20,000个唯一单词所用的维度数压缩到可能的50或100维。在这种方法中，每个特征不再具有自己的维度，而是映射到一个向量中。![密集嵌入向量](../Images/8913c8af571d99ba5f4f14898a5f3a8e.png)'
- en: 'Source: [Adrian Colyer](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[Adrian Colyer](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)
- en: So, what exactly are these features? We leave it to a neural network to determine
    the important aspects of relationships between words. Though human interpretation
    of these features would not be precisely possible, the image above provides an
    insight into what the underlying process may look like, relating to the famous
    `King - Man + Woman = Queen` [example](https://www.technologyreview.com/s/541356/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这些特征究竟是什么呢？我们将其交给神经网络来确定单词之间关系的重要方面。尽管人类对这些特征的解释不可能非常精确，但上面的图像提供了对底层过程的洞察，涉及到著名的`King
    - Man + Woman = Queen` [例子](https://www.technologyreview.com/s/541356/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/)。
- en: '**Named entity recognition** is the attempt to identify [named entities](https://en.wikipedia.org/wiki/Named_entity)
    in text data, and categorize them appropriately. Named entities categories include,
    but are not limited to, person names, locations, organizations, monetary values,
    dates and time, quantities, etc., and can include custom categories dependent
    on the application (medical, scientific, business, etc.). You can think of named
    entities as **proper noun++**.![NER](../Images/5b9fbdd9038ed540762215991c040a03.png)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**命名实体识别**是尝试识别文本数据中的[命名实体](https://en.wikipedia.org/wiki/Named_entity)，并对其进行适当分类。命名实体类别包括但不限于人名、地点、组织、货币值、日期和时间、数量等，并可以根据应用（医疗、科学、商业等）包括自定义类别。你可以将命名实体视为**专有名词++**。![NER](../Images/5b9fbdd9038ed540762215991c040a03.png)'
- en: Named entity recognition (NER) using spaCy (text excerpt taken from [here](https://www.nomadicmatt.com/travel-blogs/three-days-in-new-york-city/))
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用spaCy进行命名实体识别（文本摘自[这里](https://www.nomadicmatt.com/travel-blogs/three-days-in-new-york-city/)）
- en: '**Part-of-speech tagging** consists of assigning a category tag to the tokenized
    parts of a sentence. The most popular POS tagging would be identifying words as
    nouns, verbs, adjectives, etc.![POS Tagging](../Images/a8df09740c1f85d2d5d57166eca3d9ae.png)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**词性标注**包括将类别标签分配给句子的分词部分。最常见的词性标注是识别单词作为名词、动词、形容词等。![词性标注](../Images/a8df09740c1f85d2d5d57166eca3d9ae.png)'
- en: Part-of-speech tagging
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注
- en: 4\. Sequence to Sequence Tasks
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 序列到序列任务
- en: Many tasks in NLP can be framed as such
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多NLP任务可以被框定为这样的任务
- en: Examples are machine translation, summarization, simplification, Q&A systems
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例包括机器翻译、摘要生成、简化、问答系统
- en: Such systems are characterized by encoders and decoders, which work in complement
    to find a hidden representation of text, and to use that hidden representation
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些系统的特点是编码器和解码器，它们互补地工作以找到文本的隐藏表示，并使用该隐藏表示
- en: 5\. Dialog Systems
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 对话系统
- en: 2 main categories of dialog systems, categorized by their scope of use
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对话系统的2大主要类别，根据其使用范围进行分类
- en: Goal-oriented dialog systems focus on being useful in a particular, restricted
    domain; more precision, less generalizable
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标导向对话系统专注于在特定、受限领域中的有用性；更精确，适用性较差
- en: Conversational dialog systems are concerned with being helpful or entertaining
    in a much more general context; less precision, more generalization
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对话对话系统关注于在更广泛的上下文中提供帮助或娱乐；精确度较低，更具普遍性
- en: Whether it be in dialog systems or the practical difference between rule-based
    and more complex approaches to solving NLP tasks, note the trade-off between precision
    and generalizability; you generally sacrifice in one area for an increase in the
    other.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是在对话系统中还是在解决 NLP 任务的基于规则和更复杂方法之间的实际差异中，注意精度与泛化能力之间的权衡；通常你会在一个领域上做出牺牲，以增加另一个领域的能力。
- en: Approaches to NLP Tasks
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP 任务的方法
- en: While not cut and dry, there are 3 main groups of approaches to solving NLP
    tasks.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不是一成不变，但解决 NLP 任务的方法主要分为 3 个组。
- en: '![Dependency parse tree using spaCy](../Images/06df8524acd26c1d5673e7fcf1602ad5.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![使用 spaCy 的依存解析树](../Images/06df8524acd26c1d5673e7fcf1602ad5.png)'
- en: Dependency parse tree using spaCy
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 spaCy 的依存解析树
- en: 1\. Rule-based
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 基于规则的方法
- en: 'Rule-based approaches are the oldest approaches to NLP. Why are they still
    used, you might ask? It''s because they are tried and true, and have been proven
    to work well. Rules applied to text can offer a lot of insight: think of what
    you can learn about arbitrary text by finding what words are nouns, or what verbs
    end in -ing, or whether a pattern recognizable as Python code can be identified.
    [Regular expressions](https://en.wikipedia.org/wiki/Regular_expression) and [context
    free grammars](https://en.wikipedia.org/wiki/Context-free_grammar) are textbook
    examples of rule-based approaches to NLP.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的方法是最古老的 NLP 方法。你可能会问，为什么它们仍然被使用？因为它们经过验证，效果良好。应用于文本的规则可以提供很多洞见：想想通过找到哪些词是名词，哪些动词以
    -ing 结尾，或者是否可以识别出一个可识别为 Python 代码的模式，你可以学到什么。[正则表达式](https://en.wikipedia.org/wiki/Regular_expression)和[上下文无关文法](https://en.wikipedia.org/wiki/Context-free_grammar)是基于规则的
    NLP 方法的经典例子。
- en: 'Rule-based approaches:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的方法：
- en: tend to focus on pattern-matching or parsing
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 倾向于关注模式匹配或解析。
- en: can often be thought of as "fill in the blanks" methods
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经常可以被认为是"填空"方法。
- en: are low precision, high recall, meaning they can have high performance in specific
    use cases, but often suffer performance degradation when generalized
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精度低、召回率高，意味着它们在特定用例中表现良好，但在泛化时往往会性能下降。
- en: 2\. "Traditional" Machine Learning
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. "传统"机器学习
- en: '"Traditional" machine learning approaches include probabilistic modeling, likelihood
    maximization, and linear classifiers. Notably, these are not neural network models
    (see those below).'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '"传统"机器学习方法包括概率建模、似然最大化和线性分类器。值得注意的是，这些不是神经网络模型（见下文）。'
- en: 'Traditional machine learning approaches are characterized by:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 传统机器学习方法的特点是：
- en: training data - in this case, a corpus with markup
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据——在这种情况下，是带有标记的语料库。
- en: feature engineering - word type, surrounding words, capitalized, plural, etc.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程——词性、周围词、大小写、复数等。
- en: training a model on parameters, followed by fitting on test data (typical of
    machine learning systems in general)
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在参数上训练模型，然后在测试数据上拟合（这在机器学习系统中是典型的）。
- en: inference (applying model to test data) characterized by finding most probable
    words, next word, best category, etc.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推断（将模型应用于测试数据）特点是找到最可能的词、下一个词、最佳类别等。
- en: '"semantic slot filling"'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"语义槽填充"'
- en: 3\. Neural Networks
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 神经网络
- en: 'This is similar to "traditional" machine learning, but with a few differences:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于"传统"机器学习，但有一些区别：
- en: feature engineering is generally skipped, as networks will "learn" important
    features (this is generally one of the claimed big benefits of using neural networks
    for NLP)
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程通常被跳过，因为网络会"学习"重要的特征（这通常是使用神经网络进行 NLP 的一个主要好处之一）。
- en: instead, streams of raw parameters ("words" -- actually vector representations
    of words) without engineered features, are fed into neural networks
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，将原始参数（"词"——实际上是词的向量表示）流入神经网络，而不进行工程特征处理。
- en: very large training corpus
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常大的训练语料库
- en: Specific neural networks of use in NLP have "historically" included recurrent
    neural networks (RNNs) and convolutional neural networks (CNNs). Today, the one
    architecture that rules them all is the transformer.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NLP 中，"历史上" 使用的特定神经网络包括递归神经网络（RNNs）和卷积神经网络（CNNs）。今天，统治一切的架构是变换器（transformer）。
- en: Why use "traditional" machine learning (or rule-based) approaches for NLP?
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么在 NLP 中使用"传统"机器学习（或基于规则）的方法？
- en: still good for sequence labeling (using probabilistic modeling)
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于序列标注（使用概率建模）仍然有效。
- en: some ideas in neural networks are very similar to earlier methods (word2vec
    similar in concept to distributional semantic methods)
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络中的一些想法与早期方法非常相似（word2vec 在概念上类似于分布式语义方法）。
- en: use methods from traditional approaches to improve neural network approaches
    (for example, word alignments and attention mechanisms are similar)
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用传统方法中的方法来改进神经网络方法（例如，词对齐和注意机制是相似的）
- en: Why deep learning over "traditional" machine learning?
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么深度学习优于“传统”机器学习？
- en: SOTA in many applications (for example, machine translation)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在许多应用中处于SOTA（例如，机器翻译）
- en: a lot of research (majority?) in NLP happening here now
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在这里有很多关于NLP的研究（大多数？）
- en: '**Importantly**, both neural network and non-neural network approaches can
    be useful for contemporary NLP in their own right; they can also can be used or
    studied in tandem for maximum potential benefit'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**重要的是**，神经网络和非神经网络方法在现代NLP中各有用处；它们也可以一起使用或研究，以获得最大的潜在收益'
- en: References
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[From Languages to Information](https://web.stanford.edu/class/cs124/), Stanford'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[从语言到信息](https://web.stanford.edu/class/cs124/)，斯坦福大学'
- en: '[Natural Language Processing](https://www.coursera.org/learn/language-processing),
    National Research University Higher School of Economics (Coursera)'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[自然语言处理](https://www.coursera.org/learn/language-processing)，国家研究大学高等经济学院（Coursera）'
- en: '[Neural Network Methods for Natural Language Processing](https://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037),
    Yoav Goldberg'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[自然语言处理的神经网络方法](https://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037)，Yoav
    Goldberg'
- en: '[Natural Language Processing](https://github.com/yandexdataschool/nlp_course/),
    Yandex Data School'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[自然语言处理](https://github.com/yandexdataschool/nlp_course/)，Yandex 数据学校'
- en: '[The amazing power of word vectors](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/),
    Adrian Colyer'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[词向量的惊人力量](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)，Adrian
    Colyer'
- en: '**[Matthew Mayo](https://www.linkedin.com/in/mattmayo13/)** ([**@mattmayo13**](https://twitter.com/mattmayo13))
    is a Data Scientist and the Editor-in-Chief of KDnuggets, the seminal online Data
    Science and Machine Learning resource. His interests lie in natural language processing,
    algorithm design and optimization, unsupervised learning, neural networks, and
    automated approaches to machine learning. Matthew holds a Master''s degree in
    computer science and a graduate diploma in data mining. He can be reached at editor1
    at kdnuggets[dot]com.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Matthew Mayo](https://www.linkedin.com/in/mattmayo13/)**（[**@mattmayo13**](https://twitter.com/mattmayo13)）是数据科学家及KDnuggets的主编，该网站是开创性的在线数据科学和机器学习资源。他的兴趣包括自然语言处理、算法设计与优化、无监督学习、神经网络以及机器学习的自动化方法。Matthew拥有计算机科学硕士学位和数据挖掘研究生文凭。他可以通过editor1
    at kdnuggets[dot]com与他联系。'
- en: More On This Topic
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Explain NLP Models with LIME](https://www.kdnuggets.com/2022/01/explain-nlp-models-lime.html)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用LIME解释NLP模型](https://www.kdnuggets.com/2022/01/explain-nlp-models-lime.html)'
- en: '[Must Read NLP Papers from the Last 12 Months](https://www.kdnuggets.com/2023/03/must-read-nlp-papers-last-12-months.html)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[过去12个月必读的NLP论文](https://www.kdnuggets.com/2023/03/must-read-nlp-papers-last-12-months.html)'
- en: '[The Ultimate Guide To Different Word Embedding Techniques In NLP](https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NLP中不同词嵌入技术的终极指南](https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html)'
- en: '[The Range of NLP Applications in the Real World: A Different…](https://www.kdnuggets.com/2022/03/different-solution-problem-range-nlp-applications-real-world.html)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[现实世界中NLP应用的范围：一个不同的…](https://www.kdnuggets.com/2022/03/different-solution-problem-range-nlp-applications-real-world.html)'
- en: '[Machine Learning’s Sweet Spot: Pure Approaches in NLP and Document Analysis](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习的甜蜜点：NLP和文档分析中的纯粹方法](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
- en: '[oBERT: Compound Sparsification Delivers Faster Accurate Models for NLP](https://www.kdnuggets.com/2022/05/obert-compound-sparsification-delivers-faster-accurate-models-nlp.html)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[oBERT: 复合稀疏化提供更快的准确模型用于NLP](https://www.kdnuggets.com/2022/05/obert-compound-sparsification-delivers-faster-accurate-models-nlp.html)'
