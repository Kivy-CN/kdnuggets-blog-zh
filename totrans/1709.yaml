- en: The 10 Statistical Techniques Data Scientists Need to Master
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2017/11/10-statistical-techniques-data-scientists-need-master.html/2](https://www.kdnuggets.com/2017/11/10-statistical-techniques-data-scientists-need-master.html/2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '****6 — Dimension Reduction:****'
  prefs: []
  type: TYPE_NORMAL
- en: Dimension reduction reduces the problem of estimating *p + 1* coefficients to
    the simple problem of *M + 1* coefficients, where *M < p. *This is attained by
    computing *M* different *linear combinations,* or *projections,* of the variables.
    Then these *M* projections are used as predictors to fit a linear regression model
    by least squares. 2 approaches for this task are *principal component regression*
    and *partial least squares.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aaf5c3c81c82d9b597e23931df90d35b.png)'
  prefs: []
  type: TYPE_IMG
- en: One can describe **Principal Components Regression** as an approach for deriving
    a low-dimensional set of features from a large set of variables. The *first*principal
    component direction of the data is along which the observations vary the most.
    In other words, the first PC is a line that fits as close as possible to the data.
    One can fit *p* distinct principal components. The second PC is a linear combination
    of the variables that is uncorrelated with the first PC, and has the largest variance
    subject to this constraint. The idea is that the principal components capture
    the most variance in the data using linear combinations of the data in subsequently
    orthogonal directions. In this way, we can also combine the effects of correlated
    variables to get more information out of the available data, whereas in regular
    least squares we would have to discard one of the correlated variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PCR method that we described above involves identifying linear combinations
    of *X* that best represent the predictors. These combinations (*directions*) are
    identified in an unsupervised way, since the response *Y* is not used to help
    determine the principal component directions. That is, the response *Y* does not *supervise* the
    identification of the principal components, thus there is no guarantee that the
    directions that best explain the predictors also are the best for predicting the
    response (even though that is often assumed). **Partial least square**s (PLS)
    are a *supervised* alternative to PCR. Like PCR, PLS is a dimension reduction
    method, which first identifies a new smaller set of features that are linear combinations
    of the original features, then fits a linear model via least squares to the new *M* features.
    Yet, unlike PCR, PLS makes use of the response variable in order to identify the
    new features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '****7 — Nonlinear Models:****'
  prefs: []
  type: TYPE_NORMAL
- en: 'In statistics, nonlinear regression is a form of regression analysis in which
    observational data are modeled by a function which is a nonlinear combination
    of the model parameters and depends on one or more independent variables. The
    data are fitted by a method of successive approximations. Below are a couple of
    important techniques to deal with nonlinear models:'
  prefs: []
  type: TYPE_NORMAL
- en: A function on the real numbers is called a **step function** if it can be written
    as a finite linear combination of indicator functions of intervals. Informally
    speaking, a step function is a piecewise constant function having only finitely
    many pieces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **piecewise function** is a function which is defined by multiple sub-functions,
    each sub-function applying to a certain interval of the main function’s domain.
    Piecewise is actually a way of expressing the function, rather than a characteristic
    of the function itself, but with additional qualification, it can describe the
    nature of the function. For example, a **piecewise polynomial** function is a
    function that is a polynomial on each of its sub-domains, but possibly a different
    one on each.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/594bd23c68d30b0aa895e2e2b83540c7.png)'
  prefs: []
  type: TYPE_IMG
- en: A **spline** is a special function defined piecewise by polynomials. In computer
    graphics, spline refers to a piecewise polynomial parametric curve. Splines are
    popular curves because of the simplicity of their construction, their ease and
    accuracy of evaluation, and their capacity to approximate complex shapes through
    curve fitting and interactive curve design.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **generalized additive model**is a generalized linear model in which the linear
    predictor depends linearly on unknown smooth functions of some predictor variables,
    and interest focuses on inference about these smooth functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '****8 — Tree-Based Methods:****'
  prefs: []
  type: TYPE_NORMAL
- en: Tree-based methods can be used for both regression and classification problems.
    These involve stratifying or segmenting the predictor space into a number of simple
    regions. Since the set of splitting rules used to segment the predictor space
    can be summarized in a tree, these types of approaches are known as **decision-tree** methods.
    The methods below grow multiple trees which are then combined to yield a single
    consensus prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging** is the way decrease the variance of your prediction by generating
    additional data for training from your original dataset using combinations with
    repetitions to produce multistep of the same carnality/size as your original data.
    By increasing the size of your training set you can’t improve the model predictive
    force, but just decrease the variance, narrowly tuning the prediction to expected
    outcome.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosting** is an approach to calculate the output using several different
    models and then average the result using a weighted average approach. By combining
    the advantages and pitfalls of these approaches by varying your weighting formula
    you can come up with a good predictive force for a wider range of input data,
    using different narrowly tuned models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/8564dca67fc55bd7baf8ab9137185528.png)'
  prefs: []
  type: TYPE_IMG
- en: The **random forest** algorithm is actually very similar to bagging. Also here,
    you draw random bootstrap samples of your training set. However, in addition to
    the bootstrap samples, you also draw a random subset of features for training
    the individual trees; in bagging, you give each tree the full set of features.
    Due to the random feature selection, you make the trees more independent of each
    other compared to regular bagging, which often results in better predictive performance
    (due to better variance-bias trade-offs) and it’s also faster, because each tree
    learns only from a subset of features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '****9 — Support Vector Machines:****'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cce76979f7f43d567a5ca6b735d0d831.png)'
  prefs: []
  type: TYPE_IMG
- en: SVM is a classification technique that is listed under supervised learning models
    in Machine Learning. In layman’s terms, it involves finding the hyperplane (line
    in 2D, plane in 3D and hyperplane in higher dimensions. More formally, a hyperplane
    is n-1 dimensional subspace of an n-dimensional space) that best separates two
    classes of points with the maximum margin. Essentially, it is a constrained optimization
    problem where the margin is maximized subject to the constraint that it perfectly
    classifies the data (hard margin).
  prefs: []
  type: TYPE_NORMAL
- en: The data points that kind of “support” this hyperplane on either sides are called
    the “support vectors”. In the above picture, the filled blue circle and the two
    filled squares are the support vectors. For cases where the two classes of data
    are not linearly separable, the points are projected to an exploded (higher dimensional)
    space where linear separation may be possible. A problem involving multiple classes
    can be broken down into multiple one-versus-one or one-versus-rest binary classification
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: '****10 — Unsupervised Learning:****'
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we only have discussed supervised learning techniques, in which the
    groups are known and the experience provided to the algorithm is the relationship
    between actual entities and the group they belong to. Another set of techniques
    can be used when the groups (categories) of data are not known. They are called
    unsupervised as it is left on the learning algorithm to figure out patterns in
    the data provided. Clustering is an example of unsupervised learning in which
    different data sets are clustered into groups of closely related items. Below
    is the list of most widely used unsupervised learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61f3908308615b6f22c03bc7ae8be331.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Principal Component Analysis** helps in producing low dimensional representation
    of the dataset by identifying a set of linear combination of features which have
    maximum variance and are mutually un-correlated. This linear dimensionality technique
    could be helpful in understanding latent interaction between the variable in an
    unsupervised setting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**k-Means clustering**: partitions data into k distinct clusters based on distance
    to the centroid of a cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical clustering**: builds a multilevel hierarchy of clusters by creating
    a cluster tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This was a basic run-down of some basic statistical techniques that can help
    a data science program manager and or executive have a better understanding of
    what is running underneath the hood of their data science teams. Truthfully, some
    data science teams purely run algorithms through python and R libraries. Most
    of them don’t even have to think about the math that is underlying. However, being
    able to understand the basics of statistical analysis gives your teams a better
    approach. Have insight into the smallest parts allows for easier manipulation
    and abstraction. I hope this basic data science statistical guide gives you a
    decent understanding!
  prefs: []
  type: TYPE_NORMAL
- en: '*P.S: You can get all the lecture slides and RStudio sessions from *[*my GitHub
    source code here*](https://github.com/khanhnamle1994/statistical-learning)*. Thanks
    for the overwhelming response!*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [James Le](https://www.linkedin.com/in/khanhnamle94/)** is currently
    applying for Master of Science Computer Science programs in the US for the Fall
    2018 admission. His intended research will focus on Machine Learning and Data
    Mining. In the mean time, he is working as a freelance full-stack web developer.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/the-10-statistical-techniques-data-scientists-need-to-master-1ef6dbd531f7).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[The 10 Algorithms Machine Learning Engineers Need to Know](/2016/08/10-algorithms-machine-learning-engineers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top 10 Data Mining Algorithms, Explained](/2015/05/top-10-data-mining-algorithms-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top 10 Machine Learning Algorithms for Beginners](/2017/10/top-10-machine-learning-algorithms-beginners.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[6 Predictive Models Every Beginner Data Scientist Should Master](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Key Skills Needed To Become a Great Data Scientist](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best ETL Tools in 2021](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
