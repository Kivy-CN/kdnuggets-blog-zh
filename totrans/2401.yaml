- en: When Would Ensemble Techniques be a Good Choice?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成技术何时是一个好选择？
- en: 原文：[https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)
- en: '![When Would Ensemble Techniques be a Good Choice?](../Images/ee94621a31d8295616fa92d91a805cad.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![集成技术何时是一个好选择？](../Images/ee94621a31d8295616fa92d91a805cad.png)'
- en: '[Brett Jordan](https://unsplash.com/@brett_jordan) via Unsplash'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[布雷特·乔丹](https://unsplash.com/@brett_jordan) via Unsplash'
- en: In any decision making process, you need to gather all the different types of
    data you have, all the information, pull it apart, learn more about it, get experts
    in, and more before making a solid decision.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何决策过程中，你需要收集所有不同类型的数据和信息，拆解这些信息，深入了解，寻求专家意见，等等，然后才能做出坚实的决定。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT 部门'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: This is similar in the Machine Learning process with Ensemble techniques. Ensemble
    models combine a variety of models together to help the prediction (decision making)
    process. A single model may not have the capabilities of producing the right prediction
    for a specific data set - this raises the chance of high variance, low accuracy
    and noise and bias. By combining multiple models, we effectively have a higher
    chance to improve the level of accuracy.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这在机器学习过程中与集成技术类似。集成模型将多种模型组合在一起，以帮助预测（决策）过程。单一模型可能没有能力为特定数据集产生正确的预测 - 这增加了高方差、低准确性以及噪声和偏差的风险。通过结合多个模型，我们有效地提高了准确性的可能性。
- en: The easiest example is Decision Trees - a probability tree-like structure model
    that continuously splits data to make predictions based on the previous set of
    questions that were answered.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的例子是决策树 - 一种类似于概率树的模型，它不断分裂数据，根据先前回答的问题来进行预测。
- en: Why Would You Use An Ensemble Technique?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么使用集成技术？
- en: To answer the question of this article, “When would ensemble techniques be a
    good choice?” When you want to improve the performance of machine learning models
    - it’s that simple.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答本文的问题，“何时使用集成技术是一个好选择？”当你想提高机器学习模型的性能时 - 就这么简单。
- en: For example, if you’re working on a classification task and you wish to increase
    the accuracy of your model - use ensemble techniques. If you want to reduce your
    mean error for your regression task - use ensemble techniques.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你正在进行分类任务并希望提高模型的准确性 - 使用集成技术。如果你想减少回归任务的平均误差 - 使用集成技术。
- en: 'The main 2 reasons for using an ensemble learning algorithms is:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用集成学习算法的主要2个原因是：
- en: Improve predictions - you will achieve better predictive skill rather than just
    using a single model.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高预测 - 你将获得比仅使用单一模型更好的预测能力。
- en: Improve robustness - you will achieve better stable predictions rather than
    just using a single model.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高稳健性 - 你将获得比仅使用单一模型更稳定的预测结果。
- en: Your overall aim when using ensemble techniques should be to reduce the generalization
    error of the prediction. Therefore, using a variety of base models which are diverse
    will automatically decrease your prediction error.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用集成技术时，你的总体目标应该是减少预测的一般化误差。因此，使用多样化的基础模型将自动降低你的预测误差。
- en: It is essentially building a more stable, reliable and accurate model that you
    trust.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上是构建一个更稳定、更可靠、更准确的模型，让你信赖。
- en: 'There are 3 types of ensemble modeling techniques:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有3种类型的集成建模技术：
- en: Bagging
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bagging
- en: Boosting
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Boosting
- en: Stacking
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Stacking
- en: Bagging
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bagging
- en: Short for Bootstrap Aggregation, as the ensemble modeling technique combines
    Bootstrapping and Aggregation to form one ensemble model. It is based on creating
    multiple sets of the original training data, creating tree-like structure probability
    models which then aggregate to conclude to a final prediction.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装法的简称是 Bootstrap Aggregation，该集成建模技术结合了自助采样和聚合形成一个集成模型。它基于创建多个原始训练数据集，建立类似树的结构概率模型，然后进行聚合以得出最终预测。
- en: Each model learns about the errors produced in the previous model and uses a
    different subset of the training data set. Bagging aims to avoid overfitting of
    data and reduce the variance in the predictions and can be used for both regression
    and classification models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型学习前一个模型产生的错误，并使用训练数据集的不同子集。袋装法旨在避免数据过拟合并减少预测的方差，可用于回归和分类模型。
- en: Random Forest
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林
- en: Random Forest is an algorithm of Bagging but with a slight difference. It uses
    a subset of samples of the training data and a subset of features to build multiple
    trees that split. You can see it as multiple decision trees which fit each training
    set in a random mode.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种袋装法算法，但有一点不同。它使用训练数据的样本子集和特征子集来构建多个决策树。你可以把它看作是多个决策树，以随机方式拟合每个训练集。
- en: The decision on the split is based on a random selection of features causing
    a differentiation between each tree. This produces a more accurate aggregated
    result and final prediction.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 分割决策基于特征的随机选择，导致每棵树之间的差异。这会产生更准确的聚合结果和最终预测。
- en: 'Other example algorithm are:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其他示例算法包括：
- en: Bagged Decision Trees
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 袋装决策树
- en: Extra Trees
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Extra Trees
- en: Custom Bagging
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义袋装法
- en: Boosting
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升法
- en: Boosting is the act of converting weak learners to strong learners. A weak learner
    fails to make accurate predictions due to their capabilities. A new weak prediction
    rule is generated by applying base learning algorithms. This is done by taking
    a random sample of data which is then inputted into a model and then trained sequentially
    which aims to train the weak learners and try to correct its predecessor.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 提升法是将弱学习者转变为强学习者的过程。弱学习者由于其能力不足，无法做出准确的预测。通过应用基础学习算法生成新的弱预测规则。这是通过获取数据的随机样本并输入到模型中，然后进行顺序训练，旨在训练弱学习者并试图修正其前身。
- en: An example of Boosting is AdaBoost and XGBoost
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 提升法的一个示例是 AdaBoost 和 XGBoost
- en: AdaBoost
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AdaBoost
- en: AdaBoost is short for Adaptive Boosting and is used as a technique to boost
    the performance of a machine learning algorithm. It takes the notion of Random
    Forests weak learners and builds models on top of several weak learners.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 是自适应提升的简称，用作提升机器学习算法性能的技术。它利用随机森林的弱学习者概念，在多个弱学习者之上构建模型。
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: XGBoost
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBoost
- en: XGboosts stands for Extreme Gradient Boosting and is one of the most popular
    boosting algorithms that can be used for both regression and classification tasks.
    It is a type of supervised learning machine learning algorithm which aims to accurately
    predict a target variable by combining a set of weaker models.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 代表极端梯度提升，是最受欢迎的提升算法之一，可用于回归和分类任务。它是一种监督学习算法，旨在通过组合一组较弱的模型来准确预测目标变量。
- en: 'Other example algorithms are:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其他示例算法包括：
- en: Gradient Boosting Machine
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升机
- en: Stochastic Gradient Boosting
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机梯度提升
- en: LightGBM
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LightGBM
- en: When should I use Bagging vs Boosting?
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时使用袋装法与提升法？
- en: 'The simplest way to determine when to use bagging or boosting is:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 确定何时使用袋装法或提升法的最简单方法是：
- en: If the classifier is unstable and has high variance - use Bagging
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果分类器不稳定且有高方差 - 使用袋装法
- en: If the classifier is stable, however has high bias - use Boosting
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果分类器稳定但有高偏差 - 使用提升法
- en: Stacking
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠法
- en: Stacking is short for Stacked Generalization and is similar to boosting; with
    the aim to produce more robust predictors. This is done by taking the predictions
    from weak learners and using that to create a strong model. It does this by figuring
    out how to best combine the predictions from multiple models on the same dataset.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠法是堆叠泛化的简称，与提升法类似；目的是生成更强健的预测模型。这是通过将弱学习者的预测结果用于创建一个强模型来实现的。它通过找出如何最佳地结合多个模型在同一数据集上的预测来完成。
- en: It is basically asking you ‘If you had a variety of machine learning models
    that perform well on a specific problem, how do you choose which model is the
    best to trust?’
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 它基本上是在问你‘如果你有多种在特定问题上表现良好的机器学习模型，你如何选择最值得信赖的模型？’
- en: Voting
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 投票法
- en: Voting is an example of Stacking, however it is different for both classification
    and regression tasks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 投票是堆叠的一个示例，但在分类和回归任务中有所不同。
- en: For regression, the prediction is made based on the average of other regression
    models.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归，预测是基于其他回归模型的平均值。
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For classification, there can either be hard voting or soft voting. Hard voting
    is essentially picking the prediction with the highest number of votes, whereas
    soft voting is combining the probabilities of each prediction in each of the models
    and then picking the prediction with the highest total probability.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类任务，可以使用硬投票或软投票。硬投票本质上是选择票数最多的预测，而软投票则是结合每个模型中每个预测的概率，然后选择总概率最高的预测。
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Other example algorithms are:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其他示例算法包括：
- en: Weighted Average
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加权平均
- en: Blending
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合
- en: Stacking
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠
- en: Super Learner
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超级学习器
- en: What’s the Difference Between Stacking and Bagging/Boosting?
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠和装袋/提升有什么区别？
- en: Bagging uses decision trees, where stacking uses different models. Bagging takes
    samples from the training dataset, where stacking fits on the same dataset.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 装袋使用决策树，而堆叠使用不同的模型。装袋从训练数据集中抽取样本，而堆叠则在相同的数据集上进行拟合。
- en: Boosting uses a sequence of models that converts weak learners to strong learners
    to correct the prior models predicting, whereas stacking uses a single model to
    learn how to combine the predictions from the contributing models in the best
    way.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 提升使用一系列模型，将弱学习者转化为强学习者，以纠正之前模型的预测，而堆叠使用单一模型来学习如何最佳地结合来自贡献模型的预测。
- en: Aggregating everything
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合所有内容
- en: You will always need to understand what you’re trying to achieve before you
    attempt to solve a task. Once you do that, you will be able to determine if your
    task is a classification or regression task - in which you can then choose which
    ensemble algorithm will be the best to use to improve your models predictions
    and robustness.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你总是需要在尝试解决任务之前理解你想要实现的目标。一旦你做到这一点，你将能够确定你的任务是分类还是回归任务，从而选择最适合的集成算法来提高模型的预测能力和稳健性。
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**[尼莎·阿里亚](https://www.linkedin.com/in/nisha-arya-ahmed/)** 是一名数据科学家和自由技术作家。她特别关注提供数据科学职业建议或教程以及数据科学理论知识。她还希望探索人工智能如何/能否有助于人类寿命的不同方式。她是一个热衷学习者，寻求拓宽她的技术知识和写作技能，同时帮助指导他人。'
- en: More On This Topic
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习技术：使用 Python 中的随机森林进行详细讲解](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
- en: '[If I Had To Start Learning Data Science Again, How Would I Do It?](https://www.kdnuggets.com/2020/08/start-learning-data-science-again.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如果我必须重新开始学习数据科学，我会怎么做？](https://www.kdnuggets.com/2020/08/start-learning-data-science-again.html)'
- en: '[Ensemble Learning with Examples](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[带有示例的集成学习](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
- en: '[What makes a visualization good?](https://www.kdnuggets.com/2022/10/sphere-makes-visualization-good.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么使得可视化效果好？](https://www.kdnuggets.com/2022/10/sphere-makes-visualization-good.html)'
- en: '[Your Features Are Important? It Doesn’t Mean They Are Good](https://www.kdnuggets.com/your-features-are-important-it-doesnt-mean-they-are-good)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你的特征重要吗？这并不意味着它们好](https://www.kdnuggets.com/your-features-are-important-it-doesnt-mean-they-are-good)'
- en: '[Data Quality: The Good, The Bad, and The Ugly](https://www.kdnuggets.com/2022/01/data-quality-good-bad-ugly.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据质量：好、坏与丑](https://www.kdnuggets.com/2022/01/data-quality-good-bad-ugly.html)'
