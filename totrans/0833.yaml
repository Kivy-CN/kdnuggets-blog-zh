- en: The Importance of Data Cleaning in Data Science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/08/importance-data-cleaning-data-science.html](https://www.kdnuggets.com/2023/08/importance-data-cleaning-data-science.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![The Importance of Data Cleaning in Data Science](../Images/15e040078d2f4f18c16c97412c1f2d88.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Editor
  prefs: []
  type: TYPE_NORMAL
- en: In [data science](/2023/07/first-half-2023-data-science-ai-developments.html),
    the accuracy of predictive models is vitally important to ensure any costly errors
    are avoided and that each aspect is working to its optimal level. Once the data
    has been selected and formatted, the data needs to be cleaned, a crucial stage
    of the model development process.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will provide an overview of the importance of data cleaning
    in data science, including what it is, the benefits, the data cleaning process,
    and the commonly used tools.
  prefs: []
  type: TYPE_NORMAL
- en: What Is Data Cleaning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In data science, data cleaning is the process of identifying incorrect data
    and fixing the errors so the final dataset is ready to be used. Errors could include
    duplicate fields, incorrect formatting, incomplete fields, irrelevant or inaccurate
    data, and corrupted data.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Importance of Data Cleaning in Data Science](../Images/dd67781f627cb4da75af98a8cebd011d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://anchorcomputersoftware.com/solutions/data-quality/data-cleansing)'
  prefs: []
  type: TYPE_NORMAL
- en: In a data science project, the cleaning stage [comes before validation](https://www.tableau.com/learn/articles/what-is-data-cleaning)
    in the data pipeline. In the pipeline, each stage ingests input and creates output,
    improving the data each step of the way. The benefit of the data pipeline is that
    each step has a specific purpose and is self-contained, meaning the data is thoroughly
    checked.
  prefs: []
  type: TYPE_NORMAL
- en: The Importance of Data Cleaning in Data Science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data seldom arrives in a readily usable form; in fact, it can be confidently
    stated that data is never flawless. When collected from diverse sources and real-world
    environments, data is bound to contain numerous errors and adopt different formats.
    Hence, the significance of data cleaning arises -- to render the data error-free,
    pertinent, and easily assimilated by models.
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with extensive datasets from multiple sources, errors can occur,
    including duplication or misclassification. These mistakes greatly affect algorithm
    accuracy. Notably, data cleaning and organization can consume [up to 80% of a
    data scientist's time](https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242),
    highlighting its critical role in the data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of Data Cleaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Below are three examples of how data cleaning can fix errors within datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Formatting**'
  prefs: []
  type: TYPE_NORMAL
- en: Data formatting involves [transforming data into a specific format](https://guides.auraria.edu/datamanagement/dataformatting#:~:text=Data%20formatting%20is%20the%20decision,to%20the%20naming%20your%20files.)
    or modifying the structure of a dataset. Ensuring consistency and a well-structured
    dataset is crucial to avoid errors during data analysis. Therefore, employing
    various techniques during the cleaning process is necessary to guarantee accurate
    data formatting. This may encompass converting categorical data to numerical values
    and consolidating multiple data sources into a unified dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Empty/ Missing Values**'
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning techniques play a crucial role in resolving data issues such as
    missing or empty values. These techniques involve estimating and filling in gaps
    in the dataset using relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, consider the location field. If the field is empty, scientists
    can populate it with the average location data from the dataset or a similar one.
    Although not flawless, having the most probable location is preferable to having
    no location information at all. This approach ensures improved data quality and
    enhances the overall reliability of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Identifying Outliers**'
  prefs: []
  type: TYPE_NORMAL
- en: Within a dataset, certain data points may lack any substantive connection to
    others (e.g., in terms of value or behavior). Consequently, during data analysis,
    these outliers possess the ability to significantly distort results, leading to
    misguided predictions and flawed decision-making. However, by implementing various
    data cleaning techniques, it is possible to identify and eliminate these outliers,
    ultimately ensuring the integrity and relevance of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Importance of Data Cleaning in Data Science](../Images/50b55e1385e359bbeef572c9203af60d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://medium.com/donato-story/improving-data-quality-with-outlier-detection-techniques-a-crisp-dm-approach-to-data-preparation-5a04a9eef844)'
  prefs: []
  type: TYPE_NORMAL
- en: The Benefits of Data Cleaning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data cleaning provides a range of benefits that have a significant impact on
    the accuracy, relevance, usability, and analysis of data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy** - Using data cleaning tools and techniques significantly reduces
    errors and inaccuracies contained in a dataset. This is important for data analysis,
    helping to create models that make accurate predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Usability** - Once cleaned and correctly formatted, data can be applied to
    a number of use cases, making it much [more accessible](/2023/07/mostly-data-access-severely-lacking-synthetic-data-help.html)
    so it can be used in a range of project types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Analysis** - Clean data makes the analysis stage much more effective, allowing
    analysts to gain greater insights and deliver more reliable results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficient Data Storage** - By removing unnecessary and duplicate data, storage
    costs are reduced as only relevant, valuable data needs to be retained, whether
    that is on an on-site server or a cloud data warehouse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Governance** - Data cleaning can help organizations adhere to strict regulations
    and data governance, protecting the privacy of individuals and avoiding any penalties.
    More data compliance laws have been enacted in recent months. An example is the
    recent [Texas consumer privacy law (TDPSA)](https://www.ipvanish.com/blog/tdpsa-texas-consumer-privacy-law/),
    which prohibits certain data practices such as gathering personal customer data
    that is not reasonably necessary for the purpose of collection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Data Cleaning Process: 8 Steps'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data cleaning stage of the data pipeline is made up of eight common steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The removal of duplicates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The removal of irrelevant data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standardization of capitalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data type conversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The handling of outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fixing of errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language Translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The handling of any missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1\. The Removal of Duplicates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large datasets that utilize multiple data sources are highly likely to have
    errors, including duplicates, particularly when new entries haven't undergone
    quality checks. Duplicate data is redundant and consumes unnecessary storage space,
    necessitating data cleansing to enhance efficiency. Common instances of duplicate
    data comprise repetitive email addresses and phone numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The Removal of Irrelevant Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To optimize a dataset, it is [crucial to remove irrelevant data fields](https://segment.com/blog/data-cleaning/).
    This will result in faster model processing and enable a more focused approach
    toward achieving specific goals. During the data cleaning stage, any data that
    does not align with the scope of the project will be eliminated, retaining only
    the necessary information required to fulfill the task.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. The Standardization of Capitalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Standardizing text in datasets is crucial for ensuring consistency and facilitating
    easy analysis. Correcting capitalization is especially important, as it prevents
    the creation of false categories that could result in messy and confusing data.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Data Type Conversion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When working with CSV data using Python to manipulate it, analysts often rely
    on Pandas, the go-to data analysis library. However, there are instances where
    Pandas fall short in processing data types effectively. To guarantee accurate
    data conversion, analysts employ cleaning techniques. This ensures that the correct
    data is easily identifiable when applied to real-life projects.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. The Handling of Outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An outlier is a data point [that lacks relevance to other points](https://careerfoundry.com/en/blog/data-analytics/what-is-an-outlier/),
    deviating significantly from the overall context of the dataset. While outliers
    can occasionally offer intriguing insights, they are typically regarded as errors
    that should be removed.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. The Fixing of Errors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensuring the effectiveness of a model is crucial, and rectifying errors before
    the data analysis stage is paramount. Such errors often result from manual data
    entry without adequate checking procedures. Examples include phone numbers with
    incorrect digits, email addresses without an "@" symbol, or unpunctuated user
    feedback.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Language Translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Datasets can be gathered from various sources written in different languages.
    However, when using such data for machine translation, evaluation tools typically
    rely on [monolingual Natural Language Processing (NLP)](https://nlpcloud.com/multilingual-nlp-how-to-perform-nlp-in-non-english-languages.html)
    models, which can only handle one language at a time. Thankfully, during the data
    cleaning phase, AI tools can come to the rescue by converting all the data into
    a unified language. This ensures greater coherence and compatibility throughout
    the translation process.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. The Handling of Any Missing Values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the last steps in data cleaning involves addressing missing values. This
    can be achieved by either removing records that have missing values or employing
    statistical techniques to fill in the gaps. A comprehensive understanding of the
    dataset is crucial in making these decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The importance of data cleaning in data science can never be underestimated
    as it can significantly impact the accuracy and overall success of a data model.
    With thorough data cleaning, the data analysis stage is likely to output flawed
    results and incorrect predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Common errors that need to be rectified during the data cleaning stage are duplicate
    data, missing values, irrelevant data, outliers, and converting multiple data
    types or languages into a single form.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nahla Davies](http://nahlawrites.com/)** is a software developer and tech
    writer. Before devoting her work full time to technical writing, she managed —
    among other intriguing things — to serve as a lead programmer at an Inc. 5,000
    experiential branding organization whose clients include Samsung, Time Warner,
    Netflix, and Sony.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[The Importance of Experiment Design in Data Science](https://www.kdnuggets.com/2022/08/importance-experiment-design-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Importance of Probability in Data Science](https://www.kdnuggets.com/2023/02/importance-probability-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Berkson-Jekel Paradox and its Importance to Data Science](https://www.kdnuggets.com/2023/03/berksonjekel-paradox-importance-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Celebrating Awareness of the Importance of Data Privacy](https://www.kdnuggets.com/2022/01/celebrating-awareness-importance-data-privacy.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Importance of Pre-Processing in Machine Learning](https://www.kdnuggets.com/2023/02/importance-preprocessing-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Importance of Reproducibility in Machine Learning](https://www.kdnuggets.com/2023/06/importance-reproducibility-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
