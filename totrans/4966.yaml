- en: How Not To Program the TensorFlow Graph
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何不编程 TensorFlow 图
- en: 原文：[https://www.kdnuggets.com/2017/05/how-not-program-tensorflow-graph.html](https://www.kdnuggets.com/2017/05/how-not-program-tensorflow-graph.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/05/how-not-program-tensorflow-graph.html](https://www.kdnuggets.com/2017/05/how-not-program-tensorflow-graph.html)
- en: '**By Aaron Schumacher, Deep Learning Analytics.**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：亚伦·舒马赫，深度学习分析专家。**'
- en: Using TensorFlow from Python is like using Python to program [another computer](http://planspace.org/20170328-tensorflow_as_a_distributed_virtual_machine/).
    Some Python statements build your TensorFlow program, some Python statements execute
    that program, and of course some Python statements aren’t involved with TensorFlow
    at all. Being thoughtful about the graphs you construct can help you avoid confusion
    and performance pitfalls. Here are a few considerations.![](../Images/e09c1745abd025c932b954fd964eedfb.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python 操作 TensorFlow 就像用 Python 编程 [另一台计算机](http://planspace.org/20170328-tensorflow_as_a_distributed_virtual_machine/)。一些
    Python 语句构建你的 TensorFlow 程序，一些 Python 语句执行该程序，当然，还有一些 Python 语句完全不涉及 TensorFlow。对你构建的图保持思考可以帮助你避免混淆和性能陷阱。这里有一些考虑因素。![](../Images/e09c1745abd025c932b954fd964eedfb.png)
- en: Avoid having many identical ops
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 避免拥有多个相同的操作
- en: Lots of methods in TensorFlow create ops in the computation graph, but do not
    execute them. You may want to execute multiple times, but that doesn’t mean you
    should create lots of copies of the same ops.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 中的许多方法会在计算图中创建操作，但不会执行它们。你可能希望多次执行这些操作，但这并不意味着你应该创建许多相同操作的副本。
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 工作'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: A clear example is `tf.global_variables_initializer()`.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一个明确的例子是 `tf.global_variables_initializer()`。
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If the call to `tf.global_variables_initializer()` is repeated, for example
    directly as the argument to `session.run()`, there are downsides.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `tf.global_variables_initializer()` 的调用被重复，例如直接作为 `session.run()` 的参数，则会有一些缺点。
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A new initializer op is created every time the argument to `session.run()` here
    is evaluated. This creates multiple initializer ops in the graph. Having multiple
    copies isn’t a big deal for small ops in an interactive session, and you might
    even want to do it in the case of the initializer if you’ve created more variables
    that need to be included in initialization. But think about whether you need lots
    of duplicate ops.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每当 `session.run()` 的参数在这里被评估时，都会创建一个新的初始化操作。这在图中创建了多个初始化操作。对于小的操作来说，在交互式会话中拥有多个副本并不是什么大问题，如果你创建了更多需要在初始化中包含的变量，可能还会希望这样做。但要考虑是否真的需要大量重复的操作。
- en: Creating ops inside `session.run()`, you also don’t have a Python variable referring
    to those ops, so you can’t easily reuse them.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `session.run()` 内部创建操作时，你也没有一个 Python 变量引用这些操作，因此你无法轻易地重用它们。
- en: In Python, if you create an object that nothing refers to, it can be garbage
    collected. The abandoned object will be deleted and and memory it used will be
    freed. That doesn’t happen to things in the TensorFlow graph; everything you put
    in the graph stays there.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，如果你创建了一个没有任何引用的对象，它会被垃圾回收。被遗弃的对象将被删除，它所占用的内存将被释放。TensorFlow 图中的对象不会发生这种情况；你放入图中的所有内容都会留在那里。
- en: It’s pretty clear that `tf.global_variables_initializer()` returns an op. But
    ops are also created in less obvious ways.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，`tf.global_variables_initializer()` 返回的是一个操作。但操作也可以通过不那么明显的方式创建。
- en: Let’s compare to how NumPy works.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来比较一下 NumPy 的工作方式。
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: At this point there are two arrays in memory, `x` and `y`. The `y` has the value
    2.0, but there’s no record of *how* it came to have that value. The addition has
    left no record of itself.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 目前内存中有两个数组，`x` 和 `y`。`y` 的值是 2.0，但没有记录*如何*获得这个值。加法操作没有留下任何记录。
- en: TensorFlow is different.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是不同的。
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now only `x` is a TensorFlow variable; `y` is an `add` op, which can return
    the result of that addition if we ever run it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在只有`x`是一个TensorFlow变量；`y`是一个`add`操作，如果我们运行它，它可以返回该加法的结果。
- en: One more comparison.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 再比较一下。
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here `y` is assigned to refer to one result array `x + 1.0`, and then reassigned
    to point to a different one. The first one will be garbage collected and disappear.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里`y`被分配为一个结果数组`x + 1.0`，然后重新分配为指向不同的数组。第一个数组将被垃圾回收并消失。
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this case, `y` refers to one `add` op in the TensorFlow graph, and then `y`
    is reassigned to point to a different `add` op in the graph. Since `y` only points
    to the second `add` now, we don’t have a convenient way to work with the first
    one. But both the `add` ops are still around, in the graph, and will stay there.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`y`指的是TensorFlow图中的一个`add`操作，然后`y`被重新分配指向图中的另一个`add`操作。由于`y`现在只指向第二个`add`，我们没有方便的方法来处理第一个。然而这两个`add`操作仍然存在于图中，并且会继续存在。
- en: (As an aside, Python’s mechanism for defining class-specific addition [and so
    on](http://www.python-course.eu/python3_magic_methods.php), which is how `+` is
    made to create TensorFlow ops, is pretty neat.)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: （顺便提一下，Python用于定义类特定加法的机制[以及其他](http://www.python-course.eu/python3_magic_methods.php)，就是`+`如何用来创建TensorFlow操作的，这个机制很不错。）
- en: Especially if you’re just working with the default graph and running interactively
    in a regular REPL or a notebook, you can end up with a lot of abandoned ops in
    your graph. Every time you re-run a notebook cell that defines any graph ops,
    you aren’t just redefining ops—you’re creating new ones.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是如果你只是使用默认图并在常规REPL或笔记本中交互运行，你可能会在图中留下大量被遗弃的操作。每次重新运行定义任何图操作的笔记本单元时，你不仅仅是在重新定义操作——你是在创建新的操作。
- en: Often it’s okay to have a few extra ops floating around when you’re experimenting.
    But things can get out of hand.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验时，拥有几个额外的操作通常是可以的。但事情可能会失控。
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If `x` is a NumPy array, or just a regular Python number, this will run in constant
    memory and finish with one value for x.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`x`是一个NumPy数组，或者只是一个普通的Python数字，这将以常量内存运行，并得出一个值。
- en: But if `x` is a TensorFlow variable, there will be over a million ops in your
    TensorFlow graph, just defining a computation and not even *doing* it.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果`x`是一个TensorFlow变量，那么在你的TensorFlow图中将会有超过一百万个操作，仅仅是定义计算而已，甚至*没有执行*。
- en: One immediate fix for TensorFlow is to use a `tf.assign` op, which gives behavior
    more like what you might expect.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的一个直接修复方法是使用`tf.assign`操作，它提供了更符合你期望的行为。
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This revised version does not create any ops inside the loop, which is generally
    good advice. TensorFlow does have [control flow constructs](https://www.tensorflow.org/api_guides/python/control_flow_ops)
    including [while loops](https://www.tensorflow.org/api_docs/python/tf/while_loop).
    But only use these when really needed.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个修订版本在循环内部不创建任何操作，这通常是一个好建议。TensorFlow确实有[控制流构造](https://www.tensorflow.org/api_guides/python/control_flow_ops)，包括[while循环](https://www.tensorflow.org/api_docs/python/tf/while_loop)。但只有在真正需要时才使用这些。
- en: Be conscious of when you’re creating ops, and only create the ones you need.
    Try to keep op creation distinct from op execution. And after interactive experimentation,
    eventually get to a state, probably in a script, where you’re only creating the
    ops that you need.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建操作时要注意，只创建你需要的操作。尽量将操作创建与操作执行区分开来。在交互式实验之后，最终进入一个状态，可能是在脚本中，只创建你需要的操作。
- en: Avoid constants in the graph
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 避免图中的常量
- en: A particularly unfortunate op to needlessly add to a graph is accidental constant
    ops, especially large ones.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一种特别不幸的操作是意外常量操作，尤其是大的常量操作。
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There are a million ones in the NumPy array `many_ones`. We can add them up.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在NumPy数组`many_ones`中有一百万个1。我们可以将它们加起来。
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: What if we add them up with TensorFlow?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用TensorFlow将它们加起来呢？
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The result is the same, but the mechanism is quite different. This not only
    added some ops to the graph—it put a copy of the entire million-element array
    into the graph as a constant.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一样的，但机制完全不同。这不仅在图中添加了一些操作——它还将整个百万元素的数组作为常量放入图中。
- en: Variations on this pattern can result in accidentally loading an entire data
    set into the graph as constants. A program might still run, for small data sets.
    Or your system might fail.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式的变体可能会导致意外地将整个数据集作为常量加载到图中。程序可能仍然可以运行，适用于小数据集。或者你的系统可能会失败。
- en: One simple way to avoid storing data in the graph is to use the `feed_dict`
    mechanism.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 避免在图中存储数据的一种简单方法是使用`feed_dict`机制。
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As before, be clear about what you’re adding to the graph and when. Concrete
    data usually only enters the graph at moments of evaluation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，要明确你在何时向图中添加了什么。具体数据通常只在评估时进入图中。
- en: TensorFlow as Functional Programming
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow 作为函数式编程
- en: TensorFlow ops are like functions. This is especially clear when an op has one
    or more placeholder inputs; evaluating the op in a session is like calling a function
    with those arguments. So Python functions that return TensorFlow ops are like
    [higher-order functions](https://en.wikipedia.org/wiki/Higher-order_function).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 操作类似于函数。这在操作具有一个或多个占位符输入时尤为明显；在会话中评估操作就像用这些参数调用函数。因此，返回 TensorFlow
    操作的 Python 函数类似于[高阶函数](https://en.wikipedia.org/wiki/Higher-order_function)。
- en: You might decide that it’s worth putting a constant into the graph. For example,
    if you have to reshape a lot of tensors to 28×28, you might make an op that does
    that.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会决定将常量放入图中。例如，如果你需要将很多张量重新调整为 28×28，你可以创建一个执行此操作的操作。
- en: '[PRE12]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This is like [currying](https://en.wikipedia.org/wiki/Currying) in that the
    `shape` argument has now been set. The `[28, 28]` has become a constant in the
    graph and specified that argument. Now to evaluate `reshape_to_28` we only have
    to provide `input_tensor`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于[柯里化](https://en.wikipedia.org/wiki/Currying)，因为`shape`参数现在已经设置好了。`[28,
    28]`已成为图中的常量并指定了该参数。现在要评估`reshape_to_28`，我们只需提供`input_tensor`。
- en: Broader connections have been suggested between [neural networks, types, and
    functional programming](http://colah.github.io/posts/2015-09-NN-Types-FP/). It’s
    interesting to think of TensorFlow as a system that supports this kind of construction.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了[神经网络、类型和函数式编程](http://colah.github.io/posts/2015-09-NN-Types-FP/)之间的广泛联系。考虑
    TensorFlow 作为支持这种构建系统的系统是很有趣的。
- en: '* * *'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: I’m working on [Building TensorFlow systems from components](http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823),
    a workshop at [OSCON 2017](https://conferences.oreilly.com/oscon/oscon-tx).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在参与[从组件构建 TensorFlow 系统](http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823)，这是在
    [OSCON 2017](https://conferences.oreilly.com/oscon/oscon-tx) 上的一个研讨会。
- en: '* * *'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Find [Aaron](http://planspace.org/aaron/) on [Twitter](https://twitter.com/planarrowspace)
    | [LinkedIn](https://www.linkedin.com/in/ajschumacher) | [Google+](https://plus.google.com/112658546306232777448/)
    | [GitHub](https://github.com/ajschumacher) | [email](mailto:ajschumacher@gmail.com)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [Twitter](https://twitter.com/planarrowspace) | [LinkedIn](https://www.linkedin.com/in/ajschumacher)
    | [Google+](https://plus.google.com/112658546306232777448/) | [GitHub](https://github.com/ajschumacher)
    | [email](mailto:ajschumacher@gmail.com) 上找到 [Aaron](http://planspace.org/aaron/)
- en: '[Original](http://planspace.org/20170404-how_not_to_program_the_tensorflow_graph/).
    Reposted with permission.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](http://planspace.org/20170404-how_not_to_program_the_tensorflow_graph/)。经许可转载。'
- en: '**Bio: [Aaron Schumacher](https://www.linkedin.com/in/ajschumacher/)** is a
    Senior Data Scientist and Software Engineer at Deep Learning Analytics. He tweets
    at [@planarrowspace](https://twitter.com/planarrowspace).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介: [Aaron Schumacher](https://www.linkedin.com/in/ajschumacher/)** 是 Deep
    Learning Analytics 的高级数据科学家和软件工程师。他在 [@planarrowspace](https://twitter.com/planarrowspace)
    上发推文。'
- en: '**Related:**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关:**'
- en: '[How to Build a Recurrent Neural Network in TensorFlow](/2017/04/build-recurrent-neural-network-tensorflow.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在 TensorFlow 中构建递归神经网络](/2017/04/build-recurrent-neural-network-tensorflow.html)'
- en: '[The Gentlest Introduction to Tensorflow – Part 1](/2016/08/gentlest-introduction-tensorflow-part-1.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[最温和的 Tensorflow 入门介绍 – 第一部分](/2016/08/gentlest-introduction-tensorflow-part-1.html)'
- en: '[An Overview of Python Deep Learning Frameworks](/2017/02/python-deep-learning-frameworks-overview.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 深度学习框架概述](/2017/02/python-deep-learning-frameworks-overview.html)'
- en: More On This Topic
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[AWS AI & ML Scholarship Program Overview](https://www.kdnuggets.com/2022/09/aws-ai-ml-scholarship-program-overview.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AWS AI & ML 奖学金项目概述](https://www.kdnuggets.com/2022/09/aws-ai-ml-scholarship-program-overview.html)'
- en: '[Enroll in a 4-year Computer Science Degree Program For Free](https://www.kdnuggets.com/enroll-in-a-4-year-computer-science-degree-program-for-free)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[免费注册 4 年计算机科学学位课程](https://www.kdnuggets.com/enroll-in-a-4-year-computer-science-degree-program-for-free)'
- en: '[Join UC''s Information Session for the Master''s in Business…](https://www.kdnuggets.com/2022/10/ucincinnati-join-ucs-information-session-masters-business-analytics-program.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[加入 UC 的商业硕士信息会](https://www.kdnuggets.com/2022/10/ucincinnati-join-ucs-information-session-masters-business-analytics-program.html)'
- en: '[Maximize Your Value With The 3rd Best Online Master’s In Data…](https://www.kdnuggets.com/2023/05/bay-path-maximize-value-online-masters-data-science.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过第 3 最佳在线数据硕士课程最大化你的价值…](https://www.kdnuggets.com/2023/05/bay-path-maximize-value-online-masters-data-science.html)'
- en: '[Advance your Career with the 3rd Best Online Master''s in Data…](https://www.kdnuggets.com/2023/07/bay-path-advance-career-3rd-best-online-masters-data-science-program.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过第三名最佳在线数据科学硕士项目推动你的职业发展](https://www.kdnuggets.com/2023/07/bay-path-advance-career-3rd-best-online-masters-data-science-program.html)'
- en: '[Pursue A Master’s In Data Science With The 3rd Best Online Program](https://www.kdnuggets.com/2023/09/bay-path-pursue-masters-data-science-3rd-best-online-program)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过第三名最佳在线数据科学项目攻读硕士学位](https://www.kdnuggets.com/2023/09/bay-path-pursue-masters-data-science-3rd-best-online-program)'
