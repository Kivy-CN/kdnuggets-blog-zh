["```py\nimport pandas as pd\nimport numpy as np\nfrom os import path\nn_rows = 1_000_000\nn_cols = 1000\nfor i in range(1, 3):\n    filename = 'analysis_%d.csv' % i\n    file_path = path.join('csv_files', filename)\n    df = pd.DataFrame(np.random.uniform(0, 100, size=(n_rows, n_cols)), columns=['col%d' % i for i in range(n_cols)])\n    print('Saving', file_path)\n    df.to_csv(file_path, index=False)\ndf.head()\n\n```", "```py\n# filtering with a single column\ndf[df.col2 > 10]\n\n```", "```py\nimport glob\nimport vaex\ncsv_files = glob.glob('csv_files/*.csv')\nfor i, csv_file in enumerate(csv_files, 1):\n    for j, dv in enumerate(vaex.from_csv(csv_file, chunk_size=5_000_000), 1):\n        print('Exporting %d %s to hdf5 part %d' % (i, csv_file, j))\n        dv.export_hdf5(f'hdf5_files/analysis_{i:02}_{j:02}.hdf5')\n\n```", "```py\ndv = vaex.open('hdf5_files/*.hdf5')\n\n```", "```py\ndv.head()\n\n```", "```py\nquantile = dv.percentile_approx('col1', 10)\n\n```", "```py\ndv[‘col1_binary’] = dv.col1 > dv.percentile_approx(‘col1’, 10)\n\n```", "```py\ndv = dv[dv.col2 > 10]\n\n```", "```py\ngroup_res = dv.groupby(by=dv.col1_binary, agg={'col3_mean': vaex.agg.mean('col3')})\n\n```", "```py\nplot = dv.plot1d(dv.col3, what='count(*)', limits=[0, 100])\n\n```", "```py\nsuma = np.sum(dv.sum(dv.column_names))\n\n```", "```py\nimport dask.dataframe as dd\nds = dd.read_csv('csv_files/*.csv')\nds.to_hdf('hdf5_files_dask/analysis_01_01.hdf5', key='table')\n\n```", "```py\nimport dask.dataframe as dd\n\nds = dd.read_csv('csv_files/*.csv')\n\n```", "```py\nds.head()\n\n```", "```py\nquantile = ds.col1.quantile(0.1).compute()\n\n```", "```py\nds['col1_binary'] = ds.col1 > ds.col1.quantile(0.1)\n\n```", "```py\nds = ds[(ds.col2 > 10)]\n\n```", "```py\ngroup_res = ds.groupby('col1_binary').col3.mean().compute()\n\n```", "```py\nplot = ds.col3.compute().plot.hist(bins=64, ylim=(13900, 14400))\n\n```", "```py\nsuma = ds.sum().sum().compute()\n\n```"]