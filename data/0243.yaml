- en: Uncertainty Quantification in Artificial Intelligence-based Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能系统中的不确定性量化
- en: 原文：[https://www.kdnuggets.com/2022/04/uncertainty-quantification-artificial-intelligencebased-systems.html](https://www.kdnuggets.com/2022/04/uncertainty-quantification-artificial-intelligencebased-systems.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/04/uncertainty-quantification-artificial-intelligencebased-systems.html](https://www.kdnuggets.com/2022/04/uncertainty-quantification-artificial-intelligencebased-systems.html)
- en: Abstract
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: While Artificial Intelligence (AI) based systems hold great promise and are
    increasingly employed to assist in a variety of complex tasks, the results are
    not fully reliable due to the challenges introduced by uncertainty. Uncertainty
    quantification (UQ) plays a pivotal role in the reduction of uncertainties during
    both optimization and decision making, applied to solve a variety of real-world
    applications in science, business, and engineering. This article expediently presents
    the concepts of uncertainty, its sources, types, and measurements. The article
    summarizes the plethora of UQ methods using Bayesian techniques, shows issues
    and gaps in the literature, suggests further directions, and epitomizes AI-based
    systems within the Financial Crime domain.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于人工智能（AI）的系统前景广阔，并且越来越多地被用于协助各种复杂任务，但由于不确定性带来的挑战，这些结果并不完全可靠。不确定性量化（UQ）在优化和决策过程中减少不确定性方面发挥了关键作用，应用于解决科学、商业和工程中的各种现实世界应用。本文简明扼要地介绍了不确定性的概念、来源、类型和测量。文章总结了使用贝叶斯技术的多种UQ方法，展示了文献中的问题和不足，提出了进一步的方向，并概述了金融犯罪领域的人工智能系统。
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In recent years, there has been an increased need for the use of AI-based systems
    which by nature are active systems which required to act automatically based on
    events or changes in the environment. Such systems span many areas, from active
    databases to applications that drive the core business processes of today's enterprises.
    However, in many cases, the events to which the system must respond are not generated
    by monitoring tools but must be inferred from other events based on complex temporal
    predicates. Machine Learning (ML) models generate an optimal solution based on
    their training data. In many applications, such inference is inherently uncertain.
    However, if the uncertainty in the data and the model parameters are not considered,
    such optimal solutions have a high risk of failure in actual world deployment.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，对基于人工智能的系统的需求增加，这些系统本质上是主动的，需要根据事件或环境变化自动行动。这些系统涉及多个领域，从主动数据库到驱动现代企业核心业务流程的应用。然而，在许多情况下，系统必须响应的事件不是由监控工具生成的，而是需要基于复杂的时间谓词从其他事件中推断。机器学习（ML）模型基于其训练数据生成最佳解决方案。在许多应用中，这种推断本质上是不确定的。然而，如果不考虑数据和模型参数中的不确定性，这些最佳解决方案在实际世界中的部署风险很高。
- en: Typical AI-based system pipelines consist of collecting data, pre-processing
    it, selecting a model to learn from the data, choosing a learning algorithm to
    train a selected model, and drawing inferences from the learned model. However,
    there are inherent uncertainties associated with each of these steps. For example,
    data uncertainty may arise from the inability to collect or represent real-world
    data reliably.  Flaws in data pre-processing whether, during curation, cleaning,
    or labeling also create data uncertainty. As models only serve as a proxy for
    the real world and learning and inference algorithms rely on various simplifying
    assumptions, they introduce modeling and inferential uncertainties. The predictions
    made by an AI system are susceptible to all these sources of uncertainty. Reliable
    uncertainty estimates provide a vital diagnostic for both developers and users
    of an AI system. For example, high data uncertainty may point towards improving
    the data representation process, while a high model uncertainty may suggest the
    need to collect more data. For users, accurate uncertainties, especially when
    combined with effective communication strategies, can add a critical layer of
    transparency and trust, crucial for better AI-assisted decision making. Such trust
    in AI systems is essential for their reliable deployment in high-stakes applications
    spanning medicine, finance, and the social sciences.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的基于人工智能的系统流程包括收集数据、对数据进行预处理、选择一个模型以从数据中学习、选择一个学习算法来训练选定的模型，并从学习到的模型中得出推论。然而，这些步骤中固有的不确定性是不可避免的。例如，数据不确定性可能源于无法可靠地收集或表示真实世界的数据。数据预处理中的缺陷，无论是在策划、清理还是标注过程中，也会产生数据不确定性。由于模型仅作为真实世界的代理，而学习和推理算法依赖于各种简化假设，因此它们引入了建模和推理不确定性。人工智能系统的预测容易受到这些不确定性来源的影响。可靠的不确定性估计为人工智能系统的开发者和用户提供了重要的诊断信息。例如，高数据不确定性可能指向需要改进数据表示过程，而高模型不确定性可能建议需要收集更多数据。对于用户来说，准确的不确定性，尤其是当与有效的沟通策略相结合时，可以增加透明度和信任，这对更好的人工智能辅助决策至关重要。在医学、金融和社会科学等高风险应用中，对人工智能系统的这种信任对于其可靠部署至关重要。
- en: 'These observations have revitalized my interest in UQ research. Many approaches
    have been proposed for improved UQ in AI systems, however, choosing a particular
    UQ method depends on many factors: the underlying model, type of machine learning
    task (regression vs. classification vs segmentation), characteristics of the data,
    transparency of the machine learning models and final objectives. If inappropriately
    used, a particular UQ method may produce poor uncertainty estimates and mislead
    users. Moreover, even a highly accurate uncertainty estimate may be misleading
    if poorly communicated.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这些观察重新激发了我对不确定性量化（UQ）研究的兴趣。虽然已经提出了许多改进人工智能系统中UQ的方法，但选择特定的UQ方法取决于许多因素：基础模型、机器学习任务的类型（回归、分类或分割）、数据的特征、机器学习模型的透明性和最终目标。如果不当使用，特定的UQ方法可能会产生不良的不确定性估计，并误导用户。此外，即使是非常准确的不确定性估计，如果沟通不善，也可能产生误导。
- en: This article presents an extended introduction to types of uncertainty and characterize
    its sources, discusses the UQ approaches, formalizes uncertain modeling, and articulates
    its notion upon complex systems. The article overviews the different approaches
    used in ML to quantify uncertainty using Bayesian techniques. In addition, there
    is a focus on the evaluation of uncertainty measurements in different machine
    learning tasks such as classification, regression, and segmentation. The article
    provides a term of calibration within UQ methods, enlists open gaps in the literature,
    demonstrates UQ in a real-world application within the financial crime domain,
    and formulates a generic evaluation framework for such systems.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了对不确定性类型的扩展介绍，并描述了其来源，讨论了UQ方法，形式化了不确定建模，并阐述了其在复杂系统中的概念。文章概述了在机器学习中使用贝叶斯技术量化不确定性的不同方法。此外，还重点关注了在不同机器学习任务（如分类、回归和分割）中不确定性测量的评估。本文提供了UQ方法中的校准术语，列出了文献中的开放问题，展示了在金融犯罪领域的实际应用中的UQ，并制定了这种系统的通用评估框架。
- en: Aleatoric Uncertainty
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机不确定性
- en: Aleatoric uncertainty (aka statistical uncertainty), and is representative of
    unknowns that differ each time we run the same experiment. Aleatoric uncertainty
    refers to the inherent uncertainty due to probabilistic variability. This type
    of uncertainty is irreducible, in that there will always be variability in the
    underlying variables. These uncertainties are characterized by a probability distribution.
    For example, a single arrow shot with a mechanical bow that exactly duplicates
    each launch (the same acceleration, altitude, direction, and final velocity) will
    not all impact the same point on the target due to random and complicated vibrations
    of the arrow shaft, the knowledge of which cannot be determined sufficiently to
    eliminate the resulting scatter of impact points.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随机性不确定性（即统计不确定性），是指每次运行相同实验时所出现的未知因素的代表。随机性不确定性指的是由于概率变异性引起的固有不确定性。这种类型的不确定性是不可减少的，因为底层变量总会存在变异。这些不确定性由概率分布特征化。例如，用机械弓射出的单箭，虽然每次发射（相同的加速度、高度、方向和最终速度）都完全重复，但由于箭杆的随机和复杂的振动，箭矢的撞击点不会都落在目标的同一点上，而对这些振动的知识不足以消除撞击点的散布。
- en: Epistemic Uncertainty
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 认识论不确定性
- en: Epistemic uncertainty (aka systematic uncertainty), and is due to things one
    could in principle know but does not in practice. Epistemic uncertainty is the
    scientific uncertainty in the model of the process. It is due to limited data
    and knowledge. The epistemic uncertainty is characterized by alternative models.
    For discrete random variables, the epistemic uncertainty is modeled by alternative
    probability distributions. An example of a source of this uncertainty would be
    the pull in an experiment designed to measure the acceleration of gravity near
    the earth's surface. The commonly used gravitational acceleration of 9.8 m/s²
    ignores the effects of air resistance, but the air resistance for the object could
    be measured and incorporated into the experiment to reduce the resulting uncertainty
    in the calculation of the gravitational acceleration.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 认识论不确定性（即系统性不确定性），是由于一些原则上可以知道但实际上不知道的事情所致。认识论不确定性是对过程模型中的科学不确定性的体现。这是由于数据和知识的有限性造成的。认识论不确定性通过替代模型来表征。对于离散随机变量，认识论不确定性通过替代概率分布进行建模。这种不确定性的一个例子是设计用来测量接近地球表面引力的实验中的拉力。常用的9.8
    m/s²的引力加速度忽略了空气阻力的影响，但可以测量物体的空气阻力并将其纳入实验中，从而减少计算引力加速度的结果不确定性。
- en: Aleatoric and Epistemic Uncertainty Interaction
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机性和认识论不确定性互动
- en: Aleatoric and epistemic uncertainty can also occur simultaneously in a single
    term e.g. when experimental parameters show aleatoric uncertainty, and those experimental
    parameters are input to a computer simulation. If then for the uncertainty quantification
    a surrogate model, e.g. a Gaussian process or a [Polynomial Chaos Expansion](https://en.wikipedia.org/wiki/Polynomial_chaos#:~:text=Polynomial%20chaos%20(PC)%2C%20also,function%20of%20other%20random%20variables.),
    is learned from computer experiments, this surrogate exhibits epistemic uncertainty
    that depends on or interacts with the aleatoric uncertainty of the experimental
    parameters. Such uncertainty cannot solely be classified as aleatoric or epistemic
    anymore but is a more general inferential uncertainty. In real-life applications,
    both kinds of uncertainties are present. Uncertainty quantification intends to
    explicitly express both types of uncertainty separately.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随机性和认识论不确定性也可以在单一术语中同时出现，例如，当实验参数表现出随机性不确定性，并且这些实验参数被输入到计算机模拟中时。如果在不确定性量化中使用了替代模型，例如高斯过程或[多项式混沌展开](https://en.wikipedia.org/wiki/Polynomial_chaos#:~:text=Polynomial%20chaos%20(PC)%2C%20also,function%20of%20other%20random%20variables.)，从计算机实验中学习到的这个替代模型会展现出依赖于或与实验参数的随机性不确定性相互作用的认识论不确定性。这种不确定性不能单独归类为随机性或认识论不确定性，而是更一般的推理不确定性。在实际应用中，这两种不确定性同时存在。不确定性量化旨在明确地分别表达这两种不确定性。
- en: The quantification for the aleatoric uncertainties can be relatively straightforward,
    where traditional (frequentist) probability is the most basic form. Techniques
    such as the Monte Carlo method are frequently used. To evaluate epistemic uncertainties,
    efforts are made to understand the lack of knowledge of the system, process, or
    mechanism. Epistemic uncertainty is generally understood through the lens of Bayesian
    probability, where probabilities are interpreted as indicating how certain a rational
    person could be regarding a specific claim.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 aleatoric 不确定性的量化可以相对直接，传统的（频率学派）概率是最基本的形式。蒙特卡罗方法等技术经常被使用。为了评估 epistemic
    不确定性，努力去理解系统、过程或机制的知识缺乏。Epistemic 不确定性通常通过贝叶斯概率的视角来理解，其中概率被解释为表示理性人对特定主张的确定程度。
- en: Model and Data Uncertainty
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型和数据不确定性
- en: The model uncertainty covers the uncertainty that is caused by shortcomings
    in the model, either by errors in the training procedure, an insufficient model
    structure, or lack of knowledge due to unknown samples or a bad coverage of the
    training data set. In contrast to this, data uncertainty is related to uncertainty
    that directly stems from the data. Data uncertainty is caused by information loss
    when representing the real world within a data sample and representing the distribution.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 模型不确定性涵盖了由于模型不足所造成的不确定性，这些不足可能是由于训练过程中的错误、模型结构的不充分，或者由于样本未知或训练数据集覆盖不佳导致的知识缺乏。与此相比，数据不确定性与直接源自数据的不确定性相关。数据不确定性是由于在数据样本中表示真实世界和分布时的信息丢失所造成的。
- en: For example, in regression tasks noise in the input and target measurements
    causes data uncertainty that the network cannot learn to correct. In classification
    tasks, samples that do not contain enough information to identify one class with
    100% certainty cause data uncertainty on the prediction. The information loss
    is a result of the measurement system, e.g. by representing real-world information
    by image pixels with a specific resolution, or by errors in the labeling process.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在回归任务中，输入和目标测量中的噪声导致了数据不确定性，网络无法学会纠正。在分类任务中，样本没有足够的信息来以100%确定性识别一个类别，这会导致预测中的数据不确定性。信息丢失是测量系统的结果，例如通过特定分辨率的图像像素表示真实世界的信息，或通过标记过程中的错误。
- en: While model uncertainty can be (theoretically) reduced by improving the architecture,
    the learning process, or the training data set, the data uncertainties cannot
    be explained away.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然模型不确定性可以通过改进架构、学习过程或训练数据集（理论上）来减少，但数据不确定性是无法通过解释来消除的。
- en: Predictive Uncertainty
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测不确定性
- en: 'On the basis of the input data domain, the predictive uncertainty can also
    be classified into three main classes:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基于输入数据领域，预测不确定性还可以分为三大类：
- en: 'In-domain uncertainty: represents the uncertainty related to an input drawn
    from a data distribution assumed to be equal to the training data distribution.
    The in-domain uncertainty stems from the inability of the deep neural network
    to explain an in-domain sample due to a lack of in-domain knowledge. From a modeler
    point of view, in domain uncertainty is caused by design errors (model uncertainty)
    and the complexity of the problem at hand (data uncertainty). Depending on the
    source of the in-domain uncertainty, it might be reduced by increasing the quality
    of the training data (set) or the training process.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 域内不确定性：代表了与从假定为等于训练数据分布的数据分布中抽取的输入相关的不确定性。域内不确定性源于深度神经网络由于缺乏域内知识而无法解释域内样本。从建模者的角度来看，域内不确定性由设计错误（模型不确定性）和问题的复杂性（数据不确定性）造成。根据域内不确定性的来源，可以通过提高训练数据（集）或训练过程的质量来减少它。
- en: 'Domain-shift uncertainty: denotes the uncertainty related to an input drawn
    from a shifted version of the training distribution. The distribution shift results
    from insufficient coverage by the training data and the variability inherent to
    real-world situations. A domain shift might increase the uncertainty due to the
    inability of the DNN to explain the domain shift sample-based samples at training
    time. Some errors causing domain shift uncertainty can be modeled and can therefore
    be reduced.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 域迁移不确定性：表示与从训练分布的偏移版本中抽取的输入相关的不确定性。分布偏移源于训练数据的覆盖不足以及真实世界情况固有的变异性。域迁移可能增加不确定性，因为
    DNN 无法解释训练时基于样本的域迁移样本。一些导致域迁移不确定性的错误可以被建模，因此可以减少。
- en: 'Out-of-domain uncertainty: represents the uncertainty related to an input drawn
    from the subspace of unknown data. The distribution of unknown data is different
    and far from the training distribution. For example, when domain-shift uncertainty
    describes phenomena like a blurred picture of a dog, out-of-domain uncertainty
    describes the case when a network that learned to classify cats and dogs is asked
    to predict a bird. The out-of-domain uncertainty stems from the inability of the
    Deep Neural Networks (DNN) to explain an out-of-domain sample due to its lack
    of out-of-domain knowledge. From a modeler point of view, out-of-domain uncertainty
    is caused by input samples, where the network is not meant to give a prediction
    for or by insufficient training data.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨领域不确定性：表示与从未知数据子空间中提取的输入相关的不确定性。未知数据的分布与训练分布不同且相差甚远。例如，当领域转移不确定性描述像模糊的狗照片这样的现象时，跨领域不确定性描述的是当一个学习了猫和狗分类的网络被要求预测一只鸟的情况。跨领域不确定性源于深度神经网络（DNN）由于缺乏跨领域知识而无法解释跨领域样本。从建模者的角度来看，跨领域不确定性由输入样本造成，这些样本网络并未用于预测，或由训练数据不足造成。
- en: '![Uncertainty types](../Images/6c3c346df9ccf8e86bbfbec46336d0cd.png) Figure
    1: Uncertainty types'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![不确定性类型](../Images/6c3c346df9ccf8e86bbfbec46336d0cd.png) 图1：不确定性类型'
- en: Uncertainty versus Variability
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不确定性与变异性
- en: Technical professionals are often asked to estimate "ranges" for uncertain quantities.
    It is important that they distinguish whether they are being asked for variability
    ranges or uncertainty ranges. Likewise, it is important for modelers to know if
    they are building models of variability or uncertainty, and their relationship,
    if any.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 技术专家经常被要求对不确定量估计“范围”。重要的是要区分他们是否被要求提供变异范围或不确定范围。同样，模型构建者需要知道他们是在建立变异模型还是不确定模型，并了解其关系（如果有的话）。
- en: '**Sources of Uncertainty**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**不确定性的来源**'
- en: 'Parameter uncertainty: It comes from the model parameters that are input to
    the mathematical model but whose exact values are unknown to experimentalists
    and cannot be controlled in physical experiments, or whose values cannot be exactly
    inferred by statistical methods. For example, local free-fall acceleration in
    a falling object experiment.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数不确定性：来源于输入到数学模型中的模型参数，但实验人员的确切值未知，无法在物理实验中控制，或无法通过统计方法准确推断其值。例如，掉落物体实验中的局部自由落体加速度。
- en: 'Parametric variability: it comes from the variability of input variables of
    the model. For example, the dimensions in a datum may not be exactly as assumed,
    which would cause variability in the performance of the model trained on this
    high-dimensional dataset.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数变异性：来源于模型输入变量的变异。例如，数据中的尺寸可能与假设的不完全相同，这会导致在这个高维数据集上训练的模型表现出变异性。
- en: 'Structural uncertainty: aka model inadequacy, model bias, or model discrepancy,
    It comes from the lack of knowledge of the underlying physics or principles in
    the problem. It depends on how accurately a mathematical model describes the true
    system for a real-life situation, considering the fact that models are almost
    always only approximations to reality. One example is when modeling the process
    of a falling object using the free-fall model; the model itself is inaccurate
    since there always exists air friction. In this case, even if there is no unknown
    parameter in the model, a discrepancy is still expected between the model and
    true physics. Structural uncertainty is present when we are uncertain about the
    model output because we are uncertain about the functional form of the model.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构性不确定性：也称为模型不适应性、模型偏差或模型差异，来源于对问题中基础物理或原则知识的缺乏。它取决于数学模型在现实情况中对真实系统的描述准确程度，考虑到模型几乎总是对现实的近似。例如，当使用自由落体模型对掉落物体过程进行建模时，由于存在空气阻力，模型本身是不准确的。在这种情况下，即使模型中没有未知参数，模型和真实物理之间仍然会有差异。结构性不确定性存在于我们对模型输出不确定时，因为我们对模型的函数形式不确定。
- en: 'Algorithmic uncertainty: aka numerical uncertainty, or discrete uncertainty.
    This type comes from numerical errors and numerical approximations per implementation
    of the computer model. Most models are too complicated to solve exactly. For example,
    the finite element method or finite difference method may be used to approximate
    the solution of a partial differential equation (which introduces numerical errors).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法不确定性：也称为数值不确定性或离散不确定性。这种类型的不确定性来源于数值错误和计算机模型实现中的数值近似。大多数模型过于复杂，无法精确求解。例如，有限元法或有限差分法可能用于近似求解偏微分方程（这会引入数值错误）。
- en: 'Experimental uncertainty: aka observation error. It comes from the variability
    of experimental measurements. The experimental uncertainty is inevitable and can
    be noticed by repeating a measurement many times using exactly the same settings
    for all inputs/variables.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验不确定性：也称为观测误差。它源于实验测量的变异性。实验不确定性是不可避免的，可以通过多次重复测量并使用完全相同的设置来注意到。
- en: 'Interpolation uncertainty: This comes from a lack of available data collected
    from model simulations and/or experimental measurements. For other input settings
    that don''t have simulation data or experimental measurements, one must interpolate
    or extrapolate in order to predict the corresponding responses.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 插值不确定性：这来源于从模型模拟和/或实验测量中收集的可用数据的不足。对于没有模拟数据或实验测量的其他输入设置，必须进行插值或外推以预测相应的响应。
- en: Types of Problems
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题类型
- en: 'There are two major types of problems in uncertainty quantification: one is
    the forward propagation of uncertainty (where the various sources of uncertainty
    are propagated through the model to predict the overall uncertainty in the system
    response) and the other is the inverse assessment of model uncertainty and parameter
    uncertainty (where the model parameters are calibrated simultaneously using test
    data).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性量化中有两种主要问题：一种是前向不确定性传播（即将各种不确定性来源通过模型传播以预测系统响应中的整体不确定性），另一种是模型不确定性和参数不确定性的逆向评估（即使用测试数据同时标定模型参数）。
- en: '**Forward** ([propagation of uncertainty](https://en.wikipedia.org/wiki/Propagation_of_uncertainty))'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**前向** ([propagation of uncertainty](https://en.wikipedia.org/wiki/Propagation_of_uncertainty))'
- en: 'Uncertainty propagation is the quantification of uncertainties in system output(s)
    propagated from uncertain inputs. It focuses on the influence on the outputs from
    the parametric variability listed in the sources of uncertainty. The targets of
    uncertainty propagation analysis can be:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性传播是将不确定输入引起的不确定性量化到系统输出中。它关注于不确定性来源中列出的参数变异性对输出的影响。不确定性传播分析的目标可以是：
- en: Evaluate low-order moments of the outputs, i.e. mean and variance
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估输出的低阶矩，即均值和方差
- en: Evaluate the reliability of the outputs
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估输出的可靠性
- en: Assess the complete probability distribution of the outputs
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估输出的完整概率分布
- en: '**Inverse** ([inverse problem](https://en.wikipedia.org/wiki/Inverse_problem))'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**逆向** ([inverse problem](https://en.wikipedia.org/wiki/Inverse_problem))'
- en: Given some experimental measurements of a system and some computer simulation
    results from its mathematical model, inverse uncertainty quantification estimates
    the discrepancy between the experiment and the mathematical model (which is called
    bias correction) and estimates the values of unknown parameters in the model if
    there are any (which is called parameter calibration or simply calibration).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 给定系统的一些实验测量值和来自其数学模型的一些计算机模拟结果，逆向不确定性量化估计实验与数学模型之间的差异（即偏差修正），并估计模型中未知参数的值（即参数标定或简单地称为标定）。
- en: 'Generally, this is a much more difficult problem than forwarding uncertainty
    propagation; however, it is of great importance since it is typically implemented
    in a model updating process. There are several scenarios in inverse uncertainty
    quantification:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这比前向不确定性传播问题要困难得多；然而，它非常重要，因为它通常在模型更新过程中实施。逆向不确定性量化有几种情境：
- en: 'Bias correction only: bias correction quantifies the model inadequacy, i.e.
    the discrepancy between the experiment and the mathematical model'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅偏差修正：偏差修正量化模型的不适当性，即实验与数学模型之间的差异
- en: 'Parameter calibration only: parameter calibration estimates the values of one
    or more unknown parameters in a mathematical model.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参数标定仅：参数标定估计数学模型中一个或多个未知参数的值。
- en: 'Bias correction and parameter calibration: considers an inaccurate model with
    one or more unknown parameters, and its model updating formulation combines the
    two together: It is the most comprehensive model updating formulation that includes
    all possible sources of uncertainty, and it requires the most effort to solve.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 偏差校正和参数校准：考虑一个具有一个或多个未知参数的不准确模型，其模型更新形式将两者结合起来：这是最全面的模型更新形式，涵盖所有可能的不确定性来源，且需要最多的解决工作。
- en: '![Types of Problems within uncertainty quantification](../Images/7897631f8ff6a998ab71d7f305c1b6c2.png)
    Figure 2: Types of Problems within uncertainty quantification'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![不确定性量化中的问题类型](../Images/7897631f8ff6a998ab71d7f305c1b6c2.png) 图 2: 不确定性量化中的问题类型'
- en: Mathematical Formalization
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数学形式化
- en: 'As we say previously (Figure 1), predictive uncertainty consists of two parts:
    epistemic uncertainty, and  aleatoric uncertainty, which can be written as a sum
    of these two parts:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所述（图 1），预测不确定性由两部分组成：认知不确定性和*随机不确定性*，可以表示为这两部分的和：
- en: '![XXXXX](../Images/2a448d94ad432369453bf6b08e6ad3e9.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/2a448d94ad432369453bf6b08e6ad3e9.png)'
- en: Epistemic uncertainties can be formulated as a probability distribution over
    model parameters.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 认知不确定性可以被表述为对模型参数的概率分布。
- en: Let
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 设
- en: '![XXXXX](../Images/d61d9b41b27d3b9a6573d04381fcb7fc.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/d61d9b41b27d3b9a6573d04381fcb7fc.png)'
- en: denotes a training dataset with inputs
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 表示具有输入的训练数据集
- en: '![XXXXX](../Images/fbf147decb536dd3743e4a7107c5bcb1.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/fbf147decb536dd3743e4a7107c5bcb1.png)'
- en: where *C* represents the number of classes. The aim is to optimize the parameters
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *C* 表示类别数。目标是优化参数
- en: '![XXXXX](../Images/bb31d7fa39f8d9299b96485bfb2c2503.png)![XXXXX](../Images/d2f057cd10c5d4c44cbc65b23ab8ca6a.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/bb31d7fa39f8d9299b96485bfb2c2503.png)![XXXXX](../Images/d2f057cd10c5d4c44cbc65b23ab8ca6a.png)'
- en: 'For classification, the softmax likelihood can be used:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，可以使用 softmax 似然函数：
- en: '![XXXXX](../Images/420e87e39a215caf5e3573e5319ecb15.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/420e87e39a215caf5e3573e5319ecb15.png)'
- en: Eq.1
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Eq.1
- en: 'and the Gaussian likelihood can be assumed for regression:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，可以假设高斯似然函数：
- en: '![XXXXX](../Images/aac9e8da75f6fe27305e50740b665dcb.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/aac9e8da75f6fe27305e50740b665dcb.png)'
- en: Eq.2
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Eq.2
- en: '![XXXXX](../Images/2ebb528a6c65684f4670c31d371f220b.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/2ebb528a6c65684f4670c31d371f220b.png)'
- en: 'By applying Bayes’ theorem can be written as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用贝叶斯定理可以写成如下形式：
- en: '![XXXXX](../Images/804d6c3ba4809d62432771207d50c957.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/804d6c3ba4809d62432771207d50c957.png)'
- en: Eq.3
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Eq.3
- en: '![XXXXX](../Images/416e9f9c635e06598698649077abcca7.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/416e9f9c635e06598698649077abcca7.png)'
- en: '![XXXXX](../Images/1d0404f81cd048fbfc6c096d3cd07117.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/1d0404f81cd048fbfc6c096d3cd07117.png)'
- en: Eq.4
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Eq.4
- en: This process is called inference or marginalization. However,
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程称为推断或边际化。然而，
- en: '![XXXXX](../Images/84c09ec8626d73bba595db710a9e5a87.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/84c09ec8626d73bba595db710a9e5a87.png)'
- en: cannot be computed analytically, but it can be approximated by variational parameters
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 不能通过解析方法计算，但可以通过变分参数来近似
- en: '![XXXXX](../Images/612681c3a98057feaaf58fc7f26ab2c0.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/612681c3a98057feaaf58fc7f26ab2c0.png)'
- en: 'The aim is to approximate a distribution that is close to the posterior distribution
    obtained by the model. As such, the Kullback-Leibler (KL) divergence is needed
    to be minimized with regard to ?. The level of similarity among the two distributions
    can be measured as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是近似一个与模型获得的后验分布接近的分布。因此，需要最小化与?的 Kullback-Leibler (KL) 散度。两分布之间的相似度可以如下测量：
- en: '![Eq.5](../Images/7f0a5281e6330144441ea952f4669786.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![Eq.5](../Images/7f0a5281e6330144441ea952f4669786.png)'
- en: Eq.5
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Eq.5
- en: 'The predictive distribution can be approximated by minimizing KL divergence,
    as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 预测分布可以通过最小化 KL 散度来进行近似，如下所示：
- en: '![XXXXX](../Images/045606288218f0d26c0ead25b4027755.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/045606288218f0d26c0ead25b4027755.png)'
- en: Eq.6
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Eq.6
- en: where
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![XXXXX](../Images/01fc5e99ba0117c6ddb92398aacb03a2.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/01fc5e99ba0117c6ddb92398aacb03a2.png)'
- en: 'indicates the optimized objective. KL divergence minimization can also be rearranged
    into the *evidence lower bound* (ELBO) maximization:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表示优化的目标。KL 散度最小化也可以重新排列为*证据下界* (ELBO) 最大化：
- en: '![Eq.7](../Images/0160825c0c393a04087d25ffea3064c9.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![Eq.7](../Images/0160825c0c393a04087d25ffea3064c9.png)'
- en: Eq.7
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Eq.7
- en: where
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![XXXXX](../Images/612681c3a98057feaaf58fc7f26ab2c0.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/612681c3a98057feaaf58fc7f26ab2c0.png)'
- en: 'is able to describe the data well by maximizing the first term, and being as
    close as possible to the prior by minimizing the second term. This process is
    called variational inference (VI). Dropout VI is one of the most common approaches
    that has been widely used to approximate inference in complex models. The minimization
    objective is as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最大化第一项并尽可能最小化第二项来很好地描述数据。这个过程称为变分推断（VI）。Dropout VI 是一种最常见的方法，被广泛用于近似复杂模型中的推断。最小化目标如下：
- en: '![Eq.8](../Images/e78c8b271aa117e8ff5685435dd9b83e.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![Eq.8](../Images/e78c8b271aa117e8ff5685435dd9b83e.png)'
- en: Eq.8
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Eq.8
- en: 'where *N* and *P* represent the number of samples and dropout probability,
    respectively. To obtain data-dependent uncertainty, the precision ? in (Eq.2)
    can be formulated as a function of data. One approach to obtain epistemic uncertainty
    is to mix two functions:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *N* 和 *P* 分别代表样本数量和 dropout 概率。为了获得数据依赖的不确定性，可以将精度 ? 在（Eq.2）中制定为数据的函数。获得认识性不确定性的一种方法是混合两个函数：
- en: '![XXXXX](../Images/f98c43ce329d6b8c7018df06484830ae.png)![XXXXX](../Images/3e6791bfbd5d4b372ef36d3a482a634c.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/f98c43ce329d6b8c7018df06484830ae.png)![XXXXX](../Images/3e6791bfbd5d4b372ef36d3a482a634c.png)'
- en: A prior distribution is placed over the weights of the model, and then the amount
    of change in the weights for given data samples is computed. The Euclidian distance
    loss function can be adapted as follows:![Eq.9](../Images/a6b005c34b1b23ddf318341b9424a094.png)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对模型的权重施加先验分布，然后计算在给定数据样本下权重的变化量。欧几里得距离损失函数可以如下调整：![Eq.9](../Images/a6b005c34b1b23ddf318341b9424a094.png)
- en: Eq.9
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Eq.9
- en: 'The predictive variance can be obtained as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 预测方差可以如下获得：
- en: '![Eq.10](../Images/890956d81106099b72720714dcf0a42f.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![Eq.10](../Images/890956d81106099b72720714dcf0a42f.png)'
- en: Eq.10
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Eq.10
- en: Selective Methodologies
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择性方法
- en: Much research has been done to solve uncertainty quantification problems, though
    a majority of them deal with uncertainty propagation. During the past one to two
    decades, a number of approaches for inverse uncertainty quantification problems
    have also been developed and have proved to be useful for most small- to medium-scale
    problems.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已进行了大量研究来解决不确定性量化问题，但大多数研究处理的是不确定性传播。在过去的一到两十年里，也开发了许多针对逆不确定性量化问题的方法，并证明对大多数小到中等规模的问题非常有用。
- en: '![Figure 3: Selective methodologies for uncertainty quantification](../Images/bd9a7340a075c12d5fbe03ee953644d8.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图 3：不确定性量化的选择性方法](../Images/bd9a7340a075c12d5fbe03ee953644d8.png)'
- en: 'Figure 3: Selective methodologies for uncertainty quantification'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：不确定性量化的选择性方法
- en: Forward Propagation
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前向传播
- en: 'Simulation-based methods: Monte Carlo simulations, importance sampling, adaptive
    sampling, etc.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于模拟的方法：蒙特卡洛模拟、重要性采样、自适应采样等。
- en: 'General surrogate-based methods: In a non-intrusive approach, a surrogate model
    is learned in order to replace the experiment or the simulation with a cheap and
    fast approximation. Surrogate-based methods can also be employed in a fully Bayesian
    fashion. This approach has proven particularly powerful when the cost of sampling,
    e.g. computationally expensive simulations, is prohibitively high.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般替代模型方法：在非侵入式方法中，学习一个替代模型以用便宜和快速的近似代替实验或模拟。替代模型方法也可以以完全贝叶斯方式使用。这种方法在采样成本极高（例如计算昂贵的模拟）时特别有效。
- en: 'Local expansion-based methods: Taylor series, perturbation method, etc. These
    methods have advantages when dealing with relatively small input variability and
    outputs that don''t express high nonlinearity. These linear or linearized methods
    are detailed in the article Uncertainty propagation.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部展开方法：泰勒级数、扰动方法等。这些方法在处理相对较小的输入变异性和输出不表现高非线性的情况下具有优势。这些线性或线性化方法在《不确定性传播》一文中有详细介绍。
- en: 'Functional expansion-based methods: Neumann expansion, orthogonal or Karhunen–Loeve
    expansions (KLE), with polynomial chaos expansion (PCE) and wavelet expansions
    as special cases.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于函数展开的方法：纽曼展开、正交或 Karhunen–Loeve 展开（KLE），以及多项式混沌展开（PCE）和小波展开作为特例。
- en: 'Most probable point (MPP)-based methods: first-order reliability method (FORM)
    and second-order reliability method (SORM).'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最可能点（MPP）方法：一阶可靠性方法（FORM）和二阶可靠性方法（SORM）。
- en: 'Numerical integration-based methods: Full factorial numerical integration (FFNI)
    and dimension reduction (DR).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于数值积分的方法：完全因子数值积分（FFNI）和维度减少（DR）。
- en: For non-probabilistic approaches, interval analysis, Fuzzy theory, possibility
    theory, and evidence theory are among the most widely used.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非概率方法，区间分析、模糊理论、可能性理论和证据理论是最广泛使用的方法之一。
- en: The probabilistic approach is considered the most rigorous approach to uncertainty
    analysis in engineering design due to its consistency with the theory of decision
    analysis. Its cornerstone is the calculation of probability density functions
    for sampling statistics. This can be performed rigorously for random variables
    that are obtainable as transformations of Gaussian variables, leading to exact
    confidence intervals.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 概率方法被认为是工程设计中不确定性分析最严格的方法，因为它与决策分析理论一致。其核心是计算用于采样统计的概率密度函数。这可以对可以作为高斯变量变换获得的随机变量进行严格计算，从而得到精确的置信区间。
- en: Inverse Uncertainty
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向不确定性
- en: Frequentist**:** standard error of parameter estimates is readily available,
    which can be expanded into a confidence interval.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 频率学派**：**参数估计的标准误差是 readily available的，可以扩展为置信区间。
- en: Bayesian**:** Several methodologies for inverse uncertainty quantification exist
    under the Bayesian framework. The most complicated direction is to aim at solving
    problems with both bias correction and parameter calibration. The challenges of
    such problems include not only the influences from model inadequacy and parameter
    uncertainty but also the lack of data from both computer simulations and experiments.
    A common situation is that the input settings are not the same over experiments
    and simulations. Another common situation is that parameters derived from experiments
    are input to simulations. For computationally expensive simulations, then often
    a surrogate model, e.g. a Gaussian process or a Polynomial Chaos Expansion, is
    necessary, defining an inverse problem for finding the surrogate model that best
    approximates the simulations.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 贝叶斯**：**在贝叶斯框架下存在几种反向不确定性量化的方法。最复杂的方向是解决既有偏差校正又有参数标定的问题。这类问题的挑战不仅包括模型不适应和参数不确定性的影响，还包括来自计算机模拟和实验的数据不足。一个常见的情况是实验和模拟中的输入设置不同。另一个常见情况是从实验中得出的参数被输入到模拟中。对于计算上昂贵的模拟，通常需要一个替代模型，例如高斯过程或多项式混沌展开，定义一个反问题以找到最能逼近模拟的替代模型。
- en: 'Modular approach: An approach to inverse uncertainty quantification is the
    modular Bayesian approach. The modular Bayesian approach derives its name from
    its four-module procedure. Apart from the currently available data, the prior
    distribution of unknown parameters should be assigned.'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模块化方法：反向不确定性量化的一种方法是模块化贝叶斯方法。模块化贝叶斯方法的名称源于其四个模块的过程。除了当前可用的数据外，还应为未知参数指定先验分布。
- en: 'Gaussian process modeling for the model: To address the issue from lack of
    simulation results, the computer'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯过程建模用于模型：为了解决缺乏模拟结果的问题，计算机
- en: model is replaced with a Gaussian process (GP) model
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型被替换为高斯过程（GP）模型
- en: 'Gaussian process modeling for discrepancy function: Similarly, with the first
    module, the discrepancy'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯过程建模用于不一致函数：类似于第一个模块，不一致
- en: function is replaced with a GP model
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 函数被替换为 GP 模型
- en: 'Posterior distribution of unknown parameters: Bayes'' theorem is applied to
    calculate the posterior'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未知参数的后验分布：应用贝叶斯定理计算后验
- en: 'distribution of the unknown parameters:'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未知参数的分布：
- en: Prediction of the experimental response and discrepancy function
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验响应和不一致函数的预测
- en: 'Full approach: Fully Bayesian approach requires that not only the priors for
    unknown parameters but also the priors for the other hyperparameters should be
    assigned.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完全方法：完全贝叶斯方法要求不仅为未知参数指定先验，还需要为其他超参数指定先验。
- en: '![Uncertainty quantification using Bayesian Techniques](../Images/b9d9782932ffb4523b625aabb18437d2.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![使用贝叶斯技术的不确定性量化](../Images/b9d9782932ffb4523b625aabb18437d2.png)'
- en: 'Figure 4: Uncertainty quantification using Bayesian Techniques'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：使用贝叶斯技术的不确定性量化
- en: Uncertainty Quantification in Machine Learning
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的不确定性量化
- en: '![XXXXX](../Images/016a2eebcb95d3ddc34d69dda32de7f0.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/016a2eebcb95d3ddc34d69dda32de7f0.png)'
- en: 'Figure 5: Taxonomy of Uncertainty Quantification in Machine Learning'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：机器学习中的不确定性量化分类
- en: Evaluating Classification
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估分类
- en: 'Measuring Data Uncertainty in Classification Tasks: given prediction, probability
    vector represents a categorical distribution, i.e. it assigns a probability to
    each class to be the correct prediction. Since the prediction is not given as
    an explicit class but as a probability distribution, uncertainty estimates can
    be directly derived from the prediction. In general, this pointwise prediction
    can be seen as estimated data uncertainty. However, the model’s estimation of
    the data uncertainty is affected by model uncertainty, which has to be taken into
    account separately. In order to evaluate the amount of predicted data uncertainty,
    one can for example apply the maximal class probability or the entropy measures.
    The maximal probability represents a direct representation of certainty, while
    entropy describes the average level of information in a random variable. Though,
    one cannot tell from a single prediction how large the amount of model uncertainty
    is that affects this specific prediction as well.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分类任务中测量数据不确定性：给定预测，概率向量表示一个类别分布，即为每个类别分配一个成为正确预测的概率。由于预测不是以明确类别的形式给出，而是以概率分布的形式给出，因此可以直接从预测中得出不确定性估计。通常，这种逐点预测可以被视为估计的数据不确定性。然而，模型对数据不确定性的估计会受到模型不确定性的影响，这需要单独考虑。为了评估预测的数据不确定性的量，可以例如应用最大类别概率或熵度量。最大概率代表了确定性的直接表示，而熵描述了随机变量中的信息平均水平。不过，无法仅从单个预测中判断影响该特定预测的模型不确定性有多大。
- en: 'Measuring Model Uncertainty in Classification Tasks: an approximated posterior
    distribution on the learned model parameters can help to receive better uncertainty
    estimates. With such a posterior distribution, it is possible to evaluate variation
    i.e. uncertainty of a random variable. The most common measures for this are the
    mutual information (MI), Expected Kullback-Leibler Divergence (EKL), and the predictive
    variance. Basically, all these measures compute the expected divergence between
    the stochastic output and the expected output. MI is minimal when the knowledge
    about model parameters does not increase the information in the final prediction.
    Therefore, the MI can be interpreted as a measure of model uncertainty. The Kullback-Leibler
    divergence measures the divergence between two given probability distributions.
    The EKL can be used to measure the (expected) divergence among the possible outputs,
    which can also be interpreted as a measure of uncertainty on the model’s output
    and therefore represents the model uncertainty. And even for an analytically described
    distribution, the propagation of the parameter uncertainty into the prediction
    is in almost all cases intractable and has to be approximated for example with
    Monte Carlo approximation.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分类任务中测量模型不确定性：对学习到的模型参数的近似后验分布可以帮助获得更好的不确定性估计。通过这种后验分布，可以评估变异，即随机变量的不确定性。最常见的度量包括互信息（MI）、期望Kullback-Leibler散度（EKL）和预测方差。基本上，这些度量都是计算随机输出与期望输出之间的期望差异。当关于模型参数的知识没有增加最终预测中的信息时，MI最小。因此，MI可以被解释为模型不确定性的度量。Kullback-Leibler散度度量两个给定概率分布之间的差异。EKL可以用来测量可能输出之间的（期望）差异，这也可以被解释为对模型输出的不确定性的度量，因此代表模型不确定性。即使对于一个分析描述的分布，参数不确定性传播到预测中在几乎所有情况下都是不可处理的，必须例如通过蒙特卡罗近似来进行近似。
- en: '![XXXXX](../Images/e3ec18b810c6fa2cdfb6441fdc72725c.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/e3ec18b810c6fa2cdfb6441fdc72725c.png)'
- en: 'Figure 6: Visualization of the model and the distributional uncertainty for
    classification models. ***Source****: A Survey of Uncertainty in Deep Neural Networks,
    Jakob Gawlikowski et. al 2022*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：分类模型中模型和分布不确定性的可视化。***来源****：Jakob Gawlikowski 等人，2022年《深度神经网络不确定性调查》*
- en: 'Measuring Distributional Uncertainty in Classification Tasks: Although these
    uncertainty measures are widely used to capture the variability among several
    predictions derived from Bayesian neural networks, ensemble methods cannot capture
    distributional shifts in the input data or out-of-distribution examples, which
    could lead to a biased inference process and a falsely stated confidence. If all
    predictors attribute a high probability mass to the same (false) class label,
    this induces a low variability among the estimates. Hence, the system seems to
    be certain about its prediction, while the uncertainty in the prediction itself
    is also evaluated below.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分类任务中测量分布不确定性：尽管这些不确定性度量广泛用于捕捉来自贝叶斯神经网络的多个预测之间的变异性，但集成方法无法捕捉输入数据中的分布变化或分布外的示例，这可能导致偏倚的推断过程和错误的置信度陈述。如果所有预测器将较高的概率质量分配给相同的（错误的）类别标签，这会导致估计之间的变异性低。因此，系统似乎对其预测充满信心，而预测本身的不确定性也在下文中进行评估。
- en: '![XXXXX](../Images/9b131caebdca2a171fcca6bbec41d101.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/9b131caebdca2a171fcca6bbec41d101.png)'
- en: 'Figure 7: Visualization of the model and the distributional uncertainty for
    classification models. ***Source****: A Survey of Uncertainty in Deep Neural Networks,
    Jakob Gawlikowski et. al 2022*'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：分类模型的模型和分布不确定性的可视化。***来源****：Jakob Gawlikowski等人2022年《深度神经网络中的不确定性调查》*
- en: 'Performance Measure on Complete Data Set: while the measures described above
    evaluate the performance of individual predictions, others evaluate the usage
    of these measures on a set of samples. Measures of uncertainty can be used to
    separate between correctly and falsely classified samples or between in-domain
    and out-of-distribution samples. For that, the samples are split into two sets,
    for example in-domain and out-of-distribution or correctly classified and falsely
    classified. The two most common approaches are the Receiver Operating Characteristic
    (ROC) curve and the Precision-Recall (PR) curve. Both methods generate curves
    based on different thresholds of the underlying measure. While the ROC and PR
    curves give a visual idea of how well the underlying measures are suited to separate
    the two considered test cases, they do not give a qualitative measure. To reach
    this, the area under the curve (AUC) can be evaluated. Roughly speaking, the AUC
    gives a probability value that a randomly chosen positive sample leads to a higher
    measure than a randomly chosen negative example.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整数据集上的性能度量：虽然上述度量评估了单个预测的性能，但其他度量评估了这些度量在样本集上的使用。不确定性度量可以用于区分正确分类和错误分类的样本，或区分领域内和分布外样本。为此，样本被分为两个集合，例如领域内和分布外，或正确分类和错误分类。最常见的两种方法是接收器操作特征（ROC）曲线和精准召回（PR）曲线。这两种方法根据基础度量的不同阈值生成曲线。虽然ROC和PR曲线提供了基础度量如何适合区分两个考虑的测试案例的视觉概念，但它们没有给出定性度量。为了达到这一点，可以评估曲线下面积（AUC）。大致来说，AUC提供了一个概率值，表示随机选择的正样本比随机选择的负样本具有更高度量的可能性。
- en: Evaluating Regression
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归评估
- en: 'Measuring Data Uncertainty in Regression Predictions: in contrast to classification
    tasks, regression tasks only predict a pointwise estimation without any hint of
    data uncertainty. A common approach to overcome this is to let the network predict
    the parameters of a probability distribution, for example, a mean vector and a
    standard deviation for a normally distributed uncertainty. Doing so, a measure
    of data uncertainty is directly given. The prediction of the standard deviation
    allows an analytical description that the (unknown) true value is within a specific
    region. The interval that covers the true value with a certain probability (under
    the assumption that the predicted distribution is correct) is the quantile function,
    the inverse of the cumulative probability function. For a given probability value
    the quantile function gives a boundary. Quantiles assume some probability distribution
    and interpret the given prediction as to the expected value of the distribution.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归预测中的数据不确定性测量：与分类任务不同，回归任务只预测点估计，而没有数据不确定性的提示。一种常见的方法是让网络预测概率分布的参数，例如，正态分布不确定性的均值向量和标准差。这样，数据不确定性就被直接给出。标准差的预测允许进行分析性描述，即（未知的）真实值位于特定区域内。覆盖真实值的区间具有一定的概率（在假设预测分布正确的情况下）是分位数函数，即累积分布函数的逆函数。对于给定的概率值，分位数函数给出一个边界。分位数假定某种概率分布，并将给定预测解释为该分布的期望值。
- en: In contrast to this, other approaches directly predict a so-called prediction
    interval (PI) in which the prediction is assumed to lay. Such intervals induce
    uncertainty as a uniform distribution without giving a concrete prediction. The
    certainty of such approaches can, as the name indicates, be directly measured
    by the size of the predicted interval. The Mean Prediction Interval Width (MPIW)
    can be used to evaluate the average certainty of the model. In order to evaluate
    the correctness of the predicted intervals the Prediction Interval Coverage Probability
    (PICP) can be applied. The PCIP represents the percentage of test predictions
    that fall into a prediction interval
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相反，其他方法直接预测所谓的预测区间（PI），在该区间内假定预测值会落入。这样的区间产生的不确定性表现为均匀分布，而没有给出具体的预测。此类方法的确定性可以像名称所示，通过预测区间的大小直接测量。均值预测区间宽度（MPIW）可用于评估模型的平均确定性。为了评估预测区间的正确性，可以应用预测区间覆盖概率（PICP）。PICP
    表示落入预测区间的测试预测值的百分比。
- en: 'Measuring Model Uncertainty in Regression Predictions: model uncertainty is
    mainly caused by the model’s architecture, the training process, and underrepresented
    areas in the training data. Hence, there is no real difference in the causes and
    effects of model uncertainty between regression and classification tasks such
    that model uncertainty in regression tasks can be measured equivalently as already
    described for classification tasks, i.e. in most cases by approximating an average
    prediction and measuring the divergence among the single predictions.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归预测中的模型不确定性测量：模型不确定性主要由模型的结构、训练过程以及训练数据中未充分代表的区域引起。因此，回归任务和分类任务在模型不确定性的原因和影响上没有真正的区别，即回归任务中的模型不确定性可以与分类任务一样进行测量，即在大多数情况下，通过近似平均预测值并测量单个预测值之间的差异。
- en: '![XXXXX](../Images/da18a2598e622106a3b540f990959452.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/da18a2598e622106a3b540f990959452.png)'
- en: 'Figure 8: Visualization of the model and the distributional uncertainty for
    regression models. ***Source****: A Survey of Uncertainty in Deep Neural Networks,
    Jakob Gawlikowski et. al 2022*'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：回归模型的模型及分布不确定性的可视化。***来源****：深度神经网络中的不确定性调查，Jakob Gawlikowski 等，2022 年*
- en: '![XXXXX](../Images/bd359bb45ebb7a3d1ae7641c63bdd67d.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![XXXXX](../Images/bd359bb45ebb7a3d1ae7641c63bdd67d.png)'
- en: 'Figure 9: Visualization of the model and the distributional uncertainty for
    regression models. ***Source****: A Survey of Uncertainty in Deep Neural Networks,
    Jakob Gawlikowski et. al 2022*'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：回归模型的模型及分布不确定性的可视化。***来源****：深度神经网络中的不确定性调查，Jakob Gawlikowski 等，2022 年*
- en: 'Evaluating Uncertainty in Segmentation Tasks: evaluation of uncertainties in
    segmentation tasks is very similar to the evaluation for classification problems.
    The uncertainty is estimated in segmentation tasks using approximates of Bayesian
    inference. In the context of segmentation, the uncertainty in pixel-wise segmentation
    is measured using confidence intervals, the predictive variance, the predictive
    entropy, or the mutual information (MI). The uncertainty in structure estimation
    is obtained by averaging over all pixel-wise uncertainty estimates. The quality
    of volume uncertainties is assessed by evaluating the coefficient of variation,
    the average Dice score, or the intersection over the union. These metrics measure
    the agreement in area overlap between multiple estimates in a pairwise fashion.
    Ideally, a false segmentation should result in an increase in pixel-wise and structure
    uncertainty. To verify whether this is the case, the true positive rate at pixel
    level should be evaluated and the false detection rate as well as the ROC curves
    for the retained pixels at different uncertainty thresholds.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分割任务中评估不确定性：分割任务中的不确定性评估与分类问题的评估非常相似。分割任务中的不确定性通过贝叶斯推断的近似值来估计。在分割的背景下，像素级分割的不确定性通过置信区间、预测方差、预测熵或互信息
    (MI) 来衡量。结构估计中的不确定性通过对所有像素级不确定性估计进行平均来获得。体积不确定性的质量通过评估变异系数、平均 Dice 分数或交集并集来评估。这些指标以成对方式测量多个估计之间区域重叠的一致性。理想情况下，错误分割应导致像素级和结构不确定性的增加。要验证是否如此，应评估像素级的真正阳性率，并评估不同不确定性阈值下的假检测率以及保留像素的
    ROC 曲线。
- en: Calibration
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 校准
- en: A predictor is called well-calibrated if the derived predictive confidence represents
    a good approximation of the actual probability of correctness. Therefore, in order
    to make use of uncertainty quantification methods, one has to be sure that the
    system is well calibrated. For regression tasks, the calibration can be defined
    such that predicted confidence intervals should match the confidence intervals
    empirically computed from the dataset.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 预测器被称为校准良好，如果所得到的预测置信度能很好地近似实际的正确概率。因此，为了利用不确定性量化方法，必须确保系统已经过良好校准。对于回归任务，校准可以定义为预测的置信区间应该与从数据集中经验计算得到的置信区间相匹配。
- en: In general, calibration errors are caused by factors related to model uncertainty.
    This is intuitively clear, since as data uncertainty represents the underlying
    uncertainty that an input *x* and a target *y* represent the same real world information.
    Following, correctly predicted data uncertainty would lead to a perfectly calibrated
    system. This is clear, since these methods quantify model and data uncertainty
    separately and aim at reducing the model uncertainty on the predictions. Besides
    the methods that improve the calibration by reducing the model uncertainty, a
    large and growing body of literature has investigated methods for explicitly reducing
    calibration errors. These methods are presented in the following, followed by
    measures to quantify the calibration error. It is important to note that these
    methods do not reduce the model uncertainty, but propagate the model uncertainty
    onto the representation of the data uncertainty.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，校准误差是由与模型不确定性相关的因素引起的。这一点直观上是清楚的，因为数据不确定性表示输入 *x* 和目标 *y* 表示相同的现实世界信息的基本不确定性。接下来，正确预测的数据不确定性将导致系统完美校准。这一点很清楚，因为这些方法分别量化了模型和数据不确定性，并旨在减少预测中的模型不确定性。除了通过减少模型不确定性来改善校准的方法外，越来越多的文献研究了显式减少校准误差的方法。这些方法将在下文中介绍，随后是量化校准误差的措施。需要注意的是，这些方法并不减少模型不确定性，而是将模型不确定性传播到数据不确定性的表示上。
- en: For example, if a binary classifier is overfitted and predicts all samples of
    a test set as class A with probability 1, while half of the test samples are actually
    class B, the recalibration methods might map the network output to 0.5 in order
    to have reliable confidence. This probability of 0.5 is not equivalent to the
    data uncertainty but represents the model uncertainty propagated onto the predicted
    data uncertainty.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个二分类器过拟合，并且将测试集中的所有样本预测为类别 A，概率为 1，而测试样本中实际上有一半是类别 B，那么重新校准方法可能会将网络输出映射到
    0.5，以获得可靠的置信度。这个 0.5 的概率不等同于数据不确定性，而是代表了传播到预测数据不确定性的模型不确定性。
- en: Calibration Methods
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 校准方法
- en: 'Calibration methods can be classified into three main groups according to the
    step when they are applied:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 校准方法可以根据其应用的步骤分为三大类：
- en: 'Regularization methods applied during the training phase: these methods modify
    the objective, optimization, and/or regularization procedure in order to build
    systems and networks that are inherently calibrated.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练阶段应用的正则化方法：这些方法修改目标、优化和/或正则化过程，以构建本质上已校准的系统和网络。
- en: 'Post-processing methods applied after the training process of the model: these
    methods require a held-out calibration data set to adjust the prediction scores
    for recalibration. They only work under the assumption that the distribution of
    the left-out validation set is equivalent to the distribution, on which inference
    is done. Hence, also the size of the validation data set can influence the calibration
    result.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练过程后应用的后处理方法：这些方法需要一个独立的校准数据集来调整预测分数以进行再校准。它们仅在假设留下的验证集的分布等于进行推断的分布的情况下有效。因此，验证数据集的大小也会影响校准结果。
- en: 'Neural network uncertainty estimation methods: Approaches, that reduce the
    amount of model uncertainty on a neural network’s confidence prediction, also
    lead to a better-calibrated predictor. This is because the remaining predicted
    data uncertainty better represents the actual uncertainty on the prediction. Such
    methods are based for example on Bayesian methods or deep ensembles (Figure 4).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络不确定性估计方法：减少神经网络置信预测模型不确定性的方法，也能得到更好的校准预测器。这是因为剩余的预测数据不确定性更好地代表了预测的实际不确定性。这些方法例如基于贝叶斯方法或深度集成（见图4）。
- en: Real-World Applications
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际应用
- en: Using innovative technology to protect institutions and safeguard both consumers'
    and investors' assets, [NICE Actimize](https://www.niceactimize.com/) identifies
    financial crime, preventing fraud and providing regulatory compliance. It provides
    real-time, cross-channel fraud prevention, anti-money laundering detection, and
    trading surveillance solutions that address payment fraud, cybercrime, sanctions
    monitoring, market abuse, customer due diligence, and insider trading. AI-based
    systems and advanced analytics solutions find abnormal behavior earlier and faster,
    eliminating financial losses from theft to fraud, regulatory penalties to sanctions.
    As a result, organizations reduce losses, increase investigator efficiency, and
    improve regulatory compliance and oversight.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 利用创新技术保护机构，保障消费者和投资者的资产，[NICE Actimize](https://www.niceactimize.com/) 识别金融犯罪，防止欺诈并提供合规监管。它提供实时的跨渠道欺诈预防、反洗钱检测和交易监控解决方案，解决支付欺诈、网络犯罪、制裁监测、市场滥用、客户尽职调查和内幕交易等问题。基于人工智能的系统和先进的分析解决方案能更早更快地发现异常行为，从而消除从盗窃到欺诈、从监管处罚到制裁的财务损失。因此，组织能够减少损失，提高调查效率，并改善合规监管。
- en: With the increasing usage of AI-based systems within financial crime, quantifying
    and handling uncertainties has become more and more important. On one hand, uncertainty
    quantification plays an important role in risk minimization, which is needed in
    fraud prevention. On the other hand, there are challenging data sources that provide
    augmentation for fraud investigations, which are hard to verify. This makes the
    generation of trustworthy ground truth a very challenging task.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基于人工智能的系统在金融犯罪中的使用增加，量化和处理不确定性变得越来越重要。一方面，不确定性量化在风险最小化中扮演了重要角色，这在防止欺诈中是必要的。另一方面，有些挑战性的
    数据源提供了欺诈调查的增补，但这些数据源很难验证。这使得生成可信的真实数据成为一个非常具有挑战性的任务。
- en: Actimize’s Generic Evaluation Framework
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Actimize的通用评估框架
- en: To cope with such issues, Actimize provides an evaluation protocol containing
    various concrete baseline datasets and evaluation metrics that cover all types
    of uncertainty that help to boost research in uncertainty quantification. Also,
    the evaluation with regard to risk-averse and worst-case scenarios is considered
    as well. Such a general protocol enables data scientist researchers to easily
    compare different types of methods against an established benchmark as well as
    on real world datasets.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为应对这些问题，Actimize提供了一个包含各种具体基准数据集和评估指标的评估协议，覆盖所有类型的不确定性，助力不确定性量化研究。此外，还考虑了风险规避和最坏情况的评估。这种通用协议使数据科学研究人员能够轻松地将不同类型的方法与已建立的基准以及实际数据集进行比较。
- en: Conclusion
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Uncertainty quantification is one of the key parts of AI-based systems and decision-making
    processes. The UQ methods are becoming popular to evaluate uncertainty in various
    real-life applications. Nowadays, uncertainty has become an inseparable part of
    traditional machine and deep leering methods. This article has comprehensively
    reviewed the most important UQ concepts and methods that have been applied in
    traditional machine learning and deep learning.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性量化是基于AI的系统和决策过程的关键部分之一。UQ方法在评估各种现实应用中的不确定性时变得越来越受欢迎。如今，不确定性已成为传统机器学习和深度学习方法不可分割的一部分。本文全面回顾了在传统机器学习和深度学习中应用的最重要的UQ概念和方法。
- en: References
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: A. Ashukha, A. Lyzhov, D. Molchanov, and D. Vetrov, “Pitfalls of in-domain uncertainty
    estimation and Ensembling in deep learning,” in International Conference on Learning
    Representations, 2020.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A. Ashukha, A. Lyzhov, D. Molchanov 和 D. Vetrov，“深度学习中领域内不确定性估计和集成的陷阱”，发表在国际学习表示会议，2020年。
- en: A. G.Wilson and P. Izmailov, “Bayesian deep learning and a probabilistic perspective
    of generalization,” in Advances in Neural Information Processing Systems, H. Larochelle,
    M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, Eds., vol. 33, 2020, pp. 4697–4708.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A. G. Wilson 和 P. Izmailov，“贝叶斯深度学习和泛化的概率视角”，发表在《神经信息处理系统进展》会议论文集，H. Larochelle,
    M. Ranzato, R. Hadsell, M. F. Balcan 和 H. Lin 编， 第33卷，2020年，第4697–4708页。
- en: A. Kendall and Y. Gal, “What uncertainties do we need in bayesian deep learning
    for computer vision?” in Advances in neural information processing systems, 2017,
    pp. 5574–5584.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A. Kendall 和 Y. Gal，“计算机视觉中的贝叶斯深度学习需要哪些不确定性？” 发表在《神经信息处理系统进展》会议论文集，2017年，第5574–5584页。
- en: A. Kristiadi, M. Hein, and P. Hennig, “Learnable uncertainty under Laplace approximations,”
    arXiv preprint arXiv:2010.02720, 2020.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A. Kristiadi, M. Hein 和 P. Hennig，“在拉普拉斯近似下的可学习不确定性”，arXiv预印本 arXiv:2010.02720，2020年。
- en: A. Loquercio, M. Segu, and D. Scaramuzza, “A general framework for uncertainty
    estimation in deep learning,” IEEE Robotics and Automation Letters, vol. 5, no.
    2, pp. 3153–3160, 2020
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A. Loquercio, M. Segu 和 D. Scaramuzza，“深度学习中不确定性估计的通用框架”，IEEE Robotics and Automation
    Letters，第5卷，第2期，第3153–3160页，2020年。
- en: A. Malinin and M. Gales, “Predictive uncertainty estimation via prior networks,”
    in Advances in Neural Information Processing Systems, 2018, pp. 7047–7058.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A. Malinin 和 M. Gales，“通过先验网络进行预测不确定性估计”，发表在《神经信息处理系统进展》会议论文集，2018年，第7047–7058页。
- en: 'Chen, Wang, and Cho 2017 Chen, F.; Wang, C.; and Cho, J.-H. 2017\. Collective
    subjective logic: Scalable uncertainty-based opinion inference. In 2017 IEEE International
    Conference on Big Data (Big Data), 7–16.'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chen, Wang 和 Cho 2017 Chen, F.; Wang, C.; 和 Cho, J.-H. 2017\. 集体主观逻辑：可扩展的不确定性基础上的意见推断。发表在2017
    IEEE国际大数据会议（Big Data），第7–16页。
- en: G. Kahn, A. Villaflor, V. Pong, P. Abbeel, and S. Levine, “Uncertainty-aware
    reinforcement learning for collision avoidance,” arXiv preprint arXiv:1702.01182,
    2017.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: G. Kahn, A. Villaflor, V. Pong, P. Abbeel 和 S. Levine，“用于碰撞避免的基于不确定性的强化学习”，arXiv预印本
    arXiv:1702.01182，2017年。
- en: G. Wang, W. Li, M. Aertsen, J. Deprest, S. Ourselin, and T. Vercauteren, “Aleatoric
    uncertainty estimation with test-time augmentation for medical image segmentation
    with convolutional neural networks,” Neurocomputing, vol. 338, pp. 34–45, 2019.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: G. Wang, W. Li, M. Aertsen, J. Deprest, S. Ourselin 和 T. Vercauteren，“通过测试时间数据增强进行医学图像分割的可变不确定性估计”，Neurocomputing，第338卷，第34–45页，2019年。
- en: J. Van Amersfoort, L. Smith, Y. W. Teh, and Y. Gal, “Uncertainty estimation
    using a single deep deterministic neural network,” in Proceedings of the 37th
    International Conference on Machine Learning. PMLR, 2020, pp. 9690–9700.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: J. Van Amersfoort, L. Smith, Y. W. Teh 和 Y. Gal，“使用单一深度确定性神经网络进行不确定性估计”，发表在第37届国际机器学习会议论文集，PMLR，2020年，第9690–9700页。
- en: J. Zeng, A. Lesnikowski, and J. M. Alvarez, “The relevance of bayesian layer
    positioning to model uncertainty in deep bayesian active learning,” arXiv preprint
    arXiv:1811.12535, 2018.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: J. Zeng, A. Lesnikowski 和 J. M. Alvarez，“贝叶斯层定位对深度贝叶斯主动学习模型不确定性的相关性”，arXiv预印本
    arXiv:1811.12535，2018年。
- en: 'Jha, A.; Chandrasekaran, A.; Kim, C.; Ramprasad, R. Impact of dataset uncertainties
    on machine learning model predictions: The example of polymer glass transition
    temperatures. Model. Simul. Mater. Sci. Eng. 2019, 27, 024002.'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jha, A.; Chandrasekaran, A.; Kim, C.; Ramprasad, R. 数据集不确定性对机器学习模型预测的影响：以聚合物玻璃转变温度为例。Model.
    Simul. Mater. Sci. Eng. 2019, 27, 024002。
- en: 'Jøsang 2016 Jøsang, A. 2016\. Subjective Logic: A Formalism for Reasoning Under
    Uncertainty. Springer.'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jøsang 2016 Jøsang, A. 2016\. 主观逻辑：不确定性下推理的形式化方法。Springer。
- en: Lele, S.R. How Should We Quantify Uncertainty in Statistical Inference? Front.
    Ecol. Evol. 2020, 8.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lele, S.R. 我们应如何量化统计推断中的不确定性？Front. Ecol. Evol. 2020, 8。
- en: M. S. Ayhan and P. Berens, “Test-time data augmentation for estimation of heteroscedastic
    aleatoric uncertainty in deep neural networks,” in Medical Imaging with Deep Learning
    Conference, 2018.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: M. S. Ayhan 和 P. Berens, “测试时数据增强以估计深度神经网络中的异方差性随机不确定性，” 载于《医学成像与深度学习会议》，2018年。
- en: Meyer, V.R. Measurement uncertainty. J. Chromatogr. A 2007, 1158, 15–24.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Meyer, V.R. 测量不确定性。J. Chromatogr. A 2007, 1158, 15–24。
- en: Senel, O. Infill Location Determination and Assessment of Corresponding Uncertainty.
    Ph.D. Thesis, Texas A & M University, College Station, TX, USA, 2009.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Senel, O. 《填充位置的确定与相应不确定性的评估》。博士论文，德克萨斯农工大学，科利奇站，TX，美国，2009年。
- en: 'Siddique, T.; Mahmud, M.S. Classification of fNIRS Data Under Uncertainty:
    A Bayesian Neural Network Approach 2021\. Available online: https://ieeexplore.ieee.org/document/9398971
    (accessed on 20 November 2021)'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Siddique, T.; Mahmud, M.S. 《在不确定性下分类 fNIRS 数据：一种贝叶斯神经网络方法》 2021。在线获取： https://ieeexplore.ieee.org/document/9398971
    (访问于2021年11月20日)
- en: T. Tsiligkaridis, “Failure prediction by confidence estimation of uncertainty-aware
    Dirichlet networks,” in 2021 IEEE International Conference on Acoustics, Speech
    and Signal Processing. IEEE, 2021, pp. 3525–3529.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T. Tsiligkaridis, “通过对不确定性感知的 Dirichlet 网络的置信度估计进行故障预测，” 载于2021年 IEEE 国际声学、语音和信号处理会议。IEEE,
    2021, 页3525–3529。
- en: Y. Feldman and V. Indelman, “Bayesian viewpoint-dependent robust classification
    under model and localization uncertainty,” in 2018 IEEE International Conference
    on Robotics and Automation. IEEE, 2018, pp. 3221–3228.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Y. Feldman 和 V. Indelman, “在模型和定位不确定性下的贝叶斯视角依赖的稳健分类，” 载于2018年 IEEE 国际机器人与自动化会议。IEEE,
    2018, 页3221–3228。
- en: 'Y. Gal and Z. Ghahramani, “Dropout as a bayesian approximation: Representing
    model uncertainty in deep learning,” in international conference on machine learning,
    2016, pp. 1050–1059.'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Y. Gal 和 Z. Ghahramani, “Dropout 作为贝叶斯近似：在深度学习中表示模型不确定性，” 载于《国际机器学习会议》，2016年，页1050–1059。
- en: '**[Danny Butvinik](https://www.linkedin.com/in/danny-butvinik/)** is the Chief
    Data Scientist at NICE Actimize, providing technical and professional leadership.
    Butvinik is an expert in Artificial Intelligence and Data Science with demonstrated
    ability in building a data science vision and communicating data-driven analytics
    to all company departments. Butvinik has a strong focus on anomaly detection in
    data streams and online machine learning. He regularly publishes academic papers
    and research articles.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Danny Butvinik](https://www.linkedin.com/in/danny-butvinik/)** 是 NICE Actimize
    的首席数据科学家，提供技术和专业领导。Butvinik 是人工智能和数据科学领域的专家，具有构建数据科学愿景和向公司所有部门传达数据驱动分析的能力。Butvinik
    着重于数据流中的异常检测和在线机器学习。他定期发表学术论文和研究文章。'
- en: More On This Topic
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Navigating Today’s Data and AI Market Uncertainty](https://www.kdnuggets.com/2024/02/altair-navigating-todays-data-ai-market-uncertainty)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[导航今天的数据与人工智能市场不确定性](https://www.kdnuggets.com/2024/02/altair-navigating-todays-data-ai-market-uncertainty)'
- en: '[Ten Key Lessons of Implementing Recommendation Systems in Business](https://www.kdnuggets.com/2022/07/ten-key-lessons-implementing-recommendation-systems-business.html)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实施推荐系统的十个关键经验教训](https://www.kdnuggets.com/2022/07/ten-key-lessons-implementing-recommendation-systems-business.html)'
- en: '[OLAP vs. OLTP: A Comparative Analysis of Data Processing Systems](https://www.kdnuggets.com/2023/08/olap-oltp-comparative-analysis-data-processing-systems.html)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OLAP 与 OLTP：数据处理系统的比较分析](https://www.kdnuggets.com/2023/08/olap-oltp-comparative-analysis-data-processing-systems.html)'
- en: '[Chip Huyen shares frameworks and case studies for implementing ML systems](https://www.kdnuggets.com/2023/02/sphere-chip-huyen-shares-frameworks-case-studies-implementing-ml-systems.html)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chip Huyen 分享实施机器学习系统的框架和案例研究](https://www.kdnuggets.com/2023/02/sphere-chip-huyen-shares-frameworks-case-studies-implementing-ml-systems.html)'
- en: '[Unify Batch and ML Systems with Feature/Training/Inference Pipelines](https://www.kdnuggets.com/2023/09/hopsworks-unify-batch-ml-systems-feature-training-inference-pipelines)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过特征/训练/推理管道统一批处理和机器学习系统](https://www.kdnuggets.com/2023/09/hopsworks-unify-batch-ml-systems-feature-training-inference-pipelines)'
- en: '[Learn How to Design & Deploy Responsible AI Systems](https://www.kdnuggets.com/2023/10/teradata-design-deploy-responsible-ai-systems-whitepaper)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[了解如何设计与部署负责任的人工智能系统](https://www.kdnuggets.com/2023/10/teradata-design-deploy-responsible-ai-systems-whitepaper)'
