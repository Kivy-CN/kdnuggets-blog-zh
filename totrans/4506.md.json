["```py\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntrain_data = [\n  \"I enjoy coffee.\",\n  \"I enjoy tea.\",\n  \"I dislike milk.\",\n  \"I am going to the supermarket later this morning for some coffee.\"\n]\n\ntest_data = [\n  \"Enjoy coffee this morning.\",\n  \"I enjoy going to the supermarket.\",\n  \"Want some milk for your coffee?\"\n]\n```", "```py\nnum_words = 1000\noov_token = '<UNK>'\npad_type = 'post'\ntrunc_type = 'post'\n```", "```py\n# Tokenize our training data\ntokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\ntokenizer.fit_on_texts(train_data)\n\n# Get our training data word index\nword_index = tokenizer.word_index\n\n# Encode training data sentences into sequences\ntrain_sequences = tokenizer.texts_to_sequences(train_data)\n\n# Get max training sequence length\nmaxlen = max([len(x) for x in train_sequences])\n\n# Pad the training sequences\ntrain_padded = pad_sequences(train_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\n\n# Output the results of our work\nprint(\"Word index:\\n\", word_index)\nprint(\"\\nTraining sequences:\\n\", train_sequences)\nprint(\"\\nPadded training sequences:\\n\", train_padded)\nprint(\"\\nPadded training shape:\", train_padded.shape)\nprint(\"Training sequences data type:\", type(train_sequences))\nprint(\"Padded Training sequences data type:\", type(train_padded))\n```", "```py\nWord index:\n {'<UNK>': 1, 'i': 2, 'enjoy': 3, 'coffee': 4, 'tea': 5, 'dislike': 6, 'milk': 7, 'am': 8, 'going': 9, 'to': 10, 'the': 11, 'supermarket': 12, 'later': 13, 'this': 14, 'morning': 15, 'for': 16, 'some': 17}\n\nTraining sequences:\n [[2, 3, 4], [2, 3, 5], [2, 6, 7], [2, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 4]]\n\nPadded training sequences:\n [[ 2  3  4  0  0  0  0  0  0  0  0  0]\n [ 2  3  5  0  0  0  0  0  0  0  0  0]\n [ 2  6  7  0  0  0  0  0  0  0  0  0]\n [ 2  8  9 10 11 12 13 14 15 16 17  4]]\n\nPadded training shape: (4, 12)\nTraining sequences data type: <class 'list'>\nPadded Training sequences data type: <class 'numpy.ndarray'>\n```", "```py\ntest_sequences = tokenizer.texts_to_sequences(test_data)\ntest_padded = pad_sequences(test_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\n\nprint(\"Testing sequences:\\n\", test_sequences)\nprint(\"\\nPadded testing sequences:\\n\", test_padded)\nprint(\"\\nPadded testing shape:\",test_padded.shape)\n```", "```py\nTesting sequences:\n [[3, 4, 14, 15], [2, 3, 9, 10, 11, 12], [1, 17, 7, 16, 1, 4]]\n\nPadded testing sequences:\n [[ 3  4 14 15  0  0  0  0  0  0  0  0]\n [ 2  3  9 10 11 12  0  0  0  0  0  0]\n [ 1 17  7 16  1  4  0  0  0  0  0  0]]\n\nPadded testing shape: (3, 12)\n```", "```py\nfor x, y in zip(test_data, test_padded):\n  print('{} -> {}'.format(x, y))\n\nprint(\"\\nWord index (for reference):\", word_index)\n```", "```py\nEnjoy coffee this morning. -> [ 3  4 14 15  0  0  0  0  0  0  0  0]\nI enjoy going to the supermarket. -> [ 2  3  9 10 11 12  0  0  0  0  0  0]\nWant some milk for your coffee? -> [ 1 17  7 16  1  4  0  0  0  0  0  0]\n\nWord index (for reference): {'<UNK>': 1, 'i': 2, 'enjoy': 3, 'coffee': 4, 'tea': 5, 'dislike': 6, 'milk': 7, 'am': 8, 'going': 9, 'to': 10, 'the': 11, 'supermarket': 12, 'later': 13, 'this': 14, 'morning': 15, 'for': 16, 'some': 17}\n```"]