- en: My favorite mind-blowing Machine Learning/AI breakthroughs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我最喜欢的令人震惊的机器学习/人工智能突破
- en: 原文：[https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html](https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html](https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Jerry Chi](https://www.linkedin.com/in/jerrychi/), Data Science Manager
    at SmartNews**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Jerry Chi](https://www.linkedin.com/in/jerrychi/)，SmartNews 数据科学经理**。'
- en: Compared to other fields, machine learning / artificial intelligence seems to
    have a much higher frequency of super-interesting developments these days. Things
    that make you say “wow” or even “what a time to be alive!” (as the creator of [Two
    Minute Papers](https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg) always
    says)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他领域相比，机器学习/人工智能如今似乎有着更高频率的超级有趣的发展。那些让你说“哇”甚至“多么美好的时代！”（正如 [Two Minute Papers](https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg)
    的创作者常说）
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在IT方面'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Disclaimer: I’m not using any rigorous definition of “mind-blowing” or “breakthrough”;
    it’s a casual list.. and I might use less rigorous terminology to make this post
    more accessible'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 免责声明：我并没有使用“令人震惊”或“突破”的严格定义；这只是一个随意的列表……我可能会使用不那么严格的术语来使这篇文章更易读
- en: Amazingly accurate estimates from seemingly unusable information
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从看似无用的信息中获得惊人准确的估算
- en: Through-wall human pose estimation
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 透墙人体姿态估计
- en: '[Website/video by MIT researchers, 2018](http://rfpose.csail.mit.edu/)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[麻省理工学院研究人员的网站/视频，2018](http://rfpose.csail.mit.edu/)'
- en: '![](../Images/fd784cc2060151479f7bba89b708f3ce.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd784cc2060151479f7bba89b708f3ce.png)'
- en: We can accurately estimate how a human on the other side of a wall is standing/sitting/walking
    just from perturbations in Wifi signals caused by that human.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以仅通过Wifi信号的扰动准确估计墙另一边的人是站着、坐着还是走动。
- en: Gauging materials’ physical properties from video
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从视频中测量材料的物理属性
- en: '[Article/video by MIT researchers, 2015](http://news.mit.edu/2015/visual-microphone-identifies-structural-defects-0521)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[文章/视频由麻省理工学院研究人员提供，2015](http://news.mit.edu/2015/visual-microphone-identifies-structural-defects-0521)'
- en: The researchers first [demonstrated in 2014 ](http://news.mit.edu/2014/algorithm-recovers-speech-from-vibrations-0804)that
    they can e.g. reproduce human speech from video (with no audio) of a potato chip
    bag based on the vibrations. This part was done without machine learning. In 2015,
    they used machine learning to show that you can estimate the stiffness, elasticity,
    weight per unit area, etc. of materials just from a video (in some cases just
    the vibrations caused by the ordinary circulation of air was sufficient).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员首次在 2014 年 [演示](http://news.mit.edu/2014/algorithm-recovers-speech-from-vibrations-0804)了他们可以从视频（没有音频）中的薯片袋的振动中重现人类的声音。这一部分没有使用机器学习。在
    2015 年，他们使用机器学习展示了你可以仅从视频中（在某些情况下，仅通过空气的普通流动引起的振动就足够）估计材料的刚度、弹性、单位面积重量等。
- en: Estimating keystrokes from a smartphone next to the keyboard
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从键盘旁边的智能手机中估计按键次数
- en: '[Paper, 2015](https://www.sigmobile.org/mobicom/2015/papers/p142-liuA.pdf)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[论文，2015](https://www.sigmobile.org/mobicom/2015/papers/p142-liuA.pdf)'
- en: '![](../Images/d8cd2c0a193be9d239ff8be4a92ecc22.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8cd2c0a193be9d239ff8be4a92ecc22.png)'
- en: Researchers showed that with the audio recorded by a single, off-the-shelf smartphone
    placed next to a keyboard, one can estimate with **94% accuracy** the individual
    keystrokes. Unlike previous approaches that used supervised deep learning with
    many microphones placed around the keyboard, this paper actually uses a relatively
    simple machine learning technique (K-means clustering) and **unsupervised** learning.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员展示了使用一个放置在键盘旁边的普通智能手机录音，能够以**94%准确率**估计每个按键。与之前使用多个麦克风环绕键盘的监督深度学习方法不同，这篇论文实际上使用了一种相对简单的机器学习技术（K-means聚类）和**无监督**学习。
- en: Generative models
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成模型
- en: Realistic face generation, style-mixing, and interpolation
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 逼真的面孔生成、风格混合和插值
- en: '[Paper](https://arxiv.org/abs/1812.04948)/[video](https://www.youtube.com/watch?v=kSLJriaOumA) by
    NVIDIA researchers, 2018'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[论文](https://arxiv.org/abs/1812.04948)/[视频](https://www.youtube.com/watch?v=kSLJriaOumA)
    由NVIDIA研究人员提供，2018年'
- en: '![](../Images/d452d9c67d17a2d647fc49b9bd51e60d.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d452d9c67d17a2d647fc49b9bd51e60d.png)'
- en: The researchers combined a new architecture with tons of GPUs to create extremely
    photo-realistic artificial faces that are interpolations between other faces or
    applications of the “style” of one face to another face. The work builds upon
    past work on Generative Adversarial Networks (GANs). GANs were invented in 2014
    and have seen an explosion in research since then. The most basic concept of GANs
    is two neural networks dueling against each other (e.g. one that classifies images
    as “real” or “fake” and a second neural network that generates images in a way
    that attempts to “trick” the first neural network into wrongly classifying fake
    images as real…hence the second neural network is an “adversary” to the first).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员将一种新架构与大量GPU结合，创造出了极其逼真的人工面孔，这些面孔是其他面孔之间的插值或一种面孔“风格”应用到另一面孔上的结果。这项工作建立在生成对抗网络（GANs）之前的研究基础上。GANs于2014年发明，自那时以来研究爆炸性增长。GANs的基本概念是两个神经网络相互对抗（例如，一个将图像分类为“真实”或“虚假”，另一个神经网络生成图像以试图“欺骗”第一个神经网络，将虚假图像错误地分类为真实……因此第二个神经网络是第一个的“对手”）。
- en: In general, there is a lot of [awesome research](https://github.com/yenchenlin/awesome-adversarial-machine-learning) about
    adversarial machine learning, which has been around for more than a decade. There
    are many creepy implications for cybersecurity etc. But I digress.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，有很多关于对抗性机器学习的[惊人研究](https://github.com/yenchenlin/awesome-adversarial-machine-learning)，这种研究已经有十多年历史了。它对网络安全等领域有许多令人不安的影响。但我离题了。
- en: Teaching machines to draw
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 教授机器绘画
- en: '[Blog post by Google Brain, 2017](https://ai.googleblog.com/2017/04/teaching-machines-to-draw.html)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[Google Brain的博客文章，2017年](https://ai.googleblog.com/2017/04/teaching-machines-to-draw.html)'
- en: '![](../Images/41d17838b9f6c454295e62d20784d32f.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41d17838b9f6c454295e62d20784d32f.png)'
- en: Interpolation between 2 drawings
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 两幅图像之间的插值
- en: My acquaintance [David Ha at Google Brain](https://twitter.com/hardmaru) used
    a generative recurrent neural network (RNN) to make drawings that are vector-based
    graphics (I think of this as Adobe Illustrator except automated).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我的熟人[David Ha at Google Brain](https://twitter.com/hardmaru)使用生成对抗递归神经网络（RNN）制作了基于矢量的图形（我把这看作是自动化的Adobe
    Illustrator）。
- en: Transferring great dance moves to poor dancers
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将优秀的舞蹈动作转移到舞蹈水平差的人身上
- en: '[Website](https://carolineec.github.io/everybody_dance_now/)/[video](https://www.youtube.com/watch?v=PCBTZh41Ris) from
    UC Berkeley researchers, 2018'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[网站](https://carolineec.github.io/everybody_dance_now/)/[视频](https://www.youtube.com/watch?v=PCBTZh41Ris)
    由UC Berkeley研究人员提供，2018年'
- en: 'Think “Auto-Tune for dancing.” Using pose estimation and generative adversarial
    training, the researchers were able to make a fake video of any real person (the
    “target” person) dancing with great dance skills. The required input was only:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下“自动调音器”用于舞蹈。通过姿态估计和生成对抗训练，研究人员能够制作任何真实人物（“目标”人物）舞蹈的虚假视频，并且舞技非常高超。所需的输入仅有：
- en: a short video of someone with great dance skills dancing
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一段优秀舞者跳舞的短视频
- en: a few minutes of video of the target person dancing (typically poorly since
    most people suck at dancing)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标人物跳舞的几分钟视频（通常舞技很差，因为大多数人舞蹈不好）
- en: I also saw Jensen Huang, the CEO of NVIDIA, show a video (made with this technique)
    of himself dancing like Michael Jackson. I’m glad I attended the GPU Tech Conference,
    haha.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我还看到NVIDIA的CEO Jensen Huang展示了一段（使用此技术制作的）他像迈克尔·杰克逊一样跳舞的视频。我很高兴我参加了GPU技术大会，哈哈。
- en: Reinforcement learning
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强化学习
- en: World models — AI learning inside its own dream
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 世界模型 — AI在自己的梦境中学习
- en: '[Website by Google Brain, 2018](https://worldmodels.github.io/)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[谷歌大脑网站，2018](https://worldmodels.github.io/)'
- en: '![](../Images/acf4711d39b9c3aadfc60f86717347c5.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/acf4711d39b9c3aadfc60f86717347c5.png)'
- en: Humans do not actually know or think about all the details of the world we live
    in. We behave based on the abstraction of the world that is in our heads. For
    example, if I ride on a bike, I don’t think of the gears/nuts/bolts of the bike;
    I just have a rough sense of where the wheels, seat, and handle are and how to
    interact with them. Why not use a similar approach for AI?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 人类实际上并不真正知道或考虑我们生活的世界的所有细节。我们的行为是基于我们脑中的世界抽象。例如，如果我骑自行车，我不会考虑自行车的齿轮/螺母/螺栓；我只是对车轮、座位和把手的大致位置以及如何与它们互动有一个粗略的感知。为什么不对人工智能采用类似的方法呢？
- en: This “world models” approach (again, created by David Ha et al) allows the “agent”
    (e.g. an AI that controls a car in a racing game) to create a generative model
    of the world/environment around it which is a simplification/abstraction of the
    actual environment. So, you can think of the world model as a dream that lives
    in the head of the AI. Then the AI can train via reinforcement learning in this
    “dream” to achieve better performance. So this approach is actually combining
    generative ML with reinforcement learning. By doing this, the researchers were
    able to achieve state-of-the-art performance on certain video game-playing tasks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“世界模型”方法（再次由David Ha等人创建）允许“代理”（例如控制赛车游戏中汽车的人工智能）创建一个世界/环境的生成模型，这是实际环境的简化/抽象。因此，你可以把世界模型看作是存在于人工智能脑中的一个梦境。然后，人工智能可以在这个“梦境”中通过强化学习进行训练，以实现更好的表现。因此，这种方法实际上是将生成式机器学习与强化学习结合在一起。通过这样做，研究人员在某些视频游戏任务中达到了最先进的性能。
- en: '[Update 2019/2/15] Building upon the above “world models” approach, Google
    just revealed [PlaNet: Deep Planning Network for Reinforcement Learning](http://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html),
    which achieved 5000% better data efficiency than previous approaches.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[更新 2019/2/15] 基于上述“世界模型”方法，谷歌刚刚公布了[PlaNet：深度规划网络用于强化学习](http://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html)，其数据效率比以往方法提高了5000%。'
- en: AlphaStar — Starcraft II AI that beats the top pro players
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: AlphaStar——击败顶级职业选手的Starcraft II人工智能
- en: '[Blog post](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/), [e-sports-ish
    video](https://www.youtube.com/watch?v=cUTMhmVh1qs) by DeepMind (Google), 2019'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[博客文章](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/)，[DeepMind（谷歌）制作的电子竞技视频](https://www.youtube.com/watch?v=cUTMhmVh1qs)，2019年'
- en: We’ve come a long way from the [historic Go matches between Lee Sedol and DeepMind’s
    AlphaGo](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol) that rocked the
    world, which was a mere 3 years ago in 2016 (check out the [NetFlix documentary](https://www.netflix.com/jp-en/title/80190844),
    which made some people cry). Then, it was even more amazing that AlphaZero in
    2017 became better than AlphaGo at Go (and better than any other algorithm at
    chess, shogi AKA Japanese chess, etc.) despite not using any training data from
    human matches. But AlphaStar in 2019 is even **more** amazing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们距离[李世石与DeepMind的AlphaGo之间的历史围棋比赛](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol)已经过去很远，那只是三年前的2016年（看看[Netflix纪录片](https://www.netflix.com/jp-en/title/80190844)，让一些人感动落泪）。那时，AlphaZero在2017年在围棋方面超越了AlphaGo（并在国际象棋、将棋等方面超越了任何其他算法），尽管没有使用任何来自人类比赛的训练数据。但2019年的AlphaStar则更加**惊人**。
- en: Being a StarCraft fan myself since 1998, I can appreciate how the “…need to
    balance short and long-term goals and adapt to unexpected situations… poses a
    huge challenge.” It’s truly a difficult and complex game which requires understanding
    at multiple levels to play well. Research on Starcraft-playing algorithms have
    been ongoing since 2009.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 自1998年以来作为一个StarCraft粉丝，我能体会到“……需要平衡短期和长期目标，并适应意外情况……带来了巨大的挑战。”这是一个真正困难且复杂的游戏，需要在多个层面上理解才能玩得好。对Starcraft算法的研究自2009年以来一直在进行。
- en: AlphaStar essentially used a combination of supervised learning (from human
    matches) and reinforcement learning (playing against itself) to achieve its results.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaStar 实质上使用了监督学习（来自人类比赛）和强化学习（对抗自我）相结合的方法来实现其结果。
- en: Humans training robots
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人类训练机器人
- en: Teaching tasks to machines with a single human demonstration
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用单个人类演示来教导机器任务
- en: '[Article](https://news.developer.nvidia.com/new-ai-technique-helps-robots-work-alongside-humans/)/[video](https://www.youtube.com/watch?time_continue=1&v=B7ZT5oSnRys) by
    NVIDIA researchers, 2018'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[NVIDIA 研究人员的文章](https://news.developer.nvidia.com/new-ai-technique-helps-robots-work-alongside-humans/)
    / [视频](https://www.youtube.com/watch?time_continue=1&v=B7ZT5oSnRys)，2018年'
- en: 'I can think of 3 typical approaches to teaching robots to do something, but
    all take a lot of time/labor:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我能想到三种典型的方法来教机器人做某件事，但这些方法都需要大量时间/劳动：
- en: Manually program the robot’s joint rotations etc. for each situation
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动编程每种情况的机器人关节旋转等
- en: Let the robot try the task many, many times (reinforcement learning)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让机器人多次尝试任务（强化学习）
- en: Demonstrate a task to the robot many, many times
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向机器人演示任务很多次
- en: Typically, one major criticism of deep learning is that it’s very costly to
    produce the millions of examples (data) that make the computer perform well. But
    increasingly, there are ways to not rely on such costly data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，深度学习的一大批评是，生产使计算机表现良好的数百万个示例（数据）是非常昂贵的。但越来越多的方法可以避免依赖这种昂贵的数据。
- en: 'The researchers figured out a way for a robot arm to successfully perform a
    task (such as “pick up the blocks and stack them so that they are in the order:
    red block, blue block, orange block”) based on a **single **video of a **single **human
    demonstration (a physical real human hand moving the blocks), even if the video
    was shot from a different angle. The algorithm actually generates a human-readable
    description of the task it plans to do, which is great for troubleshooting. The
    algorithm relies on object detection with pose estimation, synthetic training
    data generation, and simulation-to-reality transfer.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员找到了一个方法，让机器人手臂能够成功执行一个任务（例如“拾起块状物并将其堆叠成以下顺序：红色块、蓝色块、橙色块”），基于**单一**视频中的**单一**人类演示（一个真实的人的手移动这些块），即使视频是从不同的角度拍摄的。这个算法实际上生成了一个人类可读的任务描述，这对故障排除非常有帮助。该算法依赖于姿态估计的对象检测、合成训练数据生成和模拟到现实的迁移。
- en: Unsupervised machine translation
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督机器翻译
- en: '[Blog post by Facebook AI Research, 2018](https://code.fb.com/ai-research/unsupervised-machine-translation-a-novel-approach-to-provide-fast-accurate-translations-for-more-languages/)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[Facebook AI Research 的博客文章，2018年](https://code.fb.com/ai-research/unsupervised-machine-translation-a-novel-approach-to-provide-fast-accurate-translations-for-more-languages/)'
- en: Typically, you would need a huge training dataset of translated documents (e.g.
    professional translations of United Nations proceedings) to do machine translation
    well (i.e. **supervised** learning). Of course, many topics and language pairs
    don’t have high-quality, plentiful training data. In this paper, researchers showed
    that it’s possible to use **unsupervised** learning (i.e. using no translation
    data and just using unrelated corpuses of text in each language), it’s possible
    to reach the translation quality of state-of-the-art **supervised** learning approaches.
    Wow.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你需要大量翻译文档的训练数据集（例如，联合国会议的专业翻译）才能很好地进行机器翻译（即**监督**学习）。当然，许多主题和语言对没有高质量、充足的训练数据。在这篇论文中，研究人员展示了使用**无监督**学习（即不使用翻译数据，仅使用每种语言中无关的文本语料库）是可能达到最先进的**监督**学习方法的翻译质量。哇。
- en: The basic idea is that, in any language, certain words/concepts will tend to
    appear in close proximity (e.g. “furry” and “cat”). They describe this as “embeddings
    of words in different languages share similar neighborhood structure.” I mean,
    OK, I get the idea, but it’s still mind-blowing that using this approach they
    can reach such high translation quality without training on translation datasets.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想是，在任何语言中，某些词汇/概念往往会紧密出现（例如“毛茸茸的”和“猫”）。他们描述这个现象为“不同语言中的词嵌入共享类似的邻域结构。” 我的意思是，好的，我明白这个概念，但令人惊叹的是，使用这种方法他们能够在没有翻译数据集的情况下达到如此高的翻译质量。
- en: Closing
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结束
- en: I hope this post made you more excited about developments in ML/AI, if you weren’t
    already. Maybe I’ll write another similar post in a year from now. Please feel
    free to leave any thoughts/comments here or e-mail me at jerrychi123 [at] gmail.com.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章让你对机器学习/人工智能的发展更加兴奋，即使你之前已经很兴奋。也许我会在一年后写另一篇类似的文章。请随时在这里留下任何想法/评论，或通过电子邮件联系我，地址是
    jerrychi123 [at] gmail.com。
- en: What a time to be alive! =D
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是多么美好的时代啊！ =D
- en: '**Bio**: [Jerry Chi](https://www.linkedin.com/in/jerrychi/) has experience
    in data science, machine learning, data engineering, and strategy in digital industries.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介**： [Jerry Chi](https://www.linkedin.com/in/jerrychi/) 在数据科学、机器学习、数据工程和数字行业战略方面有丰富经验。'
- en: '[Original](https://blog.usejournal.com/my-favorite-mind-blowing-ml-ai-breakthroughs-e7b4f3637e3d).
    Reposted with permission.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://blog.usejournal.com/my-favorite-mind-blowing-ml-ai-breakthroughs-e7b4f3637e3d)。经许可转载。'
- en: '**Resources:**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：**'
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在线和基于网络：分析、数据挖掘、数据科学、机器学习教育](https://www.kdnuggets.com/education/online.html)'
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分析、数据科学、数据挖掘和机器学习的软件](https://www.kdnuggets.com/software/index.html)'
- en: '**Related:**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Acquiring Labeled Data to Train Your Models at Low Costs](https://www.kdnuggets.com/2019/02/labeled-data-train-models.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[以低成本获取标注数据来训练你的模型](https://www.kdnuggets.com/2019/02/labeled-data-train-models.html)'
- en: '[4 Reasons Why Your Machine Learning Code is Probably Bad](https://www.kdnuggets.com/2019/02/4-reasons-machine-learning-code-probably-bad.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你的机器学习代码可能很糟糕的四个原因](https://www.kdnuggets.com/2019/02/4-reasons-machine-learning-code-probably-bad.html)'
- en: '[Artificial Neural Network Implementation using NumPy and Image Classification](https://www.kdnuggets.com/2019/02/artificial-neural-network-implementation-using-numpy-and-image-classification.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 NumPy 和图像分类实现人工神经网络](https://www.kdnuggets.com/2019/02/artificial-neural-network-implementation-using-numpy-and-image-classification.html)'
- en: More On This Topic
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并寻找目标以……](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一项90亿美元的人工智能失败案例分析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功的数据科学家的五个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么让 Python 成为初创公司理想的编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家都应该了解的三个 R 库（即使你使用 Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
