- en: A Concise Overview of Standard Model-fitting Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准模型拟合方法的简明概述
- en: 原文：[https://www.kdnuggets.com/2016/05/concise-overview-model-fitting-methods.html/2](https://www.kdnuggets.com/2016/05/concise-overview-model-fitting-methods.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/05/concise-overview-model-fitting-methods.html/2](https://www.kdnuggets.com/2016/05/concise-overview-model-fitting-methods.html/2)
- en: 3) Stochastic Gradient Descent (SGD)
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3) 随机梯度下降（SGD）
- en: In GD optimization, we compute the cost gradient based on the complete training
    set; hence, we sometimes also call it *batch GD*. In case of very large datasets,
    using GD can be quite costly since we are only taking a single step for one pass
    over the training set -- thus, the larger the training set, the slower our algorithm
    updates the weights and the longer it may take until it converges to the global
    cost minimum (note that the SSE cost function is convex).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度下降（GD）优化中，我们基于完整的训练集计算成本梯度，因此我们有时也称其为*批量GD*。在数据集非常大的情况下，使用GD可能会非常昂贵，因为我们每次只对训练集进行一次迭代——因此，训练集越大，我们的算法更新权重的速度就越慢，直到收敛到全局成本最小值所需的时间也可能更长（注意，SSE成本函数是凸的）。
- en: 'In Stochastic Gradient Descent (SGD; sometimes also referred to as *iterative* or *on-line* GD),
    we don''t accumulate the weight updates as we''ve seen above for GD:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机梯度下降（SGD；有时也称为*迭代*或*在线* GD）中，我们*不*像上面GD中看到的那样累积权重更新：
- en: '![Iterative GD](../Images/df75da212cfd6178769b879d04e8b876.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![Iterative GD](../Images/df75da212cfd6178769b879d04e8b876.png)'
- en: 'Instead, we update the weights after each training sample:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们在每个训练样本后更新权重：
- en: '![Iterative SGD](../Images/6addd7ca1e9e10f4658269976691ca14.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![Iterative SGD](../Images/6addd7ca1e9e10f4658269976691ca14.png)'
- en: 'Here, the term "stochastic" comes from the fact that the gradient based on
    a single training sample is a "stochastic approximation" of the "true" cost gradient.
    Due to its stochastic nature, the path towards the global cost minimum is not
    "direct" as in GD, but may go "zig-zag" if we are visualizing the cost surface
    in a 2D space. However, it has been shown that SGD almost surely converges to
    the global cost minimum if the cost function is convex (or pseudo-convex)[1].
    Furthermore, there are different tricks to improve the GD-based learning, for
    example:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，“随机”一词源于基于单个训练样本的梯度是“真实”成本梯度的“随机近似”。由于其随机性质，通向全局成本最小值的路径不像GD那样“直接”，而是可能在我们将成本面可视化为2D空间时“曲折”。然而，已经证明，如果成本函数是凸的（或伪凸的）[1]，SGD几乎可以肯定地收敛到全局成本最小值。此外，还有不同的技巧可以改进基于GD的学习，例如：
- en: 'An adaptive learning rate η Choosing a decrease constant *d* that shrinks the
    learning rate over time:'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自适应学习率 η 选择一个减少常数*d*，该常数随时间缩小学习率：
- en: '![](../Images/7912a1c822b9d201fb58d0b33f3e8e5d.png)'
  id: totrans-10
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/7912a1c822b9d201fb58d0b33f3e8e5d.png)'
- en: 'Momentum learning by adding a factor of the previous gradient to the weight
    update for faster updates:'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将前一个梯度的一个因子添加到权重更新中以加速更新的动量学习：
- en: '![](../Images/66a8769e0d9c7c2cfea9dbbf8481676f.png)'
  id: totrans-12
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/66a8769e0d9c7c2cfea9dbbf8481676f.png)'
- en: '**A note about shuffling**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于洗牌的说明**'
- en: 'There are several different flavors of SGD, which can be all seen throughout
    the literature. Let''s take a look at the three most common variants:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: SGD有几种不同的变体，这些变体可以在文献中看到。让我们看看三种最常见的变体：
- en: '![Shuffle A](../Images/c7e5510be283093535708988c6cc554e.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![Shuffle A](../Images/c7e5510be283093535708988c6cc554e.png)'
- en: '![Shuffle B](../Images/3b510f5554f7ff0ea8e0e76d518ef035.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![Shuffle B](../Images/3b510f5554f7ff0ea8e0e76d518ef035.png)'
- en: '![Shuffle C](../Images/ce7f311ed56b92dba8bf9225e7e5df12.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![Shuffle C](../Images/ce7f311ed56b92dba8bf9225e7e5df12.png)'
- en: In scenario A [3], we shuffle the training set only one time in the beginning;
    whereas in scenario B, we shuffle the training set after each epoch to prevent
    repeating update cycles. In both scenario A and scenario B, each training sample
    is only used once per epoch to update the model weights.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在场景A [3]中，我们仅在开始时对训练集进行一次洗牌；而在场景B中，我们在每个纪元后对训练集进行洗牌，以防止重复更新周期。在场景A和场景B中，每个训练样本仅在每个纪元中使用一次来更新模型权重。
- en: In scenario C, we draw the training samples randomly with replacement from the
    training set [2]. If the number of iterations *t*is equal to the number of training
    samples, we learn the model based on a *bootstrap sample* of the training set.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在场景C中，我们从训练集中随机抽取训练样本并进行替换[2]。如果迭代次数*t*等于训练样本的数量，我们基于训练集的*自助样本*来学习模型。
- en: 4) Mini-Batch Gradient Descent (MB-GD)
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4) 小批量梯度下降（MB-GD）
- en: Mini-Batch Gradient Descent (MB-GD) a compromise between batch GD and SGD. In
    MB-GD, we update the model based on smaller groups of training samples; instead
    of computing the gradient from 1 sample (SGD) or all *n* training samples (GD),
    we compute the gradient from *1 < k < n* training samples (a common mini-batch
    size is *k=50*).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Mini-Batch 梯度下降（MB-GD）是批量 GD 和 SGD 之间的折中。在 MB-GD 中，我们基于较小的训练样本组更新模型；而不是从 1 个样本（SGD）或所有
    *n* 个训练样本（GD）计算梯度，我们从 *1 < k < n* 个训练样本计算梯度（一个常见的 mini-batch 大小是 *k=50*）。
- en: MB-GD converges in fewer iterations than GD because we update the weights more
    frequently; however, MB-GD let's us utilize vectorized operation, which typically
    results in a computational performance gain over SGD.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: MB-GD 比 GD 收敛所需的迭代次数更少，因为我们更频繁地更新权重；然而，MB-GD 使我们可以利用矢量化操作，这通常带来比 SGD 更高的计算性能提升。
- en: '**References**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考资料**'
- en: '[1] Bottou, Léon (1998). "Online Algorithms and Stochastic Approximations".
    Online Learning and Neural Networks. Cambridge University Press. ISBN 978-0-521-65263-6'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Bottou, Léon (1998). "在线算法与随机近似". 在线学习与神经网络. 剑桥大学出版社. ISBN 978-0-521-65263-6'
- en: '[2] Bottou, Léon. "Large-scale machine learning with SGD." Proceedings of COMPSTAT''2010\.
    Physica-Verlag HD, 2010\. 177-186.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Bottou, Léon. "大规模机器学习与 SGD." COMPSTAT''2010 论文集. Physica-Verlag HD, 2010.
    177-186.'
- en: '[3] Bottou, Léon. "SGD tricks." Neural Networks: Tricks of the Trade. Springer
    Berlin Heidelberg, 2012\. 421-436.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Bottou, Léon. "SGD 技巧." 神经网络：技巧的艺术. 斯普林格柏林海德堡, 2012. 421-436.'
- en: '**Bio: [Sebastian Raschka](https://twitter.com/rasbt)** is a ''Data Scientist''
    and Machine Learning enthusiast with a big passion for Python & open source. Author
    of ''[Python Machine Learning](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning)''.
    Michigan State University.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介: [Sebastian Raschka](https://twitter.com/rasbt)** 是一位数据科学家和机器学习爱好者，对 Python
    和开源有着极大的热情。《[Python 机器学习](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning)》的作者。密歇根州立大学。'
- en: '[Original](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md).
    Reposted with permission.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md).
    已获许可转载。'
- en: '**Related:**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[When Does Deep Learning Work Better Than SVMs or Random Forests?](/2016/04/deep-learning-vs-svm-random-forest.html)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习何时优于 SVM 或随机森林？](/2016/04/deep-learning-vs-svm-random-forest.html)'
- en: '[The Development of Classification as a Learning Machine](/2016/04/development-classification-learning-machine.html)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分类的发展作为学习机器](/2016/04/development-classification-learning-machine.html)'
- en: '[Why Implement Machine Learning Algorithms From Scratch?](/2016/05/implement-machine-learning-algorithms-scratch.html)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么从头实现机器学习算法？](/2016/05/implement-machine-learning-algorithms-scratch.html)'
- en: '* * *'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 工作'
- en: '* * *'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Removing Outliers Using Standard Deviation in Python](https://www.kdnuggets.com/2017/02/removing-outliers-standard-deviation-python.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 中的标准差移除异常值](https://www.kdnuggets.com/2017/02/removing-outliers-standard-deviation-python.html)'
- en: '[Developing an Open Standard for Analytics Tracking](https://www.kdnuggets.com/2022/07/developing-open-standard-analytics-tracking.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开发用于分析跟踪的开放标准](https://www.kdnuggets.com/2022/07/developing-open-standard-analytics-tracking.html)'
- en: '[Centroid Initialization Methods for k-means Clustering](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[k-means 聚类的质心初始化方法](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
- en: '[Alternative Feature Selection Methods in Machine Learning](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的替代特征选择方法](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
- en: '[Python String Methods](https://www.kdnuggets.com/2022/12/python-string-methods.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python字符串方法](https://www.kdnuggets.com/2022/12/python-string-methods.html)'
- en: '[11 Python Magic Methods Every Programmer Should Know](https://www.kdnuggets.com/11-python-magic-methods-every-programmer-should-know)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个程序员都应该知道的11个Python魔法方法](https://www.kdnuggets.com/11-python-magic-methods-every-programmer-should-know)'
