- en: A 2019 Guide to Object Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/08/2019-guide-object-detection.html](https://www.kdnuggets.com/2019/08/2019-guide-object-detection.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)![Figure](../Images/f8b967b1fdf10c9ed86a20319f500993.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Fernando @cferdo](https://unsplash.com/@dearferdo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/search/photos/objects?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Object detection is a [computer vision](https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b) technique
    whose aim is to detect objects such as cars, buildings, and human beings, just
    to mention a few. The objects can generally be identified from [either pictures
    or video feeds](https://heartbeat.fritz.ai/detecting-objects-in-videos-and-camera-feeds-using-keras-opencv-and-imageai-c869fe1ebcdb).
  prefs: []
  type: TYPE_NORMAL
- en: '[Object detection](https://www.fritz.ai/features/object-detection.html) has
    been applied widely in video surveillance, self-driving cars, and object/[people
    tracking](https://heartbeat.fritz.ai/real-time-person-tracking-on-the-edge-with-a-raspberry-pi-93ae636af9fa).
    In this piece, we’ll look at the basics of object detection and review some of
    the most commonly-used algorithms and a few brand new approaches, as well.'
  prefs: []
  type: TYPE_NORMAL
- en: How Object Detection Works
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Object detection locates the presence of an object in an image and draws a
    bounding box around that object. This usually involves two processes; classifying
    and object’s type, and then drawing a box around that object. We’ve [covered image
    classification before](https://heartbeat.fritz.ai/a-beginners-guide-to-convolutional-neural-networks-cnn-cf26c5ee17ed),
    so let’s now review some of the common model architectures used for object detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[R-CNN](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#5401)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fast R-CNN](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#62df)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Faster R-CNN](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#6c29)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mask R-CNN](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#8300)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SSD (Single Shot MultiBox Defender)](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#b49b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[YOLO (You Only Look Once)](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#3837)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Objects as Points](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#047e)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Augmentation Strategies for Object Detection](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3#2836)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R-CNN Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This technique combines two main approaches: applying high-capacity convolutional
    neural networks to bottom-up region proposals so as to localize and segment objects;
    and supervised pre-training for auxiliary tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524?source=post_page---------------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: Object detection performance, as measured on the canonical PASCAL VOC dataset,
    has plateaued in the last few years. The...
  prefs: []
  type: TYPE_NORMAL
- en: This is followed by domain-specific fine-tuning that yields a high-performance
    boost. The authors of this paper named the algorithm R-CNN (Regions with CNN features)
    because it combines regional proposals with [convolutional neural networks](https://heartbeat.fritz.ai/a-beginners-guide-to-convolutional-neural-networks-cnn-cf26c5ee17ed).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/59b06d6c6e92c9bbfae706f2f1222ebd.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1311.2524.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: This model takes an image and extracts about 2000 bottom-up region proposals.
    It then computes the features for each proposal using a large CNN. Thereafter,
    it classifies each region using class-specific linear Support Vector Machines
    (SVMs). This model achieves a mean average precision of 53.7% on [PASCAL VOC 2010](http://host.robots.ox.ac.uk/pascal/VOC/voc2010/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: The object detection system in this model has three modules. The first one is
    responsible for generating category-independent regional proposals that define
    the set of candidate detectors available to the model’s detector. The second module
    is a large convolutional neural network responsible for extracting a fixed-length
    feature vector from each region. The third module consists of a class of support
    vector machines.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/d24a28d097e1712f433b14bf995bb069.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1311.2524.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: This model uses selective search to generate regional categories. Selective
    search groups regions that are similar based on color, texture, shape, and size.
    For feature extraction, the model uses a 4096-dimensional feature vector by applying
    the Caffe CNN implementation on each regional proposal. Forward propagating a
    227 × 227 RGB image through five convolutional layers and two fully connected
    layers computes the features. The model explained in this paper achieves a 30%
    relative improvement over the previous results on PASCAL VOC 2012.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the drawbacks of R-CNN are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training is a multi-stage pipeline.** Tuning a convolutional neural network
    on object proposals, fitting SVMs to the ConvNet features, and finally learning
    bounding box regressors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training is expensive in space and time** because of deep networks such as
    VGG16, which take up huge amounts of space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object detection is slow** because it performs a ConvNet forward pass for
    each object proposal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast R-CNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this paper, a Fast Region-based Convolutional Network method (Fast R-CNN)
    for object detection is proposed.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Fast R-CNN](https://arxiv.org/abs/1504.08083?source=post_page---------------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN)
    for object detection. Fast R-CNN...
  prefs: []
  type: TYPE_NORMAL
- en: It’s implemented in [Python and in C++ using Caffe](https://github.com/rbgirshick/fast-rcnn).
    This model achieves a mean average precision of 66% on PASCAL VOC 2012, versus
    62% for R-CNN.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/e68ab3603e1ce7abf96c668a6a6462d8.png)'
  prefs: []
  type: TYPE_IMG
- en: '[https://arxiv.org/pdf/1504.08083.pdf](https://arxiv.org/pdf/1504.08083.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: In comparison to the R-CNN, Fast R-CNN has a higher mean average precision,
    single stage training, training that updates all network layers, and disk storage
    isn’t required for feature caching.
  prefs: []
  type: TYPE_NORMAL
- en: In its architecture, a Fast R-CNN, takes an image as input as well as a set
    of object proposals. It then processes the image with convolutional and max-pooling
    layers to produce a convolutional feature map. A fixed-layer feature vector is
    then extracted from each feature map by a region of interest pooling layer for
    each region proposal.
  prefs: []
  type: TYPE_NORMAL
- en: The feature vectors are then fed to fully connected layers. These then branch
    into two output layers. One produces softmax probability estimates over several
    object classes, while the other produces four real-value numbers for each of the
    object classes. These 4 numbers represent the position of the bounding box for
    each of the objects.
  prefs: []
  type: TYPE_NORMAL
- en: Faster R-CNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**[Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497?source=post_page---------------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: State-of-the-art object detection networks depend on region proposal algorithms
    to hypothesize object locations...
  prefs: []
  type: TYPE_NORMAL
- en: This paper proposes a training mechanism that alternates fine-tuning for regional
    proposal tasks and fine-tuning for object detection.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/64825171bc64b80b110181d76c89b17c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1506.01497.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Faster R-CNN model is comprised of two modules: a deep convolutional network
    responsible for proposing the regions, and a Fast R-CNN detector that uses the
    regions. The Region Proposal Network takes an image as input and generates an
    output of rectangular object proposals. Each of the rectangles has an objectness
    score.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/1f7a4a99e387bb4ca9393c40f6b4b3e9.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1506.01497.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Mask R-CNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**[Mask R-CNN](https://arxiv.org/abs/1703.06870?source=post_page---------------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: We present a conceptually simple, flexible, and general framework for object
    instance segmentation. Our approach...
  prefs: []
  type: TYPE_NORMAL
- en: The model presented in this paper is an extension of the Faster R-CNN architecture
    described above. It also allows for the estimation of human poses.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/d59e7349c9d3b2c88fcc68bf08fd32f6.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1703.06870.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: In this model, objects are classified and localized using a bounding box and
    semantic segmentation that classifies each pixel into a set of categories. This
    model extends Faster R-CNN by adding the prediction of segmentation masks on each
    Region of Interest. The Mask R-CNN produces two outputs; a class label and a bounding
    box.
  prefs: []
  type: TYPE_NORMAL
- en: 'SSD: Single Shot MultiBox Detector'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**[SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325?source=post_page---------------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: We present a method for detecting objects in images using a single deep neural
    network. Our approach, named SSD...
  prefs: []
  type: TYPE_NORMAL
- en: This paper presents a model to predict objects in images using a single deep
    neural network. The network generates scores for the presence of each object category
    using small convolutional filters applied to feature maps.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/d37869ee91bde630ec338c0824790c64.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1512.02325.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: This approach uses a feed-forward convolutional neural network that produces
    a collection of bounding boxes and scores for the presence of certain objects.
    Convolutional feature layers are added to allow for feature detection at multiple
    scales. In this model, each feature map cell is linked to a set of default bounding
    boxes. The figure below shows how SSD512 performs on animals, vehicles, and furniture.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/ce5c9b600e729d6e72190673cacbbd2f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1512.02325.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '**You Only Look Once (YOLO)**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper proposes a single neural network to predict bounding boxes and class
    probabilities from an image in a single evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '[**You Only Look Once: Unified, Real-Time Object Detection**](https://arxiv.org/abs/1506.02640?source=post_page---------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: We present YOLO, a new approach to object detection. Prior work on object detection
    repurposes classifiers to perform...
  prefs: []
  type: TYPE_NORMAL
- en: The YOLO models process 45 frames per second in real-time. YOLO views image
    detection as a regression problem, which makes its pipeline quite simple. It’s
    extremely fast because of this simple pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: It can process a streaming video in real-time with a latency of less than 25
    seconds. During the training process, YOLO sees the entire image and is, therefore,
    able to include the context in object detection.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/29be92e8209c12e5a7016dfcc778304b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1506.02640.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: In YOLO, each bounding box is predicted by features from the entire image. Each
    bounding box has 5 predictions; x, y, w, h, and confidence. (x, y) represents
    the center of the bounding box relative to the bounds of the grid cell. *w* and *h* are
    the predicted width and height of the whole image.
  prefs: []
  type: TYPE_NORMAL
- en: This model is implemented as a convolutional neural network and evaluated on
    the PASCAL VOC detection dataset. The convolutional layers of the network are
    responsible for extracting the features, while the fully connected layers predict
    the coordinates and output probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3e8667f8e1f27accd075c933af61233.png)'
  prefs: []
  type: TYPE_IMG
- en: The network architecture for this model is inspired by the [GoogLeNet](https://ai.google/research/pubs/pub43022) model
    for image classification. The network has 24 convolutional layers and 2 fully-connected
    layers. The main challenges of this model are that it can only predict one class,
    and it doesn’t perform well on small objects such as birds.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/90aae58fca30371d8f0f137ea908d3ff.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1506.02640.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: This model achieves a mean average precision of 52.7%, it is, however, able
    to go up to 63.4%.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc635dc346ab80d50a6b142487614d29.png)'
  prefs: []
  type: TYPE_IMG
- en: Objects as Points
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper proposes modeling an object as a single point. It uses key point
    estimation to find center points and regresses to all other object properties.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Objects as Points**](https://arxiv.org/abs/1904.07850v2?source=post_page---------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Detection identifies objects as axis-aligned boxes in an image. Most successful
    object detectors enumerate a nearly...
  prefs: []
  type: TYPE_NORMAL
- en: These properties include 3D location, pose orientation, and size. It uses CenterNet,
    a center point based approach that’s faster and more accurate compared to other
    bounding box detectors.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6fd2f8d7c60edc2a759f78d0c7a68a13.png)'
  prefs: []
  type: TYPE_IMG
- en: Properties such as object size and pose are regressed from features of the image
    at the center location. In this model, an image is fed to a convolutional neural
    network which generates a heatmap. The maximum values in these heatmaps represent
    the centers of the objects in the image. In order to estimate human poses, the
    model examines 2D joint locations and regresses them at the center point location.
  prefs: []
  type: TYPE_NORMAL
- en: This model achieves 45.1% COCO average precision at 1.4 frames per second. The
    figure below shows how this compared with the results obtained in other research
    papers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3b0114b26c4cef0400b77c7f10e8ade.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Learning Data Augmentation Strategies for Object Detection**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data augmentation involves the process of creating new image data by manipulating
    the original image by, for example, rotating and resizing.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Learning Data Augmentation Strategies for Object Detection**](https://arxiv.org/abs/1906.11172v1?source=post_page---------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation is a critical component of training deep learning models.
    Although data augmentation has been shown...
  prefs: []
  type: TYPE_NORMAL
- en: While this isn’t itself a model architecture, this paper proposes the creation
    of transformations that can be applied to object detection datasets that can be
    transferred to other objection detection datasets. The transformations are usually
    applied at training time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29cf759a0f56409a6ffb612bf5e18499.png)'
  prefs: []
  type: TYPE_IMG
- en: In this model, an augmentation policy is defined as a set of n policies that
    are selected at random during the training process. Some of the operations that
    have been applied in this model include distorting color channels, distorting
    the images geometrically, and distorting only the pixel content found in the bounding
    box annotations.
  prefs: []
  type: TYPE_NORMAL
- en: Experimentation on the COCO dataset has shown that optimizing a data augmentation
    policy is able to improve the accuracy of detection by more than +2.3 mean average
    precision. This allows a single inference model to achieve an accuracy of 50.7
    mean average precision.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We should now be up to speed on some of the most common—and a couple of recent—techniques
    for performing object detection in a variety of contexts.
  prefs: []
  type: TYPE_NORMAL
- en: The papers/abstracts mentioned and linked to above also contain links to their
    code implementations. We’d be happy to see the kind of results you obtain after
    testing them.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s not limit ourselves. Object detection can also live inside your smartphone. [Learn
    how](https://fritz.ai/?utm_campaign=feature2&utm_source=heartbeat) Fritz can teach
    mobile apps to see, hear, sense, and think.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Bio: [Derrick Mwiti](https://derrickmwiti.com/)** is a data analyst, a writer,
    and a mentor. He is driven by delivering great results in every task, and is a
    mentor at Lapid Leaders Africa.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Object Detection with Luminoth](/2019/03/object-detection-luminoth.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An Overview of Human Pose Estimation with Deep Learning](/2019/06/human-pose-estimation-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Large-Scale Evolution of Image Classifiers](/2019/05/large-scale-evolution-image-classifiers.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[What Is the Difference Between SQL and Object-Relational Mapping (ORM)?](https://www.kdnuggets.com/2022/02/difference-sql-object-relational-mapping-orm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets™ News 22:n09, Mar 2: Telling a Great Data Story: A…](https://www.kdnuggets.com/2022/n09.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Beginner''s Guide to Anomaly Detection Techniques in Data Science](https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Perform Motion Detection Using Python](https://www.kdnuggets.com/2022/08/perform-motion-detection-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using Hugging Face Transformers for Emotion Detection in Text](https://www.kdnuggets.com/using-hugging-face-transformers-for-emotion-detection-in-text)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, August 17: How to Perform Motion Detection Using…](https://www.kdnuggets.com/2022/n33.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
