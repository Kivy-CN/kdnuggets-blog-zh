- en: 'Deep Learning Research Review: Natural Language Processing'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习研究回顾：自然语言处理
- en: 原文：[https://www.kdnuggets.com/2017/01/deep-learning-review-natural-language-processing.html/3](https://www.kdnuggets.com/2017/01/deep-learning-review-natural-language-processing.html/3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/01/deep-learning-review-natural-language-processing.html/3](https://www.kdnuggets.com/2017/01/deep-learning-review-natural-language-processing.html/3)
- en: '[Memory Networks](https://arxiv.org/pdf/1410.3916v11.pdf)'
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[记忆网络](https://arxiv.org/pdf/1410.3916v11.pdf)'
- en: '**Introduction**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介**'
- en: The first paper, we’re going to talk about is a quite influential publication
    in the subfield of Question Answering. Authored by Jason Weston, Sumit Chopra,
    and Antoine Bordes, this paper introduced a class of models called memory networks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的第一篇论文是问答子领域中非常有影响力的出版物。由 Jason Weston、Sumit Chopra 和 Antoine Bordes 撰写，这篇论文介绍了一类称为记忆网络的模型。
- en: The intuitive idea is that in order to accurately answer a question regarding
    a piece of text, you need to somehow store the initial information given to you.
    If I were to ask you the question “What does RNN stand for”, (assuming you’ve
    read this post fully J) you’ll be able to give me an answer because the information
    you absorbed by reading the first part of this post was stored somewhere in your
    memory. You just had to take a few seconds to locate that info and articulate
    it in words. Now, I have no clue how the brain is able to do that, but the idea
    of having a storage place for this information still remains.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 直观的想法是，为了准确回答关于一段文本的问题，你需要以某种方式存储给定的信息。如果我问你“RNN代表什么”，（假设你已经完全阅读了这篇文章J）你会能够给出答案，因为你通过阅读这篇文章的第一部分吸收的信息被存储在你的记忆中。你只需花几秒钟找到这些信息并用语言表达出来。现在，我不知道大脑是如何做到这一点的，但拥有一个存储这些信息的地方的想法依然存在。
- en: The memory network described in the paper is unique because it has an associative
    memory that it is able to read and write to. It’s interesting to note that we
    don’t have this type of memory with CNNs or with Q-Networks (for reinforcement
    learning) or with traditional neural nets. This is in part because the task of
    question answering relies so heavily upon being able to model or keep track of
    long-term dependencies, such as keeping track of the characters in a story or
    a timeline of events. With CNNs and Q-Networks, “memory” is sort of built into
    the weights of the network as it learns different filters or mappings from states
    to actions. At first look, RNNs and LSTMs could be used, but these typically aren’t
    able to remember or memorize inputs from the past (which in question answering
    is quite critical).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中描述的记忆网络是独特的，因为它具有一个可以读写的关联记忆。值得注意的是，我们在 CNNs 或 Q-Networks（用于强化学习）或传统神经网络中没有这种类型的记忆。这部分是因为问答任务非常依赖于能够建模或跟踪长期依赖关系，例如跟踪故事中的角色或事件的时间线。在
    CNNs 和 Q-Networks 中，“记忆”在网络的权重中内建，因为它们学习不同的滤波器或从状态到动作的映射。乍一看，RNNs 和 LSTMs 可能会被使用，但它们通常不能记住或记忆过去的输入（在问答中这一点是相当关键的）。
- en: '**Network Architecture**'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络架构**'
- en: Okay, so now let’s look at how this network processes the initial text it is
    given. Like with most machine learning algorithms, the first step is to convert
    the input into a feature representation. This could entail using word vectors,
    part of speech labeling, parsing, etc. It’s really just up to the programmer.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在让我们看看这个网络是如何处理它收到的初始文本的。像大多数机器学习算法一样，第一步是将输入转换为特征表示。这可能涉及使用词向量、词性标注、解析等。具体如何做完全取决于程序员。
- en: '![](../Images/18fd8b22f950e38cdc97b3182d943347.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18fd8b22f950e38cdc97b3182d943347.png)'
- en: The next step would be taking the feature representation I(x) and allowing our
    memory m to be updated to reflect the new input x we’ve received.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将特征表示 I(x) 与我们的记忆 m 更新，以反映我们收到的新输入 x。
- en: '![](../Images/0759548f7c770aaad143599f5b1f6fa0.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0759548f7c770aaad143599f5b1f6fa0.png)'
- en: You can consider the memory m to be a sort of an array that is made up of individual
    memories m[i]. Each of these individual memories m[i] can be a function of the
    memory m as a whole, the feature representation I(x), and/or itself. This function
    G can be as simple as just storing the whole representation I(x) in the individual
    memory unit m[i].  You can modify this function G to update past memories based
    on new input.  The 3^(rd) and 4^(th) steps involve reading from memory, based
    on the question, to get a feature representation o, and then decoding it to output
    a final answer r.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将记忆m视为由单个记忆m[i]组成的数组。每个单独的记忆m[i]可以是记忆m整体、特征表示I(x)和/或自身的函数。这个函数G可以简单地是将整个表示I(x)存储在单个记忆单元m[i]中。你可以修改这个函数G，以基于新输入更新过去的记忆。第3^(rd)和第4^(th)步涉及根据问题从记忆中读取以获取特征表示o，然后解码以输出最终答案r。
- en: '![](../Images/c51c92855304250ec9c570a5dd7cea49.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c51c92855304250ec9c570a5dd7cea49.png)'
- en: The function R could be an RNN that is used to convert the feature representations
    from memory into a readable and accurate answer to the question.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 函数R可以是一个RNN，用于将来自记忆的特征表示转换为对问题的可读且准确的回答。
- en: Now, let’s look at step 3 a little closer. We want this O module to output a
    feature representation that would best match a possible answer to our given question
    x. Now this question is going to be compared to every single individual memory
    unit and is going to be “scored” based on how well the memory unit supports the
    question.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更详细地看一下第3步。我们希望这个O模块输出一个最能匹配给定问题x的可能答案的特征表示。现在这个问题将与每个单独的记忆单元进行比较，并根据记忆单元对问题的支持程度进行“评分”。
- en: '![](../Images/ce837bae010f462353978c643256cac1.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce837bae010f462353978c643256cac1.png)'
- en: We take the argmax of the scoring function to find the output representation
    that best supports the question (You can also take multiple of the highest scoring
    units, doesn’t have to be limited to 1). The scoring function is one that computes
    the matrix product between different embeddings of the question and the chosen
    memory unit[s] (check paper for more details). You can think of this as when you
    multiply the word vectors of two words in order to find their similarity. This
    output representation o is then fed into an RNN or LSTM or another scoring function
    which will output a readable answer.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们取评分函数的最大值以找到最能支持问题的输出表示（你也可以选择多个最高评分单元，不必限制为1）。评分函数计算问题和所选记忆单元之间不同嵌入的矩阵积（详细信息请查阅论文）。你可以将其视为在计算两个词向量的相似度时的乘法。这个输出表示o随后会被送入RNN、LSTM或其他评分函数，以输出可读答案。
- en: This network is trained in a supervised manner where training data includes
    the original text, the question, supporting sentences, and the ground truth answer.
    Here is the objective function.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络以监督方式进行训练，其中训练数据包括原始文本、问题、支持句子和真实答案。这里是目标函数。
- en: '![](../Images/d22f11ae676374de85cb0ac24fdc5315.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d22f11ae676374de85cb0ac24fdc5315.png)'
- en: For those interested, these are some papers that built off of this memory network
    approach
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于感兴趣的人，这里有一些基于这种记忆网络方法的论文
- en: '[End to End Memory Networks](https://arxiv.org/pdf/1503.08895v5.pdf) (only
    requires supervision on outputs, not supporting sentences)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[端到端记忆网络](https://arxiv.org/pdf/1503.08895v5.pdf)（仅需对输出进行监督，无需支持句子）'
- en: '[Dynamic Memory Networks](https://arxiv.org/pdf/1506.07285v5.pdf)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[动态记忆网络](https://arxiv.org/pdf/1506.07285v5.pdf)'
- en: '[Dynamic Coattention Networks](https://arxiv.org/pdf/1611.01604v2.pdf) (Just
    got released 2 months ago and had the highest test score on Stanford’s Question
    Answering Dataset at the time)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[动态共注意网络](https://arxiv.org/pdf/1611.01604v2.pdf)（刚刚发布两个月，曾在斯坦福问答数据集上获得最高测试分数）'
- en: '[Tree LSTMs for Sentiment Analysis](https://arxiv.org/pdf/1503.00075v3.pdf)'
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[树形LSTM用于情感分析](https://arxiv.org/pdf/1503.00075v3.pdf)'
- en: '**Introduction**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介**'
- en: The next paper looks into an advancement in Sentiment Analysis, the task of
    determining whether a phrase has a positive or negative connotation/meaning. More
    formally, sentiment can be defined as “a view or attitude toward a situation or
    event”. At the time, LSTMs were the most commonly used units in sentiment analysis
    networks. Authored by Kai Sheng Tai, Richard Socher, and Christopher Manning,
    this paper introduces a novel way of chaining together LSTMs in a non-linear structure.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下一篇论文研究了情感分析的进展，即确定短语是否具有积极或消极的含义/意义。更正式地说，情感可以定义为“对某个情况或事件的看法或态度”。当时，LSTM是情感分析网络中最常用的单元。由**凯·盛·泰**、**理查德·索彻**和**克里斯托弗·曼宁**撰写，这篇论文介绍了一种在非线性结构中将LSTM串联在一起的新方法。
- en: The motivation behind this non-linear arrangement lies in the notion that natural
    language exhibits the property that words in sequence become phrases. These phrases,
    depending on the order of the words, can hold different meanings from their original
    word components. In order to represent this characteristic, a network of LSTM
    units must be arranged into a tree structure where different units are affected
    by their children nodes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种非线性排列的动机在于自然语言的特性，即词语序列成为短语。这些短语根据词语的顺序，可以具有不同于其原始词汇组件的含义。为了表示这一特性，LSTM单元的网络必须被安排成树形结构，其中不同的单元受到其子节点的影响。
- en: '**Network Architecture**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络架构**'
- en: One of the differences between a Tree-LSTM and a standard one is that the hidden
    state of the latter is a function of the current input and the hidden state at
    the previous time step. However, with a Tree-LSTM, its hidden state is a function
    of the current input and the hidden states of its child units.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Tree-LSTM和标准LSTM之间的一个区别在于，后者的隐藏状态是当前输入和前一个时间步的隐藏状态的函数。然而，Tree-LSTM的隐藏状态是当前输入和其子单元的隐藏状态的函数。
- en: '![](../Images/da2a8163cc3402a3c7fc068feab0c1af.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da2a8163cc3402a3c7fc068feab0c1af.png)'
- en: With this new tree-based structure, there are some mathematical changes including
    child units having forget gates. For those interested in the details, check the
    paper for more info. What I would like to focus on, however, is understanding
    why these models work better than a linear LSTM.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种新的树形结构，一些数学变化包括子单元具有遗忘门。对于那些对细节感兴趣的人，可以查看论文以获取更多信息。然而，我想关注的是理解为什么这些模型比线性LSTM表现更好。
- en: With a Tree-LSTM, a single unit is able to incorporate the hidden states of
    all of its children nodes. This is interesting because a unit is able to value
    each of its children nodes differently. During training, the network could realize
    that a specific word (maybe the word “not” or “very” in sentiment analysis) is
    extremely important to the overall sentiment of the sentence. The ability to value
    that node higher provides a lot of flexibility to network and could improve performance.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Tree-LSTM，一个单元能够整合其所有子节点的隐藏状态。这很有趣，因为一个单元能够对每个子节点赋予不同的权重。在训练过程中，网络可以意识到某个特定词（可能是“not”或“very”在情感分析中）对句子的整体情感非常重要。提高该节点的权重为网络提供了很大的灵活性，并可能提高性能。
- en: '[Neural Machine Translation](https://arxiv.org/pdf/1609.08144v2.pdf)'
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[神经机器翻译](https://arxiv.org/pdf/1609.08144v2.pdf)'
- en: '**Introduction**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**介绍**'
- en: The last paper we’ll look at today describes an approach to the task of Machine
    Translation. Authored by Google ML visionaries Jeff Dean, Greg Corrado, Orial
    Vinyals, and others, this paper introduced a machine translation system that serves
    as the backbone behind Google’s popular Translate service. The system reduced
    translation errors by an average of 60% compared to the previous production system
    Google used.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 今天我们要看的最后一篇论文描述了一种机器翻译任务的方法。由Google ML的远见者**杰夫·迪恩**、**格雷格·科拉多**、**奥里尔·维尼亚尔斯**等人撰写，这篇论文介绍了一个机器翻译系统，这个系统成为了Google流行翻译服务的核心。与Google之前使用的生产系统相比，该系统将翻译错误减少了平均60%。
- en: Traditional approaches to automated translation include variants of phrase-based
    matching. This approach required large amounts of linguistic domain knowledge
    and ultimately its design proved to be too brittle and lacked generalization ability.
    One of the problems with the traditional approach was that it would try to translate
    the input sentence piece by piece. It turns out the more effective approach (that
    NMT uses) is to translate the whole sentence at a time, thus allowing for a broader
    context and a more natural rearrangement of words.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的自动翻译方法包括短语匹配的变体。这种方法需要大量的语言领域知识，最终其设计证明过于脆弱，缺乏泛化能力。传统方法的问题之一是它试图逐块翻译输入句子。事实证明，更有效的方法（NMT使用的方法）是一次翻译整个句子，从而允许更广泛的上下文和更自然的词语重排。
- en: '**Network Architecture**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络架构**'
- en: The authors in this paper introduce a deep LSTM network that can be trained
    end to end with 8 encoder and decoder layers.  We can separate the system into
    3 components, the encoder RNN, decoder RNN, and attention module. From a high
    level, the encoder works on the task on transforming the input sentence to vector
    representation, the decoder produces the output representation, and then the attention
    module tells the decoder what to focus on during the task of decoding (This is
    where the idea of utilizing the whole context of the sentence comes in).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中的作者介绍了一种深度LSTM网络，该网络可以通过8个编码器和解码器层进行端到端训练。我们可以将系统分为三个组件：编码器RNN、解码器RNN和注意力模块。从高层次来看，编码器负责将输入句子转换为向量表示，解码器生成输出表示，然后注意力模块告诉解码器在解码任务中应关注什么（这就是利用整个句子上下文的想法所在）。
- en: '![](../Images/c08745cc823c2e6966908b33846bc511.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c08745cc823c2e6966908b33846bc511.png)'
- en: The rest of the paper mainly focuses on the challenges associated with deploying
    such a service at scale. Topics such as amount of computational resources, latency,
    and high volume deployment are discussed at length.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的其余部分主要集中在大规模部署此类服务所面临的挑战上。讨论了计算资源的数量、延迟以及高量级部署等主题。
- en: Conclusion
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: With that, we conclude this post on how deep learning can contribute to natural
    language processing tasks. In my mind, some future goals in the field could be
    to improve customer service chatbots, perfect machine translation, and hopefully
    get question answering systems to obtain a deeper understanding of unstructured
    or lengthy pieces of text (like Wikipedia pages).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们结束了关于深度学习如何促进自然语言处理任务的讨论。在我看来，未来该领域的一些目标可能包括改善客户服务聊天机器人，完善机器翻译，并希望使问答系统能够更深入地理解非结构化或冗长的文本（例如维基百科页面）。
- en: Special thanks to Richard Socher and the staff behind [Stanford CS 224D](http://cs224d.stanford.edu/index.html).
    Great slides (most of the images are attributed to their slides) and fantastic
    lectures.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 特别感谢Richard Socher和[斯坦福CS 224D](http://cs224d.stanford.edu/index.html)的工作人员。精彩的幻灯片（大部分图片归功于他们的幻灯片）和精彩的讲座。
- en: '**Bio: [Adit Deshpande](https://twitter.com/aditdeshpande3)** is currently
    a second year undergraduate student majoring in computer science and minoring
    in Bioinformatics at UCLA. He is passionate about applying his knowledge of machine
    learning and computer vision to areas in healthcare where better solutions can
    be engineered for doctors and patients.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Adit Deshpande](https://twitter.com/aditdeshpande3)** 目前是加州大学洛杉矶分校计算机科学专业的二年级本科生，同时辅修生物信息学。他对将机器学习和计算机视觉的知识应用于医疗保健领域充满热情，以便为医生和患者工程出更好的解决方案。'
- en: '[Original](https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-3-Natural-Language-Processing).
    Reposted with permission.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-3-Natural-Language-Processing)。经许可转载。'
- en: '**Related:**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Deep Learning Research Review: Reinforcement Learning](/2016/11/deep-learning-research-review-reinforcement-learning.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习研究回顾：强化学习](/2016/11/deep-learning-research-review-reinforcement-learning.html)'
- en: '[Deep Learning Research Review: Generative Adversarial Nets](/2016/10/deep-learning-research-review-generative-adversarial-networks.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习研究回顾：生成对抗网络](/2016/10/deep-learning-research-review-generative-adversarial-networks.html)'
- en: '[KDnuggets Top Blogger: An Interview with Adit Deshpande, Deep Learning Aficionado](/2016/10/top-blogger-interview-adit-deshpande.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets顶级博主：与深度学习爱好者Adit Deshpande的访谈](/2016/10/top-blogger-interview-adit-deshpande.html)'
- en: '* * *'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的 IT 需求'
- en: '* * *'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[建立一个坚实的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用管道编写干净的 Python 代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么让 Python 成为初创公司的理想编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
- en: '[N-gram Language Modeling in Natural Language Processing](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[N-gram 语言模型在自然语言处理中的应用](https://www.kdnuggets.com/2022/06/ngram-language-modeling-natural-language-processing.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学，找到目标，找到目标去…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
