- en: 'Optimization in Machine Learning: Robust or global minimum?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的优化：稳健还是全局最小值？
- en: 原文：[https://www.kdnuggets.com/2017/06/robust-global-minimum.html](https://www.kdnuggets.com/2017/06/robust-global-minimum.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/06/robust-global-minimum.html](https://www.kdnuggets.com/2017/06/robust-global-minimum.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png)[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png)[comments](#comments)'
- en: '**By Nikolaos Vasiloglou**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：Nikolaos Vasiloglou**'
- en: '![Optimization in Machine Learning](../Images/a067afc3bc25ba23a5ea3941a8ccf004.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![Optimization in Machine Learning](../Images/a067afc3bc25ba23a5ea3941a8ccf004.png)'
- en: 'In 2014 the Facebook best paper award @UAI was awarded to *“Universal Convexification
    via Risk-Aversion” b.y* Krishnamurthy Dvijotham; Maryam Fazel; Emanuel Todorov
    [http://www.auai.org/uai2014/proceedings/individuals/121.pdf](http://www.auai.org/uai2014/proceedings/individuals/121.pdf).
    Three years later we follow up with the authors in a brief interview:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在2014年，Facebook最佳论文奖 @UAI 授予 *“Universal Convexification via Risk-Aversion”
    b.y* Krishnamurthy Dvijotham; Maryam Fazel; Emanuel Todorov [http://www.auai.org/uai2014/proceedings/individuals/121.pdf](http://www.auai.org/uai2014/proceedings/individuals/121.pdf)。三年后，我们在一次简短的采访中跟进了作者：
- en: '**NV:** First of all congratulations for the award. We understand that in convex
    problems it is much easier to find the global optimum.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**NV:** 首先祝贺你们获奖。我们知道在凸问题中，找到全局最优解要容易得多。'
- en: '**KD, MF:** Thank you! We appreciate the opportunity to participate in this
    discussion.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**KD, MF:** 谢谢！我们很感激有机会参与这个讨论。'
- en: '**1\. NV: Does the convexified problem always have the same minimum with the
    original problem**?'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. NV: 凸化问题是否总是具有与原始问题相同的最小值？**'
- en: '**KD, MF:** No, the convexified problem can have a minimum that is quite different
    from the original problem. The motivation for our paper comes from the fact that
    in many problems (like control and reinforcement learning) one is interested in
    a “robust” minimum (a minimum such that the cost does not increase much when you
    perturb the parameters). Our method destroys non-robust minima and preserves a
    single robust minimum of the problem.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**KD, MF:** 不，凸化的问题可能具有与原始问题相当不同的最小值。我们论文的动机来源于许多问题（如控制和强化学习）中人们对“稳健”最小值（即当扰动参数时成本不会大幅增加的最小值）的兴趣。我们的方法破坏了非稳健最小值，保留了问题的唯一稳健最小值。'
- en: '**2\. NV: We have seen a lot of interest in analyzing non-convex problems that
    have all their local minimum equivalent to the global minimum. For example A.
    Anandkumar’s group has been working on how to guarantee minimums in nonConvex
    problems. How does your work compare to that direction?**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. NV: 我们已经看到很多对分析所有局部最小值等于全局最小值的非凸问题的兴趣。例如A. Anandkumar的团队一直在研究如何保证非凸问题中的最小值。你的工作与这个方向相比如何？**'
- en: '**KD:** Yes, the results along these lines are very interesting and have potential
    connections with our work. In particular, the papers [https://arxiv.org/abs/1503.03712](https://arxiv.org/abs/1503.03712),
    [https://arxiv.org/pdf/1610.09322.pdf](https://arxiv.org/pdf/1610.09322.pdf) study
    the approach of smoothing nonconvex functions to eliminate local minima, but do
    this in a gradual fashion (starting with a large amount of smoothing and gradually
    reduce it while tracking the global minimum). Our approach does this in a one-shot
    way (adds a large amount of noise/risk-aversion) to directly convexify the problem,
    but this can be improved (in the sense of allowing weaker assumptions to guarantee
    location of a robust minimum) potentially by doing this in a graduated fashion.
    We are excited to further explore these connections.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**KD:** 是的，这些结果非常有趣，并且与我们的工作有潜在的联系。特别是，论文 [https://arxiv.org/abs/1503.03712](https://arxiv.org/abs/1503.03712)
    和 [https://arxiv.org/pdf/1610.09322.pdf](https://arxiv.org/pdf/1610.09322.pdf)
    研究了平滑非凸函数以消除局部最小值的方法，但采用了逐渐的方法（从大量平滑开始，并逐渐减少，同时跟踪全局最小值）。我们的方法是一次性完成的（添加大量噪声/风险规避）以直接凸化问题，但通过逐渐的方式可能可以改进（在允许较弱假设的意义上，保证一个稳健最小值的位置）。我们期待进一步探索这些联系。'
- en: '**MF:** Just to add that our work (as well as the papers KD mentioned above)
    are not focused on the restricted case where all local minima are equivalent to
    the global minimum.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**MF:** 只是补充一下，我们的工作（以及KD上面提到的论文）并不专注于所有局部最小值等同于全局最小值的限制情况。'
- en: '**3\. NV:**  **The biggest headache is with Deep Learning. You have an example
    with MNIST on your paper. Have you tried it in other cases?**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. NV:** **最大的难题是深度学习。你的论文中有一个MNIST的例子。你们尝试过其他案例吗？**'
- en: '**KD:** The results we had on MNIST were quite preliminary and definitely need
    more careful investigation and comparison with the latest deep learning algorithms
    . Unfortunately, we did not have a chance to follow up with more detailed experiments
    – I left for a postdoc shortly after we wrote this paper and shifted focus to
    other topics.  We plan to get back to this over the next few months and write
    an extended version of this paper. However, several papers by other authors have
    appeared in the literature that explore similar ideas of adding noise to neural
    networks to improve optimization and generalization (for example, [https://openreview.net/forum?id=B1YfAfcgl](https://openreview.net/forum?id=B1YfAfcgl)).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**KD：** 我们在 MNIST 上的结果相当初步，确实需要更仔细的调查，并与最新的深度学习算法进行比较。不幸的是，我们没有机会进行更详细的实验——在我们撰写这篇论文后不久，我就离开去做博士后，并将重点转向其他主题。我们计划在接下来的几个月里重新研究这个问题，并撰写这篇论文的扩展版本。然而，文献中出现了几篇其他作者的论文，探讨了向神经网络中添加噪声以改善优化和泛化的类似想法（例如，[https://openreview.net/forum?id=B1YfAfcgl](https://openreview.net/forum?id=B1YfAfcgl)）。'
- en: '**4\. NV: The deep learning enthusiasts say that SGD works most of the times
    and with a couple of restarts you train your network. How do you comment on that?
    Why do you think your method is still better?**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**4. NV：深度学习爱好者说 SGD 大多数时候都能工作，并且经过几次重启你就能训练好网络。你对此有何评论？你认为你的方法为何仍然更好？**'
- en: '**KD:** The contribution of our paper is a specific approach that quantifies
    precisely how much noise/risk-aversion one needs to convexify the problem. We
    do not expect the approach to compete directly with SGD at least for standard
    supervised learning problems, although it may offer some insights on how much
    noise to add so that the training is less susceptible to local minima (**MF:**
    Recently, noisy gradient descent and Langevin dynamics have received attention
    for a similar goal).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**KD：** 我们论文的贡献是一个具体的方法，它精确量化了需要多少噪声/风险厌恶才能使问题凸化。我们不期望这种方法直接与 SGD 竞争，至少对于标准的监督学习问题是这样，尽管它可能会提供一些关于添加多少噪声的见解，以便训练对局部极小值的敏感性更低（**MF：**
    最近，噪声梯度下降和 Langevin 动力学因类似目标而受到关注）。'
- en: However, our main motivation for developing this was in RL and control problems
    where robustness is inherently a desirable quality (for example it is a way to
    ensure that a robotic system will not violate safety constraints in spite of perturbations
    and disturbances). Our intuition was that finding robust minima is actually easier
    and the theory developed in the paper provides some rigorous justification for
    this statement.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们开发这个方法的主要动机是在 RL 和控制问题中，其中鲁棒性本身是一种理想的特质（例如，它是一种确保机器人系统在扰动和干扰下不会违反安全约束的方法）。我们的直觉是，找到鲁棒的极小值实际上更容易，论文中提出的理论对此声明提供了一些严格的理由。
- en: '**6\. NV: How was your paper received from the community? Were there any follow
    ups? Your method seems very straightforward to implement. Do you think that a
    tensorFlow implementation (or any other popular package) would help people adopt
    it more?**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**6. NV：你的论文在社区中的反响如何？有没有后续工作？你的方法似乎很直接实现。你认为 TensorFlow 实现（或其他流行的包）会更有助于人们采纳它吗？**'
- en: '**KD:** We have received some followup from recent papers applying similar
    ideas to computer vision ([https://link.springer.com/chapter/10.1007/978-3-319-14612-6_4](https://link.springer.com/chapter/10.1007/978-3-319-14612-6_4))
    and general nonconvex optimization ([https://arxiv.org/abs/1602.02191](https://arxiv.org/abs/1602.02191)).
    However, since we did not continue to work on this ourselves, the paper was not
    well publicized. We hope to get back to it soon, improve the algorithm to make
    it practical and publish an open source implementation. The basic method in the
    paper is straightforward to implement. However, to achieve good performance in
    practice, we believe that the approach needs to be combined with variance reduction
    ideas developed by researchers working on SGD in recent years. As I said in the
    previous question, our view is that the method is most appropriate for problems
    where robustness is of direct interest as opposed to standard supervised learning
    problems. We hope to develop an implementation of the approach suitable to RL
    problems using the OpenAIgym environment ([https://gym.openai.com/](https://gym.openai.com/))
    over the next few months and make this publicly available.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**KD:** 我们收到了来自最近应用类似想法于计算机视觉的论文的一些后续跟进 ([https://link.springer.com/chapter/10.1007/978-3-319-14612-6_4](https://link.springer.com/chapter/10.1007/978-3-319-14612-6_4))
    和一般非凸优化的论文 ([https://arxiv.org/abs/1602.02191](https://arxiv.org/abs/1602.02191))。然而，由于我们没有继续进行这项工作，这篇论文没有得到广泛宣传。我们希望尽快回到这项工作中，改进算法以使其实际可用，并发布一个开源实现。论文中的基本方法是直接实现的。然而，为了在实践中取得良好的性能，我们认为这种方法需要与近年来从事SGD研究的学者们开发的方差减少思想相结合。正如我在前面的问题中所说，我们的观点是这种方法最适用于那些对鲁棒性有直接兴趣的问题，而不是标准的监督学习问题。我们希望在接下来的几个月中，开发一个适用于RL问题的实现，使用OpenAIgym环境
    ([https://gym.openai.com/](https://gym.openai.com/)) 并公开发布。'
- en: '**7\. NV: Tell us about other successful approaches in the problem of finding
    the global minimum in practice.**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**7\. NV:** 讲述一下在实际中找到全局最小值的其他成功方法。'
- en: '**KD:** Global optimization of nonconvex problems is challenging and difficult
    in general. In the optimization community, several approaches have been developed
    that rely on branch and bound, i.e., constructing and refining convex relaxations
    combined with some kind of search strategy (see, for example BARON: [http://archimedes.cheme.cmu.edu/?q=baron](http://archimedes.cheme.cmu.edu/?q=baron)).
    For problems with no additional structure, these approaches are the best one can
    hope for. However, the performance of these algorithms on any specific problem
    can vary widely (in the worst case they are doing something akin to a brute force
    search). For the case of polynomial optimization problems, the Lasserre hierarchy
    ([http://homepages.laas.fr/lasserre/book-flyer.pdf](http://homepages.laas.fr/lasserre/book-flyer.pdf))
    allows the computation of global optima via a hierarchy of semidefinite programs
    growing in size. In theory, one needs to solve very large semidefinite programs
    to find the global optimum, but in practice, for several instances of polynomial
    optimization problems, a reasonably sized semidefinite program suffices to compute
    the global optimum.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**KD:** 非凸问题的全局优化通常具有挑战性和困难。在优化社区中，已经开发出几种依赖于分支定界的方法，即构建和改进凸松弛，并结合某种搜索策略（例如
    BARON: [http://archimedes.cheme.cmu.edu/?q=baron](http://archimedes.cheme.cmu.edu/?q=baron)）。对于没有额外结构的问题，这些方法是最佳的。然而，这些算法在任何特定问题上的性能可能会有很大的变化（在最坏的情况下，它们可能类似于暴力搜索）。对于多项式优化问题，Lasserre层次结构
    ([http://homepages.laas.fr/lasserre/book-flyer.pdf](http://homepages.laas.fr/lasserre/book-flyer.pdf))
    允许通过逐渐增大的半正定程序的层次来计算全局最优解。理论上，需要解决非常大的半正定程序以找到全局最优解，但在实践中，对于一些多项式优化问题，适当大小的半正定程序足以计算全局最优解。'
- en: '**8\. NV: Is it worth looking for convex envelopes for nonconvex problems?
    How much slower would your algorithm would be compared to the convex envelope?**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**8\. NV:** 对于非凸问题，寻找凸包是否值得？与凸包相比，你的算法会慢多少？'
- en: '**KD:** This is a great question. The problem of constructing the convex envelope
    of a general nonconvex function is again NP-hard and difficult except in special
    cases. In the very special cases where constructing a convex envelope is easy,
    one can minimize this envelope if one is interested in finding the minimum of
    the original objective function. As stated earlier, however, the point of our
    approach is to find a robust minimum rather than the global minimum and it is
    not obvious that the convex envelope will be useful for this goal. It may also
    be possible to interpret our approach in terms of a different kind of envelope
    known as the Moreau envelope; we are currently investigating these connections.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**KD:** 这是一个很好的问题。构造一般非凸函数的凸包问题仍然是NP难的，除非在特殊情况下，否则很难解决。在那些构造凸包比较容易的特殊情况下，如果你有兴趣找到原始目标函数的最小值，可以最小化这个凸包。然而，正如之前所述，我们的方法的重点是找到一个鲁棒的最小值，而不是全局最小值，因此凸包是否对这个目标有用并不明显。也可能通过另一种称为莫罗包络的包络来解释我们的方法；我们目前正在研究这些联系。'
- en: '**9\. NV: In your experiments you emphasize more on the generalization error
    and less on the proximity to the global minimum.  Why is that? Does your method
    actually solve both problems?**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**9\. NV: 在你的实验中，你更强调泛化误差而不是全局最小值的接近程度。这是为什么？你的方法实际上解决了这两个问题吗？**'
- en: '**KD:** Our approach can produce an optimum that is far from the global minimum
    since it tries to find a robust minimum. The hypothesis that we tested in the
    experiments was that perhaps the robust minimum is better than the global minimum
    in terms of generalization error – the reason to expect this is that in the experiments,
    we added noise to the prediction of the SVM (w’x+b) and the intermediate neurons
    of a neural network. The idea was that adding noise of this kind and trying to
    minimize the expected learning loss over the noise  tries to ensure that the prediction
    of the algorithm remains stable not just for a given input but also for a range
    of noisy perturbations around it. Of course, this was not really justified rigorously
    in our paper, but the preliminary experiments we had indicated that this was indeed
    the case. We are investigating ways to formalize this; it has been known that
    stability of learning algorithms to perturbations is connected to generalization
    ability [http://www.jmlr.org/papers/volume2/bousquet02a/bousquet02a.pdf](http://www.jmlr.org/papers/volume2/bousquet02a/bousquet02a.pdf)).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**KD:** 我们的方法可能会产生一个远离全局最小值的最优解，因为它试图找到一个鲁棒的最小值。我们在实验中测试的假设是，也许鲁棒最小值在泛化误差方面优于全局最小值——期望这样是因为在实验中，我们对SVM（w’x+b）和神经网络的中间神经元的预测添加了噪声。这个想法是，添加这种噪声并尝试最小化噪声上的期望学习损失，试图确保算法的预测不仅对给定输入保持稳定，还对其周围的一系列噪声扰动保持稳定。当然，在我们的论文中，这一点并没有被严格证明，但我们进行的初步实验表明确实如此。我们正在研究将这一点形式化的方法；已知学习算法对扰动的稳定性与泛化能力有关
    [http://www.jmlr.org/papers/volume2/bousquet02a/bousquet02a.pdf](http://www.jmlr.org/papers/volume2/bousquet02a/bousquet02a.pdf)。'
- en: '**10\. NV: Your method adds noise to the input and optimizes over the expectation?
    Is there a link with bayesian methods? Any connections to accelerated Monte Carlo
    variants? Also does your method provide a distribution for the learned model?**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**10\. NV: 你的方法对输入添加噪声并在期望上进行优化吗？这与贝叶斯方法有联系吗？与加速的蒙特卡洛变体有任何联系吗？另外，你的方法是否提供了学习模型的分布？**'
- en: '**KD:** There is no direct link since the noise in our approach is injected
    on purpose and does not come from a probabilistic model as in the Bayesian approach
    . Also, at the end, we do not obtain a posterior distribution over the model parameters
    as in a Bayesian approach – only a point estimate. There may be ways to leverage
    Monte Carlo algorithms to obtain a better estimate of the stochastic gradient
    sample but we have not studied that yet. Our method only produces a single estimate
    of the model parameters, not an entire distribution.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**KD:** 由于我们的方法中注入噪声是有意为之，并非像贝叶斯方法中的概率模型那样产生的，因此没有直接的联系。此外，最后我们没有得到模型参数的后验分布，而只是一个点估计。可能有方法利用蒙特卡洛算法来获得更好的随机梯度样本估计，但我们还没有研究过。我们的方法仅产生模型参数的单一估计，而不是整个分布。'
- en: '**11\. NV:  Are there any connections of your method with dropout (with regards
    to improving generalization)?**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**11\. NV: 你的方法与dropout（在提高泛化能力方面）有任何联系吗？**'
- en: '**KD,MF:** We are not aware of any direct connections.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**KD,MF:** 我们不知道任何直接的联系。'
- en: '**12\. NV: The best paper at ICLR 2017 “**[Understanding Deep Learning required
    Rethinking Generalization](https://openreview.net/pdf?id=Sy8gdB9xx)**” ends with
    the following phrase: “*Another insight resulting from our experiments is that
    optimization continues to be empirically easy even if the resulting model does
    not generalize. This shows that the reasons for why optimization is empirically
    easy must be different from the true cause of generalization*”. You seem to treat
    both (optimization and generalization) at the same time. Does your approach contradict
    the findings of this paper?**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**12\. NV: ICLR 2017 最佳论文 “**[理解深度学习需要重新思考泛化](https://openreview.net/pdf?id=Sy8gdB9xx)**”
    以如下话语结束： “*从我们的实验中得到的另一个见解是，即使得到的模型不能泛化，优化仍然在经验上是容易的。这表明，优化在经验上容易的原因必须与泛化的真实原因不同*”。你似乎同时处理了这两个（优化和泛化）。你的方法是否与这篇论文的发现相矛盾？**'
- en: '**KD:** I have not studied this paper carefully, but it does have an interesting
    observation. As I mentioned earlier, our approach does seem to indicate that finding
    robust minima is easier than finding the global and there is some evidence (from
    our experiments and other learning theory papers) that robustness and generalization
    are connected. However, our experiments were fairly preliminary and used very
    small data sets and neural networks. Recent work in deep learning has shown that
    optimization issues (local minima etc.) tend to go away in the realm of big data
    and large neural networks – thus the situation studied in the recent ICLR paper
    may actually present a different regime where both optimization and generalization
    have different qualitative properties than a setup with a small number of model
    parameters and small datasets. I would not say that the two studies contradict
    each other but the new ICLR work is intriguing since it goes against the conventional
    wisdom on learning and generalization.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**KD：** 我尚未仔细研究这篇论文，但它确实有一个有趣的观察。正如我之前提到的，我们的方法确实表明，找到稳健的最小值比找到全局最小值更容易，并且有一些证据（来自我们的实验和其他学习理论论文）表明稳健性和泛化是相关的。然而，我们的实验相当初步，并使用了非常小的数据集和神经网络。最近的深度学习工作显示，在大数据和大规模神经网络领域，优化问题（局部最小值等）往往会消失——因此，最近的
    ICLR 论文所研究的情况可能实际上呈现了一个不同的状态，其中优化和泛化具有不同的定性特征，而不是少量模型参数和小数据集的设置。我不会说这两项研究相互矛盾，但新的
    ICLR 工作令人着迷，因为它违背了关于学习和泛化的传统智慧。'
- en: '[Original](https://www.linkedin.com/pulse/robust-global-minimum-nikolaos-vasiloglou).
    Reposted with permission.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.linkedin.com/pulse/robust-global-minimum-nikolaos-vasiloglou)。已获得许可转载。'
- en: '**Bio: **[Nikolaos Vasiloglou](https://www.linkedin.com/in/vasiloglou/) ****is Organizing
    Committee Member at UAI 2017 and has several years experience in building/developing
    distributed machine learning systems.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介：**[Nikolaos Vasiloglou](https://www.linkedin.com/in/vasiloglou/) 是 UAI
    2017 的组织委员会成员，并在构建/开发分布式机器学习系统方面有多年经验。'
- en: '**Related:**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Introducing Dask-SearchCV: Distributed hyperparameter optimization with Scikit-Learn](/2017/05/dask-searchcv-distributed-hyperparameter-optimization-scikit-learn.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍 Dask-SearchCV：使用 Scikit-Learn 进行分布式超参数优化](/2017/05/dask-searchcv-distributed-hyperparameter-optimization-scikit-learn.html)'
- en: '[Learning to Learn by Gradient Descent by Gradient Descent](/2017/02/learning-learn-gradient-descent.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过梯度下降学习如何学习](/2017/02/learning-learn-gradient-descent.html)'
- en: '[5 Machine Learning Projects You Can No Longer Overlook, January](/2017/01/five-machine-learning-projects-cant-overlook-january.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5 个你不容忽视的机器学习项目，1 月](/2017/01/five-machine-learning-projects-cant-overlook-january.html)'
- en: '* * *'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 需求'
- en: '* * *'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Data Science Minimum: 10 Essential Skills You Need to Know to Start…](https://www.kdnuggets.com/2020/10/data-science-minimum-10-essential-skills.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学基础：开始时需要掌握的10项关键技能](https://www.kdnuggets.com/2020/10/data-science-minimum-10-essential-skills.html)'
- en: '[Machine Learning Pipeline Optimization with TPOT](https://www.kdnuggets.com/2021/05/machine-learning-pipeline-optimization-tpot.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用TPOT进行机器学习管道优化](https://www.kdnuggets.com/2021/05/machine-learning-pipeline-optimization-tpot.html)'
- en: '[Want to Use Your Data Skills to Solve Global Problems? Here’s What…](https://www.kdnuggets.com/2022/04/jhu-want-data-skills-solve-global-problems.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[想用你的数据技能解决全球问题？这是你需要了解的](https://www.kdnuggets.com/2022/04/jhu-want-data-skills-solve-global-problems.html)'
- en: '[SQL Query Optimization Techniques](https://www.kdnuggets.com/2023/03/sql-query-optimization-techniques.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQL查询优化技术](https://www.kdnuggets.com/2023/03/sql-query-optimization-techniques.html)'
- en: '[Database Optimization: Exploring Indexes in SQL](https://www.kdnuggets.com/2023/07/database-optimization-exploring-indexes-sql.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据库优化：探索SQL中的索引](https://www.kdnuggets.com/2023/07/database-optimization-exploring-indexes-sql.html)'
- en: '[Gradient Descent: The Mountain Trekker''s Guide to Optimization with…](https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[梯度下降：山地旅行者的优化指南](https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics)'
