- en: Beginners Guide to the Three Types of Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/11/beginners-guide-three-types-machine-learning.html](https://www.kdnuggets.com/2019/11/beginners-guide-three-types-machine-learning.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Rebecca Vickery](https://www.linkedin.com/in/rebecca-vickery-20b94133/),
    Data Scientist**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/97361e3877e513bd6978ee0085f57086.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualising KMeans performance with [Yellowbrick](https://www.scikit-yb.org/en/latest/)
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning problems can generally be divided into three types. Classification
    and regression, which are known as supervised learning, and unsupervised learning
    which in the context of machine learning applications often refers to clustering.
  prefs: []
  type: TYPE_NORMAL
- en: In the following article, I am going to give a brief introduction to each of
    these three problems and will include a walkthrough in the popular python library [scikit-learn](https://scikit-learn.org/stable/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: Before I start I’ll give a brief explanation for the meaning behind the terms
    supervised and unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised Learning:** *In supervised learning, you have a known set of inputs
    (features) and a known set of outputs (labels). Traditionally these are known
    as X and y. The goal of the algorithm is to learn the mapping function that maps
    the input to the output. So that when given new examples of X the machine can
    correctly predict the corresponding y labels.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unsupervised Learning:*** In unsupervised learning, you only have a set of
    inputs (X) and no corresponding labels (y). The goal of the algorithm is to find
    previously unknown patterns in the data. Quite often these algorithms are used
    to find meaningful clusters of similar samples of X so in effect finding the categories
    intrinsic to the data.*'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In classification, the outputs (y) are categories. These can be binary, for
    example, if we were classifying spam email vs not spam email. They can also be
    multiple categories such as classifying species of [flowers](https://archive.ics.uci.edu/ml/datasets/iris),
    this is known as multiclass classification.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s walk through a simple example of classification using scikit-learn. If
    you don’t already have this installed it can be installed either via pip or conda
    as outlined [here](https://scikit-learn.org/stable/install.html).
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn has a number of datasets that can be directly accessed via the
    library. For ease in this article, I will be using these example datasets throughout.
    To illustrate classification I will use the wine dataset which is a multiclass
    classification problem. In the dataset, the inputs (X) consist of 13 features
    relating to various properties of each wine type. The known outputs (y) are wine
    types which in the dataset have been given a number 0, 1 or 2.
  prefs: []
  type: TYPE_NORMAL
- en: The imports I am using for all the code in this article are shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the below code I am downloading the data and converting to a pandas data
    frame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The next stage in a supervised learning problem is to split the data into test
    and train sets. The train set can be used by the algorithm to learn the mapping
    between inputs and outputs, and then the reserved test set can be used to evaluate
    how well the model has learned this mapping. In the below code I am using the
    scikit-learn model_selection function `train_test_split` to do this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the next step, we need to choose the algorithm that will be best suited to
    learn the mapping in your chosen dataset. In scikit-learn there are many different
    algorithms to choose from, all of which use different functions and methods to
    learn the mapping, you can view the full list [here](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning).
  prefs: []
  type: TYPE_NORMAL
- en: To determine the best model I am running the following code. I am training the
    model using a selection of algorithms and obtaining the F1-score for each one.
    The F1 score is a good indicator of the overall accuracy of a classifier. I have
    written a detailed description of the various metrics that can be used to evaluate
    a classifier [here](https://towardsdatascience.com/understanding-the-confusion-matrix-and-its-business-applications-c4e8aaf37f42).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b228fa9a0ee7af8de72a061aabe63c88.png)'
  prefs: []
  type: TYPE_IMG
- en: A perfect F1 score would be 1.0, therefore, the closer the number is to 1.0
    the better the model performance. The results above suggest that the Random Forest
    Classifier is the best model for this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In regression, the outputs (y) are continuous values rather than categories.
    An example of regression would be predicting how many sales a store may make next
    month, or what the future price of your house might be.
  prefs: []
  type: TYPE_NORMAL
- en: Again to illustrate regression I will use a dataset from scikit-learn known
    as the boston housing dataset. This consists of 13 features (X) which are various
    properties of a house such as the number of rooms, the age and crime rate for
    the location. The output (y) is the price of the house.
  prefs: []
  type: TYPE_NORMAL
- en: I am loading the data using the code below and splitting it into test and train
    sets using the same method I used for the wine dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We can use this [cheat sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) to
    see the available algorithms suited to regression problems in scikit-learn. We
    will use similar code to the classification problem to loop through a selection
    and print out the scores for each.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of different metrics used to evaluate regression models.
    These are all essentially error metrics and measure the difference between the
    actual and predicted values achieved by the model. I have used the root mean squared
    error (RMSE). For this metric, the closer to zero the value is the better the
    performance of the model. This [article](https://www.dataquest.io/blog/understanding-regression-error-metrics/) gives
    a really good explanation of error metrics for regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/112c20479514357f35a9a727672f387f.png)'
  prefs: []
  type: TYPE_IMG
- en: The RMSE score suggests that either the linear regression and ridge regression
    algorithms perform best for this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are a number of different types of unsupervised learning but for simplicity
    here I am going to focus on the [clustering methods](https://en.wikipedia.org/wiki/Cluster_analysis).
    There are many different algorithms for clustering all of which use slightly different
    techniques to find clusters of inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Probably one of the most widely used methods is Kmeans. This algorithm performs
    an iterative process whereby a specified number of randomly generated means are
    initiated. A distance metric, [Euclidean](https://en.wikipedia.org/wiki/Euclidean_distance) distance
    is calculated for each data point from the centroids, thus creating clusters of
    similar values. The centroid of each cluster then becomes the new mean and this
    process is repeated until the optimum result has been achieved.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the wine dataset we used in the classification task, with the y labels
    removed, and see how well the k-means algorithm can identify the wine types from
    the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: As we are only using the inputs for this model I am splitting the data into
    test and train using a slightly different method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As Kmeans is reliant on the distance metric to determine the clusters it is
    usually necessary to perform feature scaling (ensuring that all features have
    the same scale) before training the model. In the below code I am using the MinMaxScaler
    to scale the features so that all values fall between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With K-means you have to specify the number of clusters the algorithm should
    use. So one of the first steps is to identify the optimum number of clusters.
    This is achieved by iterating through a number of values of k and plotting the
    results on a chart. This is known as the Elbow method as it typically produces
    a plot with a curve that looks a little like the curve of your elbow. The yellowbrick [library](https://www.scikit-yb.org/en/latest/quickstart.html) (which
    is a great library for visualising scikit-learn models and can be pip installed)
    has a really nice plot for this. The code below produces this visualisation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a4ff56635ba4125cf527a067392ceba7.png)'
  prefs: []
  type: TYPE_IMG
- en: Ordinarily, we wouldn’t already know how many categories we have in a dataset
    where we are using a clustering technique. However, in this case, we know that
    there are three wine types in the data — the curve has correctly selected three
    as the optimum number of clusters to use in the model.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to initialise the K-means algorithm and fit the model to the
    training data and evaluate how effectively the algorithm has clustered the data.
  prefs: []
  type: TYPE_NORMAL
- en: One method used for this is known as the [silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering)).
    This measures the consistency of values within the clusters. Or in other words
    how similar to each other the values in each cluster are, and how much separation
    there is between the clusters. The silhouette score is calculated for each value
    and will range from -1 to +1\. These values are then plotted to form a silhouette
    plot. Again yellowbrick provides a simple way to construct this type of plot.
    The code below creates this visualisation for the wine dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/97361e3877e513bd6978ee0085f57086.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A silhouette plot can be interpreted in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: The closer the mean score (which is the red dotted line in the above) is to
    +1 the better matched the data points are within the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data points with a score of 0 are very close to the decision boundary for another
    cluster (so the separation is low).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Negative values indicate that the data points may have been assigned to the
    wrong cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The width of each cluster should be reasonably uniform if they aren’t then the
    incorrect value of k may have been used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The plot for the wine data set above shows that cluster 0 may not be as consistent
    as the others due to most data points being below the average score and a few
    data points having a score below 0.
  prefs: []
  type: TYPE_NORMAL
- en: Silhouette scores can be particularly useful in comparing one algorithm against
    another or different values of k.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, I wanted to give a brief introduction to each of the three types
    of machine learning. There are many other steps involved in all of these processes
    including feature engineering, data processing and hyperparameter optimisation
    to determine both the best data preprocessing techniques and the best models to
    use.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Rebecca Vickery](https://www.linkedin.com/in/rebecca-vickery-20b94133/)**
    is learning data science through self study. Data Scientist @ Holiday Extras.
    Co-Founder of alGo.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/beginners-guide-to-the-three-types-of-machine-learning-3141730ef45d).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Machine Learning Classification: A Dataset-based Pictorial](/2018/11/machine-learning-classification-dataset-based-pictorial.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python Libraries for Interpretable Machine Learning](/2019/09/python-libraries-interpretable-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Five Command Line Tools for Data Science](/2019/07/five-command-line-tools-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python Basics: Syntax, Data Types, and Control Structures](https://www.kdnuggets.com/python-basics-syntax-data-types-and-control-structures)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Optimizing Data Storage: Exploring Data Types and Normalization in SQL](https://www.kdnuggets.com/optimizing-data-storage-exploring-data-types-and-normalization-in-sql)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Types of Visualization Frameworks](https://www.kdnuggets.com/types-of-visualization-frameworks)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, December 14: 3 Free Machine Learning Courses for…](https://www.kdnuggets.com/2022/n48.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenAI API for Beginners: Your Easy-to-Follow Starter Guide](https://www.kdnuggets.com/openai-api-for-beginners-your-easy-to-follow-starter-guide)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
