# 常见的数据问题（及解决方案）

> 原文：[https://www.kdnuggets.com/2022/02/common-data-problems-solutions.html](https://www.kdnuggets.com/2022/02/common-data-problems-solutions.html)

![常见的数据问题（及解决方案）](../Images/c4c97c8840c97e44b817630a59d39372.png)

[Shubham Dhage](https://unsplash.com/@theshubhamdhage) 通过 Unsplash

数据科学家数量的激增纯粹是由于大量数据能够为我们提供解决现实问题的方案。然而，当涉及到数据科学时，理论与现实之间的差距并不总是一致。

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的 IT

* * *

作为数据科学家，接收大量有问题的数据并进行数据清洗、模型设计和模型执行是非常正常的。这些问题纯粹是由于用于回答问题的数据的复杂性和范围。数据中的问题可能包括特征数量、错误、特性等。

在处理数据问题时，确保问题被正确和高效地处理是至关重要的。

让我们来看看一些常见的数据问题及其解决方案。

## 数据不足

数据科学家的主要组成部分是数据；这是他们头衔的一部分。没有数据，数据科学的进展会受到限制，这对于一个现在高度依赖数据的世界来说是一个问题。

如果数据不足，这将成为一个问题，因为数据是训练算法的重要元素。如果数据有限，可能导致不准确和低效的输出，这会耗费公司大量的时间和资源。然而，有解决方案可以生成更多数据来训练你的模型。

### 1\. 随机生成

如果你知道你要找的值，有可能随机生成这些值。

一种方法是伪随机生成，这通过伪随机数生成器（PRNG）来实现。这是一种生成看似随机但仍可重复的数据的算法。可重复性是指使用与原始研究相同的数据和代码获得一致结果的能力。

你还可以使用输入分析来了解数据的分布，然后根据数据分布模仿或复制值。

### 2\. 数据增强

数据增强是一种显著增加用于训练模型的数据多样性的策略，而无需收集新数据。例如；裁剪、填充和水平翻转等技术在训练大型神经网络时被大量使用。

然而，你需要记住假设和正在解决的任务，以确保你生成的输入是有效的。

### 3\. 避免过拟合

在处理较小的数据集时，过拟合的可能性会自动增加。提醒一下：过拟合是指当一个函数过于紧密地对准一组有限的数据点时发生的建模错误。

为了避免过拟合，你可以使用特征选择、正则化或交叉验证等技术。

## 数据过多

数据不足的问题与数据过多的问题一样存在。数据更多并不保证你的模型会产生准确的输出。

计算能力、所需时间和整体输出等因素。你可能会发现使用较少的数据可以让你的模型更高效、更快速地运行。在处理大量数据时，有些事情你可以考虑。

### 1\. p值

根据[维基百科](https://en.wikipedia.org/wiki/P-value)，p值是：

> “是在零假设正确的前提下，获得至少与实际观察结果一样极端的测试结果的概率。”

通俗来说，p值是你样本数据结果发生偶然的概率，因此低p值是好的。任何小于0.5%的p值在统计上是显著的，允许我们拒绝零假设。p值依赖于被测试数据的大小：样本越大，p值越小。如果样本量很大，就更有可能找到显著的关系（如果存在的话）。随着样本量的增加，随机误差的影响被减少。

如果你的p值因数据过多而过高，你可以从数据中抽取多个样本来训练你的模型，并生成更好的p值。

### 2\. 计算能力

计算能力是计算机以速度和准确性执行某项任务的能力。当用大数据集训练模型时，这可能变得非常困难，因为你需要大量的计算能力和内存来处理数据。这需要太多时间，而且整体过程效率不高。一种解决方案是：

**分层抽样**

分层抽样是一种将数据划分为较小子组（称为层）的技术。在分层随机抽样或分层中，这些层是根据独特的属性或特征（如收入或年龄组）在普查数据中形成的。

## 多少数据算是合适的数量？

难以确定什么是“好”的数据量，因为没有固定的规则。这完全取决于数据的类型、问题的复杂性以及其他因素，如成本。然而，下面是一个你可以在决定有效样本大小时考虑的解决方案：

1.  **样本** - 从数据中提取子样本，确保每个变量的某个百分比被涵盖。一个例子就是我们上面讨论的分层抽样。

不过，这一切最终取决于你试图解决的机器学习问题。例如：

1.  **图像分类问题** - 这需要数万张图像，以创建一个具有准确输出的分类器。

1.  **情感分析问题** - 由于单词和短语的数量，这也需要数千个示例文本。N-gram模型通过计算语料库文本中出现的单词序列的频率来建立，然后估计概率。

1.  **回归问题** - 许多研究人员建议你应该有10倍于特征数量的观察值。例如，如果我们有三个独立变量，那么最好有至少30个样本。

## 目前仍无固定规则

有一个粗略的指南来自研究人员和其他从事模型创建的数据显示者，但这些并非刻板规则或黄金法则。

根据不同变量之间的相关性，你可能需要更多的数据，但也可能只需要较少的数据。在工作流程的前几个阶段，你应该问自己这些问题：

1.  **我期望的时间框架是什么？**

如果你试图预测未来5年的情况，仅有1年的数据是不够高效的。你需要至少5年的数据，并确保数据中没有大量缺失，以产生准确的结果。

1.  **我的数据粒度是什么？**

如果我需要一年的数据，那我的数据是否符合这个要求？我的数据是以天、周还是月为单位？一年的数据可以是365个数据点（天）、52个数据点（周）或12个数据点（个月）。所有这些都是有效的，关键在于你的实际问题和需求。

**[尼莎·阿里亚](https://www.linkedin.com/in/nisha-arya-ahmed/)** 是一名数据科学家和自由职业技术作家。她特别感兴趣于提供数据科学职业建议或教程，以及围绕数据科学的理论知识。她还希望探索人工智能如何有助于人类寿命的不同方式。作为一个积极的学习者，她寻求拓宽自己的技术知识和写作技能，同时帮助指导他人。

### 更多相关话题

+   [使用基本和现代算法解决计算机科学问题…](https://www.kdnuggets.com/2023/11/packt-tackle-computer-science-problems-fundamental-modern-algorithms-machine-learning)

+   [数据科学项目可以帮助你解决现实世界的问题](https://www.kdnuggets.com/2022/11/data-science-projects-help-solve-real-world-problems.html)

+   [想用你的数据技能解决全球性问题？这就是…](https://www.kdnuggets.com/2022/04/jhu-want-data-skills-solve-global-problems.html)

+   [分类问题的更多性能评估指标](https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html)

+   [识别机器学习可解决问题的4个因素](https://www.kdnuggets.com/2022/04/4-factors-identify-machine-learning-solvable-problems.html)

+   [解决5个复杂的SQL问题：难题查询解析](https://www.kdnuggets.com/2022/07/5-hardest-things-sql.html)
