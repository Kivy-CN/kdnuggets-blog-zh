- en: 'Recurrent Neural Networks (RNN): Deep Learning for Sequential Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/07/rnn-deep-learning-sequential-data.html](https://www.kdnuggets.com/2020/07/rnn-deep-learning-sequential-data.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks (RNN) are a class of Artificial Neural Networks that
    can process a sequence of inputs in [deep learning](https://blog.exxactcorp.com/category/deep-learning/) and
    retain its state while processing the next sequence of inputs. Traditional neural
    networks will process an input and move onto the next one disregarding its sequence.
    Data such as time series have a sequential order that needs to be followed in
    order to understand. Traditional feed-forward networks cannot comprehend this
    as each input is assumed to be independent of each other whereas in a time series
    setting each input is dependent on the previous input.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/ebdc0da1eb3f72438224bb2b01b4b3f4.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Illustration 1: source](https://www.dummies.com/programming/big-data/data-science/deep-learning-and-recurrent-neural-networks/)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In Illustration 1 we see that the neural network (hidden state) *A *takes an *x**t *and
    outputs a value *h**t. *The loop shows how the information is being passed from
    one step to the next. The inputs are the individual letters of ‘JAZZ’ and each
    one is passed on to the network A in the same order it is written (i.e. sequentially).
  prefs: []
  type: TYPE_NORMAL
- en: 'Recurrent Neural Networks can be used for a number of ways such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting the next word/letter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecasting financial asset prices in a temporal space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Action modeling in sports (predict the next action in a sporting event like
    soccer, football, tennis etc)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Music composition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RNN vs Autoregressive Models**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An autoregressive model is when a value from data with a temporal dimension
    are regressed on previous values up to a certain point specified by the user.
    An RNN works the same way but the obvious difference in comparison is that the
    RNN looks at all the data (i.e. it does not require a specific time period to
    be specified by the user.)
  prefs: []
  type: TYPE_NORMAL
- en: '***Y******t ******= β******0****** + β******1******y******t-1****** + Ɛ******t***'
  prefs: []
  type: TYPE_NORMAL
- en: The above AR model is an order 1 AR(1) model that takes the immediate preceding
    value to predict the next time period’s value (yt). As this is a linear model,
    it requires certain assumptions of linear regression to hold–especially due to
    the linearity assumption between the dependent and independent variables. In this
    case, ***Y******t ***and ***y******t-1 ***must have a linear relationship. In
    addition there are other checks such as autocorrelation that have to be checked
    to determine the adequate order to forecast ***Y******t******.***
  prefs: []
  type: TYPE_NORMAL
- en: An RNN will not require linearity or model order checking. It can automatically
    check the whole dataset to try and predict the next sequence. As demonstrated
    in the image below, a neural network consists of 3 hidden layers with equal weights,
    biases and activation functions and made to predict the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/17cf506bfd51128920b1b80d5e555bb9.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://hackernoon.com/rnn-or-recurrent-neural-network-for-noobs-a9afbb00e860)'
  prefs: []
  type: TYPE_NORMAL
- en: These hidden layers can then be merged to create a single recurrent hidden layer.
    A recurrent neuron now stores all the previous step input and merges that information
    with the current step input.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/13f5e5872ab6f20539c8e81982e67afb.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://hackernoon.com/rnn-or-recurrent-neural-network-for-noobs-a9afbb00e860)'
  prefs: []
  type: TYPE_NORMAL
- en: '****Advantages of an RNN****'
  prefs: []
  type: TYPE_NORMAL
- en: It can model non-linear temporal/sequential relationships.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No need to specify lags to predict the next value in comparison to and autoregressive
    process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '****Disadvantages of an RNN****'
  prefs: []
  type: TYPE_NORMAL
- en: Vanishing Gradient Problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not suited for predicting long horizons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vanishing Gradient Problem**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As more layers containing activation functions are added, the gradient of the
    loss function approaches zero. The gradient descent algorithm finds the global
    minimum of the cost function of the network. Shallow networks shouldn’t be affected
    by a too small gradient but as the network gets bigger with more hidden layers
    it can cause the gradient to be too small for model training.
  prefs: []
  type: TYPE_NORMAL
- en: Gradients of neural networks are found using the backpropagation algorithm whereby
    you find the derivatives of the network. Using the chain rule, derivatives of
    each layer are found by multiplying down the network. This is where the problem
    lies. Using an activation function like the sigmoid function, the gradient has
    a chance of decreasing as the number of hidden layers increase.
  prefs: []
  type: TYPE_NORMAL
- en: This issue can cause terrible results after compiling the model. The simple
    solution to this has been to use Long-Short Term Memory models with a ReLU activation
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '**Long-Short Term Memory Models**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Long-Short Term Memory Networks are a special type of Recurrent Neural Networks
    that are capable of handling long term dependencies without being affected by
    an unstable gradient.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/b60656880e1eccebad51931e5ae7c94b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://www.superdatascience.com/blogs/recurrent-neural-networks-rnn-lstm-variation/)'
  prefs: []
  type: TYPE_NORMAL
- en: The above diagram is a typical RNN except that the repeating module contains
    extra layers that distinguishes itself from an RNN.
  prefs: []
  type: TYPE_NORMAL
- en: The differentiation here is the horizontal line called the ‘cell state’ that
    acts as a conveyor belt of information. The LSTM will remove or add information
    to the cell state by using the 3 gates as illustrated above. The gates are composed
    of a sigmoid function and a point-wise multiplication operation that outputs between
    1 and 0 to describe how much of each component to let through the cell state.
    A value of 1 means to let all the information through while 0 means to completely
    disregard it.
  prefs: []
  type: TYPE_NORMAL
- en: '**The 3 gates are:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Input gate **'
  prefs: []
  type: TYPE_NORMAL
- en: This gate is used to discover which value will be used to modify the memory
    using the sigmoid function (by assigning a value between 0 and 1) followed by
    a tanh function which gives a weightage to the value between -1 to 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2\. Forget gate **'
  prefs: []
  type: TYPE_NORMAL
- en: The forget gate decides which value to disregard using a sigmoid function by
    using the previous state (*h**t-1*) and the input (*x**t*) by assigning a value
    between 0 and 1 for each value in *c**t-1*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3\. Output gate **'
  prefs: []
  type: TYPE_NORMAL
- en: The input of this block is used to decide the output by using a sigmoid function
    to assign a value between 0 and 1 followed by multiplying by a tanh function to
    decide its level of importance by assigning a value between -1 and 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Coding an RNN – LSTM**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[Original](https://blog.exxactcorp.com/recurrent-neural-networks-rnn-deep-learning-for-sequential-data/).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[LSTM for time series prediction](/2020/04/lstm-time-series-prediction.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Learning for NLP: ANNs, RNNs and LSTMs explained!](/2019/08/deep-learning-nlp-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Backpropagation as Applied to LSTM](/2019/05/understanding-backpropagation-applied-lstm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Neural Networks and Deep Learning: A Textbook (2nd Edition)](https://www.kdnuggets.com/2023/07/aggarwal-neural-networks-deep-learning-textbook-2nd-edition.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Neural Networks Don''t Lead Us Towards AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Explainable Forecasting and Nowcasting with State-of-the-art Deep…](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10 Simple Things to Try Before Neural Networks](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Image Classification with Convolutional Neural Networks (CNNs)](https://www.kdnuggets.com/2022/05/image-classification-convolutional-neural-networks-cnns.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
