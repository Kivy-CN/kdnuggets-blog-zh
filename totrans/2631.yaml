- en: Microsoft Explores Three Key Mysteries of Ensemble Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/02/microsoft-explores-three-key-mysteries-ensemble-learning.html](https://www.kdnuggets.com/2021/02/microsoft-explores-three-key-mysteries-ensemble-learning.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)![Figure](../Images/1ebdf4aaa12665aa2b118860856d78fc.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Source: [https://www.quora.com/What-is-ensemble-learning](https://www.quora.com/What-is-ensemble-learning)
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_BQ
  - PREF_H2
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I recently started a new newsletter focus on AI education and**already has
    over 50,000 subscribers**. TheSequence is a no-BS( meaning no hype, no news etc)
    AI-focused newsletter that takes 5 minutes to read. The goal is to keep you up
    to date with machine learning projects, research papers and concepts. Please give
    it a try by subscribing below:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[![Image](../Images/f2aed90f956dea213be7c9bbf9cd7072.png)](https://thesequence.substack.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ensemble learning** is one of the oldest and less-understood techniques used
    to improve the performance of deep learning models. The theory behind ensemble
    learning is very simple: the performance of a group of independently-trained neural
    networks should outperform the best in the group in the long term. Furthermore,
    we also know that the performance of an ensemble of models can be transferred
    to a single model using a technique known as **knowledge distillation**. The world
    of ensemble learning is super exciting, except, that we don’t quite understand
    it. Recently, Microsoft Research published a groundbreaking paper that tries that
    shade some light into the magic of ensembles by understanding **three fundamental
    mysteries of the space**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Microsoft Research works tries to understand the following two theoretical
    questions about ensemble learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '*1)* *How does ensemble improve the test-time performance in deep learning
    when we simply average over a few independently trained neural networks?*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*2)* *How can such superior test-time performance of ensemble be later “distilled”
    into a single neural network of the same architecture, simply by training the
    single model to match the output of the ensemble over the same training data set?*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The answer to those theoretical questions runs into what Microsoft Research,
    somewhat theatrically, likes to call the three mysteries of ensemble learning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mystery 1: Ensemble'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first mystery of ensemble learning is related to the performance boost. *Given
    a set of neural networks N1, N2…NM, an ensemble that simply takes an average of
    the outputs is likely to produce a significant performance boost. However that
    performance doesn’t materialize when training (N1 + N2 +…Nm)/M*. Puzzling….
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/5994f90abe1adde0378282ef8e21c384.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: [https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/](https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/)
  prefs: []
  type: TYPE_NORMAL
- en: 'Mystery 2: Knowledge Distillation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ensemble models are high performance but ridiculously expensive and slow to
    run. Knowledge distillation is a technique in which a single model can be trained
    to match the performance of the ensemble. The success of knowledge distillation
    methods leads to more the second mystery of ensembles. *Why does matching the
    outputs of the ensemble produces better test accuracy compared to matching the
    true labels?*
  prefs: []
  type: TYPE_NORMAL
- en: 'Mystery 3: Self-Distillation'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third mystery of ensemble is very related to the second one but even more
    puzzling. Knowledge distillation shows that a smaller model can match the performance
    of a larger ensemble. A parallel phenomenon known as self-distillation is even
    more puzzling. Self-distillation is based on performing knowledge distillation
    against individual models which also increases the performance! *Basically, self-distillation
    is based on training the same model using itself as a teacher. Why does that approach
    produces a performance boost remains a mystery.*
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/28553b8695abef2b45b535346d149c74.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: [https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/](https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/)
  prefs: []
  type: TYPE_NORMAL
- en: Some Answers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Microsoft Research conducted all sorts of experiments to understand some of
    the aforementioned mysteries of ensemble learning. The initial work produced some
    interesting results.
  prefs: []
  type: TYPE_NORMAL
- en: 1) Deep Learning Ensembles vs. Feature Mapping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The best-known form of ensemble learning is what is called random feature mappings
    in which models are trained in random number of features. That type of technique
    works well in linear models and is very well understood so that it can be used
    as a baseline to analyze the performance of deep learning ensembles. The initial
    results of Microsoft Research experiments showed that deep learning ensembles
    behaved very similarly to feature mappings. However, knowledge distillation doesn’t
    quite work the same.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/990bf05f80a817577d014532964cea39.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: [https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/](https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/)
  prefs: []
  type: TYPE_NORMAL
- en: 2) Multi-views Datasets are Necessary for Ensembles to work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most revealing results of the Microsoft Research study is based on
    the nature of the data. A multi-view dataset is based on a structure where each
    class of the data has multiple view features. For instance, a car image can be
    classified as a car by looking at the headlights, the wheels, or the windows.
    Microsoft Research shows that datasets with multi-view structures are likely to
    boost the performance of ensemble models whereas datasets without that structure
    does not have the same impact.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/927b0bbe029e3aa456ade2473cba6702.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: [https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/](https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/)
  prefs: []
  type: TYPE_NORMAL
- en: The Microsoft Research paper is one of the most advanced work in deep learning
    ensembles in the last few year. The paper covers many other findings but this
    summary should give you a very concrete idea of the main contributions.
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/dataseries/microsoft-explores-three-key-mysteries-of-ensemble-learning-88dc4df831bd).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[XGBoost: What it is, and when to use it](/2020/12/xgboost-what-when.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Microsoft Uses Transformer Networks to Answer Questions About Images With
    Minimum Training](/2021/01/microsoft-transformer-networks-answer-questions-minimum-training.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Implementing the AdaBoost Algorithm From Scratch](/2020/12/implementing-adaboost-algorithm-from-scratch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Thomas Miller, PhD, explores Northwestern University’s Online…](https://www.kdnuggets.com/2024/05/nwu/thomas-miller-phd-explores-northwestern-universitys-online-graduate-programs-in-data-science)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ensemble Learning with Examples](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[When Would Ensemble Techniques be a Good Choice?](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Automate Microsoft Excel and Word Using Python](https://www.kdnuggets.com/2021/08/automate-microsoft-excel-word-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
