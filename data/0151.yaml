- en: Your Ultimate Guide to Chat GPT and Other Abbreviations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你的终极指南：Chat GPT 和其他缩写
- en: 原文：[https://www.kdnuggets.com/2023/06/ultimate-guide-chat-gpt-abbreviations.html](https://www.kdnuggets.com/2023/06/ultimate-guide-chat-gpt-abbreviations.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/06/ultimate-guide-chat-gpt-abbreviations.html](https://www.kdnuggets.com/2023/06/ultimate-guide-chat-gpt-abbreviations.html)
- en: '![Your Ultimate Guide to Chat GPT and Other Abbreviations](../Images/b353b21aca4d646cbe16fe594b3854be.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![你的终极指南：Chat GPT 和其他缩写](../Images/b353b21aca4d646cbe16fe594b3854be.png)'
- en: What do all these abbreviations - ML, AI, AGI - mean?
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 所有这些缩写 - ML、AI、AGI - 代表什么？
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三名课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT。'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*ML* (machine learning) is an approach to solving difficult computational problems
    – instead of coding using a programming language you build an algorithm that “learns”
    the solution from data samples.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*ML*（机器学习）是一种解决复杂计算问题的方法——与其使用编程语言编码，不如构建一个从数据样本中“学习”解决方案的算法。'
- en: '*AI* (artificial intelligence) is a field of computer science dealing with
    problems (e.g., image classification, working with human language) that are difficult
    to solve using traditional programming. ML and AI go hand in hand, with ML being
    a tool to solve problems formulated in AI.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*AI*（人工智能）是计算机科学的一个领域，处理一些传统编程难以解决的问题（例如图像分类、处理人类语言）。ML 和 AI 是相辅相成的，ML 是解决
    AI 中提出问题的工具。'
- en: '*AGI* (artificial general intelligence) - is the correct term for what popular
    culture usually implies by AI – the ability of computers to achieve human-like
    intellectual capabilities and broad reasoning. It is still the holy grail for
    researchers working in the AI field.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*AGI*（人工通用智能）——是流行文化中通常所指的 AI 的正确术语——计算机实现类人智力能力和广泛推理的能力。它仍然是 AI 领域研究人员的终极目标。'
- en: What is a Neural Network?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络是什么？
- en: An artificial neural network (ANN) is a class of ML algorithms and data structures
    (or *models* for short*)* so called because it was inspired by the structure of
    biological neural tissue. But this doesn’t completely mimic all the biological
    mechanisms behind it. Rather, ANNs are complicated mathematical functions that
    are based on ideas from living species biology.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANN）是一类机器学习算法和数据结构（或简称*模型*），因其受到生物神经组织结构的启发而得名。但这并没有完全模拟其背后的所有生物机制。相反，ANN
    是复杂的数学函数，基于生物学中的概念。
- en: When I read “the model has 2 billion parameters” what does this mean?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当我看到“模型有 20 亿个参数”时，这是什么意思？
- en: Neural networks are layered structures consisting of uniform units interconnected
    with each other in a network. The way these units are interconnected is called
    *architecture*. Each connection has an associated number called *weight* and the
    weights store information the model learns from data. So, when you read “the model
    has 2 billion parameters,” it means that there are 2 billion connections (and
    weights) in the model, and it roughly designates the information capacity of the
    neural network.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是由统一单元组成的分层结构，这些单元在网络中相互连接。这些单元的连接方式称为*架构*。每个连接都有一个称为*权重*的相关数字，权重存储模型从数据中学习到的信息。因此，当你看到“模型有
    20 亿个参数”时，这意味着模型中有 20 亿个连接（及权重），大致表示神经网络的信息容量。
- en: What does Deep Learning mean?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习是什么意思？
- en: Neural networks have been studied since the 1980s but made a real impact when
    the computer games industry introduced cheap personal supercomputers known as
    graphical processing units (GPUs). Researchers adapted this hardware for the neural
    network training process and achieved impressive results. One of the first deep
    learning architectures, the convolutional neural network (CNN), was able to carry
    out sophisticated image recognition that was difficult with classical computer
    vision algorithms. Since then, ML with neural networks has been rebranded as *deep
    learning,* with “deep” referring to the complicated NN architectures the networks
    are able to explore.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络自1980年代以来一直在研究，但真正产生影响的是计算机游戏行业引入了便宜的个人超级计算机，即图形处理单元（GPUs）。研究人员将这一硬件适配到神经网络训练过程中，并取得了令人瞩目的成果。第一个深度学习架构之一，卷积神经网络（CNN），能够进行复杂的图像识别，这在传统的计算机视觉算法中是困难的。从那时起，带有神经网络的机器学习被重新命名为*深度学习*，“深度”指的是网络能够探索的复杂神经网络架构。
- en: Where can I get some more details on how this tech works?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我在哪里可以获得更多关于这项技术如何工作的细节？
- en: I’d recommend videos by Grant Sanderson available on his animated [math channel](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我推荐Grant Sanderson的视频，您可以在他的动画[数学频道](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)上观看。
- en: What does the Large Language Model mean?
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型（Large Language Model）是什么意思？
- en: To work with human language using computers, language must be defined mathematically.
    This approach should be sufficiently generic to include the distinctive features
    of every language.  In 2003 researchers discovered how to represent language with
    neural networks and called it the n*eural probabilistic language model* or LM
    for short. This works like predictive text in a mobile phone – given some initial
    sequence of words (or tokens), the model can predict the next possible words with
    their respective probabilities. Continuing this process using previously generated
    words as input (this is *autoregression*) – the model can generate text in the
    language for which it was trained.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要在计算机上处理人类语言，语言必须用数学方式定义。这种方法应该足够通用，以涵盖每种语言的独特特征。2003年，研究人员发现了如何用神经网络表示语言，并称之为神经概率语言模型（n*eural
    probabilistic language model*）或简称为LM。这就像手机中的预测文本——给定一些初始的单词（或标记）序列，模型可以预测下一个可能的单词及其相应的概率。继续使用先前生成的单词作为输入（这就是*自回归*）——模型可以生成其训练语言中的文本。
- en: When I read about language models, I often encounter the term “transformer”.
    What is this?
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当我阅读语言模型时，我经常遇到“变换器”这个术语。这是什么？
- en: Representing sequences of items was a challenging problem for neural networks.
    There were several attempts to solve the problem (mostly around variations of
    *recurrent neural networks*), which yielded some important ideas (e.g., *word
    embedding*, *encoder-decoder* architecture, and *attention mechanism*). In 2017
    a group of Google researchers proposed a new NN architecture that they called
    a *transformer.* It combined all these ideas with effective practical implementation.
    It was designed to solve the language translation problem (hence the name) but
    proved to be efficient for capturing the statistical properties of any sequence
    data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于神经网络来说，表示项序列是一个具有挑战性的问题。有几个尝试解决该问题的方法（主要是围绕*递归神经网络*的变体），产生了一些重要的思想（例如，*词嵌入*、*编码器-解码器*架构和*注意力机制*）。2017年，谷歌的研究团队提出了一种新的神经网络架构，称为*变换器*。它将所有这些思想与有效的实际实现结合在一起。它旨在解决语言翻译问题（因此得名），但证明对于捕捉任何序列数据的统计特性也很有效。
- en: Why everyone talks about OpenAI?
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么大家都在谈论OpenAI？
- en: 'OpenAI experimented with transformers to build a neural probabilistic language
    model. The results of their experiments are called GPT (generative pre-trained
    *transformer*) models. Pre-trained means they were training the transformer NN
    on a large body of texts mined on the Internet and then taking its *decoder* part
    for language representation and text generation. There were several generations
    of GPTs:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI尝试使用变换器（transformer）构建神经概率语言模型。他们实验的结果被称为GPT（生成预训练*变换器*）模型。预训练意味着他们在大量互联网文本上训练了变换器神经网络，然后使用其*解码器*部分进行语言表示和文本生成。GPT模型有多个版本：
- en: 'GPT-1: an initial experimental model to validate the approach'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-1：一个初步实验模型，用于验证这种方法。
- en: 'GPT-2: demonstrated ability to generate coherent human language texts and *zero-shot
    learning – the* ability to generalize to domains for which it was never specifically
    trained (e.g., language translation and text summarization, to name a few)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-2：展示了生成连贯人类语言文本的能力和*零样本学习*——将其推广到从未特别训练过的领域（例如语言翻译和文本摘要，仅举几例）的能力。
- en: GPT-3 was a scale-up of the architecture (1.5 billion parameters of the GPT-2
    vs 175 billion of the largest GPT-3) and was trained on a larger and more variate
    body of text. Its most important feature is the ability to produce texts in a
    wide range of domains by just seeing only a few examples in the prompt (hence
    the term *few short learning*) without any special *fine-tuning* or pre-training.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-3 是对架构的扩展（GPT-2 的 15 亿参数与最大 GPT-3 的 1750 亿参数），并在更大且多样的文本体上进行训练。其最重要的特点是能够通过仅仅看到几个示例来生成各种领域的文本（因此称为*少样本学习*），而无需任何特殊的*微调*或预训练。
- en: 'GPT-4: an even larger model (the exact characteristics are not disclosed),
    larger training datasets, and *multimodality* (text is augmented with image data).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4：一个更大的模型（具体特征未公开）、更大的训练数据集，以及*多模态性*（文本与图像数据相结合）。
- en: Given the enormous number of parameters GPT models have (in fact, you need a
    huge computational cluster with hundreds to thousands of GPUs to train and serve
    these models), they were called *Large* *Language Models* (LLMs).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 GPT 模型拥有巨大的参数量（实际上，你需要一个由数百到数千个 GPU 组成的大型计算集群来训练和服务这些模型），它们被称为*大型* *语言模型*（LLMs）。
- en: What’s the difference between GPT-3 and ChatGPT
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3 和 ChatGPT 有什么区别？
- en: The original GPT-3 is still a word prediction engine and thus is mostly of interest
    to AI researchers and computational linguists. Given some initial *seed* or *prompt,*
    it can generate text infinitely, which makes little practical sense. The OpenAI
    team continued to experiment with the model, trying to fine-tune it to treat prompts
    as instructions to execute. They fed in a large dataset of human-curated dialogues
    and invented a new approach (RLHF – *reinforcement learning from human feedback*)
    to significantly speed up this process with another neural network as a validator
    agent (typical in AI research). They released a model called *InstructGPT* as
    an MVP based on a smaller GPT-3 version and in November 2022 released a full-featured
    version called ChatGPT. With its simple chatbot and web UI, it changed the IT
    world.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 GPT-3 仍然是一个词预测引擎，因此主要对 AI 研究人员和计算语言学家感兴趣。给定一些初始的*种子*或*提示*，它可以无限生成文本，这在实际应用中意义不大。OpenAI
    团队继续对模型进行实验，尝试微调以将提示视为需要执行的指令。他们输入了大量由人类策划的对话数据集，并发明了一种新的方法（RLHF – *基于人类反馈的强化学习*），通过另一个神经网络作为验证代理（在
    AI 研究中很常见）显著加快了这一过程。他们发布了一个名为*InstructGPT*的 MVP 版本，基于较小的 GPT-3 版本，并于 2022 年 11
    月发布了一个功能齐全的版本，称为 ChatGPT。凭借其简单的聊天机器人和网页界面，它改变了 IT 世界。
- en: What is the language model alignment problem?
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是语言模型对齐问题？
- en: Given that LLMs are just sophisticated statistical machines, the generation
    process could go in an unexpected and unpleasant direction. This type of result
    is sometimes called an *AI hallucination,* but from the algorithmic perspective,
    it is still valid, though unexpected, by human users.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 LLM 只是复杂的统计机器，因此生成过程可能会出现意想不到且不愉快的方向。这种结果有时被称为*AI 幻觉*，但从算法的角度来看，尽管对人类用户来说是意外的，但它仍然是有效的。
- en: '*Raw* LLMs require treatment and additional fine-tuning with human validators
    and RLHF, as previously mentioned. This is to *align* LLMs with human expectations,
    and not surprisingly the process itself is called alignment. This is a long and
    tedious procedure with considerable human work involved; this could be considered
    LLM quality assurance. The alignment of the models is what distinguishes OpenAI/Microsoft
    ChatGPT and GPT-4 from their open-source counterparts.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，*原始* LLM 需要进行处理和额外的微调，包括人工验证者和 RLHF。这是为了*使* LLM 符合人类的期望，不足为奇的是，这个过程本身被称为对齐。这是一个漫长而繁琐的过程，涉及大量的人力工作；这可以被视为
    LLM 质量保证。模型的对齐是区分 OpenAI/Microsoft ChatGPT 和 GPT-4 与其开源对手的关键因素。
- en: Why there is a movement to stop the further development of language models?
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么会有停止进一步开发语言模型的运动？
- en: Neural networks are *black boxes* (a huge array of numbers with some structure
    on top). There are some methods to explore and debug their internals but the exceptional
    *generalization* qualities of GPTs remain unexplained. This is the main reason
    behind the ban movement – some researchers think we are playing with fire (science
    fiction gives us fascinating scenarios of AGI birth and *technological singularity*)
    before we get a better understanding of the processes underlying LLMs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是 *黑箱*（一个具有某种结构的大量数字）。虽然有一些方法可以探索和调试它们的内部结构，但 GPTs 的 *泛化* 特性仍未得到解释。这是禁止运动的主要原因——一些研究人员认为我们在玩火（科幻小说给我们提供了有关AGI诞生和
    *技术奇点* 的迷人情景），在我们对LLMs的基本过程有更好理解之前。
- en: What are the practical use cases of LLMs?
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs 的实际应用场景有哪些？
- en: 'The most popular include:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的包括：
- en: Large text summarization
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型文本总结
- en: Vice versa - generating text from summary
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反之亦然 - 从总结生成文本
- en: Text styling (mimicking an author or character)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本风格（模仿作者或角色）
- en: Using it as a personal tutor
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用作个人导师
- en: Solving math/science exercises
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决数学/科学练习题
- en: Answering questions on the text
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回答文本中的问题
- en: Generating programming code from short descriptions
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从简短描述生成编程代码
- en: Are the GPTs the only LLMs available now?
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现在GPT是唯一的LLM吗？
- en: GPTs are the most mature models with API access provided by OpenAI and Microsoft
    Azure OpenAI services (if you need a private subscription). But this is the frontier
    of AI and many interesting things have happened since the release of ChatGPT.
    Google has built its [PaLM-2](https://ai.google/discover/palm2) model; Meta open-sourced
    their [LLaMA](https://github.com/facebookresearch/llama) models for researchers,
    which spurred lots of tweaks and enhancements (e.g., [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)
    from Stanford) and optimization (now you can run LLMs on your [laptop](https://github.com/ggerganov/llama.cpp)
    and even [smartphone](https://github.com/Bip-Rep/sherpa)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: GPTs 是最成熟的模型，通过 OpenAI 和 Microsoft Azure OpenAI 服务（如果需要私人订阅）提供 API 访问。但这是人工智能的前沿，自
    ChatGPT 发布以来，许多有趣的事情发生了。Google 构建了 [PaLM-2](https://ai.google/discover/palm2)
    模型；Meta 为研究人员开源了他们的 [LLaMA](https://github.com/facebookresearch/llama) 模型，这激发了许多调整和改进（例如，斯坦福的
    [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)）和优化（现在你可以在你的 [laptop](https://github.com/ggerganov/llama.cpp)
    甚至 [smartphone](https://github.com/Bip-Rep/sherpa) 上运行 LLMs）。
- en: Huggingface provides [BLOOM](https://huggingface.co/docs/transformers/model_doc/bloom)
    and [StarCoder](https://huggingface.co/bigcode/starcoder) and [HuggingChat](https://huggingface.co/chat/)
    – which are completely open source, without the LLaMA research-only limitation.
    Databricks trained their own completely open-source [Dolly](https://huggingface.co/databricks/dolly-v2-12b)
    model. Lmsys.org is offering its own [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)
    LLM. Nvidia’s deep learning research team is developing its [Megatron-LM](https://github.com/bigcode-project/Megatron-LM)
    model. The [GPT4All](https://gpt4all.io/index.html) initiative is also worth mentioning.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Huggingface 提供了 [BLOOM](https://huggingface.co/docs/transformers/model_doc/bloom)
    和 [StarCoder](https://huggingface.co/bigcode/starcoder) 以及 [HuggingChat](https://huggingface.co/chat/)
    —— 这些都是完全开源的，没有LLaMA仅限研究的限制。Databricks 训练了他们自己完全开源的 [Dolly](https://huggingface.co/databricks/dolly-v2-12b)
    模型。Lmsys.org 提供了他们自己的 [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) LLM。Nvidia
    的深度学习研究团队正在开发其 [Megatron-LM](https://github.com/bigcode-project/Megatron-LM) 模型。
    [GPT4All](https://gpt4all.io/index.html) 计划也值得一提。
- en: However, all these open-source alternatives are still behind OpenAI’s major
    tech (especially in the alignment perspective) but the gap is rapidly closing.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，所有这些开源替代品仍然落后于OpenAI的主要技术（特别是在对齐方面），但差距正在迅速缩小。
- en: How can I use this technology?
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我该如何使用这些技术？
- en: The easiest way is to use OpenAI [public service](https://chat.openai.com/)
    or their platform API [playground](https://platform.openai.com/playground), which
    offers lower-level access to the models and more control over network inner workings
    (specify system context, tune generation parameters, etc). But you should carefully
    review their service agreements since they use user interactions for additional
    model improvements and training. Alternatively, you can choose Microsoft Azure
    OpenAI services, which provide the same API and tools but with private model instances.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是使用 OpenAI 的 [public service](https://chat.openai.com/) 或他们的平台 API [playground](https://platform.openai.com/playground)，它提供了对模型的低级访问权限，并对网络内部运作有更多控制（指定系统上下文、调整生成参数等）。但你应该仔细审查他们的服务协议，因为他们使用用户交互来进行额外的模型改进和训练。作为替代方案，你可以选择
    Microsoft Azure OpenAI 服务，它提供相同的 API 和工具，但具有私人模型实例。
- en: If you are more adventurous, you can try LLM models hosted by HuggingFace, but
    you’ll need to be more skilled with Python and data science tooling.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更具冒险精神，你可以尝试 HuggingFace 托管的 LLM 模型，但你需要更熟练地使用 Python 和数据科学工具。
- en: '**[Denis Shipilov](https://www.linkedin.com/in/denis-shipilov-3351512/)** is
    experienced Solutions Architect with wide range of expertise from distributed
    systems design to the BigData and Data Science related projects.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Denis Shipilov](https://www.linkedin.com/in/denis-shipilov-3351512/)** 是一位经验丰富的解决方案架构师，拥有从分布式系统设计到大数据和数据科学相关项目的广泛专业知识。'
- en: More On This Topic
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Meet Gorilla: UC Berkeley and Microsoft’s API-Augmented LLM…](https://www.kdnuggets.com/2023/06/meet-gorilla-uc-berkeley-microsoft-apiaugmented-llm-outperforms-gpt4-chatgpt-claude.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[认识 Gorilla: UC Berkeley 和微软的 API 增强型 LLM…](https://www.kdnuggets.com/2023/06/meet-gorilla-uc-berkeley-microsoft-apiaugmented-llm-outperforms-gpt4-chatgpt-claude.html)'
- en: '[Building Microservice for Multi-Chat Backends Using Llama and ChatGPT](https://www.kdnuggets.com/building-microservice-for-multichat-backends-using-llama-and-chatgpt)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Llama 和 ChatGPT 构建多聊天后端的微服务](https://www.kdnuggets.com/building-microservice-for-multichat-backends-using-llama-and-chatgpt)'
- en: '[Baize: An Open-Source Chat Model (But Different?)](https://www.kdnuggets.com/2023/04/baize-opensource-chat-model-different.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Baize: 一个开源聊天模型（但有所不同？）](https://www.kdnuggets.com/2023/04/baize-opensource-chat-model-different.html)'
- en: '[Introducing DataCamps AI-Powered Chat Interface: DataLab](https://www.kdnuggets.com/introducing-datacamps-ai-powered-chat-interface-datalab)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍 DataCamp 的 AI 驱动聊天界面：DataLab](https://www.kdnuggets.com/introducing-datacamps-ai-powered-chat-interface-datalab)'
- en: '[The Ultimate Guide to Mastering Seasonality and Boosting Business Results](https://www.kdnuggets.com/2023/08/media-mix-modeling-ultimate-guide-mastering-seasonality-boosting-business-results.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[终极指南：掌握季节性并提升商业成果](https://www.kdnuggets.com/2023/08/media-mix-modeling-ultimate-guide-mastering-seasonality-boosting-business-results.html)'
- en: '[The Ultimate Guide To Different Word Embedding Techniques In NLP](https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[终极指南：不同的词嵌入技术在 NLP 中的应用](https://www.kdnuggets.com/2021/11/guide-word-embedding-techniques-nlp.html)'
