- en: 'Ensemble Learning: 5 Main Approaches'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/01/ensemble-learning-5-main-approaches.html](https://www.kdnuggets.com/2019/01/ensemble-learning-5-main-approaches.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Diogo Menezes Borges](https://www.linkedin.com/in/diogomenezesborges/?locale=en_US),
    Data Scientist**.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Remember a few months ago when everyone was taking wild guesses at the color
    of the dress (blue or gold) or the tennis shoe (pink or grey)? For me, Ensemble
    Learning looks a bit like that. A group of weak learners comes together to form
    a strong learner, thus increasing the accuracy of any Machine Learning model.*'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Have you ever heard about *Wisdom of the Crowd*? Not the TV Series, but the
    real term. No? Ok…so, imagine you ask a complex question to thousands of random
    people. Now, aggregate their answers.
  prefs: []
  type: TYPE_NORMAL
- en: What you will probably find is that the answer given by the majority is better
    than the expert’s answer.
  prefs: []
  type: TYPE_NORMAL
- en: '*The **wisdom of the crowd** is the collective opinion of a group of individuals
    rather than that of a single expert**—**[Wikipedia](https://en.wikipedia.org/wiki/Wisdom_of_the_crowd)*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Returning to the field of Machine Learning, we can apply the same idea. For
    example, if we aggregate the predictions of a group of predictors (such as classifiers
    and regressors), we will probably get a better prediction than with the best individual
    predictor.
  prefs: []
  type: TYPE_NORMAL
- en: '*A group of predictors is called an ensemble. Therefore this Machine Learning
    technique is known as Ensemble Learning. Voilá!*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In my last post [“A breath of fresh air with Decision Trees”](https://medium.com/diogo-menezes-borges/a-breath-of-fresh-air-with-decision-trees-e660455bbfc8),
    I talked about the algorithm Decision Tree and how we can use it to make predictions.
    As you know, in Nature, a group a trees makes a forest. Hum… so that means..
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that I train a group of Decision Trees classifiers to make individual
    predictions according to different subsets of the training dataset. Later I predict
    the expected class for an observation, taking into account what the majority predicted
    for the same instance. So… that means, I am basically using the knowledge of an
    ensemble of Decision Tree or as it is mostly known a **Random Forest.**
  prefs: []
  type: TYPE_NORMAL
- en: 'For this post we will go through the most popular Ensemble methods including *bagging,
    boosting, stacking *and a few others. Before we go into detail please have the
    following in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“Ensemble methods work best when the predictors are as independent of one
    another as possible. One way to get diverse classifiers is to train them using
    very different algorithms. This increases the chance that they will make very
    different types of errors, improving the ensemble’s accuracy.”**—**“**Hands-on
    Machine Learning with Scikit-Learn & TensorFlow”, Chapter 7*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Simple Ensemble Techniques
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Hard Voting Classifier**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the most simple example of the technique and we’ve already kindly approached
    it. Voting classifiers are usually used in classification problems. Imagine you
    trained and fitted a few classifiers (Logistic Regression classifier, SVM classifiers,
    Random Forest classifier, among others) to your training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: A simple way to create a better classifier is to aggregate the prediction made
    by each one and set the majority chosen as the final prediction. Basically, we
    can consider this as if we’re retrieving the mode of all predictors.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/47f934cb71cb7bc891c09b53aa4f05c0.png)'
  prefs: []
  type: TYPE_IMG
- en: '**“Hands-on Machine Learning with Scikit-Learn & TensorFlow”, Chapter 7**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Averaging**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first example we’ve seen is mostly used for classification problems. Now
    let’s take a look at a technique used for Regression problems known as Averaging.
    Similar to Hard Voting, we take multiple predictions made by different algorithms
    and take its average to make the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '**Weighted Averaging**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This technique is equal to Averaging with the twist that all models are assigned
    different weights according to its importance to the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Ensemble techniques
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Stacking**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also known as Stacked Generalisation, this technique is based on the idea of
    training a model that would perform the usual aggregation we saw previously.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3de67bd350f8377dff6234d11b2941a2.png)'
  prefs: []
  type: TYPE_IMG
- en: We have N predictors, each making its predictions and returning a final value.
    Later, the Meta Learner or Blender will take these predictions as input and make
    the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how it works…
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c5d9ec1621328d9551334c66854e76cf.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/0ff34a0ef0985ecb605628f573f34add.png)'
  prefs: []
  type: TYPE_IMG
- en: The common approach to train the Meta Learner is a hold-out set. Firstly, you
    split the training set into two subsets. The first subset of data is used to train
    the predictors.
  prefs: []
  type: TYPE_NORMAL
- en: Later, the same predictors that trained on the first subset are used to make
    predictions on the second (held-out) set. By doing this, the cleanness of the
    predictions is ensured since those algorithms never saw the data.
  prefs: []
  type: TYPE_NORMAL
- en: With these new predictions, we are able to generate a new training set which
    will be used as input features (making the new training set three-dimensional)
    and keeping the target values.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in the end, the Meta Learner is trained on this new dataset and it
    predicts the target values having into account the values given by the first predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, you can train several Meta Learner using this methodology (e.g. one
    using Linear Regression, another one using Random Forest Regression, etc). To
    use more than one Meta Learner you have to split the training set into three or
    more subsets: the first to train the first layer of predictors, the second used
    to make predictions on unknown data and create a new dataset and the third one
    used to train the Meta Learner and make the predictions for the target values.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging and Pasting**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another approach is to use the same algorithm for every predictor (e.g. all
    decision tree), however, having different random subsets of the training set allowing
    for a more generalised result.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26bc0060612255b816373487062371f2.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Article “[Spatial downscaling of precipitation using adaptable random forests](https://www.researchgate.net/publication/309031320_Spatial_downscaling_of_precipitation_using_adaptable_random_forests?_sg=0FdXhCBDcpE3PGA1c_SnLcE36GH-G47WJbTr70zdEg1yUkijY1C0U1FyRmBlsfxbxdCOXKDiYDi-QiAXqOwjYua8wVUOz-M45Q)”**'
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the creation of the subsets, you can either proceed with [*replacement *](https://web.ma.utexas.edu/users/parker/sampling/repl.htm)or
    without. If we assume there is replacement, some observations maybe are present
    in more than one subset, the reason why we call this method *bagging *(short for [*bootstrap
    aggregating*](https://en.wikipedia.org/wiki/Bootstrap_aggregating)). When the
    sampling is performed without replacement we ensure that all the observations,
    in each subset, are unique thus guaranteeing there are no repeated observations
    in every subset.
  prefs: []
  type: TYPE_NORMAL
- en: Once all the predictors are trained, the ensemble can make a prediction for
    a new instance by aggregating the predicted values of all trained predictors,
    just like we saw in the hard voting classifier. Although each individual predictor
    has a higher bias than if it were trained on the original dataset, the aggregation
    allows the reduction of both bias and variance (check my [post ](https://medium.com/diogo-menezes-borges/whats-the-trade-off-between-bias-and-variance-c05b241b15d9)on
    the bias-variance trade-off)
  prefs: []
  type: TYPE_NORMAL
- en: With pasting, there is a high chance that these models will give the same result
    since they are getting the same input. Bootstraping introduces more diversity
    in each subset than pasting, therefore you can expect a higher bias from bagging
    which means the predictors are less correlated so the ensemble’s variance is reduced.
    Overall, the bagging model usually offers better results which explains why generally
    you hear more about bagging than pasting.
  prefs: []
  type: TYPE_NORMAL
- en: '**Boosting**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a data point is incorrectly predicted by the first model as well as the next
    (probably all models), will the combination of their results provide a better
    prediction? This is where *Boosting* comes into action.
  prefs: []
  type: TYPE_NORMAL
- en: Also known as *Hypothesis Boosting, *it refers to any Ensemble method that can
    combine weak learners into a stronger one. It’s a sequential process where each
    subsequent model attempts to fix the errors of its predecessor.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9560d9caee2ae35a7676585aaa28a88.png)'
  prefs: []
  type: TYPE_IMG
- en: '**[Source](https://blog.bigml.com/2017/03/14/introduction-to-boosted-trees/)**'
  prefs: []
  type: TYPE_NORMAL
- en: Each model would not perform well on the entire dataset. However, they do have
    great performances in parts of the dataset. Hence, from Boosting we can expect
    that each model will actually contribute to boosting the performance of the overall
    ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms based on Bagging and Boosting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The most commonly used Ensemble Learning techniques are Bagging and Boosting.
    Here are some of the most common algorithms for each of these techniques. I’ll
    write single posts presenting each one of these algorithms in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging algorithms:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Random Forest](https://medium.com/diogo-menezes-borges/random-forests-8ae226855565)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosting algorithms:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[AdaBoost](https://medium.com/diogo-menezes-borges/boosting-with-adaboost-and-gradient-boosting-9cbab2a1af81)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gradient Boosting Machine (GBM)](https://medium.com/diogo-menezes-borges/boosting-with-adaboost-and-gradient-boosting-9cbab2a1af81)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[XGBoost](https://medium.com/diogo-menezes-borges/boosting-with-adaboost-and-gradient-boosting-9cbab2a1af81)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Light GBM](https://medium.com/diogo-menezes-borges/boosting-with-adaboost-and-gradient-boosting-9cbab2a1af81)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bio**: [Diogo Menezes Borges](https://www.linkedin.com/in/diogomenezesborges/?locale=en_US)
    is a Data Scientist with a background in engineering and 2 years of experience
    using predictive modeling, data processing, and data mining algorithms to solve
    challenging business problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/diogo-menezes-borges/ensemble-learning-when-everybody-takes-a-guess-i-guess-ec35f6cb4600).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resources:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Intuitive Ensemble Learning Guide with Gradient Boosting](https://www.kdnuggets.com/2018/07/intuitive-ensemble-learning-guide-gradient-boosting.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Improving the Performance of a Neural Network](https://www.kdnuggets.com/2018/05/improving-performance-neural-network.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Tour of The Top 10 Algorithms for Machine Learning Newbies](https://www.kdnuggets.com/2018/02/tour-top-10-algorithms-machine-learning-newbies.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Ensemble Learning with Examples](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[When Would Ensemble Techniques be a Good Choice?](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Labeling for Machine Learning: Market Overview, Approaches, and Tools](https://www.kdnuggets.com/2021/12/data-labeling-ml-overview-and-tools.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning’s Sweet Spot: Pure Approaches in NLP and Document Analysis](https://www.kdnuggets.com/2022/05/machine-learning-sweet-spot-pure-approaches-nlp-document-analysis.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
