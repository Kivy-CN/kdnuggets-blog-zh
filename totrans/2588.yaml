- en: Gradient Boosted Decision Trees – A Conceptual Explanation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/04/gradient-boosted-trees-conceptual-explanation.html](https://www.kdnuggets.com/2021/04/gradient-boosted-trees-conceptual-explanation.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosted decision trees have proven to outperform other models. It’s
    because boosting involves implementing several models and aggregating their results.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosted models have recently become popular thanks to their performance
    in machine learning competitions on Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we’ll see what gradient boosted decision trees are all about.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting#:~:text=Gradient%20boosting%20is%20a%20machine,prediction%20models%2C%20typically%20decision%20trees.),
    an ensemble of weak learners is used to improve the performance of a machine learning
    model. The weak learners are usually decision trees. Combined, their output results
    in better models.
  prefs: []
  type: TYPE_NORMAL
- en: In case of regression, the final result is generated from the average of all
    weak learners. With classification, the final result can be computed as the class
    with the majority of votes from weak learners.
  prefs: []
  type: TYPE_NORMAL
- en: In gradient boosting, weak learners work sequentially. Each model tries to improve
    on the error from the previous model. This is different from the bagging technique,
    where several models are fitted on subsets of the data in a parallel manner. These
    subsets are usually drawn randomly with replacement. A great example of bagging
    is in Random Forests®.
  prefs: []
  type: TYPE_NORMAL
- en: '**The boosting process looks like this**:'
  prefs: []
  type: TYPE_NORMAL
- en: Build an initial model with the data,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run predictions on the whole data set,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the error using the predictions and the actual values,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign more weight to the incorrect predictions,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create another model that attempts to fix errors from the last model,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run predictions on the entire dataset with the new model,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create several models with each model aiming at correcting the errors generated
    by the previous one,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtain the final model by weighting the mean of all the models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting algorithms in machine learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s take a look at boosting algorithms in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AdaBoost fits a sequence of weak learners to the data. It then assigns more
    weight to incorrect predictions, and less weight to correct ones. This way the
    algorithm focuses more on observations that are harder to predict. The final result
    is obtained from the majority vote in classification, or the average in regression.
  prefs: []
  type: TYPE_NORMAL
- en: You can implement this algorithm using Scikit-learn. The `n_estimators` argument
    can be passed to it to indicate the number of weak learners needed. You can control
    the contribution of each weak learner using the `learning_rate` argument.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm uses decision trees as the base estimators by default. The base
    estimators and the parameters of the decision trees can be tuned to improve the
    performance of the model. By default, decision trees in AdaBoost have a single
    split.
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification using AdaBoost**'
  prefs: []
  type: TYPE_NORMAL
- en: You can use the `AdaBoostClassifier` from Scikit-learn to implement the AdaBoost
    model for classification problems. As you can see below, the parameters of the
    base estimator can be tuned to your preference. The classifier also accepts the
    number of estimators you want. This is the number of decision trees you need for
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Regression using AdaBoost**'
  prefs: []
  type: TYPE_NORMAL
- en: Applying AdaBoost to regression problems is similar to the classification process,
    with just a few cosmetic changes. First, you have to import the `AdaBoostRegressor`.
    Then, for the base estimator, you can use the `DecisionTreeRegressor`. Just like
    the previous one, you can tune the parameters of the decision tree regressor.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Scikit-learn gradient boosting estimator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gradient boosting is different from AdaBoost, because the loss function optimization
    is done via gradient descent. Like AdaBoost, it also uses decision trees as weak
    learners. It also sequentially fits the trees. When adding subsequent trees, loss
    is minimized using gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: In the Scikit-learn implementation, you can specify the number of trees. This
    is a parameter that should be looked at keenly, because specifying too many trees
    can lead to overfitting. On the other hand, specifying a very small number of
    trees can lead to underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm lets you specify the learning rate. This dictates how fast the
    model will learn. A low learning rate will often require more trees in the model.
    This means more training time.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now take a look at the implementation of gradient boosted trees in Scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification with the Scikit-learn gradient boosting estimator**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is implemented using the `GradientBoostingClassifier`. Some of the parameters
    expected by this algorithm include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`loss` defining the loss function to be optimized'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate` that determines the contribution of each tree'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_estimatorst` dictates the number of decision trees'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth` is the maximum depth of each estimator'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After fitting the classifier, you can obtain the importance of the features
    using the `feauture_importances_` attribute. This is usually referred to as the
    Gini importance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Gradient boosted decision tree feature import](../Images/9ad0e30669cda02fa3b7eb4daa2ad64b.png)'
  prefs: []
  type: TYPE_IMG
- en: The higher the value, the more important the feature is. The values in the obtained
    array will sum to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Impurity-based importances are not always accurate, especially when there
    are too many features. In that case, you should consider using [permutation-based
    importances](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression with the Scikit-learn gradient boosting estimator**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Scikit-learn gradient boosting estimator can be implemented for regression
    using `GradientBoostingRegressor`. It takes parameters that are similar to the
    classification one:'
  prefs: []
  type: TYPE_NORMAL
- en: loss,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: number of estimators,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: maximum depth of the trees,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: learning rate…
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: …just to mention a few.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Like the classification model, you can also obtain the feature importances for
    the regression algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: XGBoost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[XGBoost](https://neptune.ai/blog/how-to-organize-your-xgboost-machine-learning-ml-model-development-process) is
    a gradient boosting library supported for Java, Python, Java and C++, R, and Julia.
    It also uses an ensemble of weak decision trees.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s a linear model that does tree learning through parallel computations.
    The algorithm also ships with features for performing cross-validation, and showing
    the feature’s importance. The main features of this model are:'
  prefs: []
  type: TYPE_NORMAL
- en: accepts sparse input for tree booster and linear booster,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: supports custom evaluation and objective functions,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dmatrix`, its optimized data structure improves its performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a look at how you can apply XGBoost in Python. The parameters accepted
    by the algorithm include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`objective` to define the type of task, say regression or classification;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`colsample_bytree` the subsample ratio of columns when constructing each tree.
    Subsampling happens once in every iteration. This number is usually a value between
    0 and 1;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate` that determines how fast or slow the model will learn;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth` indicates the maximum depth for each tree. The more the trees,
    the greater model complexity, and the higher chances of overfitting;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha` is the [L1 regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)) on
    weights;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_estimators` is the number of decision trees to fit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification with XGBoost**'
  prefs: []
  type: TYPE_NORMAL
- en: 'After importing the algorithm, you define the parameters that you would like
    to use. Since this is a classification problem, the `binary: logistic` objective
    function is used. The next step is to use the `XGBClassifier` and unpack the defined
    parameters. You can tune these parameters until you obtain the ones that are optimal
    for your problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Regression with XGBoost**'
  prefs: []
  type: TYPE_NORMAL
- en: In regression, the `XGBRegressor` is used instead. The objective function, in
    this case, will be the `reg:squarederror`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The XGBoost models also allow you to obtain the feature importances via the
    `feature_importances_` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Regressor feature import](../Images/5fcc6666d29f1ad8c2ae319bd0f2c8e8.png)'
  prefs: []
  type: TYPE_IMG
- en: You can easily visualize them using Matplotlib. This is done using the `plot_importance`
    function from XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Gradient boosted feature-importance](../Images/516c097cd9cb492a799b06e9c75c25d4.png)'
  prefs: []
  type: TYPE_IMG
- en: The `save_model` function can be used for saving your model. You can then send
    this model to your model registry.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Check Neptune docs about integration with [XGBoost](https://docs.neptune.ai/essentials/integrations/machine-learning-frameworks/xgboost) and
    with [matplotlib](https://docs.neptune.ai/essentials/integrations/visualization-libraries/matplotlib).
  prefs: []
  type: TYPE_NORMAL
- en: LightGBM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[LightGBM](https://neptune.ai/blog/how-to-organize-your-lightgbm-ml-model-development-process-examples-of-best-practices)
    is different from other gradient boosting frameworks because it uses a leaf-wise
    tree growth algorithm. Leaf-wise tree growth algorithms are known to converge
    faster than depth-wise growth algorithms. However, they’re more prone to overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Leaf wise tree growth](../Images/3cb421772a8f89c5b77b76605f11036d.png)[*Source*](https://lightgbm.readthedocs.io/en/latest/Features.html?highlight=dart#other-features)'
  prefs: []
  type: TYPE_IMG
- en: The algorithm is [histogram-based](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html),
    so it places continuous values into discrete bins. This leads to faster training
    and efficient memory utilization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other notable features from this algorithm include:'
  prefs: []
  type: TYPE_NORMAL
- en: support for GPU training,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: native support for categorical features,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ability to handle large-scale data,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: handles missing values by default.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a look at some of the main parameters of this algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth` the maximum depth of each tree;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`objective` which defaults to regression;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate` the boosting learning rate;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_estimators` the number of decision trees to fit;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_type` whether you’re working on a CPU or GPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification with LightGBM**'
  prefs: []
  type: TYPE_NORMAL
- en: Training a binary classification model can be done by setting `binary` as the
    objective. If it’s a multi-classification problem, the `multiclass` objective
    is used.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is also converted to LightGBM’s `Dataset` format. Training the model
    is then done using the `train` function. You can also pass the validation datasets
    using the `valid_sets` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Regression with LightGBM**'
  prefs: []
  type: TYPE_NORMAL
- en: For regression with LightGBM, you just need to change the objective to `regression`.
    The boosting type is Gradient Boosting Decision Tree by default.
  prefs: []
  type: TYPE_NORMAL
- en: If you like, you can change this to the random forest algorithm, `dart` — Dropouts
    meet Multiple Additive Regression Trees, or  `goss` — Gradient-based One-Side
    Sampling.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: You can also use LightGBM to plot the model’s feature importance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![lgb.plot_importance](../Images/98b83d78b753c41c28f904a42ece95f7.png)'
  prefs: []
  type: TYPE_IMG
- en: LightGBM also has a built-in function for saving the model. That function is
    `save_model`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: CatBoost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[CatBoost](https://github.com/catboost) is a depth-wise gradient boosting library
    developed by Yandex. The algorithm grows a balanced tree using oblivious decision
    trees.'
  prefs: []
  type: TYPE_NORMAL
- en: It uses the same features to make the right and left split at each level of
    the tree.
  prefs: []
  type: TYPE_NORMAL
- en: For example in the image below, you can see that `297,value>0.5` is used through
    that level.
  prefs: []
  type: TYPE_NORMAL
- en: '![Gradient-boosting-catboost](../Images/c464403bbb3b7dff15de9e6324cf0ac7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Other notable features of [CatBoost](https://catboost.ai/news/catboost-enables-fast-gradient-boosting-on-decision-trees-using-gpus) include:'
  prefs: []
  type: TYPE_NORMAL
- en: native support for categorical features,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: supports training on multiple GPUs,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: results in good performance with the default parameters,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fast prediction via CatBoost’s model applier,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: handles missing values natively,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: support for regression and classification problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s now mention a couple of training parameters from CatBoost:'
  prefs: []
  type: TYPE_NORMAL
- en: '`loss_function` the loss to be used for classification or regression;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eval_metric` the model’s evaluation metric;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_estimators` the maximum number of decision trees;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate` determines how fast or slow the model will learn;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`depth` the maximum depth for each tree;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ignored_features` determines the features that should be ignored during training;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nan_mode` the method that will be used to deal with missing values;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cat_features` an array of categorical columns;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_features` for declaring text-based columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification with CatBoost**'
  prefs: []
  type: TYPE_NORMAL
- en: For classification problems,`CatBoostClassifier` is used. Setting `plot=True`
    during the training process will visualize the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![CatBoostClassifier](../Images/cb4a66e7e6430508805ac7c92fea9abd.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Regression with CatBoost**'
  prefs: []
  type: TYPE_NORMAL
- en: In the case of regression, the `CatBoostRegressor` is used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You can also use the `feature_importances_` to obtain the ranking of the features
    by their importance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![model.feature_importances_](../Images/1f751bb52e8a8b3b02715d3037fa158f.png)'
  prefs: []
  type: TYPE_IMG
- en: The algorithm also provides support for performing cross-validation. This is
    done using the `cv` function while passing the required parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Passing `plot=”True”` will visualize the cross-validation process. The `cv`
    function expects the dataset to be in CatBoost’s `Pool` format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: You can also use CatBoost to perform a grid search. This is done using the `grid_search`
    function. After searching, CatBoost trains on the best parameters.
  prefs: []
  type: TYPE_NORMAL
- en: You should not have fitted the model before this process. Passing the `plot=True`
    parameter will visualize the grid search process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: CatBoost also enables you to visualize a single tree in the model. This is done
    using the `plot_tree` function and passing the index of the tree you would like
    to visualize.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![Regression with catboost](../Images/6cd1b451f8b3d417236c758f41979021.png)'
  prefs: []
  type: TYPE_IMG
- en: Advantages of gradient boosting trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are several reasons as to why you would consider using gradient boosting
    tree algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: generally more accurate compare to other modes,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: train faster especially on larger datasets,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: most of them provide support handling categorical features,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: some of them handle missing values natively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disadvantages of gradient boosting trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s now address some of the challenges faced when using gradient boosted
    trees:'
  prefs: []
  type: TYPE_NORMAL
- en: 'prone to overfitting: this can be solved by applying L1 and L2 regularization
    penalties. You can try a low learning rate as well;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: models can be computationally expensive and take a long time to train, especially
    on CPUs;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: hard to interpret the final models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final thoughts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this article, we explored how to implement gradient boosting decision trees
    in your machine learning problems. We also walked through various boosting-based
    algorithms that you can start using right away.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we’ve covered:'
  prefs: []
  type: TYPE_NORMAL
- en: what is gradient boosting,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: how gradient boosting works,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: various types of gradient boosting algorithms,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: how to use gradient boosting algorithms for regression and classification problems,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the advantages of gradient boosting trees,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: disadvantages of gradient boosting trees,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: …and so much more.
  prefs: []
  type: TYPE_NORMAL
- en: You’re all set to start boosting your machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Gradient boosting in TensorFlow](https://www.tensorflow.org/tutorials/estimator/boosted_trees_model_understanding)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Histogram-based gradient boosting ](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Classification notebook ](https://colab.research.google.com/drive/1O6ChgoMcnEdr4opf2d1Qgoltw_VMYzzn?usp=sharing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Regression notebook ](https://colab.research.google.com/drive/1LE0Hj0axWfjL7DqWP04NX_tb6gzLpk-t?usp=sharing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bio: [Derrick Mwiti](https://www.linkedin.com/in/mwitiderrick/)** is a data
    scientist who has a great passion for sharing knowledge. He is an avid contributor
    to the data science community via blogs such as Heartbeat, Towards Data Science,
    Datacamp, Neptune AI, KDnuggets just to mention a few. His content has been viewed
    over a million times on the internet. Derrick is also an author and online instructor.
    He also trains and works with various institutions to implement data science solutions
    as well as to upskill their staff. You might want to check his [Complete Data
    Science & Machine Learning Bootcamp in Python course](https://www.udemy.com/course/data-science-bootcamp-in-python/?referralCode=9F6DFBC3F92C44E8C7F4).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://neptune.ai/blog/gradient-boosted-decision-trees-guide).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[LightGBM: A Highly-Efficient Gradient Boosting Decision Tree](/2020/06/lightgbm-gradient-boosting-decision-tree.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Best Machine Learning Frameworks & Extensions for Scikit-learn](/2021/03/best-machine-learning-frameworks-extensions-scikit-learn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fast Gradient Boosting with CatBoost](/2020/10/fast-gradient-boosting-catboost.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Machine Learning from Scratch: Decision Trees](https://www.kdnuggets.com/2022/11/machine-learning-scratch-decision-trees.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Decision Trees vs Random Forests, Explained](https://www.kdnuggets.com/2022/08/decision-trees-random-forests-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Generalized and Scalable Optimal Sparse Decision Trees(GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Demystifying Decision Trees for the Real World](https://www.kdnuggets.com/demystifying-decision-trees-for-the-real-world)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear vs Logistic Regression: A Succinct Explanation](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News 22:n12, March 23: Best Data Science Books for…](https://www.kdnuggets.com/2022/n12.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
