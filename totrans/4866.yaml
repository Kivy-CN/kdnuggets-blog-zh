- en: 'Getting Started with PyTorch Part 1: Understanding How Automatic Differentiation
    Works'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html/2](https://www.kdnuggets.com/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html/2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html?page=2#comments)'
  prefs: []
  type: TYPE_IMG
- en: 'Building Block #3 : Variables and Autograd'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyTorch accomplishes what we described above using the *Autograd* package.
  prefs: []
  type: TYPE_NORMAL
- en: Now, there are basically three important things to understand about how *Autograd* works.
  prefs: []
  type: TYPE_NORMAL
- en: '**Building Block #3.1 : Variable**'
  prefs: []
  type: TYPE_NORMAL
- en: The *Variable*, just like a *Tensor* is a class that is used to hold data. It
    differs, however, in the way it’s meant to be used. ***Variables* are specifically
    tailored to hold values which change during training of a neural network, i.e.
    the learnable paramaters of our network. **Tensors on the other hand are used
    to store values that are not to be learned. For example, a Tensor maybe used to
    store the values of the loss generated by each example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A *Variable* class wraps a tensor. You can access this tensor by calling ***.data***attribute
    of a Variable.
  prefs: []
  type: TYPE_NORMAL
- en: The *Variable* also stores the gradient of a scalar quantity (say, loss) with
    respect to the parameter it holds. This gradient can be accessed by calling the ***.grad*** attribute.
    This is basically the gradient computed up to this particular node, and the gradient
    of the every subsequent node, can be computed by multiplying the *edge weight *with
    the gradient computed at the node just before it.
  prefs: []
  type: TYPE_NORMAL
- en: The third attribute a *Variable* holds is a ***grad_fn***, a *Function *object
    which created the variable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db001b7058cc95475452d2b2f0c97638.png)'
  prefs: []
  type: TYPE_IMG
- en: '**NOTE: **PyTorch 0.4 merges the Variable and Tensor class into one, and Tensor
    can be made into a “Variable” by a switch rather than instantiating a new object.
    But since, we’re doing v 0.3 in this tutorial, we’ll go ahead.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Building Block #3.2 : Function**'
  prefs: []
  type: TYPE_NORMAL
- en: Did I say *Function* above? It is basically an abstraction for, well, a function.
    Something that takes an input, and returns an output. For example, if we have
    two variables, *a *and *b*, then if,
  prefs: []
  type: TYPE_NORMAL
- en: '*c = a + b*'
  prefs: []
  type: TYPE_NORMAL
- en: Then *c* is a new variable, and it’s *grad_fn *is something called *AddBackward *(PyTorch’s
    built-in function for adding two variables)*, *the function which took *a* and *b* as
    input, and created *c*.
  prefs: []
  type: TYPE_NORMAL
- en: Then, you may ask, why is a need for an entire new class, when python does provide
    a way to define function?
  prefs: []
  type: TYPE_NORMAL
- en: 'While training neural networks, there are two steps: the forward pass, and
    the backward pass. Normally, if you were to implement it using python functions,
    you will have to define two functions. One, to compute the output during forward
    pass, and another, to compute the gradient to be propagated.'
  prefs: []
  type: TYPE_NORMAL
- en: '**PyTorch abstracts the need to write two separate functions (for forward,
    and for backward pass), into two member of functions of a single class called *torch.autograd.Function.***'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch combines *Variables* and *Functions* to create a computation graph.
  prefs: []
  type: TYPE_NORMAL
- en: '**Building Block #3.3 : Autograd**'
  prefs: []
  type: TYPE_NORMAL
- en: Let us now dig into how PyTorch creates a computation graph. First, we define
    our variables.
  prefs: []
  type: TYPE_NORMAL
- en: The result of the above lines of code is,
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s dissect what the hell just happened here. If you look at the source
    code, here is how things go.
  prefs: []
  type: TYPE_NORMAL
- en: '**Define the *leaf *variables of the graph (Lines 5–9). **We start by defining
    a bunch of “variables” (Normal, python usage of language, not pytorch *Variables*).
    If you notice, the values we defined are the leaf nodes in the our computation
    graph. It only makes sense that we have to define them since these nodes aren’t
    result of any computation. At this point, these guys now occupy memory in our
    Python namespace. Means, they are hundred percent real. We **must** set the ***requires_grad***attribute
    to True, otherwise, these Variables won’t be included in the computation graph,
    and no gradients would be computed for them (and other variables, that depend
    on these particular variables for gradient flow).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create the graph (Lines 12–15)**. Till now, there is nothing such as computation
    graph in our memory. Only the leaf nodes, but as soon as you write lines 12–15,
    a graph is being generated **ON THE FLY. REALLY IMPORTANT TO NAIL THIS DETAIL.
    ON THE FLY. **When you write *b =w1*a*, it’s when the graph creation kicks in,
    and continues until line 15\. This is precisely the forward pass of our model,
    when the output is being calculated from inputs. The *forward* function of each
    variable may cache some input values to be used while computing the gradient on
    the backward pass. (For example, if our forward function computes *W*x*, then *d(W*x)/d(W)* is *x*,
    the input that needs to be cached)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, the reason I told you the graph I drew earlier wasn’t exactly accurate?
    Because when PyTorch makes a graph, it’s not the *Variable *objects that are the
    nodes of the graph. It’s a *Function *object, precisely, the *grad_fn* of each *Variable *that
    forms the nodes of the graph. So, the PyTorch graph would look like.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/bd1ab5ae2e471579c37576784c3b1341.png)'
  prefs: []
  type: TYPE_IMG
- en: Each Function is a node in the PyTorch computation graph.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve represented the leaf nodes, by their names, but they too have their *grad_fn’*s
    (which return a None value . It makes sense, as you can’t backpropagate beyond
    leaf nodes). The rest of nodes are now replaced by their *grad_fn’*s. We see that
    the single node *d* is replaced by three Functions, two multiplications, and an
    addition, while loss, is replaced by a *minus* Function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compute the Gradients (Line 18). **We now compute the gradients by calling
    the *.backward() *function on *L*. What exactly is going on here? First, the gradient
    at L, is simply 1 (*dL / dL*). **Then, we invoke it’s *backward*function, which
    basically has a job of computing the gradients of the output of the *Function *object,
    w.r.t to the inputs of the *Function*object. **Here, L is the result of 10 — d,
    which means, backwards function will compute the gradient (*dL/dd) *as -1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, this computed gradient is multiplied by the accumulated gradient (Stored
    in the *grad* attribute of the* Variable* corresponding to the current node, which
    is *dL/dL = 1 *in our case), and then sent to input node, to be stored in the ***grad *attribute
    of the Variable corresponding to input node.** Technically, what we have done
    is apply the chain rule (*dL/dL*) * (*dL/dd*) = *dL/dd.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let us understand how gradient is propagated for the *Variable* *d. d *is
    calculated from it’s inputs (w3, w4, b, c). In our graph, it consists of 3 nodes,
    2 multiplications and 1 addition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, the function *AddBackward (*representing addition operation of node *d* in
    our graph) computes the gradient of it’s output (*w3*b + w4*c*) w.r.t it’s inputs
    (*w3*b and w4*c *), which is (1 for both). Now, these *local*gradients are multiplied
    by accumulated gradient (*dL/dd *x *1* = -1 for both), and the results are saved
    in the *grad* attribute of the respective input nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, the Function *MulBackward (*representing multiplication operation of *w3*c*)
    computes the gradient of it’s input output w.r.t to it’s inputs (*w3 and c) *a*s
    (c *and *w3) *respectively. The local gradients are multiplied by accumulated
    gradient (*dL/d(w3*c)* = -1). The resultant value (*-1 *x* c *and -1 x *w3*) is
    then stored in *grad* attribute of *Variables* *w3* and *c *respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradients for all the nodes are computed in a similar fashion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gradient of *L* w.r.t any node can be accessed by calling .*grad* on the
    Variable corresponding to that node, **given it’s a leaf node **(PyTorch’s default
    behavior doesn’t allow you to access gradients of non-leaf nodes. More on that
    in a while). Now that we have got our gradients, we can update our weights using
    SGD or whatever optimization algorithm you like.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: Some Nifty Details of Autograd
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, didn’t I tell you you can’t access the *grad *attribute of non-leaf *Variables*.
    Yeah, that’s the default behavior. You can override it by calling .*retain_grad()*on
    the *Variable* just after defining it and then you’d be able to access it’s *grad *attribute.
    But really, what the heck is going on under the wraps.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dynamic Computation Graphs**'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch creates something called a **Dynamic Computation Graph, **which means
    that the graph is generated on the fly. **Until the *forward* function of a Variable
    is called, there exists no node for the *Variable (it’s grad_fn)* in the graph.** The
    graph is created as a result of *forward* function of many *Variables* being invoked.
    Only then, the buffers are allocated for the graph and intermediate values (used
    for computing gradients later). When you call *backward()*, as the gradients are
    computed, these buffers are essentially freed, and the graph is destroyed. You
    can try calling *backward*() more than once on a graph, and you’ll see PyTorch
    will give you an error. This is because the graph gets destroyed the first time *backward()* is
    called and hence, there’s no graph to call backward upon the second time.
  prefs: []
  type: TYPE_NORMAL
- en: If you call *forward* again, an entirely new graph is generated. With new memory
    allocated to it.
  prefs: []
  type: TYPE_NORMAL
- en: '**By default, only the gradients (*grad* attribute) for leaf nodes are saved,
    and the gradients for non-leaf nodes are destroyed.** But this behavior can be
    changed as described above.'
  prefs: []
  type: TYPE_NORMAL
- en: This is in contrast to the **Static Computation Graphs**, used by TensorFlow
    where the graph is declared ***before*** running the program. The dynamic graph
    paradigm allows you to make changes to your network architecture *during*runtime,
    as a graph is created only when a piece of code is run. This means a graph may
    be redefined during the lifetime for a program. This, however, is not possible
    with static graphs where graphs are created before running the program, and merely
    executed later. Dynamic graphs also make debugging way easier as the source of
    error is easily traceable.
  prefs: []
  type: TYPE_NORMAL
- en: Some Tricks of Trade
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**requires_grad**'
  prefs: []
  type: TYPE_NORMAL
- en: This is an attribute of the *Variable* class. By default, it’s False. It comes
    handy when you have to freeze some layers, and stop them from updating parameters
    while training. You can simply set the *requires_grad* to False, and these *Variable*s
    won’t be included in the computation graph. Thus, no gradient would be propagated
    to them, or to those layers which depend upon these layers for gradient flow. *requires_grad*, **when
    set to True is** **contagious**, meaning even if one operand of an operation has *requires_grad* set
    to True, so will the result.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df9e5f91cec8821b5f0aff2c1a35d7ae.png)'
  prefs: []
  type: TYPE_IMG
- en: '**b **is not included in the graph. No gradient is backpropagated through ***b*** now. **a** only
    gets gradients from **c **now. Even if **w1 **has requires_grad = True, there
    is no way it can receive gradients.'
  prefs: []
  type: TYPE_NORMAL
- en: '**volatile**'
  prefs: []
  type: TYPE_NORMAL
- en: This again is a attribute of a *Variable* class, which causes a *Variable* to
    be excluded from the computation graph when it is set to True. It might seem quite
    similar to *requires_grad*, given it’s also **contagious when set True**. But
    it has a higher precedence than *requires_grad***. A variable with *requires_grad*equals
    to True and *volatile* equals to True, would not be included in the computation
    graph.**
  prefs: []
  type: TYPE_NORMAL
- en: You might think, what’s the need of having another switch to override *requires_grad*,
    when we can simply set *requires_grad* to False? Let me digress for a while.
  prefs: []
  type: TYPE_NORMAL
- en: Not creating a graph is extremely useful when we are doing inference, and don’t
    need gradients. First, overhead to create a computation graph is eliminated, and
    the speed is boosted. Second, if we create a graph, and since there is no *backward *being
    called afterwords, the buffers used to cache values are never freed and may lead
    to you running out of memory.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, we have many layers in the a neural network, for which we might have
    set *requires_grad* to True while training. To prevent a graph from being made
    at inference, we can do either of two things. Set r*equires_grad* False on **all** the
    layers (maybe, 152 of them?). **Or, set *volatile* True only on the input, and
    we’re assured no resultant operation will result in a graph being made. **Your
    choice.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91704abbdd0a52fd54d325cb261738ee.png)'
  prefs: []
  type: TYPE_IMG
- en: No graph is created for **b or **any node that depends on** b.**
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE: **PyTorch 0.4 has no volatile argument for a combined Tensor/Variable
    class. Instead, the inference code should be put in a torch.no_grad() context
    manager.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, that was *Autograd* for you. Understanding how *Autograd* works can save
    you a lot of headache when you’re stuck somewhere, or dealing with errors when
    you’re starting out. Thanks for reading so far. I intend to write more tutorials
    on PyTorch, dealing with how to use inbuilt functions to quickly create complex
    architectures (or, maybe not so quickly, but faster than coding block by block).
    So, stay tuned!
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Understanding Backpropagation](http://neuralnetworksanddeeplearning.com/chap2.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Understanding the Chain Rule](https://www.youtube.com/watch?v=MKWBx78L7Qg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Classes in Python [Part 1](https://www.hackerearth.com/practice/python/object-oriented-programming/classes-and-objects-i/tutorial/) and [Part
    2](https://www.hackerearth.com/practice/python/object-oriented-programming/classes-and-objects-ii-inheritance-and-composition/tutorial/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PyTorch’s Official Tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Bio: [Ayoosh Kathuria](https://www.linkedin.com/in/ayoosh-kathuria-44a319132/)**
    is passionate about computer vision, and teaching machines how to extract meaningful
    information from their surroundings. He is currently working on improving object
    detection by leveraging context.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[A Simple Starter Guide to Build a Neural Network](/2018/02/simple-starter-guide-build-neural-network.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Comparing Deep Learning Frameworks: A Rosetta Stone Approach](/2018/03/deep-learning-frameworks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ranking Popular Deep Learning Libraries for Data Science](/2017/10/ranking-popular-deep-learning-libraries-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Getting Started with PyTorch Lightning](https://www.kdnuggets.com/2022/12/getting-started-pytorch-lightning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting Started with PyTorch in 5 Steps](https://www.kdnuggets.com/5-steps-getting-started-pytorch)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Ace Data Science Assessment Test by Using Automatic EDA Tools](https://www.kdnuggets.com/2022/04/ace-data-science-assessment-test-automatic-eda-tools.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How I Did Automatic Image Labeling Using Grounding DINO](https://www.kdnuggets.com/2023/05/automatic-image-labeling-grounding-dino.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How ChatGPT Works: The Model Behind The Bot](https://www.kdnuggets.com/2023/04/chatgpt-works-model-behind-bot.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Burtch Works 2023 Data Science & AI Professionals Salary Report…](https://www.kdnuggets.com/2023/08/burtch-works-2023-data-science-ai-professionals-salary-report.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
