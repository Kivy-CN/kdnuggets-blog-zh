- en: 'Classification Metrics Walkthrough: Logistic Regression with Accuracy, Precision,
    Recall, and ROC'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html](https://www.kdnuggets.com/2022/10/classification-metrics-walkthrough-logistic-regression-accuracy-precision-recall-roc.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Classification Metrics Walkthrough: Logistic Regression with Accuracy, Precision,
    Recall, and ROC](../Images/f4b3fe77b0c72d1315751064ff8166af.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Editor
  prefs: []
  type: TYPE_NORMAL
- en: Metrics are an important element of machine learning. In regard to classification
    tasks, there are different types of metrics that allow you to assess the performance
    of machine learning models. However, it can be difficult to choose the right one
    for your task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, I will be going through 4 common classification metrics: Accuracy,
    Precision, Recall, and ROC in relation to Logistic Regression.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started…
  prefs: []
  type: TYPE_NORMAL
- en: What is Logistic Regression?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic Regression is a form of Supervised Learning - when the algorithm learns
    on a labeled dataset and analyses the training data. Logistic Regression is typically
    used for binary classification problems based on its ‘logistic function’.
  prefs: []
  type: TYPE_NORMAL
- en: 'Binary classification can represent their classes as either: positive/negative,
    1/0, or True/False.'
  prefs: []
  type: TYPE_NORMAL
- en: The logistic function is also known as the Sigmoid function which takes any
    real-valued number and maps it to a value between 0 and 1\. It can be mathematically
    represented as:\
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification Metrics Walkthrough: Logistic Regression with Accuracy, Precision,
    Recall, and ROC](../Images/87fc0d0815eda9430d05037ee1681103.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![Classification Metrics Walkthrough: Logistic Regression with Accuracy, Precision,
    Recall, and ROC](../Images/bab1b7639260376b6155acf6d5282760.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Wikipedia ](https://en.wikipedia.org/wiki/Sigmoid_function)'
  prefs: []
  type: TYPE_NORMAL
- en: What are Classification Metrics?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification is about predicting a label and then identifying which category
    an object belongs to based on different parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In order to measure how well our classification model is doing at making these
    predictions, we use classification metrics. It measures the performance of our
    machine learning model, giving us the confidence that these outputs can be further
    used in decision-making processes.
  prefs: []
  type: TYPE_NORMAL
- en: The performance is normally presented in a range from 0 to 1, where a score
    of 1 represents perfection.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the threshold
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we use a range from 0 to 1 to represent the performance of our model, what
    happens when the value is 0.5? As we know from early math classes, if the probability
    is greater than 0.5, we round it up to 1 (positive) - if not, it is 0 (negative).
  prefs: []
  type: TYPE_NORMAL
- en: That sounds okay, but now when you are using classification models to help determine
    the output of real-life cases. We need to be 100% sure that the output has been
    correctly classified.
  prefs: []
  type: TYPE_NORMAL
- en: For example, logistic regression is used to detect spam emails. If the probability
    that the email is spam is based on the fact that it is above 0.5, this can be
    risky as we could potentially direct an important email into the spam folder.
    The want and need for the performance of the model to be highly accurate becomes
    more sensitive for health-related and financial tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, using the threshold concept of values above the threshold value tend
    to be 1, and a value below the threshold value tends to be 0 can cause challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Although there is the option to adjust the threshold value, it still raises
    the risk that we classify incorrectly. For example, having a low threshold will
    classify the majority of positive classes correctly, but within the positive will
    contain negative classes - vice versa if we had a high threshold.
  prefs: []
  type: TYPE_NORMAL
- en: So let’s get into how these classification metrics can help us with measuring
    the performance of our logistic regression model
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will start off with accuracy because it’s the one that’s typically used the
    most, especially for beginners.
  prefs: []
  type: TYPE_NORMAL
- en: 'Accuracy is defined as the number of correct predictions over the total predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*accuracy = correct_predictions / total_predictions*'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we can further expand on this using these:'
  prefs: []
  type: TYPE_NORMAL
- en: True Positive (TP) - you predicted positive and it’s actually positive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: True Negative (TN) - you predicted negative and it’s actually negative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False Positive (FP) - you predicted positive and it’s actually negative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False Negative (FN) - you predicted negative and it’s actually positive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So we can say the true predictions are TN+TP, while the false prediction is
    FP+FN. The equation can now be redefined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification Metrics Walkthrough: Logistic Regression with Accuracy, Precision,
    Recall, and ROC](../Images/6f18a08608b0a48a06b44118faba26ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to find the accuracy of your model, you would do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Or you can also use [sklearn library](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: However, using the accuracy metric to measure the performance of your model
    is usually not enough. This is where we need other metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Precision and Recall
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we want to further test the “accuracy” in different classes where we want
    to ensure that when the model predicts positive, it is in fact true positive -
    we use precision. We can also call this Positive Prediction Value which can be
    defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification Metrics Walkthrough: Logistic Regression with Accuracy, Precision,
    Recall, and ROC](../Images/30bf6c68e65d84ad7c5aa91da76456c1.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to further test the “accuracy” in different classes where we want
    to ensure that when the model predicts negative, it actually is negative - we
    use recall. Recall is the same formula as sensitivity and can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification Metrics Walkthrough: Logistic Regression with Accuracy, Precision,
    Recall, and ROC](../Images/3b09f192f969bd7c01713467dfe52958.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Using both precision and recall are useful metrics when there is an imbalance
    in the observations between the two classes. For example, there are more of one
    class (1) and only a few of the other class (0) in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In order to increase the precision of your model, you will need to have fewer
    FP and not have to worry about the FN. Whereas, if you want to increase recall,
    you will need to have fewer FN and not have to worry about the FP.
  prefs: []
  type: TYPE_NORMAL
- en: Raising the classification threshold reduces false positives - increasing precision.
    Raising the classification threshold reduces true positives or keeps them the
    same, whilst increasing false negatives or keeps them the same. - decreasing recall
    or keeping it constant.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, it’s not possible to have a high precision and recall value.
    If you increase precision, it will reduce recall - vice versa. This is known as
    the precision/recall tradeoff.
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification Metrics Walkthrough: Logistic Regression with Accuracy, Precision,
    Recall, and ROC](../Images/b2c2d8d2f21bdbccea84c1804b19a90a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Medium](https://medium.com/@syuumak/precision-recall-tradeoff-1f5b10cc729d)'
  prefs: []
  type: TYPE_NORMAL
- en: ROC Curve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to precision we care about lowering the FP and for recall we care
    about lowering the FN. However, there is a metric that we can use to lower both
    the FP and FN - it is called the Receiver Operating Characteristic curve, or ROC
    curve.
  prefs: []
  type: TYPE_NORMAL
- en: It plots the false positive rate (x-axis) against the true positive rate (y-axis).
  prefs: []
  type: TYPE_NORMAL
- en: True Positive Rate = TP / (TP + FN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False Positive Rate = FP / (FP + TN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The true positive rate is also known as sensitivity, and the false positive
    rate is also known as the inverted specificity rate.
  prefs: []
  type: TYPE_NORMAL
- en: Specificity = TN / (TN + FP)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the values on the x-axis consist of smaller values, this indicates lower
    FP and higher TN. If the values on the y-axis consist of larger values, this indicates
    higher TP and lower FN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ROC presents the performance of a classification model at all classification
    thresholds, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification Metrics Walkthrough: Logistic Regression with Accuracy, Precision,
    Recall, and ROC](../Images/1dfb0c57c0e2db51b60b2ed28cebefea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [Wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#/media/File:Roc_curve.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification Metrics Walkthrough: Logistic Regression with Accuracy, Precision,
    Recall, and ROC](../Images/9db4993d31fac363dc06fe698c47a376.png)'
  prefs: []
  type: TYPE_IMG
- en: AUC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When it comes to the ROC curve, you may have also heard Area Under the Curve
    (AUC). It’s exactly what it says it is - the area under the curve. If you want
    to know how good your curve is, you calculate the ROC AUC score. ??AUC measures
    the performance across all possible classification thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: The more area under the curve you have, the better - the higher the ROC AUC
    score. This is when the FN and FP are both at zero - or if we refer to the graph
    above, it’s when the true positive rate is 1 and the false positive rate is 0.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The below image shows an ascending order of logistic regression predictions.
    If the AUC value is 0.0, we can say that the predictions are completely wrong.
    If the AUC value is 1.0, we can say that the predictions are fully correct.
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification Metrics Walkthrough: Logistic Regression with Accuracy, Precision,
    Recall, and ROC](../Images/d4df7798b75dacd4037f1a34baa164ce.png)'
  prefs: []
  type: TYPE_IMG
- en: Wrapping it up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To recap, we have gone over what is Logistic Regression, what Classification
    Metrics are, and problems with the threshold with solutions, such as Accuracy,
    Precision, Recall, and the ROC Curve.
  prefs: []
  type: TYPE_NORMAL
- en: There are so many more classification metrics out there, such as confusion matrix,
    F1 score, F2 score, and more. These are all available to help you better understand
    the performance of your model.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Idiot''s Guide to Precision, Recall, and Confusion Matrix](https://www.kdnuggets.com/2020/01/guide-precision-recall-confusion-matrix.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Confusion Matrix, Precision, and Recall Explained](https://www.kdnuggets.com/2022/11/confusion-matrix-precision-recall-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, November 16: How LinkedIn Uses Machine Learning •…](https://www.kdnuggets.com/2022/n45.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Classification Metrics: Your Guide to Assessing Model…](https://www.kdnuggets.com/understanding-classification-metrics-your-guide-to-assessing-model-accuracy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Logistic Regression for Classification](https://www.kdnuggets.com/2022/04/logistic-regression-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Key Issues Associated with Classification Accuracy](https://www.kdnuggets.com/2023/03/key-issues-associated-classification-accuracy.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
