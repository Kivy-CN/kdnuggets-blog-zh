- en: Introduction to Python Libraries for Data Cleaning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 数据清理库简介
- en: 原文：[https://www.kdnuggets.com/2023/03/introduction-python-libraries-data-cleaning.html](https://www.kdnuggets.com/2023/03/introduction-python-libraries-data-cleaning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/03/introduction-python-libraries-data-cleaning.html](https://www.kdnuggets.com/2023/03/introduction-python-libraries-data-cleaning.html)
- en: '![Introduction to Python Libraries for Data Cleaning](../Images/b08bbf2a4ef0aff10901872523a2f997.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![Python 数据清理库简介](../Images/b08bbf2a4ef0aff10901872523a2f997.png)'
- en: Image by [pch.vecto](https://www.freepik.com/free-vector/male-female-cleaning-staff-with-mops-vacuum-cleaner_18733883.htm#query=cleaning&position=6&from_view=search&track=sph)
    on [Freepik](https://www.freepik.com/)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [pch.vecto](https://www.freepik.com/free-vector/male-female-cleaning-staff-with-mops-vacuum-cleaner_18733883.htm#query=cleaning&position=6&from_view=search&track=sph)
    提供，来源于 [Freepik](https://www.freepik.com/)
- en: Data cleaning is a must-do activity for any data expert because we need our
    data to be error-free, consistent, and usable for analysis. Without this step,
    the analysis result might suffer. However, data cleaning often takes a long time
    and could be repetitive. Moreover, sometimes we miss an error that we need to
    realize.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理是任何数据专家必须做的活动，因为我们需要确保数据没有错误、一致，并且适用于分析。如果没有这一步，分析结果可能会受到影响。然而，数据清理通常需要很长时间，并且可能会很重复。此外，有时我们会遗漏一个需要意识到的错误。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织进行 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: That is why we can rely on the Python packages designed for data cleaning. These
    packages were designed to improve our data cleaning experience and shorten data
    cleaning processing time. What are these packages? Let’s find out.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们可以依赖专门为数据清理设计的 Python 包。这些包旨在改善我们的数据清理体验，并缩短数据清理的处理时间。这些包是什么？让我们找出来。
- en: PyJanitor
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyJanitor
- en: 'Pandas provide many data-cleaning functions, such as fillna and dropna, but
    they could still be enhanced. [PyJanitor](https://pyjanitor-devs.github.io/pyjanitor/)
    is a Python package that provides data-cleaning APIs within the Pandas API without
    replacing them. The package provides various methods including, but not limited
    to, the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 提供了许多数据清理函数，如 fillna 和 dropna，但这些功能仍然可以得到增强。 [PyJanitor](https://pyjanitor-devs.github.io/pyjanitor/)
    是一个 Python 包，提供 Pandas API 内的数据清理 API，而不替代它们。该包提供了各种方法，包括但不限于以下内容：
- en: Cleaning Column Names,
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理列名，
- en: Identifying Duplicate Values,
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别重复值，
- en: Data Factorization,
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据因子化，
- en: Data Encoding,
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据编码，
- en: And many more. However, what is special about the PyJanitor is that the APIs
    can be executed via the chain method. Let’s test them with the example data. For
    this example, I would use the [Titanic Training data from Kaggle](https://www.kaggle.com/competitions/titanic/data?select=train.csv).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更多。然而，PyJanitor 的特别之处在于其 API 可以通过链式方法执行。让我们用示例数据来测试它们。对于这个示例，我将使用 [Kaggle
    的 Titanic 训练数据](https://www.kaggle.com/competitions/titanic/data?select=train.csv)。
- en: For starters, let’s install the PyJanitor package.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们安装 PyJanitor 包。
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then we would load the Titanic dataset.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将加载 Titanic 数据集。
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Introduction to Python Libraries for Data Cleaning](../Images/4b10ee097c6fc0674e46ea43568f037a.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![Python 数据清理库简介](../Images/4b10ee097c6fc0674e46ea43568f037a.png)'
- en: We would use the above dataset for our example. Let’s try the PyJanitor package
    to clean our data with some sample functions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用上述数据集作为示例。让我们尝试使用 PyJanitor 包来清理数据，使用一些示例函数。
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Introduction to Python Libraries for Data Cleaning](../Images/666c96e2c22edfb43c2b1e342f918b0e.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![Python 数据清理库简介](../Images/666c96e2c22edfb43c2b1e342f918b0e.png)'
- en: We transform our initial data frame with a chaining method. So, what happens
    with the code above? Let me break it down.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过链式方法转换初始数据框。那么，以上代码会发生什么？让我来分解一下。
- en: First, we transform the ‘Sex’ column into a numerical with factorize function,
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们使用factorize函数将‘Sex’列转换为数值，
- en: With the also function, we print the shape after factorization,
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用also函数，我们打印因子分解后的形状，
- en: Next, we bin the age into groups using the bin_numeric function,
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们使用bin_numeric函数将年龄分组，
- en: Same with the also function,
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与also函数相同，
- en: Lastly, we clean the column’s name by converting them to lowercase, then replaces
    all spaces with underscores using clean_names
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们通过将列名转换为小写，然后用clean_names替换所有空格为下划线来清理列名。
- en: All the above can be done with single chaining methods that directly done in
    our Pandas data frame. You can still do much more with the PyJanitor package,
    so I suggest you review their [documentation](https://pyjanitor-devs.github.io/pyjanitor/api/functions/).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上述所有操作可以通过直接在我们的Pandas数据框中进行单链方法完成。你仍然可以用PyJanitor包做更多的事情，所以我建议你查看他们的[文档](https://pyjanitor-devs.github.io/pyjanitor/api/functions/)。
- en: Feature-engine
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Feature-engine
- en: '[Feature-Engine](https://feature-engine.trainindata.com/en/latest/index.html#)
    is a Python package designed for feature engineering and selection that preserves
    the scikit-learn APIs method, such as fit and transform. The package was designed
    to provide a data transformer embedded in the machine learning pipeline.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[Feature-Engine](https://feature-engine.trainindata.com/en/latest/index.html#)是一个用于特征工程和选择的Python包，它保留了scikit-learn的API方法，如fit和transform。该包旨在提供一个嵌入机器学习管道的数据转换器。'
- en: 'The package provides various data-cleaning transformers, including but not
    limited to:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 该包提供了各种数据清洗转换器，包括但不限于：
- en: Data Imputation,
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据插补，
- en: Categorical Encoding,
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类编码，
- en: Outlier Removal,
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常值移除，
- en: Variable Selection,
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量选择，
- en: And many more functions. Let’s try the package by installing them first.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他函数。让我们先通过安装它们来尝试这个包。
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The Feature-Engine usage is easy; you only need to import them and train the
    transformer, similar to scikit-learn API. For example, I use an Imputer to fill
    the Age column missing data with the Median.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Feature-Engine的使用很简单；你只需要导入它们并训练转换器，类似于scikit-learn API。例如，我使用Imputer用中位数填充年龄列的缺失数据。
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The code above would fill our age column in the data frame with the median.
    There are so many transformers you could experiment on. Try to find the one that
    suits your data pipeline on the [documentation](https://feature-engine.trainindata.com/en/latest/api_doc/index.html).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将用中位数填充数据框中的年龄列。你可以尝试很多转换器。试着在[文档](https://feature-engine.trainindata.com/en/latest/api_doc/index.html)中找到适合你的数据管道的转换器。
- en: Cleanlab
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cleanlab
- en: '[Cleanlab](https://docs.cleanlab.ai/stable/index.html) is an open-source Python
    package to clean any issues with the machine learning dataset label. It’s designed
    to make any machine learning training with noisy labels more robust and provide
    a reliable output. Any model with probabilistic output can be trained alongside
    Cleanlab packages.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[Cleanlab](https://docs.cleanlab.ai/stable/index.html)是一个开源的Python包，用于清理机器学习数据集标签中的任何问题。它旨在使带有噪声标签的机器学习训练更加稳健，并提供可靠的输出。任何具有概率输出的模型都可以与Cleanlab包一起训练。'
- en: Let’s try out the package with a code example. First, we need to install the
    Cleanlab.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用代码示例来尝试这个包。首先，我们需要安装Cleanlab。
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As Cleanlab works to clean the label issues, let’s try to prepare the dataset
    for machine learning training.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Cleanlab用于清洗标签问题，我们来尝试准备数据集以进行机器学习训练。
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: After the dataset is ready, we would try to fit the dataset with a classifier
    model. Let’s look at the prediction metrics without cleaning the label.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集准备好后，我们将尝试用分类模型来拟合数据集。让我们看看在不清洗标签的情况下的预测指标。
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Introduction to Python Libraries for Data Cleaning](../Images/7bf04581efebbf927892b0e0bf95648e.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![Python数据清洗库简介](../Images/7bf04581efebbf927892b0e0bf95648e.png)'
- en: It’s a good result, but let’s see if we can improve the result after we clean
    the label. Let’s try to do that with the following code.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个不错的结果，但让我们看看在清洗标签后能否进一步提高结果。我们来尝试用以下代码实现这一点。
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Introduction to Python Libraries for Data Cleaning](../Images/ee0b64fa1f1008e4126bb628ca252e7d.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![Python数据清洗库简介](../Images/ee0b64fa1f1008e4126bb628ca252e7d.png)'
- en: We can see from the above result that some labels have issues because of misprediction.
    By cleaning the label, let’s see how the model metrics result.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的结果可以看出，由于预测错误，一些标签存在问题。通过清洗标签，我们来看看模型指标的结果。
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Introduction to Python Libraries for Data Cleaning](../Images/35acbf003114568e4f676c996f4c0538.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![Python数据清洗库简介](../Images/35acbf003114568e4f676c996f4c0538.png)'
- en: We can see there is an improvement in the results compared to our previous model
    without label cleaning. You could still do many things with Cleanlab; I suggest
    you visit the [documentation](https://docs.cleanlab.ai/stable/index.html) to learn
    further.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，相比于之前没有标签清洗的模型，结果有所改善。你仍然可以使用 Cleanlab 做很多事情；我建议你访问[文档](https://docs.cleanlab.ai/stable/index.html)以进一步了解。
- en: Conclusion
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'Data cleaning is a must-step for any data analysis process. Still, it often
    takes a lot of time to clean everything properly. Luckily, there are Python packages
    developed to help us clean the data properly. In this article, I present three
    packages to help clean the data: PyJanitor, Feature-Engine, and Cleanlab.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗是任何数据分析过程中的必经步骤。然而，这通常需要花费大量时间来正确清洗所有数据。幸运的是，有一些 Python 包被开发出来以帮助我们正确清洗数据。在本文中，我介绍了三个帮助清洗数据的包：PyJanitor、Feature-Engine
    和 Cleanlab。
- en: '**[Cornellius Yudha Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**
    is a data science assistant manager and data writer. While working full-time at
    Allianz Indonesia, he loves to share Python and Data tips via social media and
    writing media.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Cornellius Yudha Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**
    是一名数据科学助理经理和数据撰写员。他在全职工作于 Allianz Indonesia 的同时，喜欢通过社交媒体和写作分享 Python 和数据技巧。'
- en: More On This Topic
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多内容
- en: '[Introduction to Deep Learning Libraries: PyTorch and Lightning AI](https://www.kdnuggets.com/introduction-to-deep-learning-libraries-pytorch-and-lightning-ai)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习库简介：PyTorch 和 Lightning AI](https://www.kdnuggets.com/introduction-to-deep-learning-libraries-pytorch-and-lightning-ai)'
- en: '[Top 38 Python Libraries for Data Science, Data Visualization &…](https://www.kdnuggets.com/2020/11/top-python-libraries-data-science-data-visualization-machine-learning.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学、数据可视化及…的前 38 个 Python 库](https://www.kdnuggets.com/2020/11/top-python-libraries-data-science-data-visualization-machine-learning.html)'
- en: '[Python Libraries Data Scientists Should Know in 2022](https://www.kdnuggets.com/2022/04/python-libraries-data-scientists-know-2022.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2022 年数据科学家应该知道的 Python 库](https://www.kdnuggets.com/2022/04/python-libraries-data-scientists-know-2022.html)'
- en: '[Level 50 Data Scientist: Python Libraries to Know](https://www.kdnuggets.com/level-50-data-scientist-python-libraries-to-know)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50 级数据科学家：必知的 Python 库](https://www.kdnuggets.com/level-50-data-scientist-python-libraries-to-know)'
- en: '[7 Python Libraries Every Data Engineer Should Know](https://www.kdnuggets.com/7-python-libraries-every-data-engineer-should-know)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每位数据工程师都应该知道的 7 个 Python 库](https://www.kdnuggets.com/7-python-libraries-every-data-engineer-should-know)'
- en: '[Essential Python Libraries for Data Manipulation](https://www.kdnuggets.com/essential-python-libraries-for-data-manipulation)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据处理的基本 Python 库](https://www.kdnuggets.com/essential-python-libraries-for-data-manipulation)'
