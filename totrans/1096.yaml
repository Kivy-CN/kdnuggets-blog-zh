- en: Cutting Down Implementation Time by Integrating Jupyter and KNIME
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/12/cutting-implementation-time-integrating-jupyter-knime.html](https://www.kdnuggets.com/2021/12/cutting-implementation-time-integrating-jupyter-knime.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**By [Mahantesh Pattadkal](https://medium.com/@mpattadkal), Data Scientist
    @ KNIME**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data scientists are known for creating their own bubble within the 3I structure
    — Implement, Integrate, and Innovate. I personally lean towards the last two Is:
    Integrate new technologies for constant experimentation and Innovate to attain
    remarkable results.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: I have been working with Jupyter Notebook for the last 4–5 years and I feel
    very comfortable working with it. On the other hand, I share a lot of work projects
    with my teammate Paolo, who is an expert in building KNIME workflows. You’d think
    this could be a problem … it’s not!
  prefs: []
  type: TYPE_NORMAL
- en: '[KNIME Analytics Platform](https://www.knime.com/knime-analytics-platform) and [Jupyter
    Notebook](https://jupyter.org/) are both known for their visual appeal in solving
    data analytics problems (Fig. 1). Jupyter Notebook presents a simplified script
    interface for over 40 programming languages via a web browser, but it is in the
    end a coding platform mostly popular among Python users. KNIME Analytics Platform
    runs completely on a graphical user interface controlled by drag-and-drop operations
    and [visual programming](https://en.wikipedia.org/wiki/Visual_programming_language).
    It provides a quick understanding of the logic and structure of complex data analysis
    by representing it via a visual and transparent workflow. You can also write snippets
    of Python code in KNIME Analytics Platform; the Jupyter Notebook user experience
    simplifies this input and execution of code resulting in a UI optimized for coders.
    Now, imagine the plethora of possible applications one can build on the synergy
    of these two platforms.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we discuss two common life scenarios that require collaboration
    between Jupyter Notebook and KNIME Analytics Platform and show how simple this
    is.
  prefs: []
  type: TYPE_NORMAL
- en: Collaboration between Jupyter Notebook and KNIME Analytics Platform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section we describe two scenarios and how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Integrate a Jupyter Notebook in a KNIME workflow (scenario 1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate a KNIME workflow into Jupyter code (scenario 2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In **scenario 1,** my teammate [Paolo ](https://www.linkedin.com/in/paolo-tamagnini/)was
    in control of the project and, pressured by time, asked me for help in implementing
    a custom data transformation, which I did in Jupyter Notebook. Here, I will show
    how Paolo integrated my Jupyter script into his KNIME workflow.
  prefs: []
  type: TYPE_NORMAL
- en: In **scenario 2**, I was in charge of the project and, yet still pressured by
    time, I asked Paolo to help me with building a workflow to train a classification
    model, which he provided in KNIME Analytics Platform. Here I will show how I integrated
    Paolo’s workflow into my Python script from Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '![Cutting Down Implementation Time by Integrating Jupyter and KNIME](../Images/f06e875b4d228bd629ea66a6794ebec7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1: Here we show the user interfaces of the two data science tools: on
    the left [KNIME Analytics Platform](https://www.knime.com/knime-analytics-platform) and
    on the right [Jupyter Notebook](https://jupyter.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: Integrating a Jupyter Notebook in a KNIME Workflow — Scenario 1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Flight delay prediction with machine learning*'
  prefs: []
  type: TYPE_NORMAL
- en: Paolo was requested to classify flights by their departure delay in the [Airline
    dataset.](http://stat-computing.org/dataexpo/2009/the-data.html) Each row in the
    flight delay dataset describes a flight, through its origin, destination, scheduled
    departure time, and so on. Any flight with a departure delay > 15 minutes was
    labeled as “delayed”. The requested task was to train a machine learning model
    to classify whether a flight will be delayed at departure, considering all other
    suitable flight attributes as input features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps taken to build a workflow to train and evaluate a machine learning
    model are usually the same: import the data, transform and clean the data, partition
    them into training set and test set, train the machine learning (ML) model of
    choice on the training set, apply the trained model to the test set, and score
    its performance with the scoring metric of choice. The [Run Jupyter in KNIME](https://kni.me/w/6rzm1Wn3Zsr4NO7-) workflow
    would then look more or less like the one in Fig. 2 (you can download it from
    the [KNIME Hub](https://hub.knime.com/)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cutting Down Implementation Time by Integrating Jupyter and KNIME](../Images/8057510e37b7ae02035dcf51139adb69.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 2\. The [Run Jupyter in KNIME](https://kni.me/w/6rzm1Wn3Zsr4NO7-) training
    workflow to train and evaluate a ML model to predict departure delays in flights.
  prefs: []
  type: TYPE_NORMAL
- en: The data cleaning and data transformation part is often time consuming, since
    it depends on the data domain as well. Paolo was pressured by time and asked me
    if I could implement that part.
  prefs: []
  type: TYPE_NORMAL
- en: Well, this seemed quite easy for me to do using Jupyter Notebook. Paolo could
    then import it in his workflow using a Python Script node. Let’s have a look at
    that step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1\. Write the Python code in Jupyter Notebook.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step 2\. Set up the Python Environment in KNIME Analytics Platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step 3\. Execute the Jupyter Notebook code from the KNIME workflow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step 1\. Write the Python code in Jupyter Notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I have created a Python function called *Custom_Transformation* in Jupyter Notebook(Fig.
    3). The function implements some basic feature engineering and returns the original
    features set along with the transformed features.
  prefs: []
  type: TYPE_NORMAL
- en: '![Cutting Down Implementation Time by Integrating Jupyter and KNIME](../Images/37f87414c3a959dfd7703d8965455f51.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 3\. Python function for feature transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we need to import this code into Paolo’s KNIME workflow.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2**. **Set up the Python environment in KNIME Analytics Platform**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In KNIME Analytics Platform, click *File *→* Preferences *→* KNIME *→* Python*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the Python preference page (Fig. 4a), create a new Conda environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click *New environment* for Python 2 or Python 3 as per the installed Python
    version on your system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *New Conda environment* dialog box opens (Fig. 4b). Now you have to:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enter the environment name in the field highlighted in yellow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click *Create new environment*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the environment is created, click *Apply *and *Close *in the Preference
    page (Fig. 4a).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Cutting Down Implementation Time by Integrating Jupyter and KNIME](../Images/e86b6a0a85643049cadcd794df6728ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 4a. Python Preference window.
  prefs: []
  type: TYPE_NORMAL
- en: '![Cutting Down Implementation Time by Integrating Jupyter and KNIME](../Images/9c1460028d7762c16c7c778e0a2c3e98.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 4b. Dialog box for new Conda environment creation.
  prefs: []
  type: TYPE_NORMAL
- en: '*For a step-by-step guide of how to install the Python integration in KNIME
    and get it working, check my article *[*How to Set Up the Python Extension*](https://medium.com/low-code-for-advanced-data-science/how-to-set-up-the-python-extension-fd25ecee66e3)*.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Step 3\. Execute the Jupyter Notebook code from a KNIME workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the KNIME workflow in Fig. 2, we introduced a [Python Script](https://kni.me/n/6L0w9_dCmFzbn-rx) node
    (the second node from the left after the Table Reader node). This node loads and
    runs my code from Jupyter Notebook (Fig. 5).
  prefs: []
  type: TYPE_NORMAL
- en: 'The key instruction in the script is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This line uses the [knimepy](https://github.com/knime/knimepy) instruction
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: to locate the Jupyter Notebook *Custom_Transformation*, and to load the Jupyter
    script into the *my_notebook* variable.
  prefs: []
  type: TYPE_NORMAL
- en: The next line executes the function
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: and returns the results into a pandas DataFrame named
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '***Note.**** The code in Jupyter Notebook should always provide an output of
    type pandas DataFrame.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Cutting Down Implementation Time by Integrating Jupyter and KNIME](../Images/141ca6e68e6ea6d4a1e290849b7670d5.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 5\. Configuration dialog for the [Python Script ](https://kni.me/n/6L0w9_dCmFzbn-rx)node.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script to run the Jupyter Notebook inside the Python Script node is shown
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: After executing the Python Script node, at its output we find the transformed
    features added to the original features now ready to be passed to the next node
    in the KNIME workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating a KNIME Workflow into Jupyter Code — Scenario 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Flight delay prediction with machine learning*'
  prefs: []
  type: TYPE_NORMAL
- en: I was asked to implement the deployment application that predicts flight departure
    delays using the model previously trained on the flight delay dataset. I want
    to develop this application using Jupyter Notebook. To save time, I would like
    to borrow and integrate a deployment workflow from Paolo’s work. That is, I would
    like to integrate a KNIME workflow into my Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: This is done in three easy steps, specular to the three steps used in scenario
    1.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1\. Build the KNIME workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step 2\. Set up the KNIME environment in Jupyter Notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step 3\. Execute the KNIME workflow from Jupyter Notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step 1\. Build the KNIME workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our case Paolo already provided me with the KNIME workflow [Run KNIME in
    Jupyter](https://kni.me/w/WwfJYzXiTB-VpNql) via the KNIME Hub. The workflow deploys
    the machine learning model to predict flight departure delays (Fig. 6).
  prefs: []
  type: TYPE_NORMAL
- en: Step 2\. Set up the KNIME package in a Jupyter Notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the Command/Anaconda Prompt, enter
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This installs the latest [knimepy](https://github.com/knime/knimepy) package.
    This package enables Jupyter Notebook to read and run KNIME workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3\. Execute the KNIME workflow from a Jupyter Notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is what I need to write in my Jupyter Notebook (Fig. 7 and Fig. 8) in order
    to import and run the selected KNIME workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Import `knime` package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Import the paths to the KNIME executable, to the workspace, and to the KNIME
    workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The command `knime.Workflow(...)` visualizes the workflow. I use it to double
    check that Jupyter is pointing to the intended workflow (Fig. 7)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This instruction `wf.data_table_inputs[0]=data_set` passes the external data
    stored as DataFrame from Jupyter to the KNIME workflow (Fig. 7).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `wf.execute` command executes the KNIME workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the workflow is executed, the results are stored by default in `wf.data_table_outputs[0]` (Fig.
    8)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Note.**** Make sure that the workflow being executed from Jupyter is not
    concurrently open in KNIME. This stalls execution as the workflow is already open
    for editing in KNIME.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Cutting Down Implementation Time by Integrating Jupyter and KNIME](../Images/4259f6d5a8851d244e2f7eec6e15e2d6.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 6\. Setting up the workspace in Jupyter Notebook and displaying the selected
    KNIME workflow [Run KNIME in Jupyter](https://kni.me/w/WwfJYzXiTB-VpNql).
  prefs: []
  type: TYPE_NORMAL
- en: '![Cutting Down Implementation Time by Integrating Jupyter and KNIME](../Images/af724824b112b3ec8c0b2e66a0530ba5.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 7\. Code for executing the workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'As of KNIME Analytics Platform 4.3, I can also call and execute KNIME workflows
    residing on a KNIME Server, rather than on my local client installation. This
    has three main advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved scalability**: Execution on a [KNIME Server](https://www.knime.com/elastic-hybrid-execution) can
    exploit more computational power from another machine on premise, from a server
    in the cloud, or even parallel computation of several KNIME Executors or via [KNIME
    Edge](https://www.knime.com/sites/default/files/2021-03/living-knime-edge-model-deployment-scale.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Greater accessibility**: Anyone with credentials and an internet connection
    can use the shared KNIME workflow deployed on the KNIME Server from their Jupyter
    Notebook'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Better security and versioning:** Combining Jupyter Notebook with KNIME Analytics
    Platform usually implies data scientists with different backgrounds collaborating
    in an organization where data access might be restricted. KNIME Server offers
    a safe and consistent way for sharing workflows, data and Jupyter Notebook edit
    after edit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The only change to the previous script consists in the path to the KNIME workflow
    on the KNIME server and the required username and password to access it, as shown
    in Fig. 8\. This script is also available on [GitHub](https://github.com/knime/knimepy) repository.
    Also, in this case, there isn’t the need to specify the knime executer path like
    before.
  prefs: []
  type: TYPE_NORMAL
- en: '![Cutting Down Implementation Time by Integrating Jupyter and KNIME](../Images/67f87d1096d4cac7565157e694e1f815.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 8\. Code snippet to execute workflow on KNIME Server from Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Collaboration is key!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The fact that you don’t need to choose between Jupyter Notebook and KNIME Analytics
    Platform is an excellent feature to foster collaboration in a team. By mixing
    and matching Jupyter Notebook scripts and KNIME workflow snippets we efficiently
    produced a very advanced set of applications to train, apply, and deploy machine
    learning models for predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Coupling this strategy with the KNIME Server enterprise features enables even
    greater collaboration among data scientists with different backgrounds and tools.
  prefs: []
  type: TYPE_NORMAL
- en: Collaboration is always a key factor in a data science lab and whether you are
    looking for an open source solution or an enterprise one, KNIME software can help
    you there with its flexibility in integrating Jupyter and [many other tools](https://hub.knime.com/search?q=integration&type=Extension).
  prefs: []
  type: TYPE_NORMAL
- en: '**References:**'
  prefs: []
  type: TYPE_NORMAL
- en: Download the Run Jupyter in KNIME and Run KNIME in Jupyter workflows from the
    KNIME Hub [here](https://hub.knime.com/mpattadkal/spaces/Public/latest/Jupyter%20Webinar/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read another blog post on [KNIME and Jupyter](https://www.knime.com/blog/knime-and-jupyter) by
    Greg Landrum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Watch a recording of our webinar [Enhancing Jupyter Notebook with visual workflows
    Webinar](https://www.youtube.com/watch?v=1Rr8Q27k7cQ&t=1161s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bio: [Mahantesh Pattadkal](https://medium.com/@mpattadkal)** is a data scientist
    at KNIME. The data science techniques he is interested in are machine learning,
    natural language processing, deep learning, predictive modeling and business analytics.
    He enjoys working with Python, SQL, Tensorflow/Keras, Pytorch, Excel, and R.'
  prefs: []
  type: TYPE_NORMAL
- en: As first published in [Low Code for Advanced Data Science](https://medium.com/low-code-for-advanced-data-science).
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/low-code-for-advanced-data-science/cutting-down-implementation-time-by-integrating-jupyter-and-knime-f2fa920b25a4).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Codeless Time Series Analysis with KNIME](https://www.kdnuggets.com/2022/10/packt-codeless-time-series-analysis-knime.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introducing TPU v4: Googles Cutting Edge Supercomputer for Large…](https://www.kdnuggets.com/2023/04/introducing-tpu-v4-googles-cutting-edge-supercomputer-large-language-models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Integrating ChatGPT Into Data Science Workflows: Tips and Best Practices](https://www.kdnuggets.com/2023/05/integrating-chatgpt-data-science-workflows-tips-best-practices.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Breaking Down Quantum Computing: Implications for Data Science and AI](https://www.kdnuggets.com/breaking-down-quantum-computing-implications-for-data-science-and-ai)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Optimizing Data Analytics: Integrating GitHub Copilot in Databricks](https://www.kdnuggets.com/optimizing-data-analytics-integrating-github-copilot-in-databricks)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Integrating Generative AI in Content Creation](https://www.kdnuggets.com/integrating-generative-ai-in-content-creation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
