- en: Brewing a Domain-Specific LLM Potion
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 酿造特定领域的LLM魔药
- en: 原文：[https://www.kdnuggets.com/2023/08/brewing-domainspecific-llm-potion.html](https://www.kdnuggets.com/2023/08/brewing-domainspecific-llm-potion.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/08/brewing-domainspecific-llm-potion.html](https://www.kdnuggets.com/2023/08/brewing-domainspecific-llm-potion.html)
- en: '![Brewing a Domain-Specific LLM Potion](../Images/f59d17e898dc57d2cb666479c010020f.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![酿造特定领域的LLM魔药](../Images/f59d17e898dc57d2cb666479c010020f.png)'
- en: Image from Unsplash
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自Unsplash
- en: Arthur Clarke famously quipped that any sufficiently advanced technology is
    indistinguishable from magic. AI has crossed that line with the introduction of
    Vision and Language (V&L) models and Large Language Models (LLMs). Projects like
    [Promptbase](https://promptbase.com) essentially weave the right words in the
    correct sequence to conjure seemingly spontaneous outcomes. If “prompt engineering”
    doesn't meet the criteria of spell-casting, it's hard to say what does. Moreover,
    the quality of prompts [matter](https://toloka.ai/blog/best-stable-diffusion-prompts/).
    Better "spells" lead to better results!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 亚瑟·克拉克曾名言：“任何足够先进的技术都与魔法无异。” 人工智能通过引入视觉和语言（V&L）模型以及大语言模型（LLMs）已越过了这条界限。像[Promptbase](https://promptbase.com)这样的项目本质上是将正确的词汇以正确的顺序编织起来，以召唤看似自发的结果。如果“提示工程”不符合施法的标准，那么很难说什么符合。而且，提示的质量[很重要](https://toloka.ai/blog/best-stable-diffusion-prompts/)。更好的“咒语”能带来更好的结果！
- en: Nearly every company is keen on harnessing a share of this LLM magic. But it’s
    only magic if you can align the LLM to specific business needs, like summarizing
    information from your knowledge base.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每家公司都热衷于利用这股LLM魔力。但只有当你能够将LLM与特定业务需求对接时，它才是魔力，比如从你的知识库中总结信息。
- en: Let's embark on an adventure, revealing the recipe for creating a potent potion—an
    LLM with domain-specific expertise. As a fun example, we'll develop an LLM proficient
    in Civilization 6, a concept that’s geeky enough to intrigue us, boasts a fantastic
    [WikiFandom](https://civilization.fandom.com/wiki/Civilization_Games_Wiki) under
    a CC-BY-SA license, and isn't too complex so that even non-fans can follow our
    examples.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始一场冒险，揭示制作强效魔药的秘诀——一个具有领域特定专业知识的LLM。作为一个有趣的例子，我们将开发一个擅长《文明6》的LLM，这个概念足够极客，能引起我们的兴趣，拥有一个出色的[WikiFandom](https://civilization.fandom.com/wiki/Civilization_Games_Wiki)并且许可证为CC-BY-SA，并且不复杂，以至于即使是非粉丝也能跟随我们的例子。
- en: 'Step 1: Decipher the Documentation'
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1步：解读文档
- en: The LLM may already possess some domain-specific knowledge, accessible with
    the right prompt. However, you probably have existing documents that store knowledge
    you want to utilize. Locate those documents and proceed to the next step.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型（LLM）可能已经具备一些特定领域的知识，使用正确的提示词即可访问这些知识。然而，你可能已有现存的文档存储了你想要利用的知识。找到这些文档并继续下一步。
- en: 'Step 2: Segment Your Spells'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2步：分割你的咒语
- en: To make your domain-specific knowledge accessible to the LLM, segment your documentation
    into smaller, digestible pieces. This segmentation improves comprehension and
    facilitates easier retrieval of relevant information. For us, this involves splitting
    the Fandom Wiki markdown files into sections. Different LLMs can process prompts
    of different length. It makes sense to split your documents into pieces that would
    be significantly shorter (say, 10% or less) then the maximum LLM input length.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你的领域特定知识对LLM可访问，将你的文档分割成更小、更易消化的部分。这种分割可以提高理解度，并使相关信息的检索变得更加容易。对我们来说，这涉及到将Fandom
    Wiki的Markdown文件分割成若干部分。不同的LLM可以处理不同长度的提示。将文档拆分成长度显著小于（例如，10%或更少）最大LLM输入长度的部分是合理的。
- en: 'Step 3: Create Knowledge Elixirs and Brew Your Vector Database'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3步：创建知识精华并构建你的向量数据库
- en: Encode each segmented text piece with the corresponding embedding, using, for
    instance, [Sentence Transformers](https://sbert.net/).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相应的嵌入向量对每个分割后的文本进行编码，例如使用[Sentence Transformers](https://sbert.net/)。
- en: Store the resulting embeddings and corresponding texts in a vector database.
    You could do it DIY-style using Numpy and SKlearn's KNN, but seasoned practitioners
    often recommend vector [databases](https://towardsdatascience.com/milvus-pinecone-vespa-weaviate-vald-gsi-what-unites-these-buzz-words-and-what-makes-each-9c65a3bd0696).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 将生成的嵌入向量及其对应的文本存储在向量数据库中。你可以使用Numpy和SKlearn的KNN自行完成，但经验丰富的从业者通常推荐使用向量[数据库](https://towardsdatascience.com/milvus-pinecone-vespa-weaviate-vald-gsi-what-unites-these-buzz-words-and-what-makes-each-9c65a3bd0696)。
- en: 'Step 4: Craft Spellbinding Prompts'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4步：打造引人入胜的提示词
- en: When a user asks the LLM something about Civilization 6,  you can search the
    vector database for elements whose embedding closely matches the question embedding.
    You can use these texts in the prompt you craft.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户询问LLM关于《文明6》的问题时，你可以在向量数据库中搜索与问题嵌入相似的元素。你可以在你制作的提示中使用这些文本。
- en: 'Step 5: Manage the Cauldron of Context'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5步：管理背景的药剂锅
- en: Let's get serious about spellbinding! You can add database elements to the prompt
    until you reach the maximum context length set for the prompt. Pay close attention
    to the size of your text sections from Step 2\. There are usually significant
    trade-offs between the size of the embedded documents and how many you include
    in the prompt.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们认真对待魔法吧！你可以在提示中添加数据库元素，直到达到为提示设置的最大上下文长度。密切注意第2步中你的文本部分的大小。通常，嵌入文档的大小和你在提示中包含的数量之间存在显著的权衡。
- en: 'Step 6: Choose Your Magic Ingredient'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6步：选择你的魔法成分
- en: Regardless of the LLM chosen for your final solution, these steps apply. The
    LLM landscape is changing rapidly, so once your pipeline is ready, choose your
    success metric and run side-by-side comparisons of different models. For instance,
    we can compare [Vicuna-13b](https://medium.com/@martin-thissen/vicuna-13b-best-free-chatgpt-alternative-according-to-gpt-4-tutorial-gpu-ec6eb513a717)
    and GPT-3.5-turbo.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你为最终解决方案选择了哪个LLM，这些步骤都是适用的。LLM的格局正在迅速变化，因此，一旦你的流程准备好后，选择你的成功指标，并对不同模型进行并行比较。例如，我们可以比较[Vicuna-13b](https://medium.com/@martin-thissen/vicuna-13b-best-free-chatgpt-alternative-according-to-gpt-4-tutorial-gpu-ec6eb513a717)和GPT-3.5-turbo。
- en: 'Step 7: Test Your Potion'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7步：测试你的药剂
- en: Testing if our "potion" works is the next step. Easier said than done, as there's
    no scientific consensus on evaluating LLMs. Some researchers develop new benchmarks
    like [HELM](https://github.com/stanford-crfm/helm) or [BIG-bench](https://github.com/google/BIG-bench),
    while others advocate for human-in-the-loop assessments or assessing the output
    of domain-specific LLMs with a superior model. Each approach has pros and cons.
    For a problem involving domain-specific knowledge, you need to build an evaluation
    pipeline relevant to your business needs. Unfortunately, this usually involves
    starting from scratch.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 测试我们的“药剂”是否有效是下一步。说起来容易做起来难，因为目前尚无科学共识来评估LLM。一些研究人员开发了新的基准，如[HELM](https://github.com/stanford-crfm/helm)或[BIG-bench](https://github.com/google/BIG-bench)，而其他人则提倡人类在环评估，或通过一个更高级的模型来评估领域特定LLM的输出。每种方法都有优缺点。对于涉及领域特定知识的问题，你需要建立一个与你的业务需求相关的评估流程。不幸的是，这通常需要从零开始。
- en: 'Step 8: Unveil the Oracle and Conjure Answers and Evaluation'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8步：揭示神谕并施展答案和评估
- en: First, collect a set of questions to assess the domain-specific LLM's performance.
    This may be a tedious task, but in our Civilization example, we leveraged Google
    Suggest. We used search queries like “Civilization 6 how to …” and applied Google's
    suggestions as the questions to evaluate our solution. Then with a set of domain-related
    questions, run your QnA pipeline. Form a prompt and generate an answer for each
    question.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，收集一组问题来评估领域特定LLM的表现。这可能是一项繁琐的任务，但在我们的《文明》示例中，我们利用了Google Suggest。我们使用了像“文明6如何……”这样的搜索查询，并将Google的建议作为评估我们解决方案的问题。然后，使用一组与领域相关的问题，运行你的问答流程。为每个问题形成提示并生成答案。
- en: 'Step 9: Assess Quality Through the Seer''s Lens'
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9步：通过预言者的视角评估质量
- en: 'Once you have the answers and original queries, you must assess their alignment.
    Depending on your desired precision, you can compare your LLM''s answers with
    a superior model or use a [side-by-side comparison on Toloka](https://toloka.ai/docs/template-builder/templates/sbs-text/).
    The second option has the advantage of direct human assessment, which, if done
    correctly, safeguards against implicit bias that a superior LLM might have (GPT-4,
    for [example](https://arxiv.org/abs/2305.11206), tends to rate its responses higher
    than humans). This could be crucial for actual business implementation where such
    implicit bias could negatively impact your product. Since we''re dealing with
    a toy example, we can follow the first path: comparing Vicuna-13b and GPT-3.5-turbo''s
    answers with those of GPT-4.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你拥有了答案和原始查询，你必须评估它们的一致性。根据你期望的精确度，你可以将你的LLM答案与优越模型进行比较，或使用[Toloka上的并排比较](https://toloka.ai/docs/template-builder/templates/sbs-text/)。第二种选择的优势在于直接的人类评估，如果做得正确，可以防止优越LLM可能存在的隐性偏见（例如，[GPT-4](https://arxiv.org/abs/2305.11206)往往会将自己的回答评分更高）。这对实际业务实施可能至关重要，因为这种隐性偏见可能对你的产品产生负面影响。由于我们处理的是一个玩具示例，我们可以遵循第一条路径：将Vicuna-13b和GPT-3.5-turbo的回答与GPT-4进行比较。
- en: 'Step 10: Distill Quality Assessment'
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10步：提炼质量评估
- en: LLMs are often used in open setups, so ideally, you want an LLM that can distinguish
    questions with answers in your vector database from those without. Here is a side-by-side
    comparison of Vicuna-13b and GPT-3.5, as assessed by humans on [Toloka](https://toloka.ai/)
    (aka Tolokers) and GPT.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs通常在开放设置中使用，因此理想情况下，你希望一个LLM能够区分你的向量数据库中有答案的问题与没有答案的问题。以下是由人类在[Toloka](https://toloka.ai/)（即Tolokers）和GPT上对Vicuna-13b和GPT-3.5的并排比较。
- en: '| Method | Tolokers | GPT-4 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Tolokers | GPT-4 |'
- en: '| Model | vicuna-13b | GPT-3.5 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | vicuna-13b | GPT-3.5 |'
- en: '| Answerable, correct answer | 46.3% | 60.3% | 80.9% |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 可回答，正确答案 | 46.3% | 60.3% | 80.9% |'
- en: '| Unanswerable, AI gave no answer | 20.9% | 11.8% | 17.7% |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 无法回答，AI没有给出答案 | 20.9% | 11.8% | 17.7% |'
- en: '| Answerable, wrong answer | 20.9% | 20.6% | 1.4% |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 可回答，错误答案 | 20.9% | 20.6% | 1.4% |'
- en: '| Unanswerable, AI gave some answer | 11.9% | 7.3% | 0 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 无法回答，AI给出了一些答案 | 11.9% | 7.3% | 0 |'
- en: We can see the differences between evaluations conducted by superior models
    versus human assessment if we examine the evaluation of Vicuna-13b by Tolokers,
    as illustrated in the first column. Several key takeaways emerge from this comparison.
    Firstly, discrepancies between GPT-4 and the Tolokers are noteworthy. These inconsistencies
    primarily occur when the domain-specific LLM appropriately refrains from responding,
    yet GPT-4 grades such non-responses as correct answers to answerable questions.
    This highlights a potential evaluation bias that can emerge when an LLM's evaluation
    is not juxtaposed with human assessment.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查Tolokers对Vicuna-13b的评估，就如第一列所示，我们可以看到优越模型与人工评估之间的差异。从这次比较中可以得出几个关键点。首先，GPT-4与Tolokers之间的差异值得注意。这些不一致主要发生在领域特定的LLM适当地不作回应时，但GPT-4却将这些无回应的情况评分为可回答问题的正确答案。这突显了当LLM的评估未与人工评估对比时，可能会出现的评估偏差。
- en: Secondly, both GPT-4 and human assessors demonstrate a consensus when evaluating
    overall performance. This is calculated as the sum of the numbers in the first
    two rows compared to the sum in the second two rows. Therefore, comparing two
    domain-specific LLMs with a superior model can be an effective DIY approach to
    preliminary model assessment.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，GPT-4和人工评估者在评估整体表现时显示出一致性。这是通过比较前两行的数字总和与后两行的总和来计算的。因此，将两个领域特定的LLM与优越模型进行比较可以成为一种有效的DIY初步模型评估方法。
- en: And there you have it! You have mastered spellbinding, and your domain-specific
    LLM pipeline is fully operational.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是结果！你已经掌握了迷人的技巧，你的领域特定LLM管道已全面运行。
- en: '**[Ivan Yamshchikov](https://www.linkedin.com/in/kroniker/?originalSubdomain=de)**
    is a professor of Semantic Data Processing and Cognitive Computing at the Center
    for AI and Robotics, Technical University of Applied Sciences Würzburg-Schweinfurt.
    He also leads the Data Advocates team at Toloka AI. His research interests include
    computational creativity, semantic data processing and generative models.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**[伊万·亚姆什奇科夫](https://www.linkedin.com/in/kroniker/?originalSubdomain=de)**
    是应用科学大学维尔茨堡-施韦因富特人工智能与机器人中心的语义数据处理与认知计算教授。他还领导Toloka AI的数据倡导者团队。他的研究兴趣包括计算创造力、语义数据处理和生成模型。'
- en: '* * *'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的IT需求'
- en: '* * *'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Web LLM: Bring LLM Chatbots to the Browser](https://www.kdnuggets.com/2023/05/webllm-bring-llm-chatbots-browser.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Web LLM：将LLM聊天机器人带到浏览器](https://www.kdnuggets.com/2023/05/webllm-bring-llm-chatbots-browser.html)'
- en: '[Run an LLM Locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用LM Studio本地运行LLM](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)'
- en: '[Chatbot Arena: The LLM Benchmark Platform](https://www.kdnuggets.com/2023/05/chatbot-arena-llm-benchmark-platform.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[聊天机器人竞技场：LLM基准测试平台](https://www.kdnuggets.com/2023/05/chatbot-arena-llm-benchmark-platform.html)'
- en: '[Falcon LLM: The New King of Open-Source LLMs](https://www.kdnuggets.com/2023/06/falcon-llm-new-king-llms.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Falcon LLM：开源LLM的新霸主](https://www.kdnuggets.com/2023/06/falcon-llm-new-king-llms.html)'
- en: '[Meet Gorilla: UC Berkeley and Microsoft’s API-Augmented LLM…](https://www.kdnuggets.com/2023/06/meet-gorilla-uc-berkeley-microsoft-apiaugmented-llm-outperforms-gpt4-chatgpt-claude.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[认识Gorilla：UC Berkeley和微软的API增强型LLM…](https://www.kdnuggets.com/2023/06/meet-gorilla-uc-berkeley-microsoft-apiaugmented-llm-outperforms-gpt4-chatgpt-claude.html)'
- en: '[Free Full Stack LLM Bootcamp](https://www.kdnuggets.com/2023/06/free-full-stack-llm-bootcamp.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[免费全栈LLM训练营](https://www.kdnuggets.com/2023/06/free-full-stack-llm-bootcamp.html)'
