# Cookiecutter数据科学：如何组织你的数据科学项目

> 原文：[https://www.kdnuggets.com/2018/07/cookiecutter-data-science-organize-data-project.html](https://www.kdnuggets.com/2018/07/cookiecutter-data-science-organize-data-project.html)

![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)

**由 [DrivenData](https://www.drivendata.org/) 提供**

![Image](../Images/cd61bf44f3394ed05156d00b1e97ee5e.png)

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity Certificate](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析能力

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support Professional Certificate](https://www.kdnuggets.com/google-itsupport) - 支持你的组织进行IT工作

* * *

### 为什么使用这个项目结构？

> 我们不是在讨论缩进美学或挑剔的格式标准——最终，数据科学代码的质量关乎正确性和可重复性。

当我们考虑数据分析时，我们常常只是想到最终的报告、见解或可视化。虽然这些最终产品通常是主要事件，但很容易将注意力集中在使产品*看起来不错*上，而忽视了*生成它们的代码质量*。因为这些最终产品是程序生成的，**代码质量仍然很重要**！我们不是在讨论缩进美学或挑剔的格式标准——最终，数据科学代码的质量关乎正确性和可重复性。

好的分析往往是非常零散和偶然探索的结果，这已不是秘密。尝试性实验和快速测试可能不会奏效的方法都是达到有效成果的过程的一部分，没有任何神奇的解决方案能将数据探索转变为简单的线性进程。

尽管如此，一旦开始，这个过程并不适合仔细考虑代码或项目布局的结构，因此最好从一个干净、逻辑清晰的结构开始，并在整个过程中坚持使用。我们认为，使用像这样的标准化设置是一个相当大的胜利。原因如下：

### 其他人会感谢你

> 没有人在创建新的Rails项目之前考虑要把视图放在哪里；他们只是运行`rails new`以获得像其他人一样的标准项目骨架。

一个明确定义的标准项目结构意味着新来者可以开始理解分析而不需要深入阅读大量文档。这也意味着他们不一定要阅读100%的代码就能知道要在哪里寻找非常具体的内容。

组织良好的代码往往具有自我文档化的特点，因为组织本身提供了上下文而无需太多额外说明。人们会感谢你，因为他们可以：

+   更容易地与你协作分析

+   从你的分析中学习过程和领域

+   对分析得出的结论充满信心

这一点可以在任何主要的 web 开发框架中找到，比如 Django 或 Ruby on Rails。没有人会在创建新 Rails 项目之前考虑要将视图放在哪里；他们只需运行 `rails new` 以获得一个标准的项目骨架。因为这个默认项目结构是 *逻辑的* 和 *在大多数项目中相对标准的*，所以对从未见过某个特定项目的人来说，更容易弄清楚在哪里可以找到各种移动部件。

另一个很好的例子是 [文件系统层次结构标准](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard) 对于类似Unix的系统。`/etc` 目录有一个非常具体的目的，`/tmp` 文件夹也是如此，而且大家（或多或少）都同意遵守这一社会契约。这意味着 Red Hat 用户和 Ubuntu 用户都大致知道在哪里查找某些类型的文件，即使是在使用对方的系统——或者任何符合标准的系统！

理想情况下，当同事打开你的数据科学项目时，应该是这样的。

### 你会感谢你自己

是否曾尝试重现几个月前甚至几年前做的分析？你可能已经写了代码，但现在已经无法解读是否应使用 `make_figures.py.old`、`make_figures_working.py` 还是 `new_make_figures01.py` 来完成工作。以下是我们在感到存在主义恐惧时学会提出的一些问题：

+   我们是否应该在开始之前将列X与数据连接起来，还是这些来自于某个笔记本？

+   想一想，我们在运行绘图代码之前需要先运行哪个笔记本：是“处理数据”还是“清理数据”？

+   地理绘图的形状文件从哪里下载的？

+   *等等，乘以无穷大。*

这些问题令人痛苦，是项目杂乱的症状。一个好的项目结构鼓励一些实践，这些实践使得回到旧的工作更容易，例如关注点分离，将分析抽象为一个 [DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph)，以及像版本控制这样的工程最佳实践。

### 这里没有任何强制性

> “愚蠢的一致性是小心眼的精灵”——拉尔夫·瓦尔多·爱默生（以及 [PEP 8!](https://www.python.org/dev/peps/pep-0008/#a-foolish-consistency-is-the-hobgoblin-of-little-minds)）

对默认文件夹名称有异议？正在处理一个不完全符合当前结构的项目？是否希望使用不同于（少数）默认的包？

**去做吧！** 这是一个轻量级的结构，旨在成为许多项目的良好 *起点*。或者，正如 PEP 8 所说：

> 项目内部的一致性更为重要。在一个模块或函数内部的一致性是最重要的……然而，要知道何时可以不一致——有时候样式指南的建议并不适用。当有疑问时，运用你最好的判断力。查看其他示例并决定什么看起来最好。不要犹豫，随时询问！

### 开始使用

考虑到这一点，我们为 Python 项目的数据科学创建了一个 cookiecutter 模板。你的分析不一定要用 Python，但该模板提供了一些你可能需要删除的 Python 样板代码（例如在`src`文件夹中，以及在`docs`中的 Sphinx 文档骨架）。

### 要求

+   Python 2.7 或 3.5

+   [cookiecutter Python 包](http://cookiecutter.readthedocs.org/en/latest/installation.html) >= 1.4.0: `pip install cookiecutter`

### 启动新项目

启动一个新项目只需在命令行中运行此命令即可。无需先创建目录，cookiecutter 会为你完成这项工作。

```py
cookiecutter https://github.com/drivendata/cookiecutter-data-science

```

### 示例

### 目录结构

```py
├── LICENSE
├── Makefile           <- Makefile with commands like `make data` or `make train`
├── README.md          <- The top-level README for developers using this project.
├── data
│   ├── external       <- Data from third party sources.
│   ├── interim        <- Intermediate data that has been transformed.
│   ├── processed      <- The final, canonical data sets for modeling.
│   └── raw            <- The original, immutable data dump.
│
├── docs               <- A default Sphinx project; see sphinx-doc.org for details
│
├── models             <- Trained and serialized models, model predictions, or model summaries
│
├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
│                         the creator's initials, and a short `-` delimited description, e.g.
│                         `1.0-jqp-initial-data-exploration`.
│
├── references         <- Data dictionaries, manuals, and all other explanatory materials.
│
├── reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
│   └── figures        <- Generated graphics and figures to be used in reporting
│
├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
│                         generated with `pip freeze > requirements.txt`
│
├── setup.py           <- Make this project pip installable with `pip install -e`
├── src                <- Source code for use in this project.
│   ├── __init__.py    <- Makes src a Python module
│   │
│   ├── data           <- Scripts to download or generate data
│   │   └── make_dataset.py
│   │
│   ├── features       <- Scripts to turn raw data into features for modeling
│   │   └── build_features.py
│   │
│   ├── models         <- Scripts to train models and then use trained models to make
│   │   │                 predictions
│   │   ├── predict_model.py
│   │   └── train_model.py
│   │
│   └── visualization  <- Scripts to create exploratory and results oriented visualizations
│       └── visualize.py
│
└── tox.ini            <- tox file with settings for running tox; see tox.testrun.org

```

### 意见

项目结构中隐含了一些观点，这些观点源于我们与数据科学项目合作时的经验，了解什么有效、什么无效。一些观点涉及工作流程，另一些观点涉及使生活更轻松的工具。以下是这个项目所基于的一些信念——如果你有想法，请 [贡献或分享它们](https://drivendata.github.io/cookiecutter-data-science/#contributing)。

### 数据是不可变的

切勿编辑你的原始数据，尤其是手动编辑，更不要在 Excel 中编辑。不要覆盖你的原始数据。不要保存多个版本的原始数据。将数据（及其格式）视为不可变的。你编写的代码应该将原始数据通过一个流程传递到你的最终分析中。你不应该每次想制作新图时都运行所有步骤（参见 [分析是一个有向无环图（DAG）](https://drivendata.github.io/cookiecutter-data-science/#analysis-is-a-dag)），但任何人都应该能够仅用`src`中的代码和`data/raw`中的数据重现最终产品。

此外，如果数据是不可变的，它不需要像代码那样的源代码管理。因此，***默认情况下，数据文件夹包含在`.gitignore`文件中。*** 如果你有少量数据且不常更改，你可能想将数据包含在存储库中。Github 目前会警告文件超过 50MB，并拒绝超过 100MB 的文件。存储/同步大数据的其他选项包括[AWS S3](https://aws.amazon.com/s3/)和同步工具（例如，[`s3cmd`](http://s3tools.org/s3cmd)），[Git Large File Storage](https://git-lfs.github.com/)，[Git Annex](https://git-annex.branchable.com/)和[dat](http://dat-data.com/)。当前默认情况下，我们要求使用S3桶，并使用[AWS CLI](http://docs.aws.amazon.com/cli/latest/reference/s3/index.html)将`data`文件夹中的数据与服务器同步。

### 笔记本用于探索和沟通

像[Jupyter notebook](http://jupyter.org/)、[Beaker notebook](http://beakernotebook.com/)、[Zeppelin](http://zeppelin-project.org/)以及其他文献编程工具的笔记本包在探索性数据分析中非常有效。然而，这些工具在重现分析方面可能效果较差。当我们在工作中使用笔记本时，我们通常会细分`notebooks`文件夹。例如，`notebooks/exploratory`包含初步探索，而`notebooks/reports`则是更精细的工作，可以导出为html到`reports`目录。

由于笔记本对源代码管理（例如，`json`的差异通常不可读，合并几乎不可能）具有挑战性，我们建议不要直接与他人协作Jupyter笔记本。我们推荐使用笔记本的两个步骤：

1.  遵循一个显示所有者和分析顺序的命名约定。我们使用`<step>-<ghuser>-<description>.ipynb`格式（例如，`0.3-bull-visualize-distributions.ipynb`）。

1.  重构好的部分。不要在多个笔记本中编写完成相同任务的代码。如果这是一个数据预处理任务，请将其放在`src/data/make_dataset.py`的管道中，并从`data/interim`加载数据。如果是有用的实用代码，请重构到`src`中。

现在默认将项目转换为Python包（请参阅`setup.py`文件）。你可以导入你的代码，并使用如下单元格在笔记本中使用它：

```py
# OPTIONAL: Load the "autoreload" extension so that code can change
%load_ext autoreload

# OPTIONAL: always reload modules so that as you change code in src, it gets loaded
%autoreload 2

from src.data import make_dataset

```

### 分析是一个DAG

在分析中，经常会有长时间运行的步骤用于预处理数据或训练模型。如果这些步骤已经运行过（并且你已经将输出存储在`data/interim`目录等位置），你不希望每次都要等待重新运行。我们更喜欢[`make`](https://www.gnu.org/software/make/)来管理相互依赖的步骤，特别是长时间运行的步骤。Make是Unix平台上的常用工具（并且[在Windows上可用](https://drivendata.github.io/cookiecutter-data-science/)）。遵循[`make`文档](https://www.gnu.org/software/make/)、[Makefile约定](https://www.gnu.org/prep/standards/html_node/Makefile-Conventions.html#Makefile-Conventions)和[可移植性指南](http://www.gnu.org/savannah-checkouts/gnu/autoconf/manual/autoconf-2.69/html_node/Portable-Make.html#Portable-Make)将有助于确保你的Makefiles在各系统上有效。这里有[一些](http://zmjones.com/make/) [示例](https://blog.kaggle.com/2012/10/15/make-for-data-scientists/)以[开始](https://web.archive.org/web/20150206054212/http://www.bioinformaticszen.com/post/decomplected-workflows-makefiles/)。包括[Mike Bostock](https://bost.ocks.org/mike/make/)在内的许多数据人员将`make`作为首选工具。

还有其他管理DAG的工具，它们用Python而不是DSL编写（例如，[Paver](http://paver.github.io/paver/#)、[Luigi](http://luigi.readthedocs.org/en/stable/index.html)、[Airflow](https://pythonhosted.org/airflow/cli.html)、[Snakemake](https://bitbucket.org/snakemake/snakemake/wiki/Home)、[Ruffus](http://www.ruffus.org.uk/)或[Joblib](https://pythonhosted.org/joblib/memory.html)）。如果这些工具更适合你的分析，可以随意使用它们。

### 从环境开始构建。

重现分析的第一步始终是重现运行时的计算环境。你需要相同的工具、相同的库和相同的版本以确保一切正常配合。

一种有效的方法是使用[virtualenv](https://virtualenv.pypa.io/en/latest/)（我们推荐[virtualenvwrapper](https://virtualenvwrapper.readthedocs.org/en/latest/)来管理virtualenvs）。通过在存储库中列出所有需求（我们包括一个`requirements.txt`文件），你可以轻松跟踪重现分析所需的包。这里是一个好的工作流程：

1.  创建新项目时运行`mkvirtualenv`。

1.  `pip install`你分析所需的包。

1.  运行`pip freeze > requirements.txt`以固定用于重现分析的确切包版本。

1.  如果你发现需要安装另一个包，再次运行`pip freeze > requirements.txt`并提交更改到版本控制中。

如果你对重建环境有更复杂的要求，可以考虑基于虚拟机的方法，例如[Docker](https://www.docker.com/)或[Vagrant](https://www.vagrantup.com/)。这两种工具都使用基于文本的格式（Dockerfile 和 Vagrantfile），你可以轻松地将其添加到源代码控制中，以描述如何创建具有所需要求的虚拟机。

### 保持机密和配置不被纳入版本控制

你*真的*不希望在 Github 上泄露你的 AWS 密钥或 Postgres 用户名和密码。无需多言——请参阅[Twelve Factor App](http://12factor.net/config)的原则。以下是一种方法：

**将你的机密和配置变量存储在一个特殊文件中**

在项目根文件夹中创建一个`.env`文件。由于`.gitignore`的存在，该文件不应被提交到版本控制库中。以下是一个示例：

```py
# example .env file
DATABASE_URL=postgres://username:password@localhost:5432/dbname
AWS_ACCESS_KEY=myaccesskey
AWS_SECRET_ACCESS_KEY=mysecretkey
OTHER_VARIABLE=something

```

**使用一个包自动加载这些变量。**

如果查看`src/data/make_dataset.py`中的占位符脚本，它使用了一个名为[python-dotenv](https://github.com/theskumar/python-dotenv)的包，将此文件中的所有条目加载为环境变量，以便可以通过`os.environ.get`访问。以下是从`python-dotenv`文档中改编的示例代码片段：

```py
# src/data/dotenv_example.py
import os
from dotenv import load_dotenv, find_dotenv

# find .env automagically by walking up directories until it's found
dotenv_path = find_dotenv()

# load up the entries as environment variables
load_dotenv(dotenv_path)

database_url = os.environ.get("DATABASE_URL")
other_variable = os.environ.get("OTHER_VARIABLE")

```

**AWS CLI 配置**

使用 Amazon S3 存储数据时，管理 AWS 访问的一种简单方法是将访问密钥设置为环境变量。然而，在单台机器上管理多个密钥集（例如，当同时处理多个项目时），最好使用一个[凭证文件](https://docs.aws.amazon.com/cli/latest/userguide/cli-config-files.html)，通常位于`~/.aws/credentials`。一个典型的文件可能如下所示：

```py
[default]
aws_access_key_id=myaccesskey
aws_secret_access_key=mysecretkey

[another_project]
aws_access_key_id=myprojectaccesskey
aws_secret_access_key=myprojectsecretkey

```

初始化项目时可以添加配置文件名称；如果没有设置适用的环境变量，默认会使用配置文件凭证。

### 在更改默认文件夹结构时应保持保守。

为了使这种结构对各种不同类型的项目都适用，我们认为最佳方法是对*你的*项目在调整文件夹时采取宽松态度，但对*所有*项目的默认结构保持保守。

我们创建了一个`folder-layout`标签，专门用于提出添加、删除、重命名或移动文件夹的问题。更一般地，我们还创建了一个`needs-discussion`标签，针对那些在实施之前需要仔细讨论和广泛支持的问题。

### 贡献

Cookiecutter Data Science 项目有自己的观点，但不怕出错。最佳实践会变化，工具会发展，经验也会积累。**该项目的目标是使启动、结构化和共享分析变得更容易。** [拉取请求](https://github.com/drivendata/cookiecutter-data-science/pulls)和[提交问题](https://github.com/drivendata/cookiecutter-data-science/issues)是被鼓励的。我们希望了解哪些方法对你有效，哪些无效。

如果你使用 Cookiecutter 数据科学项目，请链接回此页面或 [给我们发消息](https://twitter.com/drivendataorg) 并 [告知我们](mailto:info@drivendata.org)!

### 相关项目和参考文献的链接

R 研究社区中讨论得更多的是项目结构和可重复性。如果你在使用 R，这里有一些项目和博客帖子可能对你有所帮助。

+   [项目模板](http://projecttemplate.net/index.html) - 一个 R 数据分析模板

+   "[设计项目](http://nicercode.github.io/blog/2013-04-05-projects/)" 在 Nice R Code 上

+   "[我的研究工作流程](http://www.carlboettiger.info/2012/05/06/research-workflow.html)" 在 Carlboettifer.info 上

+   "[组织计算生物学项目的快速指南](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000424)" 在 PLOS 计算生物学上

最后，非常感谢 [Cookiecutter](https://cookiecutter.readthedocs.org/en/latest/) 项目（[github](https://github.com/audreyr/cookiecutter)），它帮助我们减少了思考和编写模板的时间，更多时间用于完成任务。

**简介： [DrivenData](https://www.drivendata.org/)** 是一家使命驱动的数据科学公司，致力于将数据科学、机器学习和人工智能的强大能力带给应对全球重大挑战的组织。DrivenData Labs (drivendata.co) 帮助使命驱动的组织利用数据更智能地工作，提供更有影响力的服务，并充分发挥机器智能的潜力。DrivenData 还举办在线机器学习竞赛 (drivendata.org)，全球充满激情的数据科学家社区为社会影响构建算法。

[原始](https://drivendata.github.io/cookiecutter-data-science/)。经许可转载。

**相关：**

+   [与机器学习算法相关的数据结构](/2018/01/data-structures-related-machine-learning-algorithms.html)

+   [Python 正则表达式备忘单](/2018/04/python-regular-expressions-cheat-sheet.html)

+   [Python 中的函数式编程介绍](/2018/02/introduction-functional-programming-python.html)

### 更多相关内容

+   [成为优秀数据科学家所需的 5 种关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)

+   [每个初学者数据科学家都应掌握的 6 种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)

+   [2021 年最佳 ETL 工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)

+   [停止学习数据科学以寻找目的，并寻找目的去……](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [学习数据科学统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)

+   [建立一个强大的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)
