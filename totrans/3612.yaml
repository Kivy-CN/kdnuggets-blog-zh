- en: When to Retrain an Machine Learning Model? Run these 5 checks to decide on the
    schedule
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/07/retrain-machine-learning-model-5-checks-decide-schedule.html](https://www.kdnuggets.com/2021/07/retrain-machine-learning-model-5-checks-decide-schedule.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By** [**Emeli Dral**](https://twitter.com/EmeliDral)**, CTO and Co-founder
    of Evidently AI &** [**Elena Samuylova**](https://twitter.com/elenasamuylova/)**,
    CEO and Co-founder at Evidently AI**'
  prefs: []
  type: TYPE_NORMAL
- en: The world and data are not static. But most machine learning models are. Once
    they are in production, they become less relevant with time. The [data distributions
    evolve, the behavioral patterns change](https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift),
    and models need updates to keep up with new reality.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'The usual process is to retrain the models at defined intervals. It looks quite
    straightforward: take the new data, the old training pipeline, and fit the model
    again.'
  prefs: []
  type: TYPE_NORMAL
- en: But how often should we do it? Should it be done daily, weekly, or monthly?
    Or every time you get a new batch of data?
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/6633c2014ec57d3088ba2179d982447e.png)'
  prefs: []
  type: TYPE_IMG
- en: Too often, the answer is based on a gut feeling or convenience. Someone picks
    a reasonable interval and schedules a regular retraining job.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we can approach it in a more data-driven way. To be more precise when
    planning the model maintenance, we can run a few checks in advance.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on how critical the model is, you might go through all of them or
    only some.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check #1\. Is the model already good enough?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we make our retraining plans, it helps to check if we are done with training!
    Maybe, the model hasn’t reached its peak performance yet?
  prefs: []
  type: TYPE_NORMAL
- en: The simple way to do that is to look at good old learning curves.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/f0988021c2e694e56e03bef78e14da42.png)'
  prefs: []
  type: TYPE_IMG
- en: How to proceed? We can fix the test set which we use to evaluate the model performance.
    Then, run a set of experiments by training the model on different parts of the
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the random split method and iterate by changing the train size. This
    way, we focus on how the data volume impacts the performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/1353e1f20c4313d1a9f55092a6d2800a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are two things we can learn as a result: 1) how much data we need to
    reach the peak performance 2) whether the model reaches this plateau with the
    available training data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sometimes we’d learn that the model doesn’t need all the data we have.**
    For example, we have multiple years of sales data, but using just one year of
    training data gets the same quality.'
  prefs: []
  type: TYPE_NORMAL
- en: We might drop the extra data to make the model more lightweight.
  prefs: []
  type: TYPE_NORMAL
- en: '**In other cases, we could see that the model quality keeps going up and up**.
    The model is hungry for more data! There are more steps to be made to bring the
    model to its top shape.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/7d75b4bc993a559419e6f57d29fb3c70.png)'
  prefs: []
  type: TYPE_IMG
- en: Rather than think about model retraining to maintain its quality, we should
    plan for continuous improvement. As soon as we get enough new data, we can use
    it to reach better performance.
  prefs: []
  type: TYPE_NORMAL
- en: This first test also gives a sense of scale and “density” of signal in the data.
    Do we need 10, 100, or 1000 observations to see a meaningful impact on the model
    performance? Would it take us a day or a month to collect that amount of new data?
  prefs: []
  type: TYPE_NORMAL
- en: That’s a helpful thing to know!
  prefs: []
  type: TYPE_NORMAL
- en: 'Check #2\. How quickly do things change?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we create our machine learning models, we assume there is some stability
    in the real-world process. Otherwise, it would make little sense to learn from
    the past!
  prefs: []
  type: TYPE_NORMAL
- en: We also know that things change. When working with a dynamic process, we can
    also assume that there is a certain speed at which these new patterns accumulate.
  prefs: []
  type: TYPE_NORMAL
- en: We can then try to calculate this!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take our model and see how long it “lasts” if we simulate its application
    in the past.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/f57dc5e2baea3ed6c05cdd2eefc14927.png)'
  prefs: []
  type: TYPE_IMG
- en: We can train a model using some older part of the data and then “apply” it to
    the later periods. Just like we do with a hold-out set, but here we simply take
    several consecutive ones.
  prefs: []
  type: TYPE_NORMAL
- en: We can start with a single-point estimate and see how fast the performance degrades.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/886251abc0b3af2682dc5aedba2e68cd.png)'
  prefs: []
  type: TYPE_IMG
- en: If we have enough historical data, we can repeat this check several times and
    then average the results. Just keep an eye on potential outliers and rare events!
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/4187c122c80bd37ca886503946959293.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Sometimes we’d learn that an “old” model performs as good as new.** Some
    prefer to retrain the models often to keep them “fresh,” but it is not always
    justified.'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t fix what’s not broken!
  prefs: []
  type: TYPE_NORMAL
- en: If frequent retraining is not needed, you might go away with a lighter service
    architecture. You can also decrease the risks of technical errors that come with
    any change. The same goes for the organizational overhead, especially when new
    models require an approval process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/7cf2c1d846f0a3770216da28a8ebe87a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**In other cases, you can learn that the model ages very fast!** That is good
    to know in advance to set up proper monitoring and prepare the infrastructure.'
  prefs: []
  type: TYPE_NORMAL
- en: You might decide to reconsider your training approach to make the model more
    stable. For example, change feature engineering or model architecture to make
    it a bit less performant on the test set but more stable in the long run. In other
    cases, you might train the model using a shorter training period but perform frequent
    calibration or consider active learning.
  prefs: []
  type: TYPE_NORMAL
- en: We might also face constraints in our ability to retrain the model.
  prefs: []
  type: TYPE_NORMAL
- en: That brings us to the next check.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check #3\. When do we get the new data?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Image](../Images/ad4cb68a76433b6c053208581142e7d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we look at the business process rather than the data.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, we get real-world feedback almost immediately. For example, you recommend
    an article to read, and you quickly know if the user clicked on a link.
  prefs: []
  type: TYPE_NORMAL
- en: In other cases, the new data that you can use to retrain your model comes with
    a delay.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a long prediction horizon, you have to wait to know if your prediction
    was correct. With other tasks, you need a separate labeling process. Sometimes,
    the limitations come from how the data is moved or generated. For example, manual
    data entry is done once per month.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/4d963f1ef664d1112a0b9573247d2a02.png)'
  prefs: []
  type: TYPE_IMG
- en: We can find ourselves in one of the two situations.
  prefs: []
  type: TYPE_NORMAL
- en: '**In some cases, the model degrades before the new data arrives.** It becomes
    a limitation.'
  prefs: []
  type: TYPE_NORMAL
- en: If we do not get the data in time to retrain the model, we might need to reconsider
    the approach again. For example, create an ensemble of models with different retraining
    horizons or combine machine learning with rules or human-in-the-loop. As a last
    resort, we can also adjust our performance expectations and prepare to deal with
    a lowered model quality.
  prefs: []
  type: TYPE_NORMAL
- en: '**In other cases, the new data starts accumulating before the model decay.**
    In this case, we have the luxury of choice.'
  prefs: []
  type: TYPE_NORMAL
- en: We can, of course, simply initiate the retraining at any point after the data
    comes. If we want to be more precise, there is a way to do that.
  prefs: []
  type: TYPE_NORMAL
- en: Read on!
  prefs: []
  type: TYPE_NORMAL
- en: 'Check #4\. How much data do we need to see the improvement?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Say the new data arrives daily, but the model degrades only after 30 days. What
    would be the optimal action? Should we retrain daily, weekly, or once per month?
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/93da0c9a4e5c92b0642fc2c37d89f2a3.png)'
  prefs: []
  type: TYPE_IMG
- en: We can make a more precise judgment by checking if the new bucket of data brings
    the improvement we want.
  prefs: []
  type: TYPE_NORMAL
- en: The thing is, sometimes adding a small set of new data points does not change
    anything. There is some minimal required data size that has a visible impact on
    the performance.
  prefs: []
  type: TYPE_NORMAL
- en: We can evaluate this.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, we choose a test set from a period of the known decay. We know
    when the performance goes down: we can then check if retraining on the new data
    improves it.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/36e21e33e0b86cf04f120b86ea13917b.png)'
  prefs: []
  type: TYPE_IMG
- en: We can add data in small increments as it comes. Then, we see how it affects
    the test performance.
  prefs: []
  type: TYPE_NORMAL
- en: What often happens is that we have to wait a bit to collect a “useful” amount
    of data—for example, at least a week of it to capture the relevant seasonal patterns.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, our actual choice of retraining window might be more narrow than
    it seemed! On the side, it is limited by the speed of data provision. On the other
    side, by the need to collect enough data for retraining to bring effect.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/0c8d71d262bc407a56865915a599e208.png)'
  prefs: []
  type: TYPE_IMG
- en: Within this time frame, you can pick the period based on what’s practical and
    makes more sense for the use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check #5\. Should we keep the older data?'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is one more question that often comes up. How should we retrain the model?
    Should we add some new data and drop some old? Should we continuously increase
    the training set?
  prefs: []
  type: TYPE_NORMAL
- en: This is a bonus check we can run.
  prefs: []
  type: TYPE_NORMAL
- en: We can imitate model retraining at the chosen interval and then check how things
    change if we start dropping the old data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/2433693530f83ace89e16563af95b574.png)'
  prefs: []
  type: TYPE_IMG
- en: We can often see that leaving out the old data makes no difference. Then, it
    is probably a sane thing to do to keep the training more lightweight.
  prefs: []
  type: TYPE_NORMAL
- en: What’s more, sometimes it makes the model better! Keeping the old irrelevant
    data might cause the performance to go down if you have a dynamic use case. That
    is good to know in advance.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we might also see that it makes sense to keep all you have for the
    time being. You can probably repeat the check later on.
  prefs: []
  type: TYPE_NORMAL
- en: What’s next?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With these checks, you can come prepared for the model maintenance. You can
    set up your retraining pipelines to get ready for the updates at a chosen interval.
  prefs: []
  type: TYPE_NORMAL
- en: Still, we cannot simply rely on our schedule. We also need a reality check.
    This means, monitoring our models once they go live.
  prefs: []
  type: TYPE_NORMAL
- en: To get the visibility, we can build a [monitoring dashboard](https://github.com/evidentlyai/evidently)
    or schedule regular checks to calculate the actual model performance (if we have
    the ground truth) or otherwise monitor the input data for statistical distribution
    drifts and outliers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../Images/19db7390f4ca668c6a5a3a66023ce064.png)'
  prefs: []
  type: TYPE_IMG
- en: Even stable models can face [data and concept drift](https://evidentlyai.com/blog/machine-learning-monitoring-data-and-concept-drift)
    or certain rare events. In this case, we might then need to intervene earlier
    than planned.
  prefs: []
  type: TYPE_NORMAL
- en: On the other side, if things look steady, and we can skip an update for longer
    than planned.
  prefs: []
  type: TYPE_NORMAL
- en: Planning for regular retraining and complementing this with continuous monitoring
    is usually an optimal strategy to make the model live up to its promised performance!
  prefs: []
  type: TYPE_NORMAL
- en: '**Links**'
  prefs: []
  type: TYPE_NORMAL
- en: You can find an extended version of this article [here](https://evidentlyai.com/blog/retrain-or-not-retrain).
  prefs: []
  type: TYPE_NORMAL
- en: '[Machine learning monitoring checklist](/2021/03/machine-learning-model-monitoring-checklist.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Emeli Dral** is a Co-founder and CTO at Evidently AI where she creates tools
    to analyze and monitor ML models. Earlier she co-founded an industrial AI startup
    and served as the Chief Data Scientist at Yandex Data Factory. She is a co-author
    of the Machine Learning and Data Analysis curriculum at Coursera with over 100,000
    students.'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://twitter.com/EmeliDral](https://twitter.com/EmeliDral)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Elena Samuylova** is a Co-founder and CEO at Evidently AI. Earlier she co-founded
    an industrial AI startup and led business development at Yandex Data Factory.
    Since 2014, she has worked with companies from manufacturing to retail to deliver
    ML-based solutions. In 2018, Elena was named 50 Women in Product Europe by Product
    Management Festival.'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://twitter.com/elenasamuylova/](https://twitter.com/elenasamuylova/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Learning from machine learning mistakes](/2021/03/learning-from-machine-learning-mistakes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Machine Learning Model Monitoring Checklist: 7 Things to Track](/2021/03/machine-learning-model-monitoring-checklist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MLOps: Model Monitoring 101](/2021/01/mlops-model-monitoring-101.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Schedule & Run ETLs with Jupysql and GitHub Actions](https://www.kdnuggets.com/2023/05/schedule-run-etls-jupysql-github-actions.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Learning For Compliance Checks: What''s New?](https://www.kdnuggets.com/2022/05/deep-learning-compliance-checks-new.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Essential Data Quality Checks with Pandas](https://www.kdnuggets.com/7-essential-data-quality-checks-with-pandas)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learn Machine Learning From These GitHub Repositories](https://www.kdnuggets.com/2023/01/learn-machine-learning-github-repositories.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learn How to Run Alpaca-LoRA on Your Device in Just a Few Steps](https://www.kdnuggets.com/2023/05/learn-run-alpacalora-device-steps.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Run an LLM Locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
