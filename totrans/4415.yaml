- en: An Introduction to NLP and 5 Tips for Raising Your Game
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《NLP简介及提升技巧的5个建议》
- en: 原文：[https://www.kdnuggets.com/2020/09/introduction-nlp-5-tips-raising-your-game.html](https://www.kdnuggets.com/2020/09/introduction-nlp-5-tips-raising-your-game.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/09/introduction-nlp-5-tips-raising-your-game.html](https://www.kdnuggets.com/2020/09/introduction-nlp-5-tips-raising-your-game.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: For those working around Data Science, Machine Learning and/or Artificial Intelligence,
    NLP is probably one of the most exciting fields to work in.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从事数据科学、机器学习和/或人工智能工作的人来说，NLP可能是最令人兴奋的领域之一。
- en: '*NLP stands for Natural Language Processing and it’s about the interactions
    between computers and human languages. Programming algorithms capable of processing
    and analyzing large amounts of natural language data.*'
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*NLP代表自然语言处理，涉及计算机与人类语言之间的交互。编程算法能够处理和分析大量自然语言数据。*'
- en: The underlying objective may vary, but the overall goal is to get to conclusions
    about human behaviour…our intentions when writing something, what we were thinking
    or feeling when we do it, the category of an item we were writing about, and some
    other stuff like chatbots, market segmentation of customers, find duplicates and
    similarities in between elements, virtual assistants (like Siri or Alexa) and
    much more stuff.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 其基本目标可能有所不同，但总体目标是得出关于人类行为的结论……我们写作时的意图，我们在写作时的思考或感受，我们所写内容的类别，以及其他一些东西，如聊天机器人、客户市场细分、查找重复项和元素之间的相似性、虚拟助手（如Siri或Alexa）等等。
- en: Nonetheless, NLP as a subject didn’t appear much time ago, it was just in 1960
    when Alan Turing published an article called “Computing Machinery and Intelligence”
    which proposed what is now called the ‘Turing test’. The paper introduced the
    question ‘Can machines think?’ and the test proves a machine’s ability to exhibit
    intelligent behaviour equivalent to, or indistinguishable from, that of a human.
    Three participants are necessary for running the test, where a player C, the evaluator,
    is given the task of trying to determine which player — A or B — is a computer
    and which is a human.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，作为一个学科，NLP并未出现很久，直到1960年，艾伦·图灵发表了一篇名为《计算机与智能》的文章，提出了现在被称为‘图灵测试’的概念。该论文提出了“机器能思考吗？”的问题，并通过测试机器表现出的智能行为是否等同于或不可区分于人类的智能行为。进行测试需要三名参与者，其中一名玩家C，即评估者，负责确定哪个玩家——A或B——是计算机，哪个是人类。
- en: '![Figure](../Images/12bd2a60b6b24c26c85617612e7633f4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/12bd2a60b6b24c26c85617612e7633f4.png)'
- en: The Japanese robot Pepper, made by Aldebaran Robotics
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 日本机器人Pepper，由Aldebaran Robotics公司制造。
- en: The evaluator would then judge natural language conversations between a human
    and a machine designed to generate human-like responses, knowing that one of the
    two partners in conversation is a machine. The conversation would be limited to
    a text-only channel and the results do not depend on the machine’s ability to
    give correct answers to questions, only how closely its answers resemble those
    a human would give. If at the end of the test the evaluator cannot reliably tell
    the machine from the human, the machine is said to have passed the test.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 评估者将判断一个人类和一个旨在生成类似人类响应的机器之间的自然语言对话，知道对话中的两个伙伴中有一个是机器。对话将限制在仅文本的频道中，结果不依赖于机器是否能正确回答问题，而仅仅取决于其回答与人类回答的相似程度。如果在测试结束时，评估者无法可靠地区分机器和人类，则机器被认为通过了测试。
- en: Starting from there, in the past years, the field has evolved exponentially,
    going from hand-coded systems using a set of rules, to a more sophisticated statistical
    NLP. And in this context, some companies are doing some pretty exciting stuff
    in the field. For example, if you’re an Android user you’re probably familiar
    with **Swiftkey**, a startup using text prediction designed to boost the accuracy,
    fluency and speed of users’ writing. Swiftkey learns from our writing, predicting
    favourite words, emojis and even expressions. Another startup,** SignAll**, converts
    sign language into text. Helping individuals who are deaf communicate with those
    who don’t know sign language.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从那时起，在过去几年中，该领域已呈指数级发展，从使用一组规则的手工编码系统发展到更复杂的统计NLP。在这种背景下，一些公司在该领域做了一些非常令人兴奋的工作。例如，如果你是Android用户，你可能熟悉**Swiftkey**，这是一个使用文本预测的初创公司，旨在提高用户写作的准确性、流畅性和速度。Swiftkey从我们的写作中学习，预测最喜欢的单词、表情符号甚至表达方式。另一个初创公司**SignAll**，则将手语转换为文本，帮助听障人士与不会手语的人交流。
- en: And the fact is that nowadays the expansion of some open source libraries using
    Python, Tensorflow, Keras and others, has made NLP accessible and each day more
    and more businesses are using it. Some of them hiring other companies specifically
    specialized in the subject, but some others are hiring Data Scientists and Data
    Analyst in order to build their own solutions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，如今，使用 Python、Tensorflow、Keras 等开源库的扩展，使得 NLP 变得更加可及，每天越来越多的企业在使用它。其中一些公司专门聘请其他专门从事该领域的公司，而另一些则雇佣数据科学家和数据分析师来构建自己的解决方案。
- en: If any of these is your case, whether you are the company or the data specialist,
    in the next lines I will introduce some of my learning while working with NLP.
    Lucky for you, all of them are mistake-based tips! So hopefully, you will be able
    to avoid them in advance, not as it happened to me :)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到任何这些情况，无论你是公司还是数据专家，在接下来的几行中，我将介绍我在从事 NLP 工作时的一些学习经验。幸运的是，所有这些都是基于错误的提示！希望你能提前避免它们，而不是像我一样经历这些
    :)
- en: 1\. Find the right type of vectorization for you
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1\. 找到适合你的向量化类型
- en: 'In NLP usually, after lots and lots and lots (and probably lots more) of data
    cleaning, the magic starts with something called vectorization. This tool, technique,
    or however you want to call it, take a bunch of text, usually called documents,
    and transforms them in vectors according to the words appearing within each document.
    Take the following example:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，通常在经过大量的数据清理后，魔法开始于所谓的向量化。这种工具、技术或无论你怎么称呼它，将一堆文本，通常称为文档，根据每个文档中出现的单词，将它们转换为向量。请看以下示例：
- en: '![Figure](../Images/eba200a25aa4fce271629f3a64542c37.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/eba200a25aa4fce271629f3a64542c37.png)'
- en: Example created by the author using images from [https://www.oreilly.com](https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 示例由作者使用来自 [https://www.oreilly.com](https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html)
    的图像创建
- en: 'In the example above we are using a tool known as **Count Vectorizer or Bag
    of Words**. This kind of vectorization usually discards grammar, order, and structure
    in the text. It is a great option since it keeps track of all words appearing
    within the documents and their simple way of processing them by just counting
    it’s easily understandable and gives us a clear picture of the most important
    words overall. However, it presents two main problems:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，我们使用了一种称为**计数向量器或词袋模型**的工具。这种向量化方法通常会忽略文本中的语法、顺序和结构。它是一个很好的选择，因为它跟踪文档中出现的所有单词，而且通过简单地计数来处理它们的方式易于理解，并且为我们提供了对整体最重要单词的清晰认识。然而，它也存在两个主要问题：
- en: '**Data sparsity**: when counting all appearances throughout documents, we can
    easily end with a matrix composed of vectors full of zeros since of course, each
    document will only contain a small amount of all the possible words. We’ll talk
    more about this later.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据稀疏性**：在统计所有文档中的出现次数时，我们可能会得到一个由全是零的向量组成的矩阵，因为每个文档只包含所有可能单词的一小部分。我们稍后会详细讨论这个问题。'
- en: '**The future itself**: a Count Vectorizer outputs a fixed-sized matrix with
    all words (or those of certain frequencies) appearing in our current documents.
    This could be a problem if we receive further documents in the future and we don’t
    know the words we might find.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未来本身**：计数向量器输出一个固定大小的矩阵，包含我们当前文档中所有单词（或某些频率的单词）。如果我们未来收到更多文档，而我们不知道可能会遇到的单词，这可能会成为一个问题。'
- en: '**Tricky documents**: what happens if we have a document in which a specific
    word appears so many times, that it ends looking as it is the most common word
    throughout all the documents instead of just a word appearing lots of time in
    just one document?'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**棘手的文档**：如果我们有一个文档，其中一个特定的单词出现得如此频繁，以至于它看起来像是所有文档中最常见的单词，而不仅仅是一个单词在一个文档中出现了很多次，那会发生什么呢？'
- en: To solve the first problem and the second problem we could use a **Hashing Vectorize**,
    which converts a collection of text documents to a matrix of occurrences calculated
    with the [hashing trick](https://en.wikipedia.org/wiki/Feature_hashing). Each
    word is mapped to a feature with the use of a hash function that converts it to
    a number. If we encounter that word again in the text, it will be converted to
    the same hash, allowing us to count word occurrences without retaining a dictionary
    in memory. The main drawback of this trick is that it’s not possible to compute
    the inverse transform, and thus we lose information on what words the important
    features correspond to.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决第一个和第二个问题，我们可以使用**哈希向量化器**，它将文本文档集合转换为使用[哈希技巧](https://en.wikipedia.org/wiki/Feature_hashing)计算的出现矩阵。每个词汇通过哈希函数映射到一个特征，这个函数将其转换为一个数字。如果我们在文本中再次遇到该词，它将被转换为相同的哈希，从而允许我们在不保留词典的情况下计算词汇出现的次数。这个技巧的主要缺点是无法计算逆变换，因此我们失去了重要特征对应的词汇信息。
- en: To solve the third problem mentioned above, we could use **term frequency-inverse
    document frequency (tf-idf) vectorizer**. A tf-idf score tells us which words
    are most discriminating between documents. Words that occur a lot in one document
    but don’t occur in many documents contain a great deal of discriminating power.
    The inverse document frequency is a measure of how much information the word provides,
    that is, whether the term is common or rare across all documents. Enhancing terms
    highly specific of a particular document while suppressing terms that are common
    to most documents.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述第三个问题，我们可以使用**词频-逆文档频率（tf-idf）向量化器**。tf-idf 分数告诉我们哪些词在文档之间具有最强的区分能力。在一个文档中频繁出现但在许多文档中很少出现的词包含了很大的区分能力。逆文档频率是衡量词汇提供多少信息的指标，即该词在所有文档中是常见还是稀有。提升特定文档的词汇，同时抑制那些在大多数文档中常见的词汇。
- en: 'Sklearn has implementations for all these three types of vectorization:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Sklearn 对所有这三种向量化类型都提供了实现：
- en: '[Countvectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Countvectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)'
- en: '[Hashingvectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hashingvectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)'
- en: '[Tfidfvectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tfidfvectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)'
- en: 2\. Personalize stop words and be aware of the language in your data
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2\. 个性化停用词，并注意数据中的语言
- en: 'Using stop words when doing any kind of vectorization is a key step for getting
    reliable results. Passing a list of stop words to our algorithm we’re telling
    to it: ‘please ignore all these words if you find any…I don’t want to have them
    in my output matrix’. Skelarn does include a default list of stop words for us
    to use, just by passing the word ‘english’ to the ‘stop_words’ hyperparameter.
    However, there are several limitations:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行任何形式的向量化时，使用停用词是获得可靠结果的关键步骤。将停用词列表传递给我们的算法，我们是在告诉它：“如果找到这些词，请忽略它们……我不希望它们出现在我的输出矩阵中。”
    Sklearn 确实包含一个默认的停用词列表，只需将单词‘english’传递给‘stop_words’超参数即可。然而，这里有几个限制：
- en: It only includes basic words like “and”, “the”, “him”, which are presumed to
    be uninformative in representing the content of a text and which may be removed
    to avoid them being construed as a signal for prediction. However, if for example,
    you were processing descriptions of houses scraped from rental agencies websites,
    you would probably want to remove all words that d not make to the description
    of the property itself. Words as ‘opportunity’, ‘offer’, ‘amazing’, ‘great’, ‘now’
    and stuff like that
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它仅包括基本的词汇，如“and”、“the”、“him”，这些词被认为在表示文本内容时不具备信息量，可能会被删除以避免被解读为预测信号。然而，例如，如果你在处理从租赁代理网站抓取的房屋描述，你可能会希望移除所有不属于房产描述本身的词汇，如‘opportunity’、‘offer’、‘amazing’、‘great’、‘now’等。
- en: 'And what has been for me the greatest drawback being a Spanish speaker and
    working with machine learning problems in that language: It’s only available in
    English'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对我来说，作为一名西班牙语使用者，在处理该语言的机器学习问题时最大的问题是：它仅在英语中可用。
- en: So whether you want to enrich the default list of words in English to improve
    your output matrix, or you want to use a list in some other language, you can
    pass Sklearn’s algorithm a personalized list of stop words by using the hyperparameter
    ‘stop_words’. By the way, [here’s a GitHub repository](https://github.com/Alir3z4/stop-words) with
    an impressive number of lists in several languages.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，无论你是想丰富英语中的默认单词列表以改善输出矩阵，还是想使用其他语言的单词列表，你都可以通过使用超参数‘stop_words’将个性化的停用词列表传递给
    Sklearn 的算法。顺便说一下，[这是一个 GitHub 仓库](https://github.com/Alir3z4/stop-words) ，其中包含了许多语言的词表。
- en: Before jumping into the next point, just have in mind that sometimes you won’t
    want to use any stop words at all. For example, if you’re dealing with numbers,
    even the default English list of stop words within Sklearn includes all single
    numbers from 0 to 9\. So it’s important for you to ask yourself whether or not
    you’re working on an NLP problem that needs stop words.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一个点之前，请记住，有时你可能根本不需要使用任何停用词。例如，如果你处理的是数字，即使是 Sklearn 默认的英语停用词列表也包括所有从 0
    到 9 的单个数字。因此，重要的是要问自己是否正在处理需要停用词的 NLP 问题。
- en: 3\. Use stemmer for ‘grouping’ similar words
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3\. 使用词干提取器进行‘分组’相似单词
- en: 'Text normalization is the process of converting slightly different versions
    of words with essentially equivalent meaning into the same features. In some cases,
    it might be sensible to consider all possible variants of a possible word, but
    whether you’re working in English or any other language, sometimes you’ll also
    want to do some kind of pre-processing to your document in order to represent
    in the same way words that underly the same meaning. For example, consultant,
    consulting, consult, consultative and consultants could all be expressed as just
    ‘consultant’. See the next table for more examples:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 文本标准化是将稍有不同版本的具有本质上等同意义的单词转换为相同特征的过程。在某些情况下，考虑到所有可能的变体可能是合理的，但无论你是在英语还是其他语言中工作，有时你还需要对文档进行某种预处理，以相同的方式表示具有相同意义的单词。例如，consultant、consulting、consult、consultative
    和 consultants 都可以表示为‘consultant’。更多示例见下表：
- en: '![Figure](../Images/7c89acddcd3de9f9a3915d64adc25030.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/7c89acddcd3de9f9a3915d64adc25030.png)'
- en: Source: [https://www.wolfram.com](https://www.wolfram.com/language/11/text-and-language-processing/generate-and-verify-stemmed-words.html) —
    Generate and verify stemmed words
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://www.wolfram.com](https://www.wolfram.com/language/11/text-and-language-processing/generate-and-verify-stemmed-words.html)
    — 生成和验证词干化单词
- en: 'For doing this, we could use stemming. Stemmers remove morphological affixes
    from words, leaving only the word stem. Luckily for us, [NLTK library for Python](https://www.nltk.org/api/nltk.stem.html) contains
    several robust stemmers. And if you want to incorporate your specific-language
    stemmer, or other, into your vectorizer algorithm, you can just use the following
    bunch of code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们可以使用词干提取。词干提取器去除单词的形态学词缀，仅保留词干。幸运的是，[Python 的 NLTK 库](https://www.nltk.org/api/nltk.stem.html)包含了几种强大的词干提取器。如果你想将特定语言的词干提取器或其他词干提取器整合到你的向量化算法中，你可以使用以下代码：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can easily change this for using HashingVectorizer or TfidfVectorizer, just
    by changing the algorithm given to the class.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过更改分配给类的算法轻松地将其改为使用 HashingVectorizer 或 TfidfVectorizer。
- en: 4\. Avoid using Pandas DataFrames
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4\. 避免使用 Pandas DataFrames
- en: 'This piece of advice is short and sweet: if you’e working in an NLP project
    with any data larger than 5–10k thousand rows, avoid using DataFrames. Just vectorizing
    a big number of documents using Pandas returns a massive matrix that makes handling
    very slow, but also, lots of times, Natural Language Processing projects involve
    stuff like measuring distances, what tends to be very slow since it needs to compare
    elements against each other. And even though I’m myself a heavy user of Pandas’
    DataFrames, for this kind of stuff I would recommend using Numpy Arrays or Sparse
    Matrices.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这条建议简短明了：如果你在处理超过 5-10k 行的数据的 NLP 项目时，避免使用 DataFrames。仅仅使用 Pandas 对大量文档进行向量化会返回一个巨大的矩阵，处理速度非常慢，而且很多时候，自然语言处理项目涉及测量距离，这通常非常缓慢，因为需要彼此比较元素。即使我自己也大量使用
    Pandas 的 DataFrames，对于这类任务，我建议使用 Numpy Arrays 或 Sparse Matrices。
- en: 'Also. mind that you can always get your sparse matrix to an array just by using
    the ‘.toarray()’ function and vice-versa, from array to sparse matrix using:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，请注意，你总是可以通过使用‘.toarray()’函数将稀疏矩阵转换为数组，反之亦然，从数组转换为稀疏矩阵：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'By the way, if your dealing with time issues, remember you can time your code
    using the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，如果你处理时间问题，记得可以使用以下方法来计时你的代码：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '5\. Data sparsity: make your output matrix usable'
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5. 数据稀疏性：使你的输出矩阵可用
- en: '![Image for post](../Images/47f4911c29c2175f0a67e7db20befb2c.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![用于帖子图片](../Images/47f4911c29c2175f0a67e7db20befb2c.png)'
- en: 'As said before, one of the biggest problems while working with NLP is the issue
    of data sparsity…ending with matrices of dozens of thousands of columns full of
    zeros, that make it impossible for us to apply certain stuff afterwards. Here
    are a couple of tips of things I have used in the past for dealing with this problem:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前所说，处理 NLP 时最大的一个问题是数据稀疏性……结果是充满零的数万个列的矩阵，这使得之后应用某些技术变得不可能。以下是我过去处理这个问题时使用的一些技巧：
- en: '**When using TfidfVectorizer or CountVectorizer using the hyperparameter ‘max_features’**.
    For example, you could print out the words frequencies across documents and then
    set a certain threshold for them. Imagine you have set a threshold of 50, and
    your data corpus consists of 100 words. After looking at the words frequencies
    20 words occur less than 50 times. Thus, you set max_features=80 and you are good
    to go. If max_features is set to None, then the whole corpus is considered during
    the transformation. Otherwise, if you pass, say, 5 to max_features, that would
    mean creating a feature matrix out of the most 5 frequent words across text documents.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 TfidfVectorizer 或 CountVectorizer 时，设置超参数‘max_features’**。例如，你可以打印出文档中的词频，然后为其设置某个阈值。假设你设置了一个50的阈值，而你的数据语料库包含100个词。经过查看词频，发现有20个词出现次数少于50次。因此，你将
    max_features 设置为80，然后可以继续使用。如果 max_features 设置为 None，那么在转换过程中会考虑整个语料库。否则，如果你传递例如5给
    max_features，那将意味着从文本文档中最频繁的5个词中创建特征矩阵。'
- en: '**Setting up a number of ‘n_features’ in HashingVectorizer.** This hyperparameter
    sets the number of features/columns in the output matrix. Small numbers of features
    are likely to cause hash collisions, but large numbers will cause larger coefficient
    dimensions in linear learners. The number is up to you and what you need.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在 HashingVectorizer 中设置‘n_features’的数量**。这个超参数设置输出矩阵中的特征/列数。较小的特征数量可能会导致哈希冲突，但较大的特征数量会导致线性学习器中的系数维度增大。具体数量取决于你和你的需求。'
- en: '**Using dimensionality reduction**. Techniques as Principal Component Analysis
    take an output matrix with dozens of thousands of columns into a much smaller
    set capturing the variance on the original matrix could be a great idea. Just
    mind analyzing how much this dimensionality reduction affects your final results,
    to check if it’s actually useful and also to select the number of dimensions to
    be used.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用降维技术**。如主成分分析（PCA）可以将具有数万个列的输出矩阵转化为捕捉原始矩阵方差的较小集合。这可能是一个很好的主意。只要分析这种降维对最终结果的影响，检查其实际效用，并选择使用的维度数量。'
- en: I really really hope all these learnings I have had might help in your NLP project.
    More stories about NLP will come in the future, but if you enjoy this story don’t
    forget to check out some of my last articles, like [how to divide your data into
    train and test set assuring representativeness](https://medium.com/better-programming/how-to-divide-data-into-train-and-test-assuring-representativeness-c4c12c215d79?source=friends_link&sk=389e0ee2de25f5e76d31d8430e7ccc4b),
    s[urvivorship bias in Data Science](https://towardsdatascience.com/survivorship-bias-in-data-science-and-machine-learning-4581419b3bca?source=friends_link&sk=fd531decb8e07af98c506c4038c53bf9) and [using
    a cluster in the cloud for Data Science projects in 4 simple steps](https://medium.com/swlh/using-a-cluster-in-the-cloud-for-data-science-projects-in-4-simple-steps-9ee067238448?source=friends_link&sk=39832abc5b65eca87eb1089f934e87e1).
    All of them and more available within [my Medium profile](https://medium.com/@g.ferreiro.volpi).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我真的希望我所学到的这些知识能够帮助你的 NLP 项目。未来会有更多关于 NLP 的故事，但如果你喜欢这个故事，不要忘记查看我最近的一些文章，如 [如何将数据分成训练集和测试集以确保代表性](https://medium.com/better-programming/how-to-divide-data-into-train-and-test-assuring-representativeness-c4c12c215d79?source=friends_link&sk=389e0ee2de25f5e76d31d8430e7ccc4b)、[数据科学中的幸存者偏差](https://towardsdatascience.com/survivorship-bias-in-data-science-and-machine-learning-4581419b3bca?source=friends_link&sk=fd531decb8e07af98c506c4038c53bf9)
    和 [在云中使用集群进行数据科学项目的4个简单步骤](https://medium.com/swlh/using-a-cluster-in-the-cloud-for-data-science-projects-in-4-simple-steps-9ee067238448?source=friends_link&sk=39832abc5b65eca87eb1089f934e87e1)。所有这些及更多内容可在
    [我的 Medium 个人资料](https://medium.com/@g.ferreiro.volpi) 中找到。
- en: And **if you want to receive my latest articles directly on your email, just **[**subscribe
    to my newsletter**](https://gmail.us3.list-manage.com/subscribe?u=8190cded0d5e26657d9bc54d7&id=3e942158a2)** :)**
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'And a special mention to the following sources I used throughout the story:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[*https://blog.ventureradar.com/2015/09/22/top-5-companies-innovating-with-natural-language-processing/*](https://blog.ventureradar.com/2015/09/22/top-5-companies-innovating-with-natural-language-processing/)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*https://en.wikipedia.org/wiki/Natural_language_processing*](https://en.wikipedia.org/wiki/Natural_language_processing)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*https://en.wikipedia.org/wiki/Turing_test*](https://en.wikipedia.org/wiki/Turing_test)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*https://www.forbes.com/sites/bernardmarr/2019/06/03/5-amazing-examples-of-natural-language-processing-nlp-in-practice/#365b59901b30*](https://www.forbes.com/sites/bernardmarr/2019/06/03/5-amazing-examples-of-natural-language-processing-nlp-in-practice/#365b59901b30)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html*](https://www.oreilly.com/library/view/applied-text-analysis/9781491963036/ch04.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*https://stackoverflow.com/questions/46118910/scikit-learn-vectorizer-max-features*](https://stackoverflow.com/questions/46118910/scikit-learn-vectorizer-max-features)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bio:** After 5+ years of experience in eCommerce and Marketing across multiple
    industries, **[Gonzalo Ferreiro Volpi](https://www.linkedin.com/in/gferreirovolpi/)**
    pivoted into the world of Data Science and Machine Learning, and currently works
    at Ravelin Technology using a combination of machine learning and human insights
    to tackle fraud in eCommerce.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/an-introduction-to-nlp-and-5-tips-for-raising-your-game-639636188ddf).
    Reposted with permission.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[Accelerated Natural Language Processing: A Free Course From Amazon](/2020/08/accelerated-nlp-free-amazon-machine-learning-university.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4 Free Math Courses to do and Level up your Data Science Skills](/2020/06/4-free-maths-courses.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The NLP Model Forge: Generate Model Code On Demand](/2020/08/nlp-model-forge.html)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Tips to Step Up Your Data Science Game Right Away](https://www.kdnuggets.com/5-tips-to-step-up-your-data-science-game-right-away)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Step up your Python game with Fast Python for Data Science!](https://www.kdnuggets.com/2022/06/manning-step-python-game-fast-python-data-science.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提升你的 Python 技能，使用《数据科学的快速 Python》！](https://www.kdnuggets.com/2022/06/manning-step-python-game-fast-python-data-science.html)'
- en: '[Future-Proof Your Data Game: Top Skills Every Data Scientist Needs in 2023](https://www.kdnuggets.com/futureproof-your-data-game-top-skills-every-data-scientist-needs-in-2023)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为数据游戏做好未来准备：2023 年每个数据科学家需要的顶级技能](https://www.kdnuggets.com/futureproof-your-data-game-top-skills-every-data-scientist-needs-in-2023)'
- en: '[Transforming AI with LangChain: A Text Data Game Changer](https://www.kdnuggets.com/2023/08/transforming-ai-langchain-text-data-game-changer.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 LangChain 改变 AI：文本数据的游戏规则改变者](https://www.kdnuggets.com/2023/08/transforming-ai-langchain-text-data-game-changer.html)'
- en: '[Python f-Strings Magic: 5 Game-Changing Tricks Every Coder Needs to Know](https://www.kdnuggets.com/python-fstrings-magic-5-gamechanging-tricks-every-coder-needs-to-know)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python f-Strings 魔法：每个编码者需要了解的 5 个改变游戏规则的技巧](https://www.kdnuggets.com/python-fstrings-magic-5-gamechanging-tricks-every-coder-needs-to-know)'
- en: '[Kickstart Your NLP Journey with These 5 Free Courses](https://www.kdnuggets.com/kickstart-your-nlp-journey-with-these-5-free-courses)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过这 5 门免费课程启动你的 NLP 旅程](https://www.kdnuggets.com/kickstart-your-nlp-journey-with-these-5-free-courses)'
