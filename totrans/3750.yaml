- en: 7 Types of Artificial Neural Networks for Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理的7种人工神经网络类型
- en: 原文：[https://www.kdnuggets.com/2017/10/7-types-artificial-neural-networks-natural-language-processing.html/2](https://www.kdnuggets.com/2017/10/7-types-artificial-neural-networks-natural-language-processing.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/10/7-types-artificial-neural-networks-natural-language-processing.html/2](https://www.kdnuggets.com/2017/10/7-types-artificial-neural-networks-natural-language-processing.html/2)
- en: 3\. Recursive neural network (RNN)
  id: totrans-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3\. 递归神经网络（RNN）
- en: '![](../Images/bce9a470631fe036e9e9f0becdd3aa48.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bce9a470631fe036e9e9f0becdd3aa48.png)'
- en: A simple recursive neural network architecture ([https://upload.wikimedia.org/wikipedia/commons/6/60/Simple_recursive_neural_network.svg](https://upload.wikimedia.org/wikipedia/commons/6/60/Simple_recursive_neural_network.svg))
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的递归神经网络结构 ([https://upload.wikimedia.org/wikipedia/commons/6/60/Simple_recursive_neural_network.svg](https://upload.wikimedia.org/wikipedia/commons/6/60/Simple_recursive_neural_network.svg))
- en: A recursive neural network (RNN) is a type of deep neural network formed by
    applying the same set of weights recursively over a structure to make a structured
    prediction over variable-size input structures, or a scalar prediction on it,
    by traversing a given structure in topological order [[6]](https://en.wikipedia.org/wiki/Recursive_neural_network).
    In the simplest architecture, a nonlinearity such as tanh, and a weight matrix
    that is shared across the whole network are used to combine nodes into parents.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络（RNN）是一种深度神经网络，通过在一个结构上递归应用相同的权重集合，进行结构化预测或者对可变大小输入结构进行标量预测，通过以拓扑顺序遍历给定结构[[6]](https://en.wikipedia.org/wiki/Recursive_neural_network)。在最简单的架构中，使用非线性函数如tanh，以及在整个网络中共享的权重矩阵来将节点组合成父节点。
- en: 4\. Recurrent neural network (RNN)
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4\. 循环神经网络（RNN）
- en: A recurrent neural network (RNN), unlike a [feedforward neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network),
    is a variant of a recursive artificial neural network in which connections between
    neurons make a directed cycle. It means that output depends not only on the present
    inputs but also on the previous step’s neuron state. This memory lets users solve
    NLP problems like connected handwriting recognition or speech recognition. In
    a paper, [Natural Language Generation, Paraphrasing and Summarization of User
    Reviews with Recurrent Neural Networks](http://www.meanotek.ru/files/TarasovDS%282%292015-Dialogue.pdf),
    authors demonstrate a recurrent neural network (RNN) model that can generate novel
    sentences and document summaries [[7]](http://www.meanotek.ru/files/TarasovDS%282%292015-Dialogue.pdf).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）不同于[前馈神经网络](https://en.wikipedia.org/wiki/Feedforward_neural_network)，它是一种递归人工神经网络的变体，其中神经元之间的连接形成一个有向循环。这意味着输出不仅依赖于当前的输入，还依赖于前一步的神经元状态。这种记忆使得用户能够解决自然语言处理（NLP）问题，例如连写识别或语音识别。在一篇论文中，[使用循环神经网络进行自然语言生成、改写和用户评论总结](http://www.meanotek.ru/files/TarasovDS%282%292015-Dialogue.pdf)，作者展示了一种循环神经网络（RNN）模型，该模型可以生成新颖的句子和文档摘要[[7]](http://www.meanotek.ru/files/TarasovDS%282%292015-Dialogue.pdf)。
- en: Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao created a recurrent convolutional
    neural network for text classification without human-designed features and described
    it in [Recurrent Convolutional Neural Networks for Text Classification](https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745/9552).
    Their model was compared to existing text classification methods like Bag of Words,
    Bigrams + LR, SVM, LDA, Tree Kernels, Recursive neural network, and CNN. It was
    shown that their model outperforms traditional methods for all used data sets [[8]](https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745/9552).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Siwei Lai、Liheng Xu、Kang Liu 和 Jun Zhao 创建了一种用于文本分类的递归卷积神经网络，无需人工设计的特征，并在[递归卷积神经网络用于文本分类](https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745/9552)中描述了该模型。他们的模型与现有的文本分类方法如词袋模型、二元组+
    LR、SVM、LDA、树核、递归神经网络和CNN进行了比较。结果表明，他们的模型在所有使用的数据集上优于传统方法[[8]](https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745/9552)。
- en: 5\. Long short-term memory (LSTM)
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5\. 长短期记忆（LSTM）
- en: '![](../Images/fbc3b4f78b901a963f9dcd7f21f5233b.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbc3b4f78b901a963f9dcd7f21f5233b.png)'
- en: A peephole LSTM block with input, output, and forget gates. ([https://upload.wikimedia.org/wikipedia/commons/5/53/Peephole_Long_Short-Term_Memory.svg](https://upload.wikimedia.org/wikipedia/commons/5/53/Peephole_Long_Short-Term_Memory.svg))
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 带有输入、输出和遗忘门的窥视LSTM块。([https://upload.wikimedia.org/wikipedia/commons/5/53/Peephole_Long_Short-Term_Memory.svg](https://upload.wikimedia.org/wikipedia/commons/5/53/Peephole_Long_Short-Term_Memory.svg))
- en: Long Short-Term Memory (LSTM) is a specific recurrent neural network (RNN) architecture
    that was designed to model temporal sequences and their long-range dependencies
    more accurately than conventional RNNs [[9]](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenTerm1201415/sak2.pdf).
    LSTM does not use activation function within its recurrent components, the stored
    values are not modified, and the gradient does not tend to vanish during training.
    Usually, LSTM units are implemented in “blocks” with several units. These blocks
    have three or four “gates” (for example, input gate, forget gate, output gate)
    that control information flow drawing on the logistic function.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）是一种特定的递归神经网络（RNN）架构，旨在比传统的 RNN 更准确地建模时间序列及其长距离依赖关系[[9]](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenTerm1201415/sak2.pdf)。LSTM
    在其递归组件中不使用激活函数，存储的值不会被修改，并且梯度在训练过程中不会消失。通常，LSTM 单元以“块”形式实现，每个块包含多个单元。这些块有三个或四个“门”（例如，输入门、遗忘门、输出门），利用逻辑函数来控制信息流。
- en: In [Long Short-Term Memory Recurrent Neural Network Architectures for Large
    Scale Acoustic Modeling](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenTerm1201415/sak2.pdf),
    Hasim Sak, Andrew Senior, and Françoise Beaufays showed that the deep LSTM RNN
    architectures achieve state-of-the-art performance for large scale acoustic modeling.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在[用于大规模声学建模的长短期记忆递归神经网络架构](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenTerm1201415/sak2.pdf)中，Hasim
    Sak、Andrew Senior 和 Françoise Beaufays 展示了深层 LSTM RNN 架构在大规模声学建模中的先进性能。
- en: In the work, [Part-of-Speech Tagging with Bidirectional Long Short-Term Memory
    Recurrent Neural Network](https://arxiv.org/pdf/1510.06168.pdf) by Peilu Wang,
    Yao Qian, Frank K. Soong, Lei He, and Hai Zhao, a model for [part-of-speech (POS)
    tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging) was presented [[10]](https://arxiv.org/pdf/1510.06168.pdf).
    The model achieved a performance of 97.40% tagging accuracy. Apple, Amazon, Google,
    Microsoft and other companies incorporated LSTM as a fundamental element into
    their products.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Peilu Wang、Yao Qian、Frank K. Soong、Lei He 和 Hai Zhao 的[双向长短期记忆递归神经网络的词性标注](https://arxiv.org/pdf/1510.06168.pdf)工作中，提出了一种用于[词性标注（POS）](https://en.wikipedia.org/wiki/Part-of-speech_tagging)的模型[[10]](https://arxiv.org/pdf/1510.06168.pdf)。该模型实现了
    97.40% 的标注准确率。苹果、亚马逊、谷歌、微软等公司将 LSTM 作为其产品的基础元素。
- en: 6\. Sequence-to-sequence models
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6\. 序列到序列模型
- en: 'Usually, a sequence-to-sequence model consists of two recurrent neural networks:
    an [encoder](https://en.wikipedia.org/wiki/Artificial_neural_network#Encoder.E2.80.93decoder_networks) that
    processes the input and a [decoder](https://en.wikipedia.org/wiki/Artificial_neural_network#Encoder.E2.80.93decoder_networks) that
    produces the output. Encoder and decoder can use the same or different sets of
    parameters.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，序列到序列模型由两个递归神经网络组成：一个处理输入的[编码器](https://en.wikipedia.org/wiki/Artificial_neural_network#Encoder.E2.80.93decoder_networks)和一个生成输出的[解码器](https://en.wikipedia.org/wiki/Artificial_neural_network#Encoder.E2.80.93decoder_networks)。编码器和解码器可以使用相同或不同的参数集。
- en: Sequence-to-Sequence models are mainly used in question answering systems, chatbots,
    and machine translation. Such multi-layer cells have been successfully used in
    sequence-to-sequence models for translation in [Sequence to Sequence Learning
    with Neural Networks study](https://arxiv.org/pdf/1409.3215.pdf) [[11]](https://arxiv.org/pdf/1409.3215.pdf).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Sequence-to-Sequence 模型主要用于问答系统、聊天机器人和机器翻译。这种多层单元在[使用神经网络的序列到序列学习研究](https://arxiv.org/pdf/1409.3215.pdf)
    [[11]](https://arxiv.org/pdf/1409.3215.pdf)中的序列到序列模型翻译中得到了成功应用。
- en: In [Paraphrase Detection Using Recursive Autoencoder](https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf),
    a novel recursive autoencoder architecture is presented. The representations are
    vectors in an n-dimensional semantic space where phrases with similar meanings
    are close to each other [[12]](https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在[使用递归自编码器进行释义检测](https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf)中，提出了一种新颖的递归自编码器架构。这些表示是
    n 维语义空间中的向量，具有相似意义的短语彼此接近[[12]](https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf)。
- en: 7\. Shallow neural networks
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7\. 浅层神经网络
- en: Besides deep neural networks, shallow models are also popular and useful tools.
    For example, [word2vec](https://ru.wikipedia.org/wiki/Word2vec) is a group of
    shallow two-layer models that are used for producing [word embeddings](https://en.wikipedia.org/wiki/Word_embedding).
    Presented in [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf),
    word2vec takes a large corpus of text as its input and produces a vector space [[13]](https://arxiv.org/pdf/1301.3781.pdf).
    Every word in the corpus obtains the corresponding vector in this space. The distinctive
    feature is that words from common contexts in the corpus are located close to
    one another in the vector space.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 除了深度神经网络，浅层模型也是流行且有用的工具。例如，[word2vec](https://ru.wikipedia.org/wiki/Word2vec)是一组用于生成[word
    embeddings](https://en.wikipedia.org/wiki/Word_embedding)的浅层二层模型。在[Efficient Estimation
    of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)中提出，word2vec以大量文本语料作为输入，生成一个向量空间[[13]](https://arxiv.org/pdf/1301.3781.pdf)。语料中的每个词汇在这个空间中都会获得对应的向量。其显著特点是语料中来自常见上下文的词汇在向量空间中彼此靠近。
- en: Summary
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: In this paper, we described different variants of artificial neural networks,
    such as deep multilayer perceptron (MLP), convolutional neural network (CNN),
    recursive neural network (RNN), recurrent neural network (RNN), long short-term
    memory (LSTM), sequence-to-sequence model, and shallow neural networks including
    word2vec for word embeddings. We showed how these networks function and how different
    types of them are used in natural language processing tasks. We demonstrated that
    convolutional neural networks are primarily utilized for text classification tasks
    while recurrent neural networks are commonly used for natural language generation
    or machine translation. In the next part of this series, we will study existing
    tools and libraries for the discussed neural network types.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，我们描述了不同变体的人工神经网络，如深度多层感知器（MLP）、卷积神经网络（CNN）、递归神经网络（RNN）、循环神经网络（RNN）、长短期记忆（LSTM）、序列到序列模型，以及包括word2vec在内的浅层神经网络用于词嵌入。我们展示了这些网络的功能以及它们在自然语言处理任务中的不同应用。我们演示了卷积神经网络主要用于文本分类任务，而递归神经网络则常用于自然语言生成或机器翻译。在本系列的下一部分中，我们将研究讨论过的神经网络类型的现有工具和库。
- en: '**Resources**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源**'
- en: '[http://www.aclweb.org/anthology/D14-1181](http://www.aclweb.org/anthology/D14-1181)'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://www.aclweb.org/anthology/D14-1181](http://www.aclweb.org/anthology/D14-1181)'
- en: '[https://arxiv.org/pdf/1502.01710.pdf](https://arxiv.org/pdf/1502.01710.pdf)'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1502.01710.pdf](https://arxiv.org/pdf/1502.01710.pdf)'
- en: '[http://www.aclweb.org/anthology/P15-1128](http://www.aclweb.org/anthology/P15-1128)'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://www.aclweb.org/anthology/P15-1128](http://www.aclweb.org/anthology/P15-1128)'
- en: '[https://www.aclweb.org/anthology/K15-1013](https://www.aclweb.org/anthology/K15-1013)'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.aclweb.org/anthology/K15-1013](https://www.aclweb.org/anthology/K15-1013)'
- en: '[https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/CNN_ASLPTrans2-14.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/CNN_ASLPTrans2-14.pdf)'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/CNN_ASLPTrans2-14.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/CNN_ASLPTrans2-14.pdf)'
- en: '[https://en.wikipedia.org/wiki/Recursive_neural_network](https://en.wikipedia.org/wiki/Recursive_neural_network)'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Recursive_neural_network](https://en.wikipedia.org/wiki/Recursive_neural_network)'
- en: '[http://www.meanotek.ru/files/TarasovDS(2)2015-Dialogue.pdf](http://www.meanotek.ru/files/TarasovDS%282%292015-Dialogue.pdf)'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://www.meanotek.ru/files/TarasovDS(2)2015-Dialogue.pdf](http://www.meanotek.ru/files/TarasovDS%282%292015-Dialogue.pdf)'
- en: '[https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745/9552](https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745/9552)'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745/9552](https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745/9552)'
- en: '[https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenTerm1201415/sak2.pdf](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenTerm1201415/sak2.pdf)'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenTerm1201415/sak2.pdf](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenTerm1201415/sak2.pdf)'
- en: '[https://arxiv.org/pdf/1510.06168.pdf](https://arxiv.org/pdf/1510.06168.pdf)'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1510.06168.pdf](https://arxiv.org/pdf/1510.06168.pdf)'
- en: '[https://arxiv.org/pdf/1409.3215.pdf](https://arxiv.org/pdf/1409.3215.pdf)'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1409.3215.pdf](https://arxiv.org/pdf/1409.3215.pdf)'
- en: '[https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf](https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf)'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf](https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf)'
- en: '[https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)'
- en: '**[Data Monsters](https://datamonsters.co/)** helps corporations and funded
    startups research, design, and develop real-time intelligent software to improve
    their business with data technologies.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**[数据怪物](https://datamonsters.co/)** 帮助公司和获得资助的初创企业研究、设计和开发实时智能软件，以利用数据技术提升业务。'
- en: '[Original](https://medium.com/@datamonsters/artificial-neural-networks-for-natural-language-processing-part-1-64ca9ebfa3b2).
    Reposted with permission.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/@datamonsters/artificial-neural-networks-for-natural-language-processing-part-1-64ca9ebfa3b2)。已获许可转载。'
- en: '**Related:**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Machine Learning Translation and the Google Translate Algorithm](/2017/09/machine-learning-translation-google-translate-algorithm.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习翻译与谷歌翻译算法](/2017/09/machine-learning-translation-google-translate-algorithm.html)'
- en: '[5 Free Resources for Getting Started with Deep Learning for Natural Language
    Processing](/2017/07/5-free-resources-getting-started-deep-learning-nlp.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5 个免费资源，帮助你入门深度学习自然语言处理](/2017/07/5-free-resources-getting-started-deep-learning-nlp.html)'
- en: '[How I started with learning AI in the last 2 months](/2017/10/how-started-learning-ai.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我如何在过去 2 个月开始学习 AI](/2017/10/how-started-learning-ai.html)'
- en: '* * *'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的快车道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行 IT 支持'
- en: '* * *'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么让 Python 成为初创企业理想的编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学来寻找目标，找寻目标再……](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一个 90 亿美元的 AI 失败案例分析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功数据科学家的 5 个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每位数据科学家都应该了解的三个 R 语言库（即使你使用 Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
