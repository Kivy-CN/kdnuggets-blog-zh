- en: 'TensorFlow 2.0 Tutorial: Optimizing Training Time Performance'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 2.0教程：优化训练时间性能
- en: 原文：[https://www.kdnuggets.com/2020/03/tensorflow-optimizing-training-time-performance.html](https://www.kdnuggets.com/2020/03/tensorflow-optimizing-training-time-performance.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/03/tensorflow-optimizing-training-time-performance.html](https://www.kdnuggets.com/2020/03/tensorflow-optimizing-training-time-performance.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Raphael Meudec](https://www.linkedin.com/in/raphaelmeudec), Data Scientist
    @ Sicara**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[Raphael Meudec](https://www.linkedin.com/in/raphaelmeudec)，数据科学家 @ Sicara**'
- en: 'This tutorial explores how you can improve training time performance of your
    TensorFlow 2.0 model around:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程探讨了如何提升你的TensorFlow 2.0模型的训练时间性能：
- en: tf.data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tf.data
- en: Mixed Precision Training
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合精度训练
- en: Multi-GPU Training Strategy
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多GPU训练策略
- en: '* * *'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 为你的组织提供IT支持'
- en: '* * *'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: I adapted all these tricks to a custom project on image deblurring, and the
    result is astonishing. You can get a 2–10x training time speed-up depending on
    your current pipeline.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我将所有这些技巧应用到一个自定义的图像去模糊项目中，结果令人惊讶。根据你当前的流程，你可以获得2到10倍的训练时间加速。
- en: 'Usecase: Improving TensorFlow training time of an image deblurring CNN'
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用案例：提高图像去模糊CNN的TensorFlow训练时间
- en: 2 years ago, I published a blog post on [Image Deblurring with GANs in Keras](https://www.sicara.ai/blog/2018-03-20-GAN-with-Keras-application-to-image-deblurring).
    I thought it would be a nice transition to pass the repository in TF2.0 to understand
    what has changed and what are the implications on my code. In this article, I’ll
    train a simpler version of the model (the cnn part only).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 2年前，我在[使用GANs在Keras中去模糊图像](https://www.sicara.ai/blog/2018-03-20-GAN-with-Keras-application-to-image-deblurring)上发表了一篇博客文章。我觉得将TF2.0的代码库传递过来理解变化和对代码的影响是一个不错的过渡。在这篇文章中，我将训练一个更简单版本的模型（仅CNN部分）。
- en: '![Figure](../Images/50a21fb77e4806d6f4fb2d744b36ff6e.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/50a21fb77e4806d6f4fb2d744b36ff6e.png)'
- en: The model is a convolutional net which takes the (256, 256, 3) blurred patch
    and predicts the (256, 256, 3) corresponding sharp patch. It is based on the ResNet
    architecture and is fully convolutional.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是一个卷积网络，它接受(256, 256, 3)的模糊补丁，并预测(256, 256, 3)的对应清晰补丁。它基于ResNet架构，并且是完全卷积的。
- en: 'Step 1: Identify bottlenecks'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤1：识别瓶颈
- en: To optimize training speed, you want your GPUs to be running at 100% speed.
    `nvidia-smi` is nice to make sure your process is running on the GPU, but when
    it comes to GPU monitoring, there are smarter tools out there. Hence, the first
    step of this TensorFlow tutorial is to explore these better options.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化训练速度，你希望你的GPU以100%的速度运行。`nvidia-smi`很适合确保你的进程在GPU上运行，但当涉及GPU监控时，还有更智能的工具。因此，本TensorFlow教程的第一步是探索这些更好的选项。
- en: '**nvtop**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**nvtop**'
- en: If you’re using an Nvidia card, the simplest solution to monitor GPU utilization
    over time might probably be `nvtop`. Visualization is friendlier than `nvidia-smi`,
    and you can track metrics over time.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Nvidia显卡，监控GPU利用率的最简单解决方案可能就是`nvtop`。可视化比`nvidia-smi`更友好，你可以随时间跟踪指标。
- en: '![Figure](../Images/ac443b80856e4640f006573093ec785c.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/ac443b80856e4640f006573093ec785c.png)'
- en: '**TensorBoard Profiler**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorBoard Profiler**'
- en: '![Figure](../Images/1ffa437da5694cca705a5445555771e8.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/1ffa437da5694cca705a5445555771e8.png)'
- en: By simply setting `profile_batch={BATCH_INDEX_TO_MONITOR}` inside the TensorBoard
    callback, TF adds a full report on operations performed by either the CPU or GPU
    for the given batch. This can help identify if your GPU is stalled at some point
    for lack of data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 只需在TensorBoard回调中设置`profile_batch={BATCH_INDEX_TO_MONITOR}`，TF会添加一个关于CPU或GPU在给定批次上执行的操作的完整报告。这有助于识别你的GPU是否因为数据不足而在某些点停滞。
- en: '**[RAPIDS NVDashboard](https://github.com/rapidsai/jupyterlab-nvdashboard)**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**[RAPIDS NVDashboard](https://github.com/rapidsai/jupyterlab-nvdashboard)**'
- en: This is a Jupyterlab extension which gives access to various metrics. Along
    with your GPU, you can also monitor elements from your motherboard (CPU, Disks, ..).
    The advantage is you don’t have to monitor a specific batch, but rather have a
    look on performance over the whole training.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 Jupyterlab 扩展，可以访问各种指标。除了 GPU，你还可以监控来自主板的元素（CPU、磁盘等）。优势在于你不必监控特定的批次，而是查看整个训练过程的性能。
- en: '![Figure](../Images/c6697d35de27b724c15fdea6a8351c99.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/c6697d35de27b724c15fdea6a8351c99.png)'
- en: Here, we can easily spot that GPU is at 40% speed most of the time. I have activated
    only 1 of the 2 GPUs on the computer, so total utilization is around 20%.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以很容易地发现 GPU 大部分时间运行在 40% 的速度。我只激活了计算机上的 1 个 GPU，因此总利用率约为 20%。
- en: 'Step 2: Optimize your tf.data pipeline'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤 2：优化你的 tf.data 管道
- en: The first objective is to make the GPU busy 100% of the time. To do so, we want
    to reduce the data loading bottleneck. If you are using a Python generator or
    a Keras Sequence, your data loading is probably sub-optimal. Even if you’re using
    tf.data, data loading can still be an issue. In my article, I initially used Keras
    Sequences to load the images.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首要目标是让 GPU 始终 100% 繁忙。为此，我们希望减少数据加载瓶颈。如果你使用的是 Python 生成器或 Keras Sequence，你的数据加载可能并不理想。即使使用
    tf.data，数据加载仍然可能是一个问题。在我的文章中，我最初使用 Keras Sequences 加载图像。
- en: '![Figure](../Images/78d5d1936c5c60c8dd2c3ff576965df8.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/78d5d1936c5c60c8dd2c3ff576965df8.png)'
- en: You can easily spot this phenomenom using the TensorBoard profiling. GPUs will
    tend to have free time while CPUs are performing multiple operations related to
    data loading.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 TensorBoard 的性能分析功能轻松发现这一现象。GPU 在 CPU 执行多个与数据加载相关的操作时通常会有空闲时间。
- en: Making the switch from the original Keras sequences to tf.data was fairly easy.
    Most operations for data loading are pretty well supported, the only tricky part
    is to take the same patch on the blurred image and the real one.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始 Keras 序列切换到 tf.data 相对容易。数据加载的大部分操作都得到了很好的支持，唯一棘手的部分是要在模糊图像和真实图像上获取相同的补丁。
- en: 'Just switching from a Keras Sequence to tf.data can lead to a training time
    improvement. From there, we add some little tricks that you can also find in [TensorFlow''s
    documentation](https://www.tensorflow.org/guide/data_performance#optimize_performance):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Keras Sequence 切换到 tf.data 可以提高训练时间。从这里，我们添加了一些小技巧，你也可以在 [TensorFlow 文档](https://www.tensorflow.org/guide/data_performance#optimize_performance)
    中找到这些技巧：
- en: 'parallelization: Make all the `.map()` calls parallelized by adding the `num_parallel_calls=tf.data.experimental.AUTOTUNE`
    argument'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行化：通过添加 `num_parallel_calls=tf.data.experimental.AUTOTUNE` 参数，使所有 `.map()`
    调用并行化。
- en: 'cache: Keep loaded images in memory by caching datasets before the patch selection'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓存：通过在补丁选择之前缓存数据集，将加载的图像保留在内存中。
- en: 'prefetching: Start fetching elements before previous batch has ended'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预取：在前一个批次结束之前开始获取元素。
- en: 'The dataset creation now looks like this:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集创建现在看起来是这样的：
- en: Those small changes make a 5 epochs training time fall from 1000 sec (on an
    RTX2080) to 616s (full graph is below) .
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些小变化使得 5 个周期的训练时间从 1000 秒（在 RTX2080 上）下降到 616 秒（完整图表见下）。
- en: 'Step 3: Mixed Precision Training'
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤 3：混合精度训练
- en: 'By default, all variables used in our neural network training are stored on
    float32\. This means every element has to be encoded on 32 bits. The core concept
    of [Mixed Precision Training](https://arxiv.org/abs/1710.03740) is to say: we
    don''t need so much precision at all time, let''s use 16 bits sometimes.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，我们神经网络训练中使用的所有变量都存储在 float32 中。这意味着每个元素都必须编码为 32 位。[混合精度训练](https://arxiv.org/abs/1710.03740)
    的核心概念是：我们不需要始终保持如此高的精度，可以在某些情况下使用 16 位。
- en: During the Mixed Precision Training process, you keep a float32 version of the
    weights, but perform forward and backward passes on float16 versions of the weights.
    All the expensive operations to obtain the gradients are performed using float16
    elements. In the end, you use the float16 gradients to update the float32 weights.
    A loss scaling is used in the process to keep training stability.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在混合精度训练过程中，你保留了权重的 float32 版本，但在 float16 版本的权重上执行前向和后向传递。所有获取梯度的昂贵操作都是使用 float16
    元素执行的。最后，你使用 float16 梯度来更新 float32 权重。训练过程中使用了损失缩放以保持训练稳定性。
- en: '![Figure](../Images/6092e3598357ea1f80183095fb010725.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/6092e3598357ea1f80183095fb010725.png)'
- en: By keeping float32 weights, this process does not lower the accuracy of your
    models. On the contrary, they claim some performance improvements on various tasks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过保持 float32 权重，这个过程不会降低你模型的准确性。相反，他们声称在各种任务上有一些性能改进。
- en: TensorFlow makes it easy to implement from version 2.1.0, by adding different
    `Policy.` Mixed Precision Training can be activated by using these two lines before
    model instantiation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从版本 2.1.0 开始，TensorFlow 使得实现变得简单，通过添加不同的 `Policy.` 混合精度训练可以通过在模型实例化之前使用这两行代码来激活。
- en: With this method, we can reduce the 5 epochs training time to 480 sec.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，我们可以将 5 个 epoch 的训练时间缩短到 480 秒。
- en: 'Step 4: Multi-GPU Strategies'
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第四步：多 GPU 策略
- en: Last topic concerns how to perform multi-GPU training with TF2.0\. If you don't
    adjust your code for multi-GPU, you won't reduce your TensorFlow training time
    because they won't be efficiently used.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一部分讨论如何使用 TF2.0 进行多 GPU 训练。如果你不调整你的代码以适应多 GPU，你不会减少 TensorFlow 的训练时间，因为它们不会被有效利用。
- en: The easiest way to perform multi-GPU training is to use the `MirroredStrategy`.
    It instantiates your model on each GPU. At each step, different batches are sent
    to the GPUs which run the backward pass. Then, gradients are aggregated to perform
    weights update, and the updated values are propagated to each model instantiated.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 执行多 GPU 训练的最简单方法是使用 `MirroredStrategy`。它会在每个 GPU 上实例化你的模型。在每一步，不同的批量数据被发送到 GPUs，这些
    GPU 执行反向传播。然后，梯度被聚合以进行权重更新，更新后的值被传播到每个实例化的模型。
- en: The distribution strategy is again fairly easy with TensorFlow 2.0\. You should
    only think of multiplying the usual batch size by the number of available GPUs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 分布策略在 TensorFlow 2.0 中仍然相当简单。你只需要考虑将通常的批量大小乘以可用 GPU 的数量。
- en: If you use TPUs, you might consider taking a deeper look at the official [Tensorflow
    tutorial from documentation](https://www.tensorflow.org/api_docs/python/tf/distribute)
    on training distribution.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 TPU，可能需要更深入地查看官方的 [Tensorflow 文档教程](https://www.tensorflow.org/api_docs/python/tf/distribute)
    关于训练分布。
- en: Wrap-up on tips to improve your TensorFlow training time
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于提升 TensorFlow 训练时间的技巧总结
- en: All those steps lead to a massive reduction of your model training time. This
    graph traces the 5 epochs training time after each improvement of the training
    pipeline. I hope you enjoy this TensorFlow tutorial on training time performance.
    You can ping me on Twitter (@raphaelmeudec) if you have any feedback!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些步骤都导致了你模型训练时间的巨大减少。此图跟踪了每次训练管道改进后的 5 个 epoch 训练时间。希望你喜欢这个关于 TensorFlow 训练时间性能的教程。如果你有任何反馈，可以在
    Twitter 上@我（@raphaelmeudec）！
- en: '![Figure](../Images/2c96d07baa79ec897336a36174bb3e3e.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/2c96d07baa79ec897336a36174bb3e3e.png)'
- en: Impacts of tf.data, MPT and GPU Strategy on training time
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: tf.data、MPT 和 GPU 策略对训练时间的影响
- en: '**Bio: [Raphael Meudec](https://www.linkedin.com/in/raphaelmeudec)** ([**@raphaelmeudec**](https://twitter.com/raphaelmeudec))
    is a Data Scientist at Sicara.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介: [Raphael Meudec](https://www.linkedin.com/in/raphaelmeudec)** （[**@raphaelmeudec**](https://twitter.com/raphaelmeudec)）是
    Sicara 的数据科学家。'
- en: '[Original](https://www.sicara.ai/blog/tensorflow-tutorial-training-time). Reposted
    with permission.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.sicara.ai/blog/tensorflow-tutorial-training-time)。已获许可转载。'
- en: '**Related:**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Hands on Hyperparameter Tuning with Keras Tuner](/2020/02/hyperparameter-tuning-keras-tuner.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Keras Tuner 进行超参数调整](/2020/02/hyperparameter-tuning-keras-tuner.html)'
- en: '[Easy Image Dataset Augmentation with TensorFlow](/2020/02/easy-image-dataset-augmentation-tensorflow.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 TensorFlow 简单图像数据集增强](/2020/02/easy-image-dataset-augmentation-tensorflow.html)'
- en: '[Transfer Learning Made Easy: Coding a Powerful Technique](/2019/11/transfer-learning-coding.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[转移学习简化：编码强大的技术](/2019/11/transfer-learning-coding.html)'
- en: More On This Topic
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Optimizing Python Code Performance: A Deep Dive into Python Profilers](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化 Python 代码性能：深入探讨 Python 性能分析器](https://www.kdnuggets.com/2023/02/optimizing-python-code-performance-deep-dive-python-profilers.html)'
- en: '[Optimizing Your LLM for Performance and Scalability](https://www.kdnuggets.com/optimizing-your-llm-for-performance-and-scalability)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化你的 LLM 性能和可扩展性](https://www.kdnuggets.com/optimizing-your-llm-for-performance-and-scalability)'
- en: '[Strategies for Optimizing Performance and Costs When Using Large…](https://www.kdnuggets.com/strategies-for-optimizing-performance-and-costs-when-using-large-language-models-in-the-cloud)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用大型语言模型时优化性能和成本的策略](https://www.kdnuggets.com/strategies-for-optimizing-performance-and-costs-when-using-large-language-models-in-the-cloud)'
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 TensorFlow 和 Keras 构建和训练你的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
- en: '[Optimizing Genes with a Genetic Algorithm](https://www.kdnuggets.com/2022/04/optimizing-genes-genetic-algorithm.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用遗传算法优化基因](https://www.kdnuggets.com/2022/04/optimizing-genes-genetic-algorithm.html)'
- en: '[Optimizing Data Storage: Exploring Data Types and Normalization in SQL](https://www.kdnuggets.com/optimizing-data-storage-exploring-data-types-and-normalization-in-sql)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化数据存储：探索 SQL 中的数据类型和规范化](https://www.kdnuggets.com/optimizing-data-storage-exploring-data-types-and-normalization-in-sql)'
