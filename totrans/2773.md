# 优化生产环境中机器学习API的响应时间

> 原文：[https://www.kdnuggets.com/2020/05/optimize-response-time-machine-learning-api-production.html](https://www.kdnuggets.com/2020/05/optimize-response-time-machine-learning-api-production.html)

[评论](#comments)

**由[Yannick Wolf](https://www.linkedin.com/in/yannick-wolff-457616124/)，Sicara的首席数据科学家**

这篇文章演示了如何通过构建一个更智能的API来服务深度学习模型，从而最小化响应时间。

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织IT

* * *

<picture>![](../Images/e8acc09138d252f613079422b466a468.png)</picture>

你的团队为给定任务（比如：通过计算机视觉检测商店内购买的产品）努力构建了一个深度学习模型。很好。

然后你开发并部署了一个集成此模型的API（我们继续以自助结账机为例，这些机器会调用这个API）。太棒了！

新产品运行良好，你感觉所有工作都完成了。

但由于经理决定安装更多自助结账机（我真的很喜欢这个例子），用户们开始抱怨每次扫描产品时出现的巨大延迟。

你能做什么？购买10倍更快且10倍更贵的GPU？让数据科学家尝试在不降低模型准确性的情况下减少模型的深度？

存在更便宜、更简单的解决方案，正如你将在这篇文章中看到的。

### 一个基本的API与一个大虚拟模型

首先，我们需要一个推理时间较长的模型来进行工作。下面是我如何使用[TensorFlow 2](https://www.tensorflow.org/)'s Keras API来实现这个目标的方式（如果你对这个深度学习框架不熟悉，可以跳过这段代码）：

当在我的[GeForce RTX 2080](https://www.nvidia.com/fr-fr/geforce/graphics-cards/rtx-2080/) GPU上测试模型时，我测得推理时间为303毫秒。这可以称为一个大模型。

现在，我们需要一个非常简单的API来提供我们的模型，只需要一个路由来请求预测。Python中一个非常标准的API框架是[Flask](https://www.palletsprojects.com/p/flask/)。这是我选择的框架，还有一个叫做[Gunicorn](https://gunicorn.org/)的[WSGI HTTP服务器](https://www.fullstackpython.com/wsgi-servers.html)。

我们独特的路由解析来自请求的输入，调用实例化的模型，并将输出发送回用户。

我们可以使用以下命令运行我们的深度学习API：

```py
gunicorn wsgi:app
```

好的，我现在可以向我的 API 发送一些随机数字，它会用其他随机数字回应我。问题是：多快？

### 让我们对我们的 API 进行负载测试

我们确实想知道我们的 API 响应速度，尤其是在每秒请求数量增加时。这样的负载测试可以使用 [Locust](https://locust.io/) Python 库进行。

这个库与一个 locustfile.py 文件一起工作，该文件指示要模拟的行为：

Locust 模拟多个用户，同时执行不同的任务。在我们的案例中，我们只定义一个任务，即用随机输入调用 /predict 路径。

我们还可以参数化 wait_time，即模拟用户在收到 API 响应后等待的时间，然后再发送下一个请求。为了模拟自助结账用例，我将这个时间设置为 1 到 3 秒之间的随机值。

用户数量通过 Locust 的仪表板选择，其中所有统计数据实时显示。可以通过运行以下命令来启动此仪表板：

```py
locust --host=[http://localhost:8000](http://localhost:8000/) # host of the API you want to load test
```

Locust 仪表板

那么，当我们增加用户数量时，我们的原始 API 如何反应？正如我们预期的那样，非常糟糕：

+   在 1 个用户的情况下，我测量的平均响应时间为 332 毫秒（略高于之前测量的隔离推理时间，没什么意外）

+   在 5 个用户的情况下，这个平均时间稍微增加了一些：431 毫秒

+   在 20 个用户的情况下，响应时间达到了 4.1 秒（比 1 个用户多了 12 倍以上）

<picture>![gunicorn-flask-tensorflow-api-response-time](../Images/e9ae12e8925c2b179622755f9d2e5010.png)</picture>

当我们考虑到 API 如何处理请求时，这些结果并不那么令人惊讶。默认情况下，Gunicorn 启动 2 个进程：一个主进程监听传入请求，将其存储到 [FIFO](https://en.wikipedia.org/wiki/FIFO_and_LIFO_accounting#FIFO) 队列中，并将其依次发送到每个工作进程，每次它可用。后者负责运行你的代码，以计算每个请求的响应。

由于工作进程是单线程的，请求逐个处理。如果同时到达 5 个请求，第 5 个请求将在 5 x 300 毫秒 = 1500 毫秒后收到响应，这解释了 20 个用户情况下的高平均响应时间。

### 越多越快

幸运的是，Gunicorn 提供了通过增加以下方式来扩展 API 的两种方法：

+   处理请求的线程数量

+   工作进程的数量

多线程选项可能不是最有帮助的，因为 TensorFlow 不允许线程访问在另一个线程中初始化的会话图。

另一方面，设置多个工作进程会创建多个进程，每个工作进程都初始化整个 Flask 应用程序。每个工作进程都实例化其自己的 TensorFlow 会话和模型。

<picture>![multi-workers-gunicorn-flask-tensorflow-api-schema](../Images/1c4723ca5844a91abd1575e67f684d17.png)</picture>

我们可以通过运行以下命令来尝试这种方法：

```py
gunicorn wsgi:app --workers=5
```

这是我们在多工作进程API下获得的新性能：

<picture>![多工作进程-gunicorn-flask-tensorflow-api-响应时间](../Images/69dfec94e4ccee61dcc3858dfb78e933.png)</picture>

你可以看到，使用5个工作进程而不是1个工作进程对响应时间的影响是显著的：20个用户的4.1秒平均时间几乎减半了。是减半而不是减为5分之一，因为请求之间的等待时间——如果模拟用户在收到响应后立即再次请求API，它将被减少到5分之一。

你可能会想为什么我停在了5个工作进程：为什么不设置20个工作进程，以便能够处理所有20个用户同时请求API的情况？原因是每个工作进程都是一个独立的进程，在GPU内存中实例化自己的模型版本。

20个工作进程将消耗20倍的模型大小仅用于初始化权重，推理计算还需要更多内存。对于一个2GiB的模型和一个8GiB的GPU，我们有些受限。

![图示](../Images/781550b5a91b86e5e76a34b60c897835.png)

每个工作进程消耗2349MiB的GPU内存

可以通过调整每个工作进程的内存分配来最大化运行的工作进程数量，使用TensorFlow参数（TensorFlow 1.x中的`per_process_gpu_memory_fraction`，TensorFlow 2.x中的`memory_limit`——我只测试了第一个），将其设置为不会因内存不足错误而导致预测失败的最小值。

根据库的警告，减少可用内存理论上可能会通过禁用优化来增加推理时间。然而，我个人并没有注意到任何显著的变化。

无论如何，这不会让我们在使用2个 GPU的情况下运行20个工作进程。那么，我们要再买2个吗？别慌，你还有更多的办法。

### 欲速则不达

现在让我们回到单工作进程的情况。如果你考虑当两个或更多请求几乎同时到达这个 API 时的情况，会发现有些地方未得到优化：

![图示](../Images/06d468e84c3a8937dd518aa81510fca8.png)

3个请求间隔100毫秒到达时，平均响应时间为1433毫秒

API 按顺序将3个输入通过模型。然而，由于深度学习模型的数学操作性质，它们可以同时处理多个输入而不会增加推理时间——这就是我们所说的批量处理，也是我们在训练阶段通常做的事。

那么，为什么不将这3个输入批量处理呢？这意味着在接收到第一个请求后，API 会稍等一下再运行模型，以便在处理所有请求之前先接收接下来的两个请求。这会增加第一个请求的响应时间，但平均响应时间会减少。

![图示](../Images/c266286ba19aad8c0e6421495b0655e0.png)

如果 API 在批量处理请求之前等待 300 毫秒，我们会得到一个 600 毫秒的平均响应时间。

实际上，我们不知道未来的请求何时会到达，因此决定等待潜在下一个请求的时间有些复杂。触发处理排队请求的一个合理规则是：

+   如果排队请求的数量达到了一个最大值，对应于 GPU 内存限制或最大用户数（如果已知）

或者

+   如果队列中最旧的请求比超时值更久。这一值需要通过经验来微调，以最小化平均响应时间。

这里有一种实现这种行为的方法，仍然使用 Gunicorn - Flask - TensorFlow 堆栈，使用 Python 队列和专门处理批量请求的线程。

这个新的 API 必须运行与我们希望能够批量处理的请求最大数量相等的线程：

```py
gunicorn wsgi:app --threads=20
```

这是我在快速微调超时值后获得的非常令人满意的结果：

<picture>![batching-gunicorn-flask-tensorflow-api-response-time](../Images/5141b3adf790bc580e4c443de1d5c6ef.png)</picture>

正如你所看到的，当仅模拟 1 个用户时，这个新的 API 设置具有 500 毫秒的超时会将响应时间增加 500 毫秒（超时值），显然没有用。但对于 20 个用户，平均响应时间已被缩短了 6 倍。

请注意，为了获得更好的结果，这种批处理技术可以与上述多工作者技术并行使用。

### 我们不是在重新发明轮子吗？

作为开发者，从零开始编写一个可能已经被现有工具实现的功能是一种有点罪恶的乐趣，这也是我们在整篇文章中一直在做的事情。

确实存在一个工具用于在 API 中服务 TensorFlow 模型。它由 TensorFlow 开发，叫做…… [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)。

它不仅允许为模型启动一个预测 API，而不需要编写除模型本身之外的任何其他代码，还提供了你可能会感兴趣的几种优化，比如并行处理和请求批处理！

要用 TF Serving 服务一个模型，首先需要将其导出到磁盘（使用 TensorFlow 格式，而不是 Keras 格式），可以使用以下代码完成：

那么，启动服务 API 的一种简单方法是使用 TF Serving [docker 镜像](https://www.tensorflow.org/tfx/serving/docker)。

以下命令将启动一个预测 API，你可以通过请求此端点来测试：[http://localhost:8501/v1/models/my_model:predict](http://localhost:8501/v1/models/my_model:predict)

```py
docker run -p 8501:8501 \
--mount type=bind,source=/path/to/my_model/,target=/models/my_model \
-e MODEL_NAME=my_model -it tensorflow/serving
```

请注意，它默认不使用 GPU。你可以在 [此文档](https://www.tensorflow.org/tfx/serving/docker#serving_with_docker_using_your_gpu) 中找到如何使其工作的信息。

那批处理呢？你需要编写这样的批处理配置文件：

```py
max_batch_size { value: 20 }
batch_timeout_micros { value: 500000 }
max_enqueued_batches { value: 100 }
num_batch_threads { value: 8 }
```

它包括，特别是我们自定义实现中使用的2个参数：max_batch_size和batch_timeout_micros参数。后者是我们所说的超时，必须以微秒为单位。

最终，启用批处理的TF Serving命令是：

```py
docker run -p 8501:8501 \
--mount type=bind,source=/path/to/my_model/,target=/models/my_model \
--mount type=bind,source=/path/to/batching.cfg,target=/batching.cfg \
-e MODEL_NAME=my_model -it tensorflow/serving \
--enable_batching --batching_parameters_file=/batching.cfg
```

我进行了测试，感谢一些黑魔法，我得到的结果甚至比我的自定义批处理API更好：

<picture>![batching-tensorflow-serving-api-response-time](../Images/01ab4df8dea678bba57bcbd7fde2432f.png)</picture>

我希望你能利用本文中介绍的各种技巧来提升你自己的机器学习API。有很多我没有探索的方面，特别是模型本身的优化（而非API），如[模型剪枝](https://towardsdatascience.com/pruning-deep-neural-network-56cae1ec5505)或[训练后量化](https://www.tensorflow.org/model_optimization/guide/quantization)。

**简介: [Yannick Wolf](https://www.linkedin.com/in/yannick-wolff-457616124/)** 是Sicara的首席数据科学家。

[原文](https://www.sicara.ai/blog/optimize-response-time-api)。经许可转载。

**相关：**

+   [机器学习部署的软件接口](/2020/03/software-interfaces-machine-learning-deployment.html)

+   [TensorFlow 2.0教程：优化训练时间性能](/2020/03/tensorflow-optimizing-training-time-performance.html)

+   [如何在3个简单步骤中对任何Python脚本进行超参数调优](/2020/04/hyperparameter-tuning-python.html)

### 更多相关主题

+   [优化和管理机器学习生命周期的十大MLOps工具](https://www.kdnuggets.com/2022/10/top-10-mlops-tools-optimize-manage-machine-learning-lifecycle.html)

+   [将机器学习模型部署到云中的生产环境]((https://www.kdnuggets.com/deploying-your-ml-model-to-production-in-the-cloud)

+   [如何优化SQL查询以加快数据检索速度](https://www.kdnuggets.com/2023/06/optimize-sql-queries-faster-data-retrieval.html)

+   [如何优化Dockerfile指令以加快构建时间](https://www.kdnuggets.com/how-to-optimize-dockerfile-instructions-for-faster-build-times)

+   [机器学习算法的完整端到端部署…](https://www.kdnuggets.com/2021/12/deployment-machine-learning-algorithm-live-production-environment.html)

+   [将机器学习从概念验证到生产环境的运作化](https://www.kdnuggets.com/2022/05/operationalizing-machine-learning-poc-production.html)
