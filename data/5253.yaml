- en: 'PySpark SQL Cheat Sheet: Big Data in Python'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark SQL 备忘单：Python 中的大数据
- en: 原文：[https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html](https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html](https://www.kdnuggets.com/2017/11/pyspark-sql-cheat-sheet-big-data-python.html)
- en: '**By Karlijn Willems, [DataCamp](https://www.datacamp.com/).**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 Karlijn Willems， [DataCamp](https://www.datacamp.com/)。**'
- en: Big Data With Python
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用 Python 处理大数据
- en: 'Big data is everywhere and is traditionally characterized by three V’s: Velocity,
    Variety and Volume. Big data is fast, is varied and has a huge volume. As a data
    scientist, data engineer, data architect, ... or whatever the role is that you’ll
    assume in the data science industry, you’ll definitely get in touch with big data
    sooner or later, as companies now gather an enormous amount of data across the
    board.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据无处不在，通常以三个 V 特征来描述：速度（Velocity）、多样性（Variety）和体积（Volume）。大数据是快速的、多样的，并且体积庞大。作为数据科学家、数据工程师、数据架构师等角色，无论你在数据科学行业中担任何种角色，你都将早晚接触到大数据，因为公司现在收集了大量的数据。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前 3 个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - 支持你所在组织的
    IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Data doesn’t always mean information, though, and that is where you, data science
    enthusiast, come in. You can make use of Apache Spark, “a fast and general engine
    for large-scale data processing” to start to tackle the challenges that big data
    poses to you and the company you’re working for.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据并不总是意味着信息，这就是你，数据科学爱好者，发挥作用的地方。你可以利用 Apache Spark，这是一款“用于大规模数据处理的快速且通用的引擎”，来开始解决大数据给你和你所在公司带来的挑战。
- en: 'Nevertheless, doubts may always arise when you’re working with Spark and when
    they do, take a look at DataCamp’s [Apache Spark Tutorial: ML with PySpark tutorial](https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning)
    or download the [cheat sheet](https://www.datacamp.com/community/blog/pyspark-sql-cheat-sheet)
    for free!'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，当你在使用 Spark 时，疑问可能会不断出现。遇到问题时，可以查看 DataCamp 的 [Apache Spark 教程：ML with PySpark
    tutorial](https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning)
    或免费下载 [备忘单](https://www.datacamp.com/community/blog/pyspark-sql-cheat-sheet)！
- en: In what follows, we’ll dive deeper into the structure and the contents of the
    cheat sheet.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨备忘单的结构和内容。
- en: PySpark Cheat Sheet
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PySpark 备忘单
- en: PySpark is the Spark Python API exposes the Spark programming model to Python.
    Spark SQL, then, is a module of PySpark that allows you to work with structured
    data in the form of DataFrames. This stands in contrast to RDDs, which are typically
    used to work with unstructured data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 是 Spark 的 Python API，将 Spark 编程模型暴露给 Python。Spark SQL 是 PySpark 的一个模块，允许你以
    DataFrames 的形式处理结构化数据。这与通常用于处理非结构化数据的 RDDs 相对。
- en: '![PySpark cheat sheet](../Images/f31b5ce6452a08f76098ca13c56b14cb.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 备忘单](../Images/f31b5ce6452a08f76098ca13c56b14cb.png)'
- en: '**Tip:** if you want to learn more about the differences between RDDs and DataFrames,
    but also about how Spark DataFrames differ from pandas DataFrames, you should
    definitely check out the [Apache Spark in Python: Beginner''s Guide](https://www.datacamp.com/community/tutorials/apache-spark-python).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示：** 如果你想了解更多关于 RDD 和 DataFrame 之间的差异，以及 Spark DataFrames 与 pandas DataFrames
    的不同之处，你一定要查看 [Apache Spark in Python: 初学者指南](https://www.datacamp.com/community/tutorials/apache-spark-python)。'
- en: Initializing SparkSession
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化 SparkSession
- en: 'If you want to start working with Spark SQL with PySpark, you’ll need to start
    a SparkSession first: you can use this to create DataFrames, register DataFrames
    as tables, execute SQL over the tables and read parquet files. Don’t worry if
    all of this sounds very new to you - You’ll read more about this later on in this
    article!'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想使用 PySpark 开始进行 Spark SQL 工作，你需要先启动一个 SparkSession：你可以用它来创建 DataFrames、将
    DataFrames 注册为表、对表执行 SQL 查询以及读取 parquet 文件。即使这些对你来说听起来非常陌生，也不用担心——你会在本文后面了解更多关于这些的内容！
- en: You start a SparkSession by first importing it from the sql module that comes
    with the pyspark package. Next, you can initialize a variable spark, for example,
    to not only build the SparkSession, but also give the application a name, set
    the config and then use the getOrCreate() method to either get a SparkSession
    if there is already one running or to create one if that isn’t the case yet! That
    last method will come in extremely handy, especially for future reference, because
    it will prevent you from running multiple SparkSessions at the same time!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你通过首先从 pyspark 包附带的 sql 模块中导入 SparkSession 来启动一个 SparkSession。接下来，你可以初始化一个变量
    spark，例如，不仅构建 SparkSession，还给应用程序命名，设置配置，然后使用 getOrCreate() 方法来获取已存在的 SparkSession，或者在尚不存在的情况下创建一个！最后这个方法将非常实用，特别是为了将来参考，因为它会防止你同时运行多个
    SparkSessions！
- en: Cool, isn’t it?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 很酷，不是吗？
- en: '![PySpark cheat sheet](../Images/3c86b85673b094ab3ed9a4afa5142260.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark Cheat Sheet](../Images/3c86b85673b094ab3ed9a4afa5142260.png)'
- en: Inspecting the Data
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查数据
- en: When you have imported your data, it’s time to inspect the Spark DataFrame by
    using some of the built-in attributes and methods. This way, you’ll get to know
    your data a little bit better before you start manipulating the DataFrames.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当你导入数据后，使用一些内置属性和方法检查 Spark DataFrame 是时候了。这样，你可以在开始操作 DataFrames 之前，更好地了解你的数据。
- en: Now, you’ll probably already know most of the methods and attributes mentioned
    in this section of the cheat sheet  from working with pandas DataFrames or NumPy,
    such as dtypes, head(), describe(), count(),... There are also some methods that
    might be new to you, such as the take() or printSchema() method, or the schema
    attribute.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能已经从使用 pandas DataFrames 或 NumPy 的经验中了解了本节中提到的大多数方法和属性，例如 dtypes、head()、describe()、count()，等等。还有一些可能对你来说是新的方法，例如
    take() 或 printSchema() 方法，或 schema 属性。
- en: '![PySpark cheat sheet](../Images/2b87b5681e4fc864b638d73298521c50.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark Cheat Sheet](../Images/2b87b5681e4fc864b638d73298521c50.png)'
- en: Nevertheless, you’ll see that the ramp-up is quite modest, as you can leverage
    your previous knowledge on data science packages maximally.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，你会发现提升过程相当温和，因为你可以最大限度地利用你在数据科学包方面的前期知识。
- en: Duplicate Values
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重复值
- en: As you’re inspecting your data, you might find that there are some duplicate
    values. To remediate this, you can use the dropDuplicates() method, for example,
    to drop duplicate values in your Spark DataFrame.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查数据时，你可能会发现一些重复值。为了解决这个问题，你可以使用 dropDuplicates() 方法，例如，在你的 Spark DataFrame
    中删除重复值。
- en: Queries
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询
- en: You might remember that one of the main reasons for using Spark DataFrames in
    the first place is that you have a more structured way of dealing with your data
    - Querying is one of those examples of handling your data in a more structured
    way, whether you’re using SQL in a relational database, SQL-like language in No-SQL
    databases, the [query()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html?highlight=query#pandas.DataFrame.query)
    method on Pandas DataFrames, and so on.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，使用 Spark DataFrames 的主要原因之一是你有一种更结构化的数据处理方式——查询就是处理数据的一种更结构化的方式，无论你是在关系型数据库中使用
    SQL、在 No-SQL 数据库中使用类似 SQL 的语言、还是在 Pandas DataFrames 中使用 [query()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.query.html?highlight=query#pandas.DataFrame.query)
    方法，等等。
- en: '![PySpark cheat sheet](../Images/8a57af6b8937cff2c56c8dfbf857efe5.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark Cheat Sheet](../Images/8a57af6b8937cff2c56c8dfbf857efe5.png)'
- en: 'Now that we’re talking about Pandas DataFrames, you’ll notice that Spark DataFrames
    follow a similar principle: you’re using methods to get to know your data better.
    In this case, though, you won’t use query(). Instead, you’ll have to make use
    of other methods to get the results you want to get back: in a first instance,
    select() and show() will be your friends whenever you want to retrieve information
    from your DataFrames.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在讨论 Pandas DataFrames 时，你会注意到 Spark DataFrames 遵循类似的原则：你使用方法来更好地了解你的数据。然而，在这种情况下，你不会使用
    query()。相反，你需要利用其他方法来获取你想要的结果：在第一次使用时，select() 和 show() 将是你获取 DataFrames 信息的好帮手。
- en: 'Within these methods, you can build up your query. As with standard SQL, you
    specify what columns you exactly want to get back within select(). You can do
    this with a regular string or by specifying the column name with the help of your
    DataFrame itself, as in the following example: df.lastName or df[“firstName”].'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些方法中，你可以构建你的查询。和标准 SQL 一样，你可以在 select() 中指定你想要返回的确切列。你可以使用常规字符串来完成，也可以借助 DataFrame
    本身来指定列名，如以下示例：df.lastName 或 df[“firstName”]。
- en: Note that the latter approach will give you some more freedom in some cases
    where you want to specify what information you exactly want to retrieve with additional
    functions such as isin() or startswith(), for example.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，后者的方法在某些情况下会给你更多的自由，特别是当你想用额外的函数如 isin() 或 startswith() 精确指定你要检索的信息时。
- en: 'Besides select(), you can also make use of the functions module in PySpark
    SQL to specify, for example, a when clause in your query, as demonstrated in the
    table below:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 select() 外，你还可以利用 PySpark SQL 的 functions 模块来指定查询中的 when 子句，例如，如下表所示：
- en: '![PySpark cheat sheet](../Images/d5234f75f8ad26d04f778d4b0bbf5d52.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 备忘单](../Images/d5234f75f8ad26d04f778d4b0bbf5d52.png)'
- en: All in all, you can see that there are a lot of possibilities for those who
    want to handle and get to know their data to a larger extent with the help of
    select(), help() and the functions of the functions module.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，你可以看到，使用 select()、help() 和 functions 模块的功能，有很多可能性可以帮助你更大程度地处理和了解你的数据。
- en: Add, Update & Remove Columns
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加、更新和删除列
- en: You might also want to look into adding, updating or removing some columns from
    your Spark DataFrame. You can easily do this with the withColumn(), withColumnRenamed()
    and drop() methods. You’ll probably know by now that you also have a drop() method
    at your disposal when you’re working with Pandas DataFrames. You can read more
    [here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html?highlight=drop#pandas.DataFrame.drop).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还希望查看如何向 Spark DataFrame 添加、更新或删除某些列。你可以使用 withColumn()、withColumnRenamed()
    和 drop() 方法轻松完成。你现在可能已经知道，当你使用 Pandas DataFrames 时，你也可以使用 drop() 方法。你可以在[这里](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html?highlight=drop#pandas.DataFrame.drop)阅读更多信息。
- en: '![PySpark cheat sheet](../Images/751902d5d047e7357ce04767eb6020aa.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 备忘单](../Images/751902d5d047e7357ce04767eb6020aa.png)'
- en: GroupBy
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 按组
- en: Again, just like with Pandas DataFrames, you might want to group by certain
    values and aggregate some values - The example below shows how you use the groupBy()
    method, in combination with count() and show() to retrieve and show your data,
    grouped by age, together with the number of people who have that certain age.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，就像 Pandas DataFrames 一样，你可能希望按某些值分组并聚合一些值 - 下面的示例展示了如何使用 groupBy() 方法，并结合
    count() 和 show() 来检索和显示你的数据，以年龄为分组，并显示具有该年龄的人数。
- en: '![PySpark cheat sheet](../Images/283cb11c84f4170ae04c5534ce466532.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![PySpark 备忘单](../Images/283cb11c84f4170ae04c5534ce466532.png)'
- en: More On This Topic
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题
- en: '[PySpark for Data Science](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于数据科学的 PySpark](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)'
- en: '[Data Validation for PySpark Applications using Pandera](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Pandera 对 PySpark 应用进行数据验证](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
- en: '[Data Cleaning with Python Cheat Sheet](https://www.kdnuggets.com/2023/02/data-cleaning-python-cheat-sheet.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 进行数据清理的备忘单](https://www.kdnuggets.com/2023/02/data-cleaning-python-cheat-sheet.html)'
- en: '[Best Python Tools for Building Generative AI Applications Cheat Sheet](https://www.kdnuggets.com/2023/08/best-python-tools-generative-ai-cheat-sheet.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建生成式 AI 应用的最佳 Python 工具备忘单](https://www.kdnuggets.com/2023/08/best-python-tools-generative-ai-cheat-sheet.html)'
- en: '[Python Control Flow Cheat Sheet](https://www.kdnuggets.com/2022/11/python-control-flow-cheatsheet.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 控制流备忘单](https://www.kdnuggets.com/2022/11/python-control-flow-cheatsheet.html)'
- en: '[KDnuggets News, July 5: A Rotten Data Science Project • 10 AI…](https://www.kdnuggets.com/2023/n24.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，7月5日：一个糟糕的数据科学项目 • 10 人工智能…](https://www.kdnuggets.com/2023/n24.html)'
