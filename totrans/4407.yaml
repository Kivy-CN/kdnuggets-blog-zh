- en: The Most Complete Guide to PyTorch for Data Scientists
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/09/most-complete-guide-pytorch-data-scientists.html](https://www.kdnuggets.com/2020/09/most-complete-guide-pytorch-data-scientists.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)![Header](../Images/43c438dc46265ffdd4bc06ff3d0f68a3.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '***PyTorch*** has sort of became one of the de facto standards for creating
    Neural Networks now, and I love its interface. Yet, it is somehow a little difficult
    for beginners to get a hold of.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: I remember picking PyTorch up only after some extensive experimentation a couple
    of years back. To tell you the truth, it took me a lot of time to pick it up but
    am I glad that I moved from [Keras to PyTorch](https://towardsdatascience.com/moving-from-keras-to-pytorch-f0d4fff4ce79).* With
    its high customizability and pythonic syntax, *PyTorch is just a joy to work with,
    and I would recommend it to anyone who wants to do some heavy lifting with Deep
    Learning.
  prefs: []
  type: TYPE_NORMAL
- en: So, in this PyTorch guide, ***I will try to ease some of the pain with PyTorch
    for starters*** and go through some of the most important classes and modules
    that you will require while creating any Neural Network with Pytorch.
  prefs: []
  type: TYPE_NORMAL
- en: But, that is not to say that this is aimed at beginners only as ***I will also
    talk about the*** ***high customizability PyTorch provides and will talk about
    custom Layers, Datasets, Dataloaders, and Loss functions***.
  prefs: []
  type: TYPE_NORMAL
- en: So let’s get some coffee ☕ ️and start it up.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Tensors](https://www.kdnuggets.com/2018/05/wtf-tensor.html) are the basic
    building blocks in PyTorch and put very simply, they are NumPy arrays but on GPU.
    In this part, I will list down some of the most used operations we can use while
    working with Tensors. This is by no means an exhaustive list of operations you
    can do with Tensors, but it is helpful to understand what tensors are before going
    towards the more exciting parts.'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Create a Tensor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can create a PyTorch tensor in multiple ways. This includes converting to
    tensor from a NumPy array. Below is just a small gist with some examples to start
    with, but you can do a whole lot of [more things](https://pytorch.org/docs/stable/tensors.html) with
    tensors just like you can do with NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image for post](../Images/be8c3218e6ad5ab93ddbc41fa18b206d.png)'
  prefs: []
  type: TYPE_IMG
- en: 2\. Tensor Operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Again, there are a lot of operations you can do on these tensors. The full list
    of functions can be found [here](https://pytorch.org/docs/stable/torch.html?highlight=mm#math-operations).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image for post](../Images/ba8d46e45d52acf8c3009404823db6fd.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Note: **What are PyTorch Variables? In the previous versions of Pytorch,
    Tensor and Variables used to be different and provided different functionality,
    but now the Variable API is [deprecated](https://pytorch.org/docs/stable/autograd.html#variable-deprecated),
    and all methods for variables work with Tensors. So, if you don’t know about them,
    it’s fine as they re not needed, and if you know them, you can forget about them.'
  prefs: []
  type: TYPE_NORMAL
- en: The nn.Module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Figure](../Images/cbff37d276c1915bc74c150f60bef3d3.png)Photo by [Fernand
    De Canne](https://unsplash.com/@fernanddecanne?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
  prefs: []
  type: TYPE_IMG
- en: Here comes the fun part as we are now going to talk about some of the most used
    constructs in Pytorch while creating deep learning projects. nn.Module lets you
    create your Deep Learning models as a class. You can inherit from `nn.Module`
    to define any model as a class. Every model class necessarily contains an `__init__` procedure
    block and a block for the `forward` pass.
  prefs: []
  type: TYPE_NORMAL
- en: In the `__init__` part, the user can define all the layers the network is going
    to have but doesn't yet define how those layers would be connected to each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the `forward` pass block, the user defines how data flows from one layer
    to another inside the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, put simply, any network we define will look like:'
  prefs: []
  type: TYPE_NORMAL
- en: Here we have defined a very simple Network that takes an input of size 784 and
    passes it through two linear layers in a sequential manner. But the thing to note
    is that we can define any sort of calculation while defining the forward pass,
    and that makes PyTorch highly customizable for research purposes. For example,
    in our crazy experimentation mode, we might have used the below network where
    we arbitrarily attach our layers. Here we send back the output from the second
    linear layer back again to the first one after adding the input to it(skip connection)
    back again(I honestly don’t know what that will do).
  prefs: []
  type: TYPE_NORMAL
- en: We can also check if the neural network forward pass works. I usually do that
    by first creating some random input and just passing that through the network
    I have created.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A word about Layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pytorch is pretty powerful, and you can actually create any new experimental
    layer by yourself using `nn.Module`. For example, rather than using the predefined
    Linear Layer `nn.Linear` from Pytorch above, we could have created our **custom
    linear layer**.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see how we wrap our weights tensor in `nn.Parameter.` This is done
    to make the tensor to be considered as a model parameter. From PyTorch [docs](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter):'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters are `[*Tensor*](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)` subclasses,
    that have a very special property when used with `*Module*` - when they’re assigned
    as Module attributes they are automatically added to the list of its parameters,
    and will appear in `*parameters()*` iterator
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As you will later see, the `model.parameters()` iterator will be an input to
    the optimizer. But more on that later.
  prefs: []
  type: TYPE_NORMAL
- en: Right now, we can now use this custom layer in any PyTorch network, just like
    any other layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'But then again, Pytorch would not be so widely used if it didn’t provide a
    lot of ready to made layers used very frequently in wide varieties of Neural Network
    architectures. Some examples are: `[nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)`,
    `[nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)`,
    `[nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)`,
    `[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)`, `[nn.BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d)`,
    `[nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout)`,
    `[nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding)`, `[nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU)/[nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM)`,
    `[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax)`, `[nn.LogSoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax)`,
    `[nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention)`,
    `[nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html#torch.nn.TransformerEncoder)`,
    `[nn.TransformerDecoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder)`'
  prefs: []
  type: TYPE_NORMAL
- en: I have linked all the layers to their source where you could read all about
    them, but to show how I usually try to understand a layer and read the docs, I
    would try to look at a very simple convolutional layer here.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image for post](../Images/c6dbe2d59cfe06a6d60c85ce6574c285.png)'
  prefs: []
  type: TYPE_IMG
- en: So, a Conv2d Layer needs as input an Image of height H and width W, with `Cin` channels.
    Now, for the first layer in a convnet, the number of `in_channels` would be 3
    (RGB), and the number of `out_channels` can be defined by the user. The `kernel_size`
    mostly used is 3x3, and the `stride` normally used is 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check a new layer which I don’t know much about, I usually try to see the
    input as well as output for the layer like below where I would first initialize
    the layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: And then pass some random input through it. Here 100 is the batch size.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: So, we get the output from the convolution operation as required, and I have
    sufficient information on how to use this layer in any Neural Network I design.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets and DataLoaders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'How would we pass data to our Neural nets while training or while testing?
    We can definitely pass tensors as we have done above, but Pytorch also provides
    us with pre-built Datasets to make it easier for us to pass data to our neural
    nets. You can check out the complete list of datasets provided at [torchvision.datasets](https://pytorch.org/docs/stable/torchvision/datasets.html) and [torchtext.datasets](https://pytorch.org/text/datasets.html).
    But, to give a concrete example for datasets, let’s say we had to pass images
    to an Image Neural net using a folder which has images in this structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use `torchvision.datasets.ImageFolder` dataset to get an example image
    like below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image for post](../Images/8f36e3e7f1066b9c820066cbac26ddbf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This dataset has 847 images, and we can get an image and its label using an
    index. Now we can pass images one by one to any image neural network using a for
    loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '***But that is not optimal. We want to do batching. ***We can actually write
    some more code to append images and labels in a batch and then pass it to the
    Neural network. But Pytorch provides us with a utility iterator `torch.utils.data.DataLoader` to
    do precisely that. Now we can simply wrap our `train_dataset` in the Dataloader,
    and we will get batches instead of individual examples.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can simply iterate with batches using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'So actually, the whole process of using datasets and Dataloaders becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: You can look at this particular example in action in my previous blogpost on
    Image classification using Deep Learning [here](https://towardsdatascience.com/end-to-end-pipeline-for-setting-up-multiclass-image-classification-for-data-scientists-2e051081d41c).
  prefs: []
  type: TYPE_NORMAL
- en: This is great, and Pytorch does provide a lot of functionality out of the box.
    But the main power of Pytorch comes with its immense customization. We can also
    create our own custom datasets if the datasets provided by PyTorch don’t fit our
    use case.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Custom Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To write our custom datasets, we can make use of the abstract class `torch.utils.data.Dataset` provided
    by Pytorch. We need to inherit this `Dataset` class and need to define two methods
    to create a custom Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '`__len__` : a function that returns the size of the dataset. This one is pretty
    simple to write in most cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__getitem__`: a function that takes as input an index `i` and returns the
    sample at index `i`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, we can create a simple custom dataset that returns an image and
    a label from a folder. See that most of the tasks are happening in `__init__` part
    where we use `glob.glob` to get image names and do some general preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that we open our images one at a time in the` __getitem__` method
    and not while initializing. This is not done in `__init__` because we don't want
    to load all our images in the memory and just need to load the required ones.
  prefs: []
  type: TYPE_NORMAL
- en: We can now use this dataset with the utility `Dataloader` just like before.
    It works just like the previous dataset provided by PyTorch but without some utility
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Custom DataLoaders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**This particular section is a little advanced and can be skipped going through
    this post as it will not be needed in a lot of situations.** But I am adding it
    for completeness here.'
  prefs: []
  type: TYPE_NORMAL
- en: So let’s say you are looking to provide batches to a network that processes
    text input, and the network could take sequences with any sequence size as long
    as the size remains constant in the batch. For example, we can have a BiLSTM network
    that can process sequences of any length. It’s alright if you don’t understand
    the layers used in it right now; just know that it can process sequences with
    variable sizes.
  prefs: []
  type: TYPE_NORMAL
- en: This network expects its input to be of shape (`batch_size`, `seq_length`) and
    works with any `seq_length`. We can check this by passing our model two random
    batches with different sequence lengths(10 and 25).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, we want to provide tight batches to this model, such that each batch has
    the same sequence length based on the max sequence length in the batch to minimize
    padding. This has an added benefit of making the neural net run faster. It was,
    in fact, one of the methods used in the winning submission of the Quora Insincere
    challenge in Kaggle, where running time was of utmost importance.
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we do this? Let’s write a very simple custom dataset class first.
  prefs: []
  type: TYPE_NORMAL
- en: Also, let’s generate some random data which we will use with this custom Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/09c6e3ad648b1c216c3dffeec8a2eb70.png)Example of one random
    sequence and label. Each integer in the sequence corresponds to a word in the
    sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the custom dataset now using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If we now try to use the Dataloader on this dataset with `batch_size`>1, we
    will get an error. Why is that?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Image for post](../Images/f16ed32a2e9e8eb50c11b791e59b59ff.png)'
  prefs: []
  type: TYPE_IMG
- en: This happens because the sequences have different lengths, and our data loader
    expects our sequences of the same length. Remember that in the previous image
    example, we resized all images to size 224 using the transforms, so we didn’t
    face this error.
  prefs: []
  type: TYPE_NORMAL
- en: '***So, how do we iterate through this dataset so that each batch has sequences
    with the same length, but different batches may have different sequence lengths?***'
  prefs: []
  type: TYPE_NORMAL
- en: We can use `collate_fn` parameter in the DataLoader that lets us define how
    to stack sequences in a particular batch. To use this, we need to define a function
    that takes as input a batch and returns (`x_batch`, `y_batch` ) with padded sequence
    lengths based on `max_sequence_length` in the batch. The functions I have used
    in the below function are simple NumPy operations. Also, the function is properly
    commented so you can understand what is happening.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now use this `collate_fn` with our Dataloader as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure](../Images/071eb46c3a18b3d3e7c289a3cb5d1471.png)See that the batches
    have different sequence lengths now'
  prefs: []
  type: TYPE_NORMAL
- en: It will work this time as we have provided a custom `collate_fn.` And see that
    the batches have different sequence lengths now. Thus we would be able to train
    our BiLSTM using variable input sizes just like we wanted.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Neural Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We know how to create a neural network using `nn.Module.` But how to train
    it? Any neural network that has to be trained will have a training loop that will
    look something similar to below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the above code, we are running five epochs and in each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: We iterate through the dataset using a data loader.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In each iteration, we do a forward pass using `model(x_batch)`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We calculate the Loss using a `loss_criterion`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We back-propagate that loss using `loss.backward()` call. We don't have to worry
    about the calculation of the gradients at all, as this simple call does it all
    for us.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take an optimizer step to change the weights in the whole network using `optimizer.step()`.
    This is where weights of the network get modified using the gradients calculated
    in `loss.backward()` call.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We go through the validation data loader to check the validation score/metrics.
    Before doing validation, we set the model to eval mode using `model.eval().`Please
    note we don't back-propagate losses in eval mode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Till now, we have talked about how to use `nn.Module` to create networks and
    how to use Custom Datasets and Dataloaders with Pytorch. So let's talk about the
    various options available for Loss Functions and Optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pytorch provides us with a variety of [loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) for
    our most common tasks, like Classification and Regression. Some most used examples
    are `[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`,
    `[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)`,
    `[nn.KLDivLoss](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss)`
    and `[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss).`
    You can read the documentation of each loss function, but to explain how to use
    these loss functions, I will go through the example of `[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)`
  prefs: []
  type: TYPE_NORMAL
- en: '![Image for post](../Images/cfb45e3ebca92aab225660e6ddccde92.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The documentation for NLLLoss is pretty succinct. As in, this loss function
    is used for Multiclass classification, and based on the documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: the input expected needs to be of size (`batch_size` x `Num_Classes` ) — These
    are the predictions from the Neural Network we have created.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to have the log-probabilities of each class in the input — To get log-probabilities
    from a Neural Network, we can add a `LogSoftmax` Layer as the last layer of our
    network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target needs to be a tensor of classes with class numbers in the range(0,
    C-1) where C is the number of classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, we can try to use this Loss function for a simple classification network.
    Please note the `LogSoftmax` layer after the final linear layer. If you don't
    want to use this `LogSoftmax` layer, you could have just used `[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)`
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s define a random input to pass to our network to test it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'And pass it through the model to get predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now get the loss as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Custom Loss Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Defining your custom loss functions is again a piece of cake, and you should
    be okay as long as you use tensor operations in your loss function. For example,
    here is the `customMseLoss`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You can use this custom loss just like before. But note that we don’t instantiate
    the loss using criterion this time as we have defined it as a function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If we wanted, we could have also written it as a class using `nn.Module` ,
    and then we would have been able to use it as an object. Here is an NLLLoss custom
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once we get gradients using the `loss.backward()` call, we need to take an
    optimizer step to change the weights in the whole network. Pytorch provides a
    variety of different ready to use optimizers using the `torch.optim` module. For
    example: `[torch.optim.Adadelta](https://pytorch.org/docs/stable/optim.html#torch.optim.Adadelta)`,
    `[torch.optim.Adagrad](https://pytorch.org/docs/stable/optim.html#torch.optim.Adagrad)`,
    `[torch.optim.RMSprop](https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop)`
    and the most widely used `[torch.optim.Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam).`
    To use the most used Adam optimizer from PyTorch, we can simply instantiate it
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: And then use `optimizer**.**zero_grad()` and `optimizer.step()` while training
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: I am not discussing how to write custom optimizers as it is an infrequent use
    case, but if you want to have more optimizers, do check out the [pytorch-optimizer](https://pytorch-optimizer.readthedocs.io/en/latest/) library,
    which provides a lot of other optimizers used in research papers. Also, if you
    anyhow want to create your own optimizers, you can take inspiration using the
    source code of implemented optimizers in [PyTorch](https://github.com/pytorch/pytorch/tree/master/torch/optim) or [pytorch-optimizers](https://github.com/jettify/pytorch-optimizer/tree/master/torch_optimizer).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/8fa38ecb11772198ed87d804bc7c312e.png)Other optimizers from [pytorch-optimizer](https://github.com/jettify/pytorch-optimizer) library'
  prefs: []
  type: TYPE_NORMAL
- en: Using GPU/Multiple GPUs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Till now, whatever we have done is on the CPU. If you want to use a GPU, you
    can put your model to GPU using `model.to('cuda')`. Or if you want to use multiple
    GPUs, you can use `nn.DataParallel`. Here is a utility function that checks the
    number of GPUs in the machine and sets up parallel training automatically using `DataParallel` if
    needed.
  prefs: []
  type: TYPE_NORMAL
- en: The only thing that we will need to change is that we will load our data to
    GPU while training if we have GPUs. It’s as simple as adding a few lines of code
    to our training loop.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pytorch provides a lot of customizability with minimal code. While at first,
    it might be hard to understand how the whole ecosystem is structured with classes,
    in the end, it is simple Python. In this post, I have tried to break down most
    of the parts you might need while using Pytorch, and I hope it makes a little
    more sense for you after reading this.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code for this post here on my [GitHub](https://github.com/MLWhiz/data_science_blogs/tree/master/pytorch_guide) repo,
    where I keep codes for all my blogs.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about Pytorch using a course based structure, take
    a look at the [Deep Neural Networks with PyTorch](https://www.coursera.org/learn/deep-neural-networks-with-pytorch?ranMID=40328&ranEAID=lVarvwc5BD0&ranSiteID=lVarvwc5BD0-Mh_whR0Q06RCh47zsaMVBQ&siteID=lVarvwc5BD0-Mh_whR0Q06RCh47zsaMVBQ&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0) course
    by IBM on Coursera. Also, if you want to know more about Deep Learning, I would
    like to recommend this excellent course on [Deep Learning in Computer Vision](https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0) in
    the [Advanced machine learning specialization](https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&utm_content=2&utm_medium=partners&utm_source=linkshare&utm_campaign=lVarvwc5BD0).
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for the read. I am going to be writing more beginner-friendly posts in
    the future too. Follow me up at [**Medium**](https://medium.com/@rahul_agarwal?source=post_page---------------------------) or
    Subscribe to my [**blog**](http://eepurl.com/dbQnuX?source=post_page---------------------------) to
    be informed about them. As always, I welcome feedback and constructive criticism
    and can be reached on Twitter [**@mlwhiz**](https://twitter.com/MLWhiz?source=post_page---------------------------).
  prefs: []
  type: TYPE_NORMAL
- en: Also, a small disclaimer — There might be some affiliate links in this post
    to relevant resources, as sharing knowledge is never a bad idea.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Rahul Agarwal](https://www.linkedin.com/in/rahulagwl/)** is Senior
    Statistical Analyst at WalmartLabs. Follow him on Twitter [@mlwhiz](https://twitter.com/MLWhiz).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/minimal-pytorch-subset-for-deep-learning-for-data-scientists-8ccbd1ccba6b).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[6 bits of advice for Data Scientists](/2019/09/advice-data-scientists.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Hitchhiker’s Guide to Feature Extraction](/2019/06/hitchhikers-guide-feature-extraction.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Classification Evaluation Metrics Every Data Scientist Must Know](/2019/10/5-classification-evaluation-metrics-every-data-scientist-must-know.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[The Complete Free PyTorch Course for Deep Learning](https://www.kdnuggets.com/2022/10/complete-free-pytorch-course-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Build a Data Science Enablement Team: A Complete Guide](https://www.kdnuggets.com/2022/10/build-data-science-enablement-team-complete-guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Geocoding in Python: A Complete Guide](https://www.kdnuggets.com/2022/11/geocoding-python-complete-guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Complete Guide To Decision Tree Software](https://www.kdnuggets.com/2022/08/complete-guide-decision-tree-software.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Practical Guide to Transfer Learning using PyTorch](https://www.kdnuggets.com/2023/06/practical-guide-transfer-learning-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Beginner''s Guide to PyTorch](https://www.kdnuggets.com/a-beginners-guide-to-pytorch)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
