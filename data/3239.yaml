- en: 5 Ways to Get Started with Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习入门的5种方法
- en: 原文：[https://www.kdnuggets.com/2017/09/5-ways-get-started-reinforcement-learning.html](https://www.kdnuggets.com/2017/09/5-ways-get-started-reinforcement-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/09/5-ways-get-started-reinforcement-learning.html](https://www.kdnuggets.com/2017/09/5-ways-get-started-reinforcement-learning.html)
- en: '![](../Images/b1f1892f4476b0d4d7ff79f1901c2b3a.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1f1892f4476b0d4d7ff79f1901c2b3a.png)'
- en: Artwork by [Robert Aguilera](http://robertaguileradesign.com/)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 艺术作品由 [Robert Aguilera](http://robertaguileradesign.com/)
- en: Machine learning algorithms, and neural networks in particular, are considered
    to be the cause of a new AI ‘revolution’. In this article I will introduce the
    concept of reinforcement learning but with limited technical details so that readers
    with a variety of backgrounds can understand the essence of the technique, its
    capabilities and limitations.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法，特别是神经网络，被认为是引发新一轮人工智能“革命”的原因。在本文中，我将介绍强化学习的概念，但技术细节有限，以便具有不同背景的读者能够理解该技术的本质、能力和局限性。
- en: '*At the end of the article, I will provide* ***links*** *to a few* ***resources***
    *for implementing RL.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*在文章末尾，我将提供* ***链接*** *到一些* ***资源*** *，用于实施强化学习。*'
- en: What is Reinforcement Learning?
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是强化学习？
- en: 'Broadly speaking, data-driven algorithms can be categorized into three types:
    **Supervised**, **Unsupervised**, and **Reinforcement** learning.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 广义上讲，数据驱动的算法可以分为三种类型：**监督学习**、**无监督学习**和**强化学习**。
- en: The first two are generally used to perform tasks such as image classification,
    detection, etc. While their accuracy is remarkable, these tasks differ from those
    that we would expect from an ‘intelligent’ being.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 前两种方法通常用于执行图像分类、检测等任务。虽然它们的准确性很高，但这些任务与我们期望的“智能”存在的任务有所不同。
- en: 'This is where reinforcement learning comes in. The concept itself is very simple,
    and much like our evolutionary process: the environment rewards the agent for
    things that it gets right and penalizes it for things that it gets wrong. The
    main challenge is developing the capacity to learn several million possible ways
    of doing things.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是强化学习的作用所在。其概念非常简单，就像我们的进化过程一样：环境奖励代理正确的行为，并惩罚其错误的行为。主要的挑战是开发出学习数百万种可能行为的能力。
- en: Q Learning & Deep Q Learning
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Q学习与深度Q学习
- en: Q learning is a widely used reinforcement learning algorithm. Without going
    into the detailed math, the given quality of an action is determined by what state
    the agent is in. The agent usually performs the action which gives it the maximum
    reward. The detailed math can be found [**here**](https://en.wikipedia.org/wiki/Q-learning).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习是一种广泛使用的强化学习算法。在不涉及详细数学的情况下，某个行动的质量由代理所在的状态决定。代理通常执行能获得最大奖励的行动。详细的数学内容可以在[**这里**](https://en.wikipedia.org/wiki/Q-learning)找到。
- en: In this algorithm, the agent learns the quality(Q value) of each action (action
    is also called policy) based on how much reward the environment gave it. The value
    of each environment’s state, along with the Q value is usually stored in a table.
    As the agent interacts with the environment, the Q values get updated from random
    values to values that actually help maximize reward.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个算法中，代理根据环境给予的奖励量来学习每个行动（行动也称为策略）的质量（Q值）。每个环境状态的值以及Q值通常存储在表格中。随着代理与环境的交互，Q值会从随机值更新为实际有助于最大化奖励的值。
- en: Deep Q Learning
  id: totrans-13
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 深度Q学习
- en: The problem with using Q learning with tables is that it doesn’t scale well.
    If the number of states is too high, the table will not fit in memory. This is
    where Deep Q learning could be applied. Deep learning is basically just a universal
    approximation machine which can understand and come up with abstract representations.
    Deep learning can be used to approximate Q values, and it can also easily learn
    optimal Q values by using gradient descent.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Q学习和表格的问题在于其扩展性较差。如果状态数量过多，表格将无法存储在内存中。这时可以应用深度Q学习。深度学习基本上是一种通用的逼近机器，它可以理解并生成抽象表示。深度学习可以用来逼近Q值，也可以通过梯度下降轻松学习最优Q值。
- en: '*Fun Fact:*'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*趣味事实：*'
- en: 'Google has a patent on some elements of Deep Q learning: [US20150100530](https://www.google.com/patents/US20150100530)'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 谷歌对深度Q学习的一些元素拥有专利：[US20150100530](https://www.google.com/patents/US20150100530)
- en: Exploration vs Exploitation
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索与利用
- en: It is often the case that the agent memorizes one path and will never try to
    explore any other paths. In general, we would like an agent to not only exploit
    good paths, but also sometimes explore new paths that it can perform actions in.
    Therefore, a hyper-parameter, named *ε,* is used to govern how much to explore
    new paths vs how much to exploit old paths.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 代理通常会记住一条路径而不会尝试探索其他路径。一般来说，我们希望代理不仅能利用好的路径，还能有时探索新的路径以执行操作。因此，一个名为*ε*的超参数用于控制探索新路径与利用旧路径的平衡。
- en: Experience Replay
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 经验重放
- en: When training a neural network, data imbalance plays a very important role.
    If a model is trained as the agent interacts with the environment, there will
    be imbalances. The most recent play will obviously have more bearing than older
    plays.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，数据不平衡发挥着非常重要的作用。如果模型是在代理与环境交互时进行训练的，就会存在不平衡。最新的操作显然比旧的操作有更多的影响。
- en: Therefore, all the states, along with related data, is stored in the memory,
    and the neural network can randomly pick a batch of some interactions and learn
    (this makes it very similar to supervised learning).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，所有状态及相关数据都存储在内存中，神经网络可以随机挑选一批交互并进行学习（这使得它与监督学习非常相似）。
- en: The Training Framework
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练框架
- en: This is what the whole framework for deep Q learning looks like. Note the *γ*.
    This represents the discounted reward. It is a hyperparameter that controls how
    much weight the future reward will have. The symbol***´*** denotes next. e.g.
    s´ denotes next state.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是深度 Q 学习的整个框架。注意到*γ*。这表示折扣奖励。它是一个超参数，用于控制未来奖励的权重。符号***´*** 表示下一个。例如，s´ 表示下一个状态。
- en: '![](../Images/bec61ce4815420ff287ea8593a29a62e.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bec61ce4815420ff287ea8593a29a62e.png)'
- en: '**Figure 1.0** Deep Q Learning training framework. Credit: [Robert Aguilera](http://robertaguileradesign.com/)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.0** 深度 Q 学习训练框架。致谢：[Robert Aguilera](http://robertaguileradesign.com/)'
- en: Extending Reinforcement Learning
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展强化学习
- en: Reinforcement learning works well with many things (such as [**AlphaGo**](https://deepmind.com/research/alphago/)),
    but it often fails in places where the feedback is sparse. The agent will not
    explore behaviors that are actually beneficial in the long term. Sometimes, exploring
    some actions is needed for its own sake (intrinsic motivation) instead of directly
    trying to solve problems.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习在许多方面表现良好（如 [**AlphaGo**](https://deepmind.com/research/alphago/)），但在反馈稀疏的地方经常失败。代理不会探索在长期内实际有益的行为。有时，探索一些行动是为了其自身的目的（内在动机），而不是直接解决问题。
- en: Doing this allows the agent to perform complicated actions and essentially allows
    the agent to ‘plan’ things. [**Hierarchical Learning**](https://arxiv.org/pdf/1604.06057.pdf)
    allows for such kinds of abstract learning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做允许代理执行复杂的动作，并基本上允许代理‘规划’事务。[**层次化学习**](https://arxiv.org/pdf/1604.06057.pdf)
    允许这种抽象学习。
- en: '![](../Images/df0237426aeb85f512c80ad4af86a8de.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df0237426aeb85f512c80ad4af86a8de.png)'
- en: '**Figure 2.0 Hierarchical Deep Q learning**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.0 层次化深度 Q 学习**'
- en: In this kind of a setup, there are two Q networks. They are represented as the
    controller and meta-controller. The meta controller looks at the raw states and
    calculates which ‘goal’ to follow. The controller takes in the states along with
    the goal and outputs a policy to solve the goal. The critic checks if the goal
    is reached and gives some reward to the controller. The controller stops when
    the episode ends, or when the goal is reached. The meta controller then chooses
    a new goal, and this repeats.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置中，有两个 Q 网络。它们分别表示为控制器和元控制器。元控制器查看原始状态并计算要跟随的‘目标’。控制器接受状态和目标，并输出解决目标的策略。评论者检查目标是否已达到，并给予控制器一些奖励。控制器在回合结束或目标达到时停止。然后，元控制器选择一个新目标，这个过程会重复进行。
- en: The ‘goal’ is something that will eventually help the agent get to the final
    reward. This is better because it’s possible to have Q learning on top of Q learning
    in a hierarchical fashion.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ‘目标’是最终帮助代理获得最终奖励的东西。这更好，因为可以在层次结构中进行 Q 学习。
- en: Introductory Resources for Reinforcement Learning
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强化学习的入门资源
- en: 'This list will be helpful for those who are looking to get started with Reinforcement
    Learning:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表对那些希望开始强化学习的人将会有所帮助：
- en: '[**The Basics of Deep Q Learning**](https://www.intelnervana.com/demystifying-deep-reinforcement-learning/).
    Very helpful for understanding the math and processes of Reinforcement Learning.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**深度 Q 学习基础**](https://www.intelnervana.com/demystifying-deep-reinforcement-learning/)。对理解强化学习的数学和过程非常有帮助。'
- en: '[**The Hierarchical Learning paper**](https://arxiv.org/pdf/1604.06057.pdf),
    for those who want to understand Hierarchical Learning in detail.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**层次学习论文**](https://arxiv.org/pdf/1604.06057.pdf)，适合那些想详细了解层次学习的人。'
- en: '[**Hierarchical Learning paper explanation**](https://www.youtube.com/watch?v=tyRUql_ZR7Q)
    video from the authors.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**层次学习论文解释**](https://www.youtube.com/watch?v=tyRUql_ZR7Q) 来自作者的视频。'
- en: '[**Deep RL: An Overview**](https://arxiv.org/abs/1701.07274) What I would consider
    the Reinforcement Learning handbook. It covers nearly every aspect of RL that
    is required to understand the current level of research. It delves deep into the
    math, but also provides high-level overviews.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**深度 RL：概述**](https://arxiv.org/abs/1701.07274) 我认为这是强化学习的手册。它涵盖了理解当前研究水平所需的几乎所有方面。它深入探讨了数学内容，但也提供了高层次的概述。'
- en: '[**Implementing Deep Q learning with a single python script.**](https://gist.github.com/EderSantana/c7222daa328f0e885093#file-qlearn-py-L157)
    Perhaps the simplest deep Q learning implementation. This is very easy to read
    and a great starting point.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**使用单个 python 脚本实现深度 Q 学习。**](https://gist.github.com/EderSantana/c7222daa328f0e885093#file-qlearn-py-L157)
    也许是最简单的深度 Q 学习实现。这非常易读，是一个很好的起点。'
- en: '![](../Images/3373f1c2c0122aa4b193371fc9365e4d.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3373f1c2c0122aa4b193371fc9365e4d.png)'
- en: '**Figure 3.0 Deep Q Learning in action.** Output of python script in Point
    5'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 3.0 深度 Q 学习实际操作。** Python 脚本在第 5 点的输出'
- en: Call to Action
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 行动召唤
- en: If you have comments or questions, feel free to respond to this article below.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有评论或问题，请随时在下方回复这篇文章。
- en: '***Big thanks to*** [***Robert Aguilera***](http://robertaguileradesign.com/)
    ***for making the Artwork and the flowchart.***'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '***特别感谢*** [***罗伯特·阿吉莱拉***](http://robertaguileradesign.com/) ***制作了艺术作品和流程图。***'
- en: '[Original](https://buzzrobot.com/5-ways-to-get-started-with-reinforcement-learning-b96d1989c575).
    Reposted with permission.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://buzzrobot.com/5-ways-to-get-started-with-reinforcement-learning-b96d1989c575)。经许可转载。'
- en: '**Related:**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Transforming from Autonomous to Smart: Reinforcement Learning Basics](/2017/08/transforming-autonomous-smart-reinforcement-learning-basics.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从自主学习到智能学习：强化学习基础](/2017/08/transforming-autonomous-smart-reinforcement-learning-basics.html)'
- en: '[Deep Learning Zero to One: 5 Awe-Inspiring Demos with Code for Beginners](/2017/06/deep-learning-demos-code-beginners.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从零到一的深度学习：5 个令人惊叹的演示和代码，适合初学者](/2017/06/deep-learning-demos-code-beginners.html)'
- en: '[The Next Challenges for Reinforcement Learning](/2017/03/next-challenges-reinforcement-learning.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[强化学习的下一个挑战](/2017/03/next-challenges-reinforcement-learning.html)'
- en: '* * *'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的 3 个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的 IT 工作'
- en: '* * *'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题
- en: '[3 Possible Ways to Get into Data Science](https://www.kdnuggets.com/2022/03/3-possible-ways-get-data-science.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[进入数据科学的 3 种可能途径](https://www.kdnuggets.com/2022/03/3-possible-ways-get-data-science.html)'
- en: '[Federated Learning: Collaborative Machine Learning with a Tutorial…](https://www.kdnuggets.com/2021/12/federated-learning-collaborative-machine-learning-tutorial-get-started.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[联邦学习：带教程的协作机器学习…](https://www.kdnuggets.com/2021/12/federated-learning-collaborative-machine-learning-tutorial-get-started.html)'
- en: '[7 Beginner-Friendly Projects to Get You Started with ChatGPT](https://www.kdnuggets.com/2023/08/7-beginnerfriendly-projects-get-started-chatgpt.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7 个适合初学者的 ChatGPT 项目](https://www.kdnuggets.com/2023/08/7-beginnerfriendly-projects-get-started-chatgpt.html)'
- en: '[10 Free Must-Take Data Science Courses to Get Started](https://www.kdnuggets.com/10-free-must-take-data-science-courses-to-get-started)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10 门免费的必修数据科学课程以供入门](https://www.kdnuggets.com/10-free-must-take-data-science-courses-to-get-started)'
- en: '[Why Upskilling in Data Vis Matters (& How to Get Started)](https://www.kdnuggets.com/2022/07/sphere-upskilling-data-vis-matters.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么提升数据可视化技能很重要（& 如何入门）](https://www.kdnuggets.com/2022/07/sphere-upskilling-data-vis-matters.html)'
- en: '[3 Benefits to A/B Testing (+ Where to Get Started)](https://www.kdnuggets.com/2022/08/sphere-3-benefits-ab-testing-get-started.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[A/B 测试的 3 个好处（+ 从哪里开始）](https://www.kdnuggets.com/2022/08/sphere-3-benefits-ab-testing-get-started.html)'
