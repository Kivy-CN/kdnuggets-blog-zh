- en: 'Feature Selection: Where Science Meets Art'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择：科学与艺术的交汇点
- en: 原文：[https://www.kdnuggets.com/2021/12/feature-selection-science-meets-art.html](https://www.kdnuggets.com/2021/12/feature-selection-science-meets-art.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/12/feature-selection-science-meets-art.html](https://www.kdnuggets.com/2021/12/feature-selection-science-meets-art.html)
- en: '**By [Mahbubul Alam](https://www.linkedin.com/in/mab-alam/), Data Scientist,
    Economist and Quantitative Researcher**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Mahbubul Alam](https://www.linkedin.com/in/mab-alam/)，数据科学家、经济学家和定量研究员**'
- en: '![Figure](../Images/cb4fde334a43c5979b8e5d08f990d42a.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/cb4fde334a43c5979b8e5d08f990d42a.png)'
- en: Photo by [David Clode](https://unsplash.com/@davidclode?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [David Clode](https://unsplash.com/@davidclode?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行 IT 管理'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Some people say feature selection and engineering is the most important part
    of data science projects. In many cases it’s not sophisticated algorithms, rather
    it’s feature selection that makes all the difference in model performance.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有人说特征选择和工程是数据科学项目中最重要的部分。在许多情况下，不是复杂的算法，而是特征选择在模型性能中起到了决定性作用。
- en: Too few features can under-fit a model. For example, if you want to predict
    house prices, knowing just the number of bedrooms and floor area is not good enough.
    You are omitting many important variables a buyer cares about such as location,
    school district, property age etc.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 特征过少可能导致模型欠拟合。例如，如果你想预测房价，仅知道卧室数量和楼层面积是不够的。你忽略了买家关心的许多重要变量，例如位置、学区、房产年龄等。
- en: You can also come from the other direction and choose 100 different features
    that describe every tiny detail such as names of trees on the property. Instead
    of adding more information, these features add noise and complexity. Many of the
    features chosen might be outright irrelevant. On top of that, too many features
    add computational costs to train a model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以从另一个角度出发，选择100个不同的特征来描述每一个细节，例如物业上的树木名称。与其增加更多信息，这些特征却会增加噪声和复杂性。许多选择的特征可能根本不相关。此外，特征过多还会增加训练模型的计算成本。
- en: So to build a good predictive model, what is the right number of features and
    how to choose which features to keep and which features to drop and which new
    features to add? This is an important consideration in machine learning projects
    in managing what’s known as the [bias-variance tradeoff](https://towardsdatascience.com/bias-variance-tradeoff-in-machine-learning-an-intuition-da85228c5074).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，要构建一个好的预测模型，应该选择多少特征，如何决定保留哪些特征、丢弃哪些特征以及添加哪些新特征？这是在机器学习项目中管理所谓的[偏差-方差权衡](https://towardsdatascience.com/bias-variance-tradeoff-in-machine-learning-an-intuition-da85228c5074)时的重要考虑因素。
- en: This is also where “science” meets “arts.”
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是“科学”与“艺术”相遇的地方。
- en: The purpose of this article is to demystify feature selection techniques with
    some simple implementation. The techniques I’m describing below should equally
    work in regression and classification problems. Unsupervised classification (e.g.
    clustering) can be a bit tricky, so I’ll talk about it separately.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目的是揭示特征选择技术，并进行一些简单的实现。我在下面描述的技术应该在回归和分类问题中同样有效。无监督分类（例如聚类）可能有点棘手，所以我会单独讨论。
- en: Heuristic approach
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启发式方法
- en: 'We don’t talk much about heuristics in data science but it is quite relevant.
    Let’s look at the definition (source: [Wikipedia](https://en.wikipedia.org/wiki/Heuristic)):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在数据科学中不常谈论启发式方法，但它实际上是相当相关的。我们来看一下定义（来源：[Wikipedia](https://en.wikipedia.org/wiki/Heuristic)）：
- en: A heuristic or heuristic technique **…. **employs a practical method that is
    not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient
    for reaching an immediate, short-term goal or approximation.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 启发式或启发式技术**….**采用了一种不保证最优、完美或理性的实用方法，但仍足以达到即时的短期目标或近似值。
- en: This definition equally applies to feature selection, if done based on intuition.
    Just by looking at a dataset, you’ll get the gut feeling that such and such features
    are strong predictors and some others have nothing to do with the dependent variable
    and you feel that it’s safe to eliminate them.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果基于直觉进行特征选择，这个定义同样适用。仅通过观察数据集，你会有一种直觉，感觉某些特征是强预测因子，而其他一些与依赖变量无关，你会觉得可以安全地去除它们。
- en: If you are not sure, you can go a step further to check the correlation between
    features and the dependent variable.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不确定，可以进一步检查特征与依赖变量之间的相关性。
- en: With too many features in the dataset, just these heuristics— intuition and
    correlation — will get most of your job done in choosing the right features.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有太多特征时，仅凭这些启发式方法——直觉和相关性——就可以完成大部分选择合适特征的工作。
- en: As an example, let’s say your company is allocating budgets for advertising
    in different channels (TV, radio, newspaper). You want to predict which channel
    is most effective as an advertisement platform and what’s the expected return.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你的公司正在为不同渠道（电视、广播、报纸）分配广告预算。你想预测哪个渠道作为广告平台最有效，以及预期的回报是什么。
- en: Before you build a model you look at the historical data and found the following
    relationship between ad expenses on different platforms and corresponding sales
    revenue.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在你建立模型之前，你查看了历史数据，并发现了不同平台上的广告费用与相应销售收入之间的关系。
- en: '![](../Images/8c2c5f121cd9f6ffe2cc60f48d9be253.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c2c5f121cd9f6ffe2cc60f48d9be253.png)'
- en: 'Bivariate scatterplots showing relationships between sales and ad expenditure
    on different platforms (figure source: author; data source: [ISLR](https://rdrr.io/cran/ISLR/),
    license: GLP 2, public domain).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 双变量散点图显示了不同平台上的销售和广告支出之间的关系（图源：作者；数据源：[ISLR](https://rdrr.io/cran/ISLR/)，许可证：GLP
    2，公共领域）。
- en: Based on the scatterplots what do you think are the best features to explain
    ad revenue? Clearly, newspaper ads do not have any significant impact on the outcome,
    so you may want to drop it from the model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 基于散点图，你认为哪些特征最能解释广告收入？显然，报纸广告对结果没有显著影响，因此你可能想从模型中去除它。
- en: Automated feature selection
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化特征选择
- en: We will now get into automated feature selection techniques. Most of them are
    integrated within the `sklearn` module, so you can implement feature selection
    only in a few lines of code in a standard format.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将介绍自动化特征选择技术。大多数这些技术都集成在`sklearn`模块中，因此你只需用几行代码就能实现特征选择，格式也很标准。
- en: 'For demonstration, I’ll use the ‘iris’ dataset (source: [Kaggle/UCI Machine
    Learning](https://www.kaggle.com/uciml/iris/metadata), license: CC0 public domain).
    It’s a simple dataset and has only 5 columns, nevertheless, you’ll get the key
    points across.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 作为演示，我将使用‘iris’数据集（来源：[Kaggle/UCI机器学习](https://www.kaggle.com/uciml/iris/metadata)，许可证：CC0
    公共领域）。这是一个简单的数据集，只有5列，不过你会掌握关键点。
- en: Let’s load the dataset from `seaborn` library.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`seaborn`库加载数据集。
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/63644ab4ea21380679aaa4629cd6d2b4.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63644ab4ea21380679aaa4629cd6d2b4.png)'
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the dataset “species” is the one that we want to predict and the remaining
    4 columns are predictors. Let’s confirm the number of features programmatically:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中，“species”是我们想预测的目标，其余的4列是预测变量。让我们以编程方式确认特征的数量：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s now go ahead and implement a few feature selection techniques.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来实现一些特征选择技术。
- en: 1) Chi-squared based technique
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1) 基于卡方的技术
- en: The chi-squared-based technique selects a specific number of user-defined features
    (k) based on some scores. These scores are determined by computing chi-squared
    statistics between X (independent) and y (dependent) variables.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 基于卡方的技术根据一些评分选择用户定义的特征数量（k）。这些评分通过计算X（独立）和y（依赖）变量之间的卡方统计量来确定。
- en: '`sklearn` has built-in methods for chi-square-based feature selection. All
    you have to do is determine how many features you want to keep (let’s say, k=3
    for the iris dataset).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn`有内置的卡方特征选择方法。你需要做的就是确定要保留多少个特征（例如，对于鸢尾花数据集，k=3）。'
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let’s now confirm that we’ve got the 3 best features out of 4.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们确认我们从4个特征中选择了3个最佳特征。
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For a large number of features, you can rather specify a certain percentage
    of features you want to keep or drop. It works in a similar fashion as above.
    Let’s say we want to keep 75% of features and drop the remaining 25%.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大量特征，你可以指定希望保留或丢弃的特征百分比。这与上述方法类似。假设我们希望保留75%的特征，丢弃剩余的25%。
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 2) Impurity-based feature selection
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2) 基于杂质的特征选择
- en: Tree-based algorithms (e.g. Random Forest Classifier) have built-in `feature_importances_` attribute.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的算法（例如随机森林分类器）具有内置的`feature_importances_`属性。
- en: A Decision Tree would split data using a feature that decreases the impurity
    (measured in terms of [Gini impurity](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) or [information
    gain](https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain)).
    That means, finding the best feature is a key part of how the algorithm works
    to solve classification problems. We can then access the best features via `feature_importances_` attribute.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树会使用减少杂质的特征来划分数据（杂质的度量标准为[基尼杂质](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity)或[信息增益](https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain)）。这意味着，找到最佳特征是算法解决分类问题的关键部分。我们可以通过`feature_importances_`属性来访问最佳特征。
- en: Let’s first fit the “iris” dataset to a Random Forest Classifier with 200 estimators.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们将“鸢尾花”数据集拟合到具有200个估计器的随机森林分类器上。
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now let’s access feature importance by the attribute call.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过属性调用来访问特征的重要性。
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output above shows the importance of each of the 4 features at reducing
    impurity at each node/split.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出显示了每个特征在减少每个节点/分裂的杂质方面的重要性。
- en: Since the Random Forest Classifier has many estimators (e.g. 200 decision trees
    above), we can calculate an estimate of the relative importance with a confidence
    interval. Let’s visualize it.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随机森林分类器有许多估计器（例如，上述200棵决策树），我们可以计算相对重要性的估计值及其置信区间。让我们将其可视化。
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/b5061526f9889c48e31fcf0b514a0175.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5061526f9889c48e31fcf0b514a0175.png)'
- en: 'Figure: Importance of features in impurity measures (source: author)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图：特征在杂质度量中的重要性（来源：作者）
- en: Now that we know the importance of each feature, we can manually (or visually)
    determine which features to keep and which one to drop.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了每个特征的重要性，我们可以手动（或通过视觉）确定要保留哪些特征，删除哪些特征。
- en: Alternatively, we can take advantage of Scikit-Learn’s meta transformer `SelectFromModel` to
    do this job for us.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们可以利用Scikit-Learn的元转换器`SelectFromModel`来为我们完成这项工作。
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 3) Regularization
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3) 正则化
- en: Regularization is an important concept in machine learning to reduce overfitting
    (read: [Avoid Overfitting with Regularization](https://towardsdatascience.com/avoid-overfitting-with-regularization-6d459c13a61f)).
    If you have too many features, regularization controls their effect, either by
    shrinking feature coefficients (called L2 regularization/Ridge Regression) or
    by setting some feature coefficients to zero(called L1 regularization/LASSO Regression).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是机器学习中的一个重要概念，用于减少过拟合（详见：[通过正则化避免过拟合](https://towardsdatascience.com/avoid-overfitting-with-regularization-6d459c13a61f)）。如果特征过多，正则化可以控制它们的影响，方法是缩小特征系数（称为L2正则化/岭回归）或将某些特征系数设为零（称为L1正则化/LASSO回归）。
- en: Some linear models have built-in L1 regularization as a hyperparameter to penalize
    features. Those features can be eliminated using the meta transformer `SelectFromModel`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一些线性模型具有内置的L1正则化作为超参数来惩罚特征。这些特征可以通过元转换器`SelectFromModel`来消除。
- en: Let’s implement the`LinearSVC` algorithm with hyperparameter *penalty = ‘l1’*.
    We’ll then use `SelectFromModel`to remove some features.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现`LinearSVC`算法，超参数为*penalty = ‘l1’*。然后我们将使用`SelectFromModel`来移除一些特征。
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 4) Sequential selection
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4) 顺序选择
- en: Sequential feature selection is an age-old statistical technique. In this case,
    you add (or remove) features to/from the model one by one, and check your model
    performance, and then heuristically choose which one to keep.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序特征选择是一种古老的统计技术。在这种情况下，你一个接一个地将特征添加到模型中（或从模型中移除），检查模型性能，然后启发式地选择要保留的特征。
- en: Sequential selection has two variants. The *forward selection* technique starts
    with zero feature, then adds one feature which minimizes the error the most; then
    adds another feature, and so on. The *backward selection *works in the opposite
    direction. The model starts with all features and calculates error; then it eliminates
    one feature which minimizes error even further, and so on, until the desired number
    of features remains.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序选择有两种变体。*前向选择*技术从零特征开始，然后添加一个最小化误差的特征；然后再添加另一个特征，以此类推。*后向选择*则在相反的方向上工作。模型从所有特征开始并计算误差；然后它会删除一个进一步最小化误差的特征，以此类推，直到剩下所需数量的特征。
- en: Scikit-Learn module has `SequentialFeatureSelector `meta transformer to make
    life easier. Note that it works for `sklearn`v0.24 or later.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 模块有`SequentialFeatureSelector`元变换器来简化操作。请注意，它适用于`sklearn`v0.24或更高版本。
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Alternative techniques…**'
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**替代技术……**'
- en: Aside from the techniques I just described, there are few other methods you
    can try. Some of them are not exactly designed for feature selection, but if you
    dig a bit deeper you will find out how they can be creatively applied for feature
    selection.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我刚才描述的技术，还有一些其他方法可以尝试。其中一些方法并不是专门为特征选择设计的，但如果你深入挖掘，你会发现它们可以被创造性地应用于特征选择。
- en: '**Beta coefficients**: the coefficients you get after running a linear regression
    (the beta coefficients) show the relative sensitivity of the dependent variable
    to each feature. From here you can choose the features with high coefficient values.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Beta系数**：你在进行线性回归后得到的系数（Beta系数）显示了因变量对每个特征的相对敏感性。从这里你可以选择具有高系数值的特征。'
- en: '**p-value**: If you implement regression in a classical statistical package
    (e.g. `statsmodels` ), you’ll notice that the model output includes p-values for
    each feature ([check this out](https://www.statsmodels.org/stable/regression.html)).
    The p-value tests the null hypothesis that the coefficient is exactly zero. So
    you can eliminate the features associated with high p-values.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**p值**：如果你在经典统计软件包（例如`statsmodels`）中实现回归，你会发现模型输出包括每个特征的p值（[查看此内容](https://www.statsmodels.org/stable/regression.html)）。p值测试的是系数是否恰好为零的原假设。因此，你可以排除那些与高p值相关的特征。'
- en: '[**Variance Inflation Factor (VIF)**](https://etav.github.io/python/vif_factor_python.html):
    Typically VIF is used to detect multicollinearity in the dataset. Statisticians
    usually remove variables with high VIF to meet a key assumption of linear regression.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**方差膨胀因子（VIF）**](https://etav.github.io/python/vif_factor_python.html)：VIF通常用于检测数据集中的多重共线性。统计学家通常会删除具有高VIF的变量，以满足线性回归的关键假设。'
- en: '[**Akaike and Bayesian Information Criteria (AIC/BIC)**](https://en.wikipedia.org/wiki/Akaike_information_criterion):
    Generally AIC and BIC are used to compare performances between two models. But
    you can use it to your advantage for feature selection, for example by choosing
    certain features that get you a better model quality measured in terms of AIC/BIC.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**赤池信息量准则（AIC/BIC）**](https://en.wikipedia.org/wiki/Akaike_information_criterion)：通常AIC和BIC用于比较两个模型之间的性能。但你可以利用它们进行特征选择，例如选择某些特征以获得在AIC/BIC度量下更好的模型质量。'
- en: '**Principal Component Analysis (PCA)**: If you know what PCA is, you guessed
    it right. It’s not exactly a feature selection technique, but the dimensionality
    reduction properties of PCA can but used to that effect, without eliminating features
    entirely.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析（PCA）**：如果你知道PCA是什么，你猜对了。它不完全是特征选择技术，但PCA的降维特性可以用于这一效果，而不是完全消除特征。'
- en: '**And many others:** There are quite a few other feature selection classes
    that come with `sklearn` module, [check out the documentation](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection).
    A clustering-based algorithm has also been proposed recently in a [scientific
    paper](https://www.sciencedirect.com/science/article/pii/S2314717218300059). Fisher’s
    Score is yet another technique available.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**以及其他许多：** 有相当多的其他特征选择类，这些类随`sklearn`模块一起提供，[查看文档](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)。最近也在一篇[科学论文](https://www.sciencedirect.com/science/article/pii/S2314717218300059)中提出了一种基于聚类的算法。费舍尔评分（Fisher’s
    Score）也是另一种可用的技术。'
- en: How about clustering?
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类怎么样？
- en: Clustering is an unsupervised machine learning algorithm, meaning you feed your
    data into a clustering algorithm and the algorithm will figure out how to segment
    the data into different clusters based on some “property”. These properties actually
    come from the features.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种无监督的机器学习算法，这意味着你将数据输入到聚类算法中，算法会根据某些“属性”来确定如何将数据分成不同的簇。这些属性实际上来自于特征。
- en: Does clustering require feature selection? Of course. Without appropriate features,
    the clusters could be useless. Let’s say you want to segment customers in order
    to sell high-end, mid-range and low-end products. That means you are implicitly
    using *customer income* as a factor. You could also through *education *into the
    mix. Their *age *and years of *experience*? Sure. But as you increase the number
    of features, the algorithm becomes confused as to what you are trying to achieve
    and therefore the outputs may not be exactly what you were looking for.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是否需要特征选择？当然需要。没有合适的特征，聚类可能会毫无用处。假设你想要对客户进行细分，以便销售高端、中端和低端产品。这意味着你隐含地使用了*客户收入*作为一个因素。你还可以将*教育*因素考虑进去。他们的*年龄*和工作*经验*呢？当然。但随着特征数量的增加，算法可能会对你想要达到的目标感到困惑，因此输出结果可能不会完全符合你的期望。
- en: All that is to say, data scientists do not run clustering algorithms in a vacuum,
    they often have a hypothesis or question in mind. So the features must correspond
    to that need.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，数据科学家在运行聚类算法时并不是在孤立的环境中进行，他们通常有一个假设或问题在心中。因此，特征必须对应于这个需求。
- en: Summary
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Data scientists take feature selection very seriously because of the impact
    it has on model performance. For a low dimensional dataset heuristics and intuition
    work perfectly, however, for high dimensional data, there are automated techniques
    to do the job. Most useful techniques include chi-squared and impurity-based algorithms
    as well as regularization and sequential feature selection. In addition, there
    are alternative techniques that can be made useful such as beta coefficients in
    regression, p-value, VIF, AIC/BIC and dimensionality reduction.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家非常重视特征选择，因为它对模型性能有很大影响。对于低维数据集，启发式方法和直觉效果很好，但对于高维数据，有自动化技术可以完成这项工作。最有用的技术包括卡方检验和基于杂质的算法以及正则化和序列特征选择。此外，还有其他替代技术，如回归中的贝塔系数、p值、VIF、AIC/BIC和降维。
- en: In the title of this article, I said “science meets art”. It is because there’s
    no right or wrong answer when it comes to feature selection. We can use science
    tools, but in the end, it can be a subjective decision made by a data scientist.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章的标题中，我提到“科学遇见艺术”。这是因为特征选择没有对错之分。我们可以使用科学工具，但最终，可能是由数据科学家做出的主观决策。
- en: Thanks for reading. Feel free to [subscribe](https://mab-datasc.medium.com/subscribe) to
    be notified of my forthcoming articles or simply connect with me via [Twitter ](https://twitter.com/DataEnthus)or [LinkedIn](https://www.linkedin.com/in/mab-alam/).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读。欢迎 [订阅](https://mab-datasc.medium.com/subscribe) 以接收我即将发布的文章，或通过 [Twitter](https://twitter.com/DataEnthus) 或 [LinkedIn](https://www.linkedin.com/in/mab-alam/) 与我联系。
- en: '**Bio: [Mahbubul Alam](https://www.linkedin.com/in/mab-alam/)** has 8+ years
    of work experience applying data science in making policy and business decisions,
    and has worked closely with stakeholders in government and non-government organizations
    and helped make better decisions with data-driven solutions. Mab is passionate
    about statistical modeling and all aspects of data science – from survey design
    and data collection to building advanced predictive models. Mab is also teaching/mentoring
    a data science and business analytics course designed for early to mid-career
    analysts, team leads and IT professionals who are transitioning to data science
    or building data science capabilities within their organizations.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**简历：[Mahbubul Alam](https://www.linkedin.com/in/mab-alam/)** 拥有超过8年的数据科学工作经验，主要应用于政策和商业决策，并与政府和非政府组织的利益相关者密切合作，帮助通过数据驱动的解决方案做出更好的决策。Mab
    对统计建模和数据科学的各个方面充满热情——从调查设计和数据收集到构建先进的预测模型。Mab 还教授/指导了一门数据科学和商业分析课程，专为早期到中期职业分析师、团队负责人以及转型为数据科学的IT专业人员或在其组织内构建数据科学能力的人员设计。'
- en: '[Original](https://towardsdatascience.com/feature-selection-where-science-meets-art-639757d91293).
    Reposted with permission.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/feature-selection-where-science-meets-art-639757d91293)。经许可转载。'
- en: '**Related:**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Be Wary of Automated Feature Selection — Chi Square Test of Independence Example](/2021/08/automated-feature-selection-chi-square-test-independence-example.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[警惕自动特征选择 — 卡方独立性检验示例](/2021/08/automated-feature-selection-chi-square-test-independence-example.html)'
- en: '[Feature Selection – All You Ever Wanted To Know](/2021/06/feature-selection-overview.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[特征选择 – 你想了解的一切](/2021/06/feature-selection-overview.html)'
- en: '[From Scratch: Permutation Feature Importance for ML Interpretability](/2021/06/from-scratch-permutation-feature-importance-ml-interpretability.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从头开始：ML 可解释性的排列特征重要性](/2021/06/from-scratch-permutation-feature-importance-ml-interpretability.html)'
- en: More On This Topic
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Alternative Feature Selection Methods in Machine Learning](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的替代特征选择方法](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
- en: '[Advanced Feature Selection Techniques for Machine Learning Models](https://www.kdnuggets.com/2023/06/advanced-feature-selection-techniques-machine-learning-models.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习模型的高级特征选择技术](https://www.kdnuggets.com/2023/06/advanced-feature-selection-techniques-machine-learning-models.html)'
- en: '[Feature Store Summit 2022: A free conference on Feature Engineering](https://www.kdnuggets.com/2022/10/hopsworks-feature-store-summit-2022-free-conference-feature-engineering.html)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[特征存储峰会2022：关于特征工程的免费会议](https://www.kdnuggets.com/2022/10/hopsworks-feature-store-summit-2022-free-conference-feature-engineering.html)'
- en: '[Retrieval Augmented Generation: Where Information Retrieval Meets…](https://www.kdnuggets.com/retrieval-augmented-generation-where-information-retrieval-meets-text-generation)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[检索增强生成：信息检索与文本生成的结合](https://www.kdnuggets.com/retrieval-augmented-generation-where-information-retrieval-meets-text-generation)'
- en: '[Linear Regression Model Selection: Balancing Simplicity and Complexity](https://www.kdnuggets.com/2023/02/linear-regression-model-selection-balancing-simplicity-complexity.html)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性回归模型选择：平衡简洁性与复杂性](https://www.kdnuggets.com/2023/02/linear-regression-model-selection-balancing-simplicity-complexity.html)'
- en: '[Ensuring Reliable Few-Shot Prompt Selection for LLMs](https://www.kdnuggets.com/2023/07/ensuring-reliable-fewshot-prompt-selection-llms.html)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[确保 LLMs 的可靠少量样本提示选择](https://www.kdnuggets.com/2023/07/ensuring-reliable-fewshot-prompt-selection-llms.html)'
