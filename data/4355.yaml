- en: 'Data Compression via Dimensionality Reduction: 3 Main Methods'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过维度缩减的数据压缩：3 种主要方法
- en: 原文：[https://www.kdnuggets.com/2020/12/data-compression-dimensionality-reduction.html](https://www.kdnuggets.com/2020/12/data-compression-dimensionality-reduction.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/12/data-compression-dimensionality-reduction.html](https://www.kdnuggets.com/2020/12/data-compression-dimensionality-reduction.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '![](../Images/b9897b1c6998c7f5aab989b3f5ac3a1e.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9897b1c6998c7f5aab989b3f5ac3a1e.png)'
- en: '*Photo by [Anna Tarazevich](https://www.pexels.com/@anntarazevich?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels) from [Pexels](https://www.pexels.com/photo/gray-and-black-metal-machine-5963136/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels).*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*照片来自 [Anna Tarazevich](https://www.pexels.com/@anntarazevich?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels) ，来自 [Pexels](https://www.pexels.com/photo/gray-and-black-metal-machine-5963136/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)。*'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织中的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Why reduce the dimensions of data?
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么要减少数据的维度？
- en: 'If you are in data science for quite some time, you must have heard this phrase:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在数据科学领域工作了一段时间，你一定听说过这个短语：
- en: '*Dimensionality is a curse.*'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*维度是一种诅咒。*'
- en: This is also referred to as the curse of dimensionality. You can learn more
    about this term [here](https://en.wikipedia.org/wiki/Curse_of_dimensionality).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这也被称为维度灾难。你可以在[这里](https://en.wikipedia.org/wiki/Curse_of_dimensionality)了解更多关于这个术语的信息。
- en: 'Overall, some of the common disadvantages of high dimensional data are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，高维数据的一些常见缺点包括：
- en: Overfitting the model
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型过拟合
- en: Difficulties in [clustering algorithms](https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e)
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[聚类算法](https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e)中的困难'
- en: Very hard to visualize
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 非常难以可视化
- en: There might be some useless features in your data
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你的数据中可能存在一些无用的特征
- en: Complex and costly models
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复杂且昂贵的模型
- en: There are other disadvantages too, which you can search and look for details
    on Google.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他缺点，你可以在 Google 上搜索并查看详细信息。
- en: In this article, we are going to discuss 3 important and famous techniques,
    which will help you in reducing the dimensions of your data while maintaining
    useful features or information.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将讨论三种重要而著名的技术，这些技术将帮助你在减少数据维度的同时保持有用的特征或信息。
- en: These 3 essential techniques are divided into 2 parts.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种基本技术分为两部分。
- en: '**Linear dimensionality reduction**'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**线性维度缩减**'
- en: PCA (Principal Component Analysis)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA（主成分分析）
- en: LDA (Linear Discriminant Analysis)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDA（线性判别分析）
- en: '**Non-linear dimensionality reduction**'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**非线性维度缩减**'
- en: KPCA (Kernel Principal Component Analysis)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KPCA（核主成分分析）
- en: We will discuss the basic idea behind each technique, practical implementation
    in sklearn, and the results of each technique.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论每种技术的基本思想、在 sklearn 中的实际应用，以及每种技术的结果。
- en: PCA (Principal Component Analysis)
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PCA（主成分分析）
- en: Principal Component Analysis is one of the most famous data compression technique
    that is used for unsupervised data compression.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析是用于无监督数据压缩的最著名数据压缩技术之一。
- en: PCA helps us to identify the patterns in the dataset based on the correlation
    between them. Or simply, it is a technique for feature extraction that combines
    our input variables in such a way that we can drop the least important ones while
    retaining the important information in the dataset. PCA finds the direction of
    the maximum variance and projects the data into lower dimensions. The principal
    components of the new subspace can be interpreted as the direction of maximum
    variance given the constraint that the new feature axes are orthogonal to each
    other.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: PCA帮助我们识别数据集中的模式，基于它们之间的相关性。或者简单来说，它是一种特征提取技术，通过这种方式将输入变量结合起来，使我们能够丢弃不重要的特征，同时保留数据集中重要的信息。PCA找到最大方差的方向，并将数据投影到较低维度的空间中。新子空间的主成分可以解释为在新特征轴彼此正交的约束下的最大方差方向。
- en: '![](../Images/998e450e556e6774afaf52cbf8c0d94f.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/998e450e556e6774afaf52cbf8c0d94f.png)'
- en: '*Image Credits: [Python Machine Learning repo](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/ch05.ipynb).*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片来源：[Python机器学习repo](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/ch05.ipynb).*'
- en: Here, x1 and x2 are the original feature axes, and PC1 and PC2 are the principal
    components.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，x1 和 x2 是原始特征轴，PC1 和 PC2 是主成分。
- en: When using PCA for dimensionality reduction, we construct a transformation matrix **W**,
    which is ***d *x *k ***dimensions, that allows us to map a sample vector **x** onto
    a new k-dimensional feature subspace that has fewer dimensions than the original *d*–dimensional
    feature space.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用PCA进行降维时，我们构建一个变换矩阵 **W**，其维度为 ***d *x *k***，这允许我们将样本向量 **x** 映射到一个新的 k 维特征子空间中，该子空间的维度比原始
    *d* 维特征空间要少。
- en: '![](../Images/fb838787031f2de697d85a1028c5ab3c.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb838787031f2de697d85a1028c5ab3c.png)'
- en: Typically *k* is very less than *d*, so the first principal component has the
    maximum variance, and so on.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，*k* 远小于 *d*，因此第一个主成分具有最大的方差，以此类推。
- en: PCA is highly sensitive to data scaling, so before using PCA, we have to standardize
    our features and bring them on the same scale.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: PCA对数据缩放非常敏感，因此在使用PCA之前，我们必须标准化特征，并将它们调整到相同的尺度。
- en: PCA is simple to implement from scratch in Python, and it is given as a built-in
    function in sklearn. To check a from scratch implementation, refer to this [repo](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/ch05.ipynb).
    We will review the implementation in sklearn.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: PCA在Python中从头实现很简单，且在sklearn中提供了内置函数。要查看从头实现的代码，请参考这个 [repo](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/ch05.ipynb)。我们将回顾sklearn中的实现。
- en: '**Sklearn Code:**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sklearn代码：**'
- en: First of all, we have to import a dataset and preprocess it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须导入数据集并进行预处理。
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/56e87c85a384463ff69a0cb3a09baf28.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56e87c85a384463ff69a0cb3a09baf28.png)'
- en: This is our dataframe with class labels (1,2,3) in 0th column and features in
    1–13 columns. Let’s assign those to **X** and **y**.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的数据框，其中第0列包含类别标签（1,2,3），第1到13列包含特征。让我们将它们分别分配给 **X** 和 **y**。
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now let’s split the dataset into training and testing portions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将数据集拆分为训练集和测试集。
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now since we have split the dataset, let’s apply a Standard Scaler to standardize
    our features.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经分割了数据集，让我们应用标准化缩放器来标准化我们的特征。
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, all we have to do is to perform PCA on it.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们要做的就是对它执行PCA。
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, we can pass either how much percent of variance do we want to keep or the
    number of components. For example, if we want to store 80% of the information
    on our data, we can do *pca = PCA(n_components=0.8)*, or if we want to have 4
    features in our dataset, we can do *pca = PCA(n_components=4)*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以传递我们想要保留多少百分比的方差，或者主成分的数量。例如，如果我们想要保留数据的80%信息，我们可以使用 *pca = PCA(n_components=0.8)*，或者如果我们希望数据集中有4个特征，我们可以使用
    *pca = PCA(n_components=4)*。
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/134bf39927377d23763de3a0884777da.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/134bf39927377d23763de3a0884777da.png)'
- en: And we can see that we have jumped from 13 features to 2 features. In order
    to check how much information we are saving, we can do
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们已经从13个特征减少到2个特征。为了检查我们保存了多少信息，我们可以做
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/8f6e36b2be04cb61c8be2b5b6cdbfc4f.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f6e36b2be04cb61c8be2b5b6cdbfc4f.png)'
- en: For visualization purposes, we did 2 features, as 2 features can be easily visualized.
    And we can see that the first feature has ~36% of the information and 2nd feature
    has ~18% of the information. So we lost 11 features while we still have 55% of
    the information, which is really cool.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化目的，我们使用了两个特征，因为两个特征可以很容易地进行可视化。我们可以看到，第一个特征有约36%的信息，而第二个特征有约18%的信息。因此，我们丢失了11个特征，但仍然保留了55%的信息，这真的很酷。
- en: Let’s plot these 2 features.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这两个特征。
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/6309aff29d0d1306210507bc3c65c3d0.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6309aff29d0d1306210507bc3c65c3d0.png)'
- en: Now you can apply any Machine Learning algorithm, and it will converge a lot
    faster than your algorithm, which was initially trained on 13 features.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以应用任何机器学习算法，它将比最初在13个特征上训练的算法收敛得更快。
- en: Similarly, if I want to retain 80% of the information, all I have to do is
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果我想保留80%的信息，我只需要做的是
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now we can check the dimensions of *X_train_pca*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以检查*X_train_pca*的维度。
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: And first 5 rows of the resultant dataset are
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 结果数据集的前5行是
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/038547e905f755db309fb941f07bb0d5.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/038547e905f755db309fb941f07bb0d5.png)'
- en: And the explained variance ratio via
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过解释方差比
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/3b08d381abeb51d338525491f5202e70.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b08d381abeb51d338525491f5202e70.png)'
- en: '**How to choose the number of dimensions in PCA**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何选择PCA中的维度数**'
- en: In practice, when you have good computational power, we normally use Grid Search
    to find the optimum number of hyperparameters, and we also use it to find the
    number of principal components, which give the best performance.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，当你拥有良好的计算能力时，我们通常使用网格搜索来找到最佳超参数数量，也用它来找到提供最佳性能的主成分数量。
- en: If we lack good computational power, we have to choose the number of principal
    components by a tradeoff between the performance of the classifier and the computational
    efficiency.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果计算能力不足，我们必须在分类器性能和计算效率之间权衡，以选择主成分的数量。
- en: Supervised Data Compression via Linear Discriminant Analysis (LDA)
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过线性判别分析（LDA）进行监督数据压缩
- en: LDA or Linear Discriminant Analysis is one of the famous supervised data compressions.
    As the name supervised might have given you the idea, it takes into account the
    class labels that are absent in PCA. It is also a linear transformation technique,
    just like PCA**.**
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: LDA或线性判别分析是著名的监督数据压缩方法之一。正如“监督”这个词可能已经给你暗示的那样，它考虑了PCA中缺失的类别标签。它也是一种线性变换技术，就像PCA**一样**。
- en: LDA uses quite a similar approach to PCA*, *in the sense that we decompose the
    matrix into eigenvectors and eigenvalues, which then forms a lower-dimensional
    feature space. However, it uses a supervised approach, meaning that it takes into
    account the class labels.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 使用的方法与PCA*相似*，即我们将矩阵分解为特征值和特征向量，然后形成一个低维特征空间。然而，它使用了一种监督方法，意味着它考虑了类别标签。
- en: Before applying LDA to our data, we assume that our data is normally distributed,
    the classes have identical covariance matrices, and the training examples are
    statistically independent of each other. It has been proved from some research
    papers that if one, or more, of these assumptions are violated slightly, LDA still
    works pretty well.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数据应用LDA之前，我们假设数据是正态分布的，类别具有相同的协方差矩阵，并且训练样本彼此统计独立。一些研究论文证明，如果这些假设中的一个或多个假设稍微被违反，LDA仍然表现良好。
- en: '![](../Images/82295939649572b0e9526ad02361abfe.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82295939649572b0e9526ad02361abfe.png)'
- en: '*Source: [Python Machine Learning Github Repo](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/ch05.ipynb).*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*来源：[Python机器学习Github Repo](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/ch05.ipynb)。*'
- en: This graphic summarizes the concept of LDA for the 2-class problem, where circles
    are class 1 and + are class 2\. A linear discriminant LD 1 (*x*-axis) would separate
    the 2 normally distributed classes well. Whereas the linear discriminant LD 2
    captures a lot of variance in the dataset, it would fail as it would not be able
    to gather any class discrimination information.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图形总结了2类问题的LDA概念，其中圆圈代表类别1，+代表类别2。线性判别LD 1（*x*-轴）将很好地分离这两个正态分布的类别。而线性判别LD 2捕获了数据集中的大量方差，但由于无法获得任何类别区分信息，它会失败。
- en: In LDA, what we basically do is compute the within-class and between-class scatter
    matrices. Then, we compute the eigenvectors and corresponding eigenvalues for
    the scatter matrices, sort them in decreasing order, and select the top ***k ***eigenvectors
    that correspond to the ***k*** largest eigenvalues for constructing a ***d ***x ***k***-dimensional
    transformation matrix ***W***. We then project the examples in new feature subspace.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LDA 中，我们基本上是计算类内和类间散布矩阵。然后，我们计算散布矩阵的特征向量和相应的特征值，对其按降序排序，并选择前 ***k*** 个特征向量，这些特征向量对应于
    ***k*** 个最大特征值，用于构建 ***d*** x ***k*** 维的变换矩阵 ***W***。然后，我们将样本投影到新的特征子空间中。
- en: 'We can compute the ***within-class scatter matrix*** using the following formula:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下公式计算 ***类内散布矩阵***：
- en: '![](../Images/f06bed5d4e46a7e98a3b78d3953a0f94.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f06bed5d4e46a7e98a3b78d3953a0f94.png)'
- en: 'where **c **is the total number of distinct classes, and ***Si ***is the individual
    scatter matrix of class ***i ***calculated via:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 **c** 是不同类别的总数，***Si*** 是类 ***i*** 的个别散布矩阵，通过以下公式计算：
- en: '![](../Images/eaa08685135955aef33b03f51bc96380.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eaa08685135955aef33b03f51bc96380.png)'
- en: where ***mi ***is the mean vector that stores the mean feature value **μm **with
    respect to the example of class i.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ***mi*** 是存储类 i 的样本的均值特征值 **μm** 的均值向量。
- en: '![](../Images/244fda843a7a5a1b3c270ce122742b24.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/244fda843a7a5a1b3c270ce122742b24.png)'
- en: Similarly, we can compute the between-class scatter matrix **S*b:***
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以计算类间散布矩阵 **S*b:***
- en: '![](../Images/e37246eb170a979ab81c82b1ab33632a.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e37246eb170a979ab81c82b1ab33632a.png)'
- en: Here, **m** is the overall mean that is computed, including examples from all **c **classes.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**m** 是计算出的整体均值，包括所有 **c** 类别的样本。
- en: This whole work is not that difficult in Python and Numpy, and a lot of people
    have already done it. If you want to look at in detailed from-scratch implementation,
    have a look at this [repo](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/ch05.ipynb).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 和 Numpy 中，整个工作并不难，很多人已经完成了。如果你想详细查看从零实现的细节，可以看看这个 [repo](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/ch05.ipynb)。
- en: Let’s have a look at its implementation in sklearn in Python.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它在 Python 的 sklearn 中的实现。
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*LinearDiscriminantAnalysis*is a class implemented in sklearn’s *discriminant_analysis*
    package. You can have a look at the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*LinearDiscriminantAnalysis* 是在 sklearn 的 *discriminant_analysis* 包中实现的一个类。你可以在
    [这里](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)
    查看文档。'
- en: The first step is to create an LDA object.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建一个 LDA 对象。
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: An important thing to notice here is that in *fit_transform* function, we are
    passing the labels of the data set, and, as discussed earlier, it is a supervised
    algorithm.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的一点是，在 *fit_transform* 函数中，我们传递了数据集的标签，并且正如前面讨论的，这是一个监督算法。
- en: Here we haven’t set *n_components* a parameter, so it will automatically decide
    to pick from the formula *min(n_features, n_classes — 1)*. If we implicitly provide
    any value greater than this, it will give us an error. Check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) for
    more details.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们没有设置 *n_components* 参数，因此它会自动决定从公式 *min(n_features, n_classes — 1)* 中选择。如果我们隐式提供了大于此的任何值，它将给我们一个错误。有关更多详细信息，请查看
    [文档](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)。
- en: We can check out the resultant shape via
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式检查结果形状
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Since it is in 2 dimensions, we can easily plot all the classes. We will use
    the same code as above for plotting.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它是二维的，我们可以轻松地绘制所有类别。我们将使用上面相同的代码进行绘图。
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/7ca5199dc77b85d9d5ed36861f2dfaac.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ca5199dc77b85d9d5ed36861f2dfaac.png)'
- en: Now, as we have compressed the data, we can easily apply any machine learning
    algorithm to it. We can see that this data is easily linearly separable, so Logistic
    Regression would give us quite a good accuracy.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于我们已经压缩了数据，我们可以轻松地对其应用任何机器学习算法。我们可以看到这些数据是容易线性可分的，因此逻辑回归会给我们相当不错的准确性。
- en: One thing we have to note in LDA via sklearn is that we can not provide *n_components* in
    probabilities as we can do in PCA. It must be an integer, and it must fulfill
    the condition *min(n_features, n_classes — 1)*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在使用sklearn的LDA时需要注意的一点是，我们不能像在PCA中那样以概率形式提供*n_components*。它必须是整数，并且必须满足条件*min(n_features,
    n_classes — 1)*。
- en: LDA vs PCA
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LDA与PCA
- en: One thought you might have encountered is that since LDA also keeps in mind
    the class labels as its supervised algorithm, the results in some cases show that
    PCA performs better than LDA, such as in tasks like image recognition when each
    class consists of a small number of examples (PCA Vrtsis LDA, A. M. Martinez,
    and A. C. Kak, *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    23(2)L 228-233, 2001).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '你可能会遇到这样的想法：由于LDA也考虑了类标签作为其监督算法，结果在某些情况下显示PCA比LDA表现更好，比如在每个类包含少量示例的图像识别任务中（PCA
    Vs LDA, A. M. Martinez, and A. C. Kak, *IEEE Transactions on Pattern Analysis
    and Machine Intelligence*, 23(2): 228-233, 2001）。'
- en: Kernel PCA
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核PCA
- en: As discussed earlier, both PCA and LDA are linear dimensionality reduction techniques.
    Similarly, most machine learning algorithms make assumptions about the linear
    separability of the data to converge perfectly.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，PCA和LDA都是线性降维技术。同样，大多数机器学习算法对数据的线性可分性做出假设，以便完美收敛。
- en: But the real-world is not always linear, and most of the time, you have to deal
    with nonlinear datasets. So, we can see that linear dimensionality reduction techniques
    might not be the best choice for these kinds of datasets.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 但现实世界并不总是线性的，大多数时候，你必须处理非线性数据集。因此，我们可以看到线性降维技术可能不是这些数据集的最佳选择。
- en: '![](../Images/9fb0f7695219f669790a579b6fa26c6f.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9fb0f7695219f669790a579b6fa26c6f.png)'
- en: '*Image Credits: [statistics4u](http://www.statistics4u.info/fundstat_eng/cc_linvsnonlin.html).*'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*图片来源： [statistics4u](http://www.statistics4u.info/fundstat_eng/cc_linvsnonlin.html)。*'
- en: Using KPCA, we can transform the data that is nonlinear to a linear subspace
    on which classifiers perform really well.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用KPCA，我们可以将非线性的数据显示到一个线性子空间中，在这个子空间上分类器表现得非常好。
- en: The main idea behind KPCA is that we have to perform a nonlinear mapping that
    transforms the data into a higher-dimensional space.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: KPCA背后的主要思想是我们必须执行一个非线性映射，将数据转换到一个更高维的空间。
- en: In other words, we transform our *d-*dimensional data to this higher *k*-dimensional
    space, and we define a nonlinear mapping function ϕ.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们将我们的*d*-维数据转换到这个更高的*k*-维空间，并定义一个非线性映射函数ϕ。
- en: '![](../Images/dfc7cdbe585a28cbf65933e54edd493d.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfc7cdbe585a28cbf65933e54edd493d.png)'
- en: Think of ϕ as a function that maps the original *d*-dimensional dataset onto
    a larger, *k*-dimensional dataset via some nonlinear mappings.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 把ϕ看作一个函数，它通过一些非线性映射将原始*d*-维数据集映射到更大的*k*-维数据集上。
- en: In other words, we perform a nonlinear mapping to transform the dataset to a
    higher dimension where we use standard PCA to project the data back to a lower-dimension
    linearly separable space.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们执行一个非线性映射，将数据集转换到一个更高维度的空间，在那里我们使用标准PCA将数据投影回到一个较低维度的线性可分空间。
- en: One downside of this approach is that it is very computationally expensive.
    To overcome this problem, we have to use the **kernel trick**, via which we can
    compute the similarity between two high-dimension feature vectors in the original
    feature space.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个缺点是计算成本非常高。为了克服这个问题，我们必须使用**核技巧**，通过它我们可以计算原始特征空间中两个高维特征向量之间的相似性。
- en: Now the mathematical details and intuition behind the kernel trick are difficult
    to explain in a written manner. I suggest you watch [this](https://www.youtube.com/watch?v=HbDHohXPLnU&t=30s&ab_channel=caltech) video
    by the California Institute of Technology via which you can develop a good understanding
    of the mathematical intuition behind KPCA and kernel trick.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，核技巧背后的数学细节和直觉很难用书面方式解释。我建议你观看加州理工学院的[this](https://www.youtube.com/watch?v=HbDHohXPLnU&t=30s&ab_channel=caltech)视频，通过它你可以很好地理解KPCA和核技巧背后的数学直觉。
- en: If you want to look at the implementation in Python and scipy, I recommend you
    to go through this [repo](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/ch05.ipynb) where
    it is implemented step by step.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看Python和scipy中的实现，我推荐你查看这个[repo](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/ch05.ipynb)，其中逐步实现了这个过程。
- en: We will have a look at its implementation in sklearn in Python. What we are
    going to do is to convert a nonlinear 2-D dataset to a linear 2-D dataset. Remember,
    what KPCA will do is to map it to a higher dimension and then map it to a linearly
    separable lower dimension.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看它在Python的sklearn中的实现。我们要做的是将一个非线性2-D数据集转换为线性2-D数据集。请记住，KPCA会将其映射到更高维度，然后再映射到线性可分的较低维度。
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here we have imported important libraries and datasets.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们导入了重要的库和数据集。
- en: '[PRE17]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](../Images/1d93d490ca9f2ac7e5091dd704b5a48d.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d93d490ca9f2ac7e5091dd704b5a48d.png)'
- en: And we can see that our dataset has 100 rows and 2 columns. Let us see the number
    of classes and their values in our dataset.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到数据集有100行和2列。让我们查看数据集中类别的数量及其值。
- en: '[PRE18]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/d918d880187905fe339437ea00b51be8.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d918d880187905fe339437ea00b51be8.png)'
- en: Since we have 2 classes, we can easily plot them.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有2个类别，我们可以轻松绘制它们。
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/0d3408d938b02e932e12978aceb3dbdc.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d3408d938b02e932e12978aceb3dbdc.png)'
- en: Here we can see that both classes are not linearly separable. We can confirm
    it by using any linear classifier.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们可以看到两个类别并不线性可分。我们可以通过任何线性分类器来确认。
- en: '![](../Images/db490ba5efb40d88365394a91f330dc9.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db490ba5efb40d88365394a91f330dc9.png)'
- en: Hence, to make it linearly separable, we will cast it to a higher dimension,
    then map it back to a lower dimension, all using KPCA.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了使其线性可分，我们将其转换为更高维度，然后再映射回较低维度，全部使用KPCA。
- en: '[PRE20]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here we have used the *kernel='rbf' *and *gamma=15*. There are different kernel
    functions that are to be used. Some of them are *linear*, *poly*, and *rbf*. Similarly, *gamma *is
    the kernel coefficient for the rbf, poly, and sigmoid kernels. There is a detailed
    answer on stats.StackExchange about which kernel to choose that you can read [here](https://stats.stackexchange.com/questions/131142/how-to-choose-a-kernel-for-kernel-pca).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们使用了*kernel='rbf'*和*gamma=15*。有不同的核函数可供使用。它们包括*linear*、*poly*和*rbf*。同样，*gamma*是rbf、poly和sigmoid核的核系数。关于选择哪个核的详细答案可以在stats.StackExchange上阅读，[点击这里](https://stats.stackexchange.com/questions/131142/how-to-choose-a-kernel-for-kernel-pca)。
- en: Now we simply have to transform our data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只需对数据进行变换。
- en: '[PRE21]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Let us plot to check whether it is Linearly separable now or not.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制图形以检查现在是否线性可分。
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/ccde042b4b9a559c7fae58b67bb925ad.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ccde042b4b9a559c7fae58b67bb925ad.png)'
- en: And we can see that our data is now linearly separable, and we can even confirm
    it via any linear classifier.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的数据现在是线性可分的，甚至可以通过任何线性分类器来确认。
- en: '![](../Images/e448dd009ed8dfadd92f53c1e41ad108.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e448dd009ed8dfadd92f53c1e41ad108.png)'
- en: In this example, we have converted a nonlinear dataset into a linear dataset
    by using KPCA. In the coming example, we will decrease the dimensions of the wine
    dataset that we used previously in PCA and LDA.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们通过使用KPCA将一个非线性数据集转换为线性数据集。在接下来的例子中，我们将减少之前在PCA和LDA中使用的葡萄酒数据集的维度。
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: And this will transform our previous 13-dimensional data into 2 dimensional
    linearly separable data. We can now use any linear classifier to get good decision
    boundaries and good results.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把我们之前的13维数据转变为2维线性可分的数据。我们现在可以使用任何线性分类器来获得良好的决策边界和良好的结果。
- en: '![](../Images/402c6857fad92a61078d5b0314c41b95.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/402c6857fad92a61078d5b0314c41b95.png)'
- en: '**Learning Outcomes**'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习成果**'
- en: In this article, you learned the basics of 3 dimensionality reduction techniques
    in which 2 are linear, and 1 is nonlinear or uses the kernel trick. You have also
    learned their implementation in one of the most famous Python libraries, sklearn.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你学习了3种维度降低技术的基础，其中2种是线性的，1种是非线性的或使用了核技巧。你还学习了它们在最著名的Python库sklearn中的实现。
- en: '**Related:**'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Dimensionality Reduction with Principal Component Analysis (PCA)](https://www.kdnuggets.com/2020/05/dimensionality-reduction-principal-component-analysis.html)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[主成分分析（PCA）中的维度降低](https://www.kdnuggets.com/2020/05/dimensionality-reduction-principal-component-analysis.html)'
- en: '[What Is Dimension Reduction In Data Science?](https://www.kdnuggets.com/2019/01/dimension-reduction-data-science.html)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中的维度降低是什么？](https://www.kdnuggets.com/2019/01/dimension-reduction-data-science.html)'
- en: '[Must-Know: What is the curse of dimensionality?](https://www.kdnuggets.com/2017/04/must-know-curse-dimensionality.html)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[必知：什么是维度诅咒？](https://www.kdnuggets.com/2017/04/must-know-curse-dimensionality.html)'
- en: More On This Topic
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Dimensionality Reduction Techniques in Data Science](https://www.kdnuggets.com/2022/09/dimensionality-reduction-techniques-data-science.html)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中的降维技术](https://www.kdnuggets.com/2022/09/dimensionality-reduction-techniques-data-science.html)'
- en: '[AI, Analytics, Machine Learning, Data Science, Deep Learning…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能、分析、机器学习、数据科学、深度学习…](https://www.kdnuggets.com/2021/12/developments-predictions-ai-machine-learning-data-science-research.html)'
- en: '[Data Science & Analytics Industry Main Developments in 2021 and Key…](https://www.kdnuggets.com/2021/12/developments-predictions-data-science-analytics-industry.html)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学与分析行业在2021年的主要发展和关键…](https://www.kdnuggets.com/2021/12/developments-predictions-data-science-analytics-industry.html)'
- en: '[Main 2021 Developments and Key 2022 Trends in AI, Data Science,…](https://www.kdnuggets.com/2021/12/trends-ai-data-science-ml-technology.html)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2021年的主要发展及2022年人工智能、数据科学的关键趋势](https://www.kdnuggets.com/2021/12/trends-ai-data-science-ml-technology.html)'
- en: '[Data Science Methods Drive Business Success](https://www.kdnuggets.com/2023/10/nwu-data-science-methods-drive-business-success)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学方法推动业务成功](https://www.kdnuggets.com/2023/10/nwu-data-science-methods-drive-business-success)'
- en: '[Centroid Initialization Methods for k-means Clustering](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[k-means聚类的质心初始化方法](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
