- en: Explainable Artificial Intelligence (Part 2) – Model Interpretation Strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/12/explainable-ai-model-interpretation-strategies.html](https://www.kdnuggets.com/2018/12/explainable-ai-model-interpretation-strategies.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](/2018/12/explainable-ai-model-interpretation-strategies.html?page=2#comments)![Figure](../Images/259329bfc9d56d238c06bba517984824.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Pixabay'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This article is a continuation in my series of articles aimed at ***‘Explainable
    Artificial Intelligence (XAI)’***. If you haven’t checked out the first article,
    I would definitely recommend you to take a quick glance at [***‘Part I — The Importance
    of Human Interpretable Machine Learning’***](https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476)which
    covers the what and why of human interpretable machine learning and the need and
    importance of model interpretation along with its scope and criteria. In this
    article, we will be picking up from where we left off and expand further into
    the criteria of machine learning model interpretation methods and explore techniques
    for interpretation based on scope. The aim of this article is to give you a good
    understanding of existing, traditional model interpretation methods, their limitations
    and challenges. We will also cover the classic model accuracy vs. model interpretability
    trade-off and finally take a look at the major strategies for model interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: Briefly, we will be covering the following aspects in this article.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional Techniques for Model Interpretation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges and Limitations of Traditional Techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Accuracy vs. Interpretability trade-off
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Interpretation Techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This should get us set and ready for the detailed hands-on guide to model interpretation
    coming in Part 3, so stay tuned!
  prefs: []
  type: TYPE_NORMAL
- en: Traditional Techniques for Model Interpretation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model interpretation at heart, is to find out ways to understand model decision
    making policies better. This is to enable fairness, accountability and transparency
    which will give humans enough confidence to use these models in real-world problems
    which a lot of impact to business and society. Hence, there are techniques which
    have existed for a long time now, which can be used to understand and interpret
    models in a better way. These can be grouped under the following two major categories.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploratory analysis and visualization techniques** like *clustering *and *dimensionality
    reduction*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model performance evaluation metrics** like [*precision, recall, accuracy*](https://en.wikipedia.org/wiki/Confusion_matrix)*, *[*ROC
    curve and the AUC*](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) (for
    classification models) and the [*coefficient of determination (R-square)*](https://en.wikipedia.org/wiki/Coefficient_of_determination)*, *[*root
    mean-square error, mean absolute error*](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d) (for
    regression models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s briefly take a more detailed look at these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploratory Analysis and Visualization**'
  prefs: []
  type: TYPE_NORMAL
- en: The idea of exploratory analysis is not something entirely new. For years now,
    data visualization has been one of the most effective tools for getting latent
    insights from data. Some of these techniques can help us in identifying key features
    and meaningful representations from our data which can give an indication of what
    might be influential for a model to take decisions in a human-interpretable form.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction techniques are very useful here since often we deal
    with a very large feature space (curse of dimensionality) and reducing the feature
    space helps us visualize and see what might be influencing a model to take specific
    decisions. Some of these techniques are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dimensionality reduction: **[Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis), [Self-organizing
    maps (SOM)](https://en.wikipedia.org/wiki/Self-organizing_map), [Latent Semantic
    Indexing](https://nlp.stanford.edu/IR-book/html/htmledition/latent-semantic-indexing-1.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Manifold Learning**](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction)**:** t-Distributed
    Stochastic Neighbor Embedding ([t-SNE](https://distill.pub/2016/misread-tsne/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variational autoencoders:** An automated generative approach using [variational
    autoencoders](https://arxiv.org/pdf/1606.05908.pdf) (VAE)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering:** [Hierarchical Clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of this in a real-world problem could be trying to visualize which
    text features could be influential in a model by checking their semantic similarity
    using word embeddings and visualizing the same using t-SNE as depicted below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/a170df9d9993c6d08f152e05380b6078.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing word embeddings using t-SNE (Source: [*Understanding Feature Engineering
    (Part 4) — Deep Learning Methods for Text Data — Towards Data Science*](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa))
  prefs: []
  type: TYPE_NORMAL
- en: You can also use t-SNE to visualize the famous MNIST hand-written digits dataset
    as depicted in the following figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/f673989b5ae5a27375f97dae683444a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing MNIST data using t-SNE using sklearn. Image courtesy of Pramit Choudhary
    and the Datascience.com team.
  prefs: []
  type: TYPE_NORMAL
- en: Another example is visualizing the famous IRIS dataset by leveraging PCA for
    dimension reduction as depicted in the following figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/52b4d6050191a5f88b4b2591d6eaee84.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Principal Component Analysis on the IRIS dataset](http://scikit-learn.org/stable/modules/decomposition.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Besides visualizing data and features, certain more intuitive and interpretable
    models like decision trees help us in visualizing how exactly it might take a
    certain decision. A sample tree is depicted below which helps us visualize the
    exact rules in a human-interpretable manner.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/74fc9f8e2e28cf9c70e62ababfa2c625.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing human-interpretable rules for a decision tree model (Source: [*Practical
    Machine Learning with Python, Apress 2018*](https://github.com/dipanjanS/practical-machine-learning-with-python))
  prefs: []
  type: TYPE_NORMAL
- en: However, like we discussed we may not get these rules for other models which
    are not as interpretable as tree based models. Also huge decision trees always
    become very difficult to visualize and interpret.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Performance Evaluation Metrics**'
  prefs: []
  type: TYPE_NORMAL
- en: '[***Model performance evaluation***](https://www.cs.cornell.edu/courses/cs578/2003fa/performance_measures.pdf) is
    a key step in any data science lifecycle for choosing the best model. This enables
    us to see how well our models are performing, compare various performance metrics
    across models and choose the best models. This also enables us to [***tune and
    optimize hyper-parameters***](https://en.wikipedia.org/wiki/Hyperparameter_optimization)in
    a model to obtain a model which gives the best performance on the data we are
    dealing with. Typically there exist certain standard evaluation metrics based
    on the type of problem we are dealing with.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised Learning — Classification:** For classification problems, our
    main objective is to predict a discrete categorical response variable. The confusion
    matrix is extremely useful here from which we can derive a whole bunch of useful
    metrics including accuracy, precision, recall, F1-Score as depicted in the following
    example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure](../Images/2c8ccd191f418a6b656e14ff7bf12402.png)'
  prefs: []
  type: TYPE_IMG
- en: Classification Model Performance Metrics (Source: [*Practical Machine Learning
    with Python, Apress 2018*](https://github.com/dipanjanS/practical-machine-learning-with-python))
  prefs: []
  type: TYPE_NORMAL
- en: Besides these metrics, we can also use some other techniques like the ROC curve
    and the AUC score as depicted in a wine-quality prediction system in the following
    figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/9c01adeae214aa7cbb8e63cfe26d9bd0.png)'
  prefs: []
  type: TYPE_IMG
- en: ROC Curve and AUC scores (Source: [*Practical Machine Learning with Python,
    Apress 2018*](https://github.com/dipanjanS/practical-machine-learning-with-python))
  prefs: []
  type: TYPE_NORMAL
- en: 'The area under the ROC curve is a very popular technique to objectively evaluate
    the performance of a classifier. Here we typically try to balance the True Positive
    Rate (TPR) and the False Positive Rate (FPR). The above figure tells us that the
    AUC score for a wine of class `**‘high’**` is `**0.9**` which means that the model’s
    probability of assigning a higher score to a wine of class `**‘high’**`(positive
    class) than to class `**NOT ‘high’ **`which is either `**‘medium’**` or `**‘low’**`(negative
    class) is 90%. At times, the results may be misleading and difficult to interpret
    if the ROC curves cross(Source: [*Measuring classifier performance: a coherent
    alternative to the area under the ROC curve*](https://link.springer.com/article/10.1007%2Fs10994-009-5119-5)).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised Learning — Regression:** For regression problems we can use standard
    metrics like the [coefficient of determination (R-square)](https://en.wikipedia.org/wiki/Coefficient_of_determination),
    the [root mean-square error (RMSE)](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d) and
    the [mean absolute error (MAE)](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised Learning — Clustering:** For unsupervised learning problems
    based on clustering we can use metrics like the [silhouette coefficient](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html), [homogeneity,
    completeness, V-measure](http://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness-and-v-measure) and
    the [Calinski-Harabaz index](http://scikit-learn.org/stable/modules/clustering.html#calinski-harabaz-index).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations of Traditional Techniques and Motivation for Better Model Interpretation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The techniques we discussed in the previous techniques are definitely helpful
    in trying to understand more about our data, features as well as which models
    might be effective. However, they are quite limiting in terms of trying to discern
    human-interpretable ways of how a model works. In any data science problem, we
    usually build a model on a stationary dataset and get our objective function (optimized
    loss function) which is usually deployed when it meets certain criteria based
    on model performance and business requirements. Usually we leverage the above
    techniques of exploratory analysis and evaluation metrics for deciding the overall
    model performance on our data. However, in the real-world, a model’s performance
    often decreases and plateaus over time after deployment due to variability in
    data features, added constraints and noise. This might include things like changes
    in the environment, changes in features as well as added constraints. Hence simply
    re-training a model on the same feature set will not be sufficient and we need
    to constantly check for how important features are in deciding model predictions
    and how well they might be working on new data points.
  prefs: []
  type: TYPE_NORMAL
- en: For example, an [intrusion detection system](https://ir.library.louisville.edu/etd/2790/)(IDS),
    a cyber-security application, is prone to evasion attacks where an attacker may
    use adversarial inputs to beat the secure system (note: [adversarial inputs](https://arxiv.org/abs/1602.02697) are
    examples that are intentionally engineered by an attacker to trick the machine
    learning model to make false predictions). The model’s objective function in such
    cases may act as a weak surrogate to the real-world objectives. A better interpretation
    might be needed to identify the blind spots in the algorithms to build a secure
    and safe model by fixing the training data set prone to adversarial attacks (for
    further reading, see Moosavi-Dezfooli, et al., 2016, [*DeepFool*](https://arxiv.org/pdf/1511.04599.pdf) and
    Goodfellow, et al., 2015, [*Explaining and harnessing adversarial examples*](https://arxiv.org/abs/1412.6572)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/1409f75c34f0565044b40b8018016d08.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: [https://medium.com/@jrodthoughts/using-adversarial-attacks-to-make-your-deep-learning-model-look-stupid-24fb872f06fd](https://medium.com/@jrodthoughts/using-adversarial-attacks-to-make-your-deep-learning-model-look-stupid-24fb872f06fd)
  prefs: []
  type: TYPE_NORMAL
- en: Besides, often bias exists in models due to the nature of data we are dealing
    with like in a rare-class prediction problem (fraud or intrusion detection). Metrics
    don’t help us justify the true story of a model’s prediction decisions. Also these
    traditional forms of model interpretation might be easy for a data scientist to
    understand but since they are inherently theoretical and often mathematical, there
    is a substantial gap in trying to explain these to (non-technical) business stakeholders
    and trying to decide the success criteria of a project based on these metrics
    alone. Just telling the business, *“I have a model with 90% accuracy”* is not
    sufficient information for them to start trusting the model when deployed in the
    real world. We need human-interpretable interpretations (HII) of a model’s decision
    policies which could be explained with proper and intuitive inputs and outputs.
    This would enable insightful information to be easily shared with peers (analysts,
    managers, data scientists, data engineers). Using such forms of explanations,
    which could be explained based on inputs and outputs, might help facilitate better
    communication and collaboration, enabling businesses to make more confident decisions
    (e.g., [risk assessment/audit risk analysis in financial institutions](https://www.journalofaccountancy.com/issues/2006/jul/assessingandrespondingtorisksinafinancialstatementaudit.html)).
  prefs: []
  type: TYPE_NORMAL
- en: To reiterate, we define model interpretation (new approaches) as being able
    to account for ***fairness ***(unbiasedness/non-discriminative), ***accountability***(reliable
    results), and ***transparency ***(being able to query and validate predictive
    decisions) of a predictive model — currently in regard to supervised learning
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Should You Become a Freelance Artificial Intelligence Engineer?](https://www.kdnuggets.com/2021/12/ucsd-become-freelance-artificial-intelligence-engineer.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Artificial Intelligence Project Ideas for 2022](https://www.kdnuggets.com/2022/01/artificial-intelligence-project-ideas-2022.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Artificial Intelligence and the Metaverse](https://www.kdnuggets.com/2022/02/artificial-intelligence-metaverse.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Uncertainty Quantification in Artificial Intelligence-based Systems](https://www.kdnuggets.com/2022/04/uncertainty-quantification-artificial-intelligencebased-systems.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How Artificial Intelligence Can Transform Data Integration](https://www.kdnuggets.com/2022/04/artificial-intelligence-transform-data-integration.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Most In-demand Artificial Intelligence Skills To Learn In 2022](https://www.kdnuggets.com/2022/08/indemand-artificial-intelligence-skills-learn-2022.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
