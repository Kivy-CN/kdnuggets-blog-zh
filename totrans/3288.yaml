- en: Understanding Deep Learning Requires Re-thinking Generalization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解深度学习需要重新思考泛化
- en: 原文：[https://www.kdnuggets.com/2017/06/understanding-deep-learning-rethinking-generalization.html](https://www.kdnuggets.com/2017/06/understanding-deep-learning-rethinking-generalization.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2017/06/understanding-deep-learning-rethinking-generalization.html](https://www.kdnuggets.com/2017/06/understanding-deep-learning-rethinking-generalization.html)
- en: '[Understanding deep learning requires re-thinking generalization](https://openreview.net/forum?id=Sy8gdB9xx&noteId=Sy8gdB9xx)
    Zhang et al., *ICLR’17*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[理解深度学习需要重新思考泛化](https://openreview.net/forum?id=Sy8gdB9xx&noteId=Sy8gdB9xx)
    Zhang et al., *ICLR’17*'
- en: 'This paper has a wonderful combination of properties: the results are easy
    to understand, somewhat surprising, and then leave you pondering over what it
    all might mean for a long while afterwards!'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文具有令人惊叹的组合特性：结果易于理解，略显惊讶，并且会让你在之后很长一段时间内思考其意义！
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析水平'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT 方面的工作'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'The question the authors set out to answer was this:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提出要回答的问题是：
- en: What is it that distinguishes neural networks that generalize well from those
    that don’t? A satisfying answer to this question would not only help to make neural
    networks more interpretable, but it might also lead to more principled and reliable
    model architecture design.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 什么因素使得神经网络能够很好地泛化？对这个问题的令人满意的答案不仅有助于使神经网络更具可解释性，还可能导致更有原则和可靠的模型架构设计。
- en: By “generalize well,” the authors simply mean “what causes a network that performs
    well on training data to also perform well on the (held out) test data?” (As opposed
    to transfer learning, which involves applying the trained network to a related
    but different problem). If you think about that for a moment, the question pretty
    much boils down to “why do neural networks work as well as they do?” Generalisation
    is the difference between just memorising portions of the training data and parroting
    it back, and actually developing some meaningful intuition about the dataset that
    can be used to make predictions. So it would be somewhat troubling, would it not,
    if the answer to the question “why do neural networks work (generalize) as well
    as they do?” turned out to be “we don’t really know!”
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 作者所说的“良好泛化”仅仅是指“什么原因使得一个在训练数据上表现良好的网络也能在（保留的）测试数据上表现良好？”（而不是迁移学习，迁移学习涉及将训练好的网络应用于一个相关但不同的问题）。如果你稍加思考，这个问题实际上就是“为什么神经网络的表现如此之好？”
    泛化是仅仅记忆训练数据的一部分并将其重复的差别，以及实际上对数据集有某种有意义的直觉并能用来做预测。所以，如果问题的答案是“我们真的不知道”，那不就有点令人担忧吗？
- en: The curious case of the random labels
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机标签的奇怪案例
- en: Our story begins in a familiar place – the CIFAR 10 (50,000 training images
    split across 10 classes, 10,000 validation images) and the ILSVRC (ImageNet) 2012
    (1,281,167 training, 50,000 validation images, 1000 classes) datasets and variations
    of the [Inception](https://blog.acolyer.org/2017/03/21/convolution-neural-nets-part-2/)
    network architecture.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的故事开始于一个熟悉的地方——CIFAR 10（50,000 张训练图像分为 10 个类别，10,000 张验证图像）和 ILSVRC（ImageNet）2012（1,281,167
    张训练图像，50,000 张验证图像，1000 个类别）数据集，以及 [Inception](https://blog.acolyer.org/2017/03/21/convolution-neural-nets-part-2/)
    网络架构的变体。
- en: Train the networks using the training data, and you won’t be surprised to hear
    that they can reach zero errors on the *training set*. This is highly indicative
    of *overfitting* – memorising training examples rather than learning true predictive
    features. We can use techniques such as regularisation to combat overfitting,
    leading to networks that generalise better. More on that later.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练数据训练网络，你不会感到惊讶地听到它们在*训练集*上可以达到零误差。这高度表明了*过拟合*——记住训练示例而不是学习真正的预测特征。我们可以使用诸如正则化等技术来对抗过拟合，从而得到泛化更好的网络。稍后会详细讨论。
- en: Take the same training data, but this time randomly jumble the labels (i.e.,
    such that there is no longer any genuine correspondence between the label and
    what’s in the image). Train the networks using these random labels and what do
    you get? *Zero training error!*
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的训练数据，但这次随机打乱标签（即，使标签与图像内容之间不再存在真正的对应关系）。使用这些随机标签训练网络，你会得到什么？*零训练误差！*
- en: In [this] case, there is no longer any relationship between the instances and
    the class labels. As a result, learning is impossible. Intuition suggests that
    this impossibility should manifest itself clearly during training, e.g., by training
    not converging or slowing down substantially. To our suprise, several properties
    of the training process for multiple standard architectures is largely unaffected
    by this transformation of the labels.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在[这种]情况下，实例和类别标签之间不再存在任何关系。因此，学习是不可能的。直觉上，应该在训练过程中清晰地表现出这种不可能性，例如，训练不收敛或显著减慢。令我们惊讶的是，对于多个标准架构的训练过程的几个特性在标签转换下基本保持不变。
- en: 'As the authors succinctly put it, “*Deep neural networks easily fit random
    labels*.” Here are three key observations from this first experiment:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如作者简洁地表述的，“*深度神经网络很容易拟合随机标签*。”以下是这个第一次实验的三个关键观察：
- en: The effective capacity of neural networks is sufficient for memorising the entire
    data set.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络的有效容量足以记住整个数据集。
- en: Even optimisation on *random labels* remains easy. In fact, training time increases
    by only a small constant factor compared with training on the true labels.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 即使在*随机标签*上的优化仍然很简单。实际上，与在真实标签上训练相比，训练时间仅增加了一个小的常数因子。
- en: Randomising labels is solely a data transformation, leaving all other properties
    of the learning problem unchanged.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机化标签仅仅是数据转换，保留了学习问题的所有其他属性不变。
- en: 'If you take the network trained on random labels, and then see how well it
    performs on the test data, it of course doesn’t do very well at all because it
    hasn’t truly learned anything about the dataset. A fancy way of saying this is
    that it has a high generalisation error. Put all this together and you realise
    that:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你拿到一个在随机标签上训练的网络，然后看看它在测试数据上的表现，它当然会表现得非常差，因为它并没有真正学到任何关于数据集的东西。换句话说，它有很高的*泛化误差*。将这些因素综合起来，你会意识到：
- en: … by randomizing labels alone we can force the generalization error of a model
    to jump up considerably *without changing the model, its size, hyperparameters,
    or the optimizer.* We establish this fact for several different standard architectures
    trained on the CIFAR 10 and ImageNet classification benchmarks. (Emphasis mine).
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …仅通过随机化标签，我们可以使模型的泛化误差大幅上升，*而不改变模型、模型的大小、超参数或优化器*。我们在多个不同的标准架构上进行了验证，这些架构在CIFAR
    10和ImageNet分类基准上进行训练。（强调由我加上）。
- en: 'Or in other words: the model, its size, hyperparameters, and the optimiser
    cannot explain the generalisation performance of state-of-the-art neural networks.
    This must be the case because the generalisation performance can vary significantly
    while they all remain unchanged.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说：模型、模型的大小、超参数和优化器无法解释最先进神经网络的泛化性能。这必然是因为尽管它们保持不变，泛化性能却可以显著变化。
- en: The even more curious case of the random images
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更加令人好奇的随机图像案例
- en: What happens if we don’t just mess with the labels, but we also mess with the
    images themselves. In fact, what if just replace the true images with random noise??
    In the figures this is labeled as the ‘Gaussian’ experiment because a Gaussian
    distribution with matching mean and variance to the original image dataset is
    used to generate random pixels for each image.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不仅仅改变标签，还改变图像本身会发生什么？实际上，如果用随机噪声替换真实图像会怎么样？在这些图中，这被标记为‘高斯’实验，因为使用与原始图像数据集均值和方差匹配的高斯分布来生成每个图像的随机像素。
- en: In turns out that what happens is the networks train to zero training error
    still, but they get there even faster than the random labels case! A hypothesis
    for why this happens is that the random pixel images are more separated from each
    other than the random label case of images that originally all belonged to the
    same class, but now must be learned as differing classes due to label swaps.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 结果发现，网络仍然训练到零训练误差，但比随机标签的情况更快！一个假设是，这种情况发生的原因是，随机像素图像之间的分离程度比随机标签的情况更大，因为后者的图像原本都属于同一类别，但由于标签交换现在必须被学习为不同的类别。
- en: 'The team experiment with a spectrum of changes introducing different degrees
    and kinds of randomisation into the dataset:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 团队对数据集引入了不同程度和种类的随机化变化进行了一系列实验：
- en: true labels (original dataset without modification)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实标签（原始数据集没有修改）
- en: partially corrupted labels (mess with some of the labels)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部分损坏的标签（对一些标签进行扰动）
- en: random labels (mess with all of the labels)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机标签（对所有标签进行扰动）
- en: shuffled pixels (choose a pixel permutation, and then apply it uniformly to
    all images)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打乱像素（选择一个像素排列，然后均匀地应用到所有图像）
- en: random pixels (apply a different random permutation to each image independently)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机像素（对每张图片独立应用不同的随机排列）
- en: Guassian (just make stuff up for each image, as described previously)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯（如前所述，为每张图片生成虚假的数据）
- en: '![](../Images/e95aab14ed016d23369c11732e68ee35.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e95aab14ed016d23369c11732e68ee35.png)'
- en: All the way along the spectrum, the networks are still able to perfectly fit
    the training data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个范围内，网络仍然能够完美地拟合训练数据。
- en: We furthermore vary the amount of randomization, interpolating smoothly between
    the case of no noise and complete noise. This leads to a range of intermediate
    learning problems where there remains some level of signal in the labels. We observe
    a steady deterioration of the generalization error as we increase the noise level.
    This shows that neural networks are able to capture the remaining signal in the
    data, while at the same time fit the noisy part using brute-force.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们进一步改变了随机化的数量，在没有噪声和完全噪声的情况下平滑地插值。这导致了一系列中间学习问题，其中标签中仍然保留了一定程度的信号。我们观察到，随着噪声水平的增加，泛化误差稳步恶化。这表明神经网络能够捕捉数据中剩余的信号，同时用蛮力拟合噪声部分。
- en: For me that last sentence is key. Certain choices we make in model architecture
    clearly do make a difference in the ability of a model to generalise (otherwise
    all architectures would generalise the same). The best generalising network in
    the world is still going to have to fallback on memorisation when there is no
    other true signal in the data though. So maybe we need a way to tease apart the
    true potential for generalisation that exists in the dataset, and how efficient
    a given model architecture is at capturing this latent potential. A simple way
    of doing that is to train different architectures on the same dataset! (Which
    we do all the time of course). That still doesn’t help us with the original quest
    though – understanding *why* some models generalise better than others.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，最后一句话是关键。我们在模型架构中做出的某些选择显然会影响模型的泛化能力（否则所有架构的泛化效果都会相同）。不过，世界上最具泛化能力的网络仍然必须在数据中没有其他真实信号时依赖于记忆。所以也许我们需要一种方法来区分数据集中存在的真正泛化潜力，以及给定模型架构在捕捉这种潜在能力方面的效率。一种简单的方法是对相同的数据集训练不同的架构！（我们当然一直在这样做）。不过，这仍然没有帮助我们解决最初的问题——理解*为什么*有些模型比其他模型更具泛化能力。
- en: Regularization to the rescue?
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则化来救援？
- en: The model architecture itself is clearly not a sufficient regulariser (can’t
    prevent overfitting / memorising). But what about commonly used regularisation
    techniques?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构本身显然不是一个充分的正则化器（不能防止过拟合/记忆）。那么常用的正则化技术呢？
- en: 'We show that explicit forms of regularization, such as weight decay, dropout,
    and data augmentation, do not adequately explain the generalization error of neural
    networks: *Explicit regularization may improve generalization performance, but
    is neither necessary nor by itself sufficient for controlling generalization error.*'
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们展示了显式正则化形式，如权重衰减、丢弃法和数据增强，并不能充分解释神经网络的泛化误差：*显式正则化可能改善泛化性能，但既不是必要的，也不是单独足够的来控制泛化误差。*
- en: '![](../Images/d58e95f0f54f415be1f649119ec37010.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d58e95f0f54f415be1f649119ec37010.png)'
- en: 'Explicit regularisation seems to be more of a tuning parameter that helps improve
    generalisation, but its absence does not necessarily imply poor generalisation
    error. It is certainly not the case that not all models that fit the training
    data generalise well though. An interesting piece of analysis in the paper shows
    that we pick up a certain amount of regularisation just through the process of
    using gradient descent:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 显式正则化似乎更多的是一种调节参数，有助于改善泛化，但其缺乏并不一定意味着泛化误差差。确实，并不是所有适合训练数据的模型都能很好地泛化。论文中的一个有趣分析表明，我们在使用梯度下降的过程中会得到一定程度的正则化：
- en: We analyze how SGD acts as an implicit regularizer. For linear models, SGD always
    converges to a solution with small norm. Hence, the algorithm itself is implicitly
    regularizing the solution… Though this doesn’t explain why certain architectures
    generalize better than other architectures, it does suggest that more investigation
    is needed to understand exactly what the properties are that are inherited by
    models trained using SGD.
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们分析了SGD如何作为隐式正则化器。对于线性模型，SGD总是会收敛到具有小范数的解。因此，算法本身隐式地对解进行正则化… 尽管这并不能解释为什么某些架构比其他架构泛化得更好，但它确实表明需要更多的研究来准确理解使用SGD训练的模型所继承的特性。
- en: The effective capacity of machine learning models
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习模型的有效容量
- en: 'Consider the case of neural networks working with a finite sample size of *n*.
    If a network has *p* parameters, where *p* is greater than *n*, then *even a simple
    two-layer neural network can represent any function of the input sample.* The
    authors prove (in an appendix), the following theorem:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到神经网络在有限样本大小为*n*的情况下的情形。如果一个网络具有*p*个参数，其中*p*大于*n*，那么*即使是一个简单的两层神经网络也能表示输入样本的任何函数。*
    作者在附录中证明了以下定理：
- en: There exists a two-layer neural network with ReLU activations and *2n + d* weights
    that can represent any function on a sample of size *n* in *d* dimensions.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 存在一个具有ReLU激活和*2n + d* 权重的两层神经网络，可以表示*在*d维中大小为*n*的样本上的任何函数。
- en: Even depth-2 networks of linear size can already represent any labeling of the
    training data!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是深度为2的线性网络也已经能够表示训练数据的任何标记！
- en: So where does this all leave us?
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 那么，这一切将把我们带到哪里？
- en: This situation poses a conceptual challenge to statistical learning theory as
    traditional measures of model complexity struggle to explain the generalization
    ability of large artificial neural networks. We argue that we have yet to discover
    a precise formal measure under which these enormous models are simple. Another
    insight resulting from our experiments is that optimization continues to be empirically
    easy even if the resulting model does not generalize. This shows that the reasons
    for why optimization is empirically easy must be different from the true cause
    of generalization.
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这种情况对统计学习理论提出了概念上的挑战，因为传统的模型复杂度度量难以解释大型人工神经网络的泛化能力。我们认为我们尚未发现一个精确的正式度量，这些巨大的模型在该度量下是简单的。另一个实验结果是，即使结果模型无法泛化，优化过程仍然在经验上容易。这表明，优化在经验上容易的原因必须不同于泛化的真正原因。
- en: '[Original](https://blog.acolyer.org/2017/05/11/understanding-deep-learning-requires-re-thinking-generalization/).
    Reposted with permission.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://blog.acolyer.org/2017/05/11/understanding-deep-learning-requires-re-thinking-generalization/)。已获许可转载。'
- en: '**Related:**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Deep Learning Papers Reading Roadmap](/2017/06/deep-learning-papers-reading-roadmap.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习论文阅读路线图](/2017/06/deep-learning-papers-reading-roadmap.html)'
- en: '[Deep Learning 101: Demystifying Tensors](/2017/06/deep-learning-demystifying-tensors.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习101：揭开张量的神秘面纱](/2017/06/deep-learning-demystifying-tensors.html)'
- en: '[Why Does Deep Learning Not Have a Local Minimum?](/2017/06/deep-learning-local-minimum.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么深度学习没有局部最小值？](/2017/06/deep-learning-local-minimum.html)'
- en: More On This Topic
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多关于此话题
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并通过寻找目标…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学的统计学顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[一个90亿美元的AI失败，审视](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
- en: '[Building a solid data team](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[建立一个高效的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)'
- en: '[Write Clean Python Code Using Pipes](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用管道编写清晰的 Python 代码](https://www.kdnuggets.com/2021/12/write-clean-python-code-pipes.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功数据科学家的五个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
