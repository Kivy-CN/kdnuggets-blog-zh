- en: Deploy your PyTorch model to Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/03/deploy-pytorch-model-production.html](https://www.kdnuggets.com/2019/03/deploy-pytorch-model-production.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Nicolás Metallo](https://www.linkedin.com/in/nicolas-metallo/?originalSubdomain=uk),
    Audatex**'
  prefs: []
  type: TYPE_NORMAL
- en: Following the last article about [Training a Choripan Classifier](https://medium.com/@nicolas.metallo/train-a-choripan-classifier-with-fast-ai-v1-in-google-colab-6e438817656a)
    with PyTorch and Google Colab, we will now talk about what are some steps that
    you can do if you want to deploy your recently trained model as an API. The discussion
    on how to do this with Fast.ai is [currently ongoing](https://forums.fast.ai/t/using-a-fast-ai-model-in-production/12033)
    ([more](https://forums.fast.ai/t/productionizing-models-thread/28353/54)) and
    will most likely continue until PyTorch releases their official 1.0 version. You
    can find more information in the Fast.ai Forums, PyTorch Documentation/Forums,
    and their respective GitHub repositories.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Saving and Loading Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s recommended that you take a look at the [PyTorch Documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html)
    as it’s a great place to start, but in short, there are two ways to serialize
    and restore a model. One is loading only the weights and the other loading the
    entire model (and weights). You will need to first create a model to define its
    architecture otherwise you will end up with an `OrderedDict` with just the weight
    values. Both options would work for inference and/or for resuming a model's training
    from a previous checkpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Using `torch.save()` and `torch.load()`
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This save/load process uses the most intuitive syntax and involves the least
    amount of code. Saving a model in this way will save the entire module using Python’s
    [pickle](https://docs.python.org/3/library/pickle.html) module. The disadvantage
    of this approach is that the serialized data is bound to the specific classes
    and the exact directory structure used when the model is saved. The reason for
    this is because pickle does not save the model class itself. Rather, it saves
    a path to the file containing the class, which is used during load time. Because
    of this, your code can break in various ways when used in other projects or after
    refactors.
  prefs: []
  type: TYPE_NORMAL
- en: '**Save model**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Sometimes `pickle` is not able to serialize some model creations functions (e.g.
    `resnext_50_32x4d` which is found in previous versions of Fastai) so you need
    to use `dill` instead. Here's the fix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can read more about the limitations of `pickle` in this [article](invalid#zSoyz).
    A common PyTorch convention is to save models using either a `.pt` or `.pth` file
    extension.
  prefs: []
  type: TYPE_NORMAL
- en: '**Load model**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**2\. Using `state_dict`**'
  prefs: []
  type: TYPE_NORMAL
- en: In PyTorch, the learnable parameters (e.g. weights and biases) of an `torch.nn.Module`
    model are contained in the model’s *parameters* (accessed with `model.parameters()`).
    A *state_dict* is simply a Python dictionary object that maps each layer to its
    parameter tensor. Note that only layers with learnable parameters (convolutional
    layers, linear layers, etc.) have entries in the model’s *state_dict*.
  prefs: []
  type: TYPE_NORMAL
- en: We will need to re-initialize our model in the same way it was originally defined
    and created when we saved the weights, making sure the variables, classes, functions
    that go into creating the model are available, whether through module imports
    or directly within the same script/file. One potential advantage of using this
    method is that you can use updated scripts to load the old model if the parameters
    are the same, and it’s also the [recommended](https://pytorch.org/docs/master/notes/serialization.html)
    approach by the official documentation. One thing you should also remember is
    that `state_dict` takes a dictionary object, not a path to a saved object, so
    you can't load using `model.load_state_dict(PATH)`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Save model**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Load model**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You run `model.eval()` after loading because you usually have `BatchNorm` and
    `Dropout` layers that by default are in train mode on construction. You don't
    need to call `model.eval()` if you want to resume your model training.
  prefs: []
  type: TYPE_NORMAL
- en: As we have done our training with Fastai, we can call `[Learner.save](https://docs.fast.ai/basic_train.html#Learner.save)`
    and `[Learner.load](https://docs.fast.ai/basic_train.html#Learner.load)` to save
    and load models (more info in the [documentation](https://docs.fast.ai/basic_train.html#Saving-and-loading-models))
    . This is running `state_dict()` in the back so only the model parameters will
    be saved and not the architecture. This means that you will need to run the create_cnn
    method to get a pre-trained model from a given architecture (the same that you
    used before to train your model, e.g. *models.resnet34*) with a custom head that
    is suitable for your data. Models are saved and loaded from the `path`/`model_dir`
    directory, and the `.pth` extension is automatically added for both operations.
  prefs: []
  type: TYPE_NORMAL
- en: Simple Deployment With Flask
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After we have trained our classifier using the free GPU available in Google
    Colab, we are ready to do inference on our end. We can do it locally or in the
    cloud and there are many different options (AWS, Paperspace, Google Cloud, etc)
    that we can choose from. As I have some free Amazon AWS credits remaining I’ll
    be using an Amazon AMI that comes with several ML libraries already installed
    hosted on a t2.medium instance. Here are some simple instructions to run a Docker
    image on your end (should be around the same when we are not doing GPU training).
  prefs: []
  type: TYPE_NORMAL
- en: '**Ready-to-run Docker images**'
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter Docker Stacks are a great way to get a notebook up and going in no time
    with the latest libraries. These are ready-to-run Docker images that contain Jupyter
    applications and interactive computing tools. Learn more about them in the [Official
    Documentation](https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html#jupyter-datascience-notebook).
    We will use the **Jupyter Notebook Data Science Stack** from [this repository](https://github.com/jupyter/docker-stacks).
  prefs: []
  type: TYPE_NORMAL
- en: Once you have [installed](https://docs.docker.com/install/) Docker, open the
    terminal, `cd` into your working directory, and run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This will create a server that you can log into where you will connect to a
    Jupyter notebook. You can run some commands directly from there or you can also
    run bash or any command in a Docker container by getting the `container id`, typing
    `docker ps` in the terminal and then running
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Install PyTorch & [Fastai](https://github.com/fastai/fastai)**'
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your machine configuration you will want to run inference on either
    GPU or CPU. In our example, we are going to run everything on the CPU, so you
    need to run the following to install the latest [PyTorch](https://pytorch.org/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now you can install Fastai with `pip install fastai`
  prefs: []
  type: TYPE_NORMAL
- en: '**Create a Flask Application**'
  prefs: []
  type: TYPE_NORMAL
- en: Install the Flask library by running
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We will create a folder called `flask_app` and two new python files `server.py`
    with our code to load the model weights and run the inference server and `settings.py`
    that sets some basic params to give us more flexibility in the future. The following
    is an example of what `flask_app/settings.py` might look like. We will then import
    this with `from settings import *` into `server.py` .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We can now go through the `flask_app/server.py`. This first part will import
    the libraries and settings.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In order to run our single image inference prediction, we first need to create
    a new model that follows the same folder structure that we used when we trained
    it. That’s why we are going to create a new empty dir based on the labels that
    we have set before in `settings.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Once that `path` is defined, we are going to create a new `learn` model and
    download the pre-trained weights for the [Choripan Classifier](https://medium.com/@nicolas.metallo/train-a-choripan-classifier-with-fast-ai-v1-in-google-colab-6e438817656a).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: There are already many great tutorials [online](http://flask.pocoo.org/) that
    are amazingly detailed so I won’t explain much of how Flask works. I created a
    `predict` function that takes the URL that you receive as INPUT, runs it through
    `learn.predict(img)` to get the predicted class, and then returns a `json`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Once we have that done, we can head over to the terminal, cd into the `flask_app`
    dir, and run `python server.py`. We should see something like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**That’s it!** Now we can run commands like these from the terminal (I’m running
    an AWS instance). Let’s take this image for example.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1bc93c133e36d2aa8f5cfd14033110ae.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: And this is how it looks from the server side
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**Not bad!** You now have your very own “Choripan/Not Choripan” API. If you
    want to move to the next level, please check out [this tutorial](http://flask.pocoo.org/docs/1.0/tutorial/deploy/)
    from the Flask Documentation to deploy to Production and/or [this other tutorial](http://containertutorials.com/docker-compose/flask-simple-app.html)
    if you want to Dockerize your Flask application (you can also use [docker-compose](http://containertutorials.com/docker-compose/flask-compose.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: Other Ways to Deploy to Production
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 1\. Image Classification Example Using [Clipper](http://clipper.ai)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There’s a great `ipynb` that you can follow in the [ClipperTutorials](invalid#zSoyz)
    GitHub with the basics of how everything works. They provide a Docker image or
    you can just run their Amazon AMI. Sadly, this is only working with PyTorch 0.4.0
    which makes it a real pain to convert to when your models have been trained with
    the latest preview versions of PyTorch and Fastai. Works great with the example
    pre-trained model though.
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating a ClipperConnection**'
  prefs: []
  type: TYPE_NORMAL
- en: To start Clipper, you must first create a `[ClipperConnection](http://docs.clipper.ai/en/develop/#clipper-connection)`
    object with the type of `ContainerManager` you want to use. In this case, you
    will be using the `DockerContainerManager`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '**Starting Clipper**'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a `ClipperConnection` object, you can start a Clipper cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command will start 3 Docker containers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Query Frontend: The Query Frontend container listens for incoming prediction
    requests and schedules and routes them to the deployed models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The Management Frontend: The Management Frontend container manages and updates
    the cluster’s internal configuration state, such as tracking which models are
    deployed and which application endpoints have been registered.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A Redis instance: Redis is used to persistently store Clipper’s internal configuration
    state. By default, Redis is started on port 6380 instead of the standard Redis
    default port 6379 to avoid collisions with any Redis instances that are already
    running.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Take a look at the containers Clipper has started.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '**Create an Application**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: When you list the applications registered with Clipper, you should see the newly
    registered `squeezenet-classifier` application show up.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '**Load an example pre-trained PyTorch model**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: PyTorch models cannot just be pickled and loaded. Instead, they must be saved
    using PyTorch’s native serialization API. Because of this, you cannot use the
    generic Python model deployer to deploy the model to Clipper. Instead, you will
    use the Clipper PyTorch deployer to deploy it. The Docker container will load
    and reconstruct the model from the serialized model checkpoint when the container
    is started.
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessing**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '**Define a predict function and add metrics**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Clipper must download this Docker image from the internet, so this may take
    a minute
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Now link the generated `pytorch-model` to the application `squeezenet-classsifier`
    we created before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '**That’s it!**'
  prefs: []
  type: TYPE_NORMAL
- en: '**How to query the API with Requests**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '**Stopping Clipper**'
  prefs: []
  type: TYPE_NORMAL
- en: If you run into issues and want to completely stop Clipper, you can do this
    by calling `[ClipperConnection.stop_all()](http://docs.clipper.ai/en/latest/#clipper_admin.ClipperConnection.stop_all)`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: When you list all the Docker containers a final time, you should see that all
    of the Clipper containers have been stopped.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Using Now from Zeit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another option under discussion in the Forums is to use the [Now](https://zeit.co/now)
    service from [Zeit](https://zeit.co/). You can follow [this guide](https://course-v3.fast.ai/deployment_zeit.html)
    in the Fast.ai Documentation. I have tried this but have not gotten accurate results
    (probably because of normalization). Seems very promising.
  prefs: []
  type: TYPE_NORMAL
- en: You will need to run these commands only one time. The first installs Now’s
    CLI (Command Line Interface).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: And the next downloads a **starter pack** for model deployment based on Fast.ai
    Lesson 2.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '**Upload your trained model file**'
  prefs: []
  type: TYPE_NORMAL
- en: Upload your trained model file (for example `stage-2.pth`) to a cloud service
    like Google Drive or Dropbox. Copy the download link for the file. **Note:** the
    download link is the one which starts the file download directly—and is normally
    different than the share link which presents you with a view to download the file
    (use [https://rawdownload.now.sh/](https://rawdownload.now.sh/) if needed)
  prefs: []
  type: TYPE_NORMAL
- en: '**Customize the app for your model**'
  prefs: []
  type: TYPE_NORMAL
- en: Open up the file `server.py` inside the `app` directory and update the `model_file_url`
    variable with the url copied above
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the same file, update the line `classes = ['black', 'grizzly', 'teddys']`
    with the classes you are expecting from your model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Deploy**'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the terminal, make sure you are in the `zeit` directory, then type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The first time you run this, it will prompt for your email address and create
    your Now account for you. After your account is created, run it again to deploy
    your project.
  prefs: []
  type: TYPE_NORMAL
- en: Every time you deploy with `now` it’ll create a unique **Deployment URL** for
    the app. It has a format of `xxx.now.sh`, and is shown while you are deploying
    the app.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Using `Torch Script` and `PyTorch C++ API`
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These are some of the most recent changes will come with the official 1.0 version
    of PyTorch. You can follow the instructions in [this](https://pytorch.org/tutorials/advanced/cpp_export.html)
    article from the Documentation or check out the last chapter of the [Intro to
    Deep Learning with PyTorch](https://classroom.udacity.com/courses/ud188) class
    in Udacity where they go through this step by step. This is just an overview of
    the main steps.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '**Build a minimal C++ application**'
  prefs: []
  type: TYPE_NORMAL
- en: Follow [these steps](https://pytorch.org/tutorials/advanced/cpp_export.html)
    and build `example-app.cpp` and `CMakeLists.txt`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install [Anaconda](https://www.anaconda.com/download/) and get CMAKE to run
    on your machine. You can install it through their [binaries](https://cmake.org/download/)
    or, if you are running MacOS, type `brew install cmake` (install `homebrew` through
    [these](https://brew.sh/) instructions). If you have some problems with CMAKE,
    remember to download the X-Code command line tools as a `.dmg` directly from [here](https://developer.apple.com/download/more/)
    (MacOS 10.14 in my case).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install Caffe2 from [here](https://caffe2.ai/docs/getting-started.html?platform=mac)
    and run `conda install pytorch-nightly-cpu -c pytorch`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Build the application](https://pytorch.org/tutorials/advanced/cpp_export.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PyTorch, Libtorch, C++, and NodeJS**'
  prefs: []
  type: TYPE_NORMAL
- en: Look at [http://blog.christianperone.com/2018/10/pytorch-1-0-tracing-jit-and-libtorch-c-api-to-integrate-pytorch-into-nodejs/](http://blog.christianperone.com/2018/10/pytorch-1-0-tracing-jit-and-libtorch-c-api-to-integrate-pytorch-into-nodejs/)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I tried my best to summarize some of the options available to deploy your recently
    trained PyTorch models. Hope this is useful and looking forward to reading your
    comments.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Nicolás Metallo](https://www.linkedin.com/in/nicolas-metallo/?originalSubdomain=uk)**
    is an award-winning entrepreneur with nearly 10 years of professional experience.
    He graduated from New York University with an MSc in Technology Management & Innovation
    and he works as a management consultant and freelance deep learning engineer.
    Nicolas is also the co-founder of INVIP Labs Inc., a social enterprise that helps
    blind and low vision individuals understand their environment better through computer
    vision. His particular interest in data science is focused on providing cities
    with data to make them more connected, efficient, resilient, vibrant, and prosperous.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/datadriveninvestor/deploy-your-pytorch-model-to-production-f69460192217).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[How to build an API for a machine learning model in 5 minutes using Flask](https://www.kdnuggets.com/2019/01/build-api-machine-learning-model-using-flask.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to PyTorch for Deep Learning](https://www.kdnuggets.com/2018/11/introduction-pytorch-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Learning Cheat Sheets](https://www.kdnuggets.com/2018/11/deep-learning-cheat-sheets.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Deploying Your Machine Learning Model to Production in the Cloud](https://www.kdnuggets.com/deploying-your-ml-model-to-production-in-the-cloud)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Detecting Data Drift for Ensuring Production ML Model Quality Using Eurybia](https://www.kdnuggets.com/2022/07/detecting-data-drift-ensuring-production-ml-model-quality-eurybia.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Managing Model Drift in Production with MLOps](https://www.kdnuggets.com/2023/05/managing-model-drift-production-mlops.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[From Zero to Hero: Create Your First ML Model with PyTorch](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How to Successfully Deploy Data Science Projects](https://www.kdnuggets.com/2022/01/successfully-deploy-data-science-projects.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learn How to Design & Deploy Responsible AI Systems](https://www.kdnuggets.com/2023/10/teradata-design-deploy-responsible-ai-systems-whitepaper)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
