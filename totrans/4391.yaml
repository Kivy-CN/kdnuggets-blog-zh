- en: Roadmap to Natural Language Processing (NLP)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/10/roadmap-natural-language-processing-nlp.html](https://www.kdnuggets.com/2020/10/roadmap-natural-language-processing-nlp.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)![Figure](../Images/b8ef7d8ca3d872590868efad60b8c60d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Photo by [Kelly Sikkema](https://unsplash.com/@kellysikkema?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the development of [Big Data](https://towardsdatascience.com/big-data-analysis-spark-and-hadoop-a11ba591c057) during
    the last decade. organizations are now faced with analysing large amounts of data
    coming from a wide variety of sources on a daily basis.
  prefs: []
  type: TYPE_NORMAL
- en: Natural Language Processing (NLP) is the area of research in Artificial Intelligence
    focused on processing and using Text and Speech data to create smart machines
    and create insights.
  prefs: []
  type: TYPE_NORMAL
- en: One of nowadays most interesting NLP application is creating machines able to
    discuss with humans about complex topics. [IBM Project Debater](https://www.research.ibm.com/artificial-intelligence/project-debater/) represents
    so far one of the most successful approaches in this area.
  prefs: []
  type: TYPE_NORMAL
- en: 'Video 1: IBM Project Debater'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some of the most common techniques which are applied in order to prepare text
    data for inference are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tokenization: **is used to segment the input text into its constituents words
    (tokens). In this way, it becomes easier to then convert our data into a numerical
    format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stop Words Removal: **is applied in order to remove from our text all the
    prepositions (eg. “an”, “the”, etc…) which can just be considered as a source
    of noise in our data (since they do not carry additional informative information
    in our data).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stemming: **is finally used in order to get rid of all the affixes in our
    data (eg. prefixes or suffixes). In this way, it can in fact become much easier
    for our algorithm to not consider as distinguished words which have actually similar
    meaning (eg. insight ~ insightful).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these preprocessing techniques can be easily applied to different types
    of texts using standard Python NLP libraries such as [NLTK](https://www.nltk.org/) and [Spacy](https://spacy.io/).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, in order to extrapolate the language syntax and structure of our
    text, we can make use of techniques such as Parts of Speech (POS) Tagging and
    Shallow Parsing (Figure 1). Using these techniques, in fact, we explicitly tag
    each word with its lexical category (which is based on the phrase syntactic context).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/951cdc5d92cd5446926fe321dc97c9ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Parts of Speech Tagging Example [1].'
  prefs: []
  type: TYPE_NORMAL
- en: Modelling Techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bag of Words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bag of Words is a technique used in Natural Language Processing and [Computer
    Vision](https://towardsdatascience.com/roadmap-to-computer-vision-79106beb8be4) in
    order to create new features for training classifiers (Figure 2). This technique
    is implemented by constructing a histogram counting all the words in our document
    (not taking into account the word order and syntax rules).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/1a4c7a49429f7625947a25b13bb77af0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Bag of Words [2]'
  prefs: []
  type: TYPE_NORMAL
- en: One of the main problems which can limit the efficacy of this technique is the
    presence of prepositions, pronouns, articles, etc… in our text. In fact, these
    can all be considered as words which are likely to appear frequently in our text
    even without necessarily being really informative in finding out what are the
    main characteristics and topics in our document.
  prefs: []
  type: TYPE_NORMAL
- en: In order to solve this type of problem, a technique called “Term Frequency-Inverse
    Document Frequency” (TFIDF) is commonly used. TFIDF aims to rescale the words
    count frequency in our text by considering how frequently each of the words in
    our text appears overall in a large sample of texts. Using this technique, we
    will then reward words (scaling up their frequency value) which appear quite commonly
    in our text but rarely in other texts, while punishing words (scaling down their
    frequency value) which appear frequently in both our text and other texts (such
    as prepositions, pronouns, etc…).
  prefs: []
  type: TYPE_NORMAL
- en: Latent Dirichlet Allocation (LDA)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Latent Dirichlet Allocation (LDA) is a type of Topic Modelling technique. Topic
    Modelling is a field of research focused on finding out ways to cluster documents
    in order to discover latent distinguishing markers which can characterize them
    based on their content (Figure 3). Therefore, Topic Modelling can also be considered
    in this ambit as a [dimensionality reduction technique](https://towardsdatascience.com/feature-extraction-techniques-d619b56e31be) since
    it allows us to reduce our initial data to a limited set of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/cdec39b743e04d96d3f4444ce0fb6fb0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Topic Modelling [3]'
  prefs: []
  type: TYPE_NORMAL
- en: Latent Dirichlet Allocation (LDA) is an unsupervised learning technique used
    to find out latent topics which can characterize different documents and cluster
    together similar ones. This algorithm takes as input the number ***N*** of topics
    which are believed exists and then groups the different documents into ***N*** clusters
    of documents which are closely related to each other.
  prefs: []
  type: TYPE_NORMAL
- en: What characterises LDA from other clustering techniques such as K-Means Clustering
    is that LDA is a soft-clustering technique (each document is assigned to a cluster
    based on a probability distribution). For example, a document can be assigned
    to a Cluster A because the algorithm determines that it is 80% likely that this
    document belongs to this class, while still taking into account that some characteristics
    embedded into this document (the remaining 20%) are more likely to belong instead
    to a second Cluster B.
  prefs: []
  type: TYPE_NORMAL
- en: Word Embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Word Embeddings are one of the most common ways to encode words as vectors of
    numbers which can then fed in into our Machine Learning models for inference.
    Word Embeddings aim to reliably transform our words into a vector space so that
    similar words are represented by similar vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/1d23da0d54d8af70e47056a3cf0967c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Word Embedding [4]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nowadays, there are three main techniques used in order to create Word Embeddings:
    [Word2Vec](https://en.wikipedia.org/wiki/Word2vec), [GloVe](https://en.wikipedia.org/wiki/GloVe_(machine_learning))
    and [fastText](https://en.wikipedia.org/wiki/FastText). All these three techniques,
    use a shallow neural network in order to create the desired word embedding.'
  prefs: []
  type: TYPE_NORMAL
- en: In case you can be interested in finding out more about how Word Embeddings
    works, [this article is a great place where to start.](https://machinelearningmastery.com/what-are-word-embeddings/)
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sentiment Analysis is an NLP technique commonly used in order to understand
    if some form of text expresses positive, negative or neutral sentiment about a
    topic. This can be particularly useful to do when for example trying to find out
    what is the general public opinion (through online reviews, tweets, etc…) about
    a topic, product or a company.
  prefs: []
  type: TYPE_NORMAL
- en: In sentiment analysis, sentiments in texts are usually represented as a value
    between -1 (negative sentiment) and 1 (positive sentiment) referred to as polarity.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment Analysis can be considered as an Unsupervised Learning technique since
    we are not usually provided with handcrafted labels for our data. In order to
    overcome this obstacle, we make use of prelabeled lexicons (a book of words) which
    had been created to quantify the sentiment of a large number of words in different
    contexts. Some examples of widely used lexicons in sentiment analysis are [TextBlob](https://github.com/sloria/TextBlob/tree/eb08c120d364e908646731d60b4e4c6c1712ff63) and [VADER](https://github.com/cjhutto/vaderSentiment).
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Transformers](http://jalammar.github.io/illustrated-transformer/) represent
    the current state of the art NLP models in order to analyse text data. Some examples
    of widely known Transformers models are [BERT](https://arxiv.org/abs/1810.04805) and [GTP2](https://openai.com/blog/better-language-models/).'
  prefs: []
  type: TYPE_NORMAL
- en: Before the creation of Transformers, Recurrent Neural Networks (RNNs) represented
    the most efficient way to analyse sequentially text data for prediction but this
    approach found quite difficult to reliably make use of long term dependencies
    (eg. our network might find difficult to understand if a word fed in several iterations
    ago might result to be useful for the current iteration).
  prefs: []
  type: TYPE_NORMAL
- en: Transformers successfully managed to overcome this limitation thanks to a mechanism
    called [Attention](https://arxiv.org/pdf/1706.03762.pdf) (which is used in order
    to determine which parts of the text to focus on and give more weight). Additionally,
    Transformers made easier to process text data in parallel rather than sequentially
    (therefore improving execution speed).
  prefs: []
  type: TYPE_NORMAL
- en: Transformers can nowadays be easily implemented in Python thanks to [Hugging
    Face library](https://huggingface.co/).
  prefs: []
  type: TYPE_NORMAL
- en: Text Prediction Demonstration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Text prediction is one of the tasks which can be easily implemented using Transformers
    such as GPT2\. In this example, we will give as input a quote from “[The Shadow
    of the Wind](https://en.wikipedia.org/wiki/The_Shadow_of_the_Wind)” by Carlos
    Ruiz Zafón and our transformer will then generate other 50 characters which should
    logically follow our input data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As can be seen from our example output shown above, our GPT2 model performed
    quite well in creating a resealable continuation for our input string.
  prefs: []
  type: TYPE_NORMAL
- en: An example notebook which you can run in order to generate your own text is
    available at [this link.](https://drive.google.com/open?id=1UVfieBsf4gb6J_s_FaRA93D0ueO4-7JE)
  prefs: []
  type: TYPE_NORMAL
- en: '*I hope you enjoyed this article, thank you for reading!*'
  prefs: []
  type: TYPE_NORMAL
- en: Contacts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you want to keep updated with my latest articles and projects [follow me
    on Medium](https://medium.com/@pierpaoloippolito28?source=post_page---------------------------) and
    subscribe to my [mailing list](http://eepurl.com/gwO-Dr?source=post_page---------------------------).
    These are some of my contacts details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Linkedin](https://uk.linkedin.com/in/pier-paolo-ippolito-202917146?source=post_page---------------------------)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Personal Blog](https://pierpaolo28.github.io/blog/?source=post_page---------------------------)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Personal Website](https://pierpaolo28.github.io/?source=post_page---------------------------)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Medium Profile](https://towardsdatascience.com/@pierpaoloippolito28?source=post_page---------------------------)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GitHub](https://github.com/pierpaolo28?source=post_page---------------------------)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kaggle](https://www.kaggle.com/pierpaolo28?source=post_page---------------------------)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bibliography
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[1] Extract Custom Keywords using NLTK POS tagger in python, Thinkinfi, Anindya
    Naskar. Accessed at: [https://www.thinkinfi.com/2018/10/extract-custom-entity-using-nltk-pos.html](https://www.thinkinfi.com/2018/10/extract-custom-entity-using-nltk-pos.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Comparison of word bag model BoW and word set model SoW, ProgrammerSought.
    Accessed at: [http://www.programmersought.com/article/4304366575/;jsessionid=0187F8E68A22612555B437068028C012](http://www.programmersought.com/article/4304366575/;jsessionid=0187F8E68A22612555B437068028C012)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Topic Modeling: Art of Storytelling in NLP,'
  prefs: []
  type: TYPE_NORMAL
- en: TechnovativeThinker. Accessed at: [https://medium.com/@MageshDominator/topic-modeling-art-of-storytelling-in-nlp-4dc83e96a987](https://medium.com/@MageshDominator/topic-modeling-art-of-storytelling-in-nlp-4dc83e96a987)
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Word Mover’s Embedding: Universal Text Embedding from Word2Vec, IBM Research
    Blog. Accessed at: [https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/](https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Pier Paolo Ippolito](https://www.linkedin.com/in/pierpaolo28/)** is
    a Data Scientist and MSc in Artificial Intelligence graduate from the University
    of Southampton. He has a strong interest in AI advancements and machine learning
    applications (such as finance and medicine). Connect with him on [Linkedin](https://www.linkedin.com/in/pierpaolo28/).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/roadmap-to-natural-language-processing-nlp-38a81dcff3a6).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Accelerated Natural Language Processing: A Free Course From Amazon](/2020/08/accelerated-nlp-free-amazon-machine-learning-university.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An Introduction to NLP and 5 Tips for Raising Your Game](/2020/09/introduction-nlp-5-tips-raising-your-game.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting Started with PyTorch](/2020/10/getting-started-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
