- en: 'Getting Started with PyTorch Part 1: Understanding How Automatic Differentiation
    Works'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 PyTorch 第 1 部分：理解自动微分是如何工作的
- en: 原文：[https://www.kdnuggets.com/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html/2](https://www.kdnuggets.com/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html/2](https://www.kdnuggets.com/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html/2)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html?page=2#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html?page=2#comments)'
- en: 'Building Block #3 : Variables and Autograd'
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '构建块 #3：变量和 Autograd'
- en: PyTorch accomplishes what we described above using the *Autograd* package.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 使用*Autograd*包实现了我们上面描述的功能。
- en: Now, there are basically three important things to understand about how *Autograd* works.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，了解*Autograd*如何工作有三个重要的方面。
- en: '**Building Block #3.1 : Variable**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**构建块 #3.1：变量**'
- en: The *Variable*, just like a *Tensor* is a class that is used to hold data. It
    differs, however, in the way it’s meant to be used. ***Variables* are specifically
    tailored to hold values which change during training of a neural network, i.e.
    the learnable paramaters of our network. **Tensors on the other hand are used
    to store values that are not to be learned. For example, a Tensor maybe used to
    store the values of the loss generated by each example.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*变量*就像一个*张量*，是一个用于存储数据的类。然而，它在使用方式上有所不同。***变量***专门用于保存在神经网络训练过程中会变化的值，即我们网络的可学习参数。**张量**则用于存储不会被学习的值。例如，张量可能用于存储每个示例生成的损失值。'
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A *Variable* class wraps a tensor. You can access this tensor by calling ***.data***attribute
    of a Variable.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*变量*类封装了一个张量。你可以通过调用变量的***.data***属性来访问这个张量。'
- en: The *Variable* also stores the gradient of a scalar quantity (say, loss) with
    respect to the parameter it holds. This gradient can be accessed by calling the ***.grad*** attribute.
    This is basically the gradient computed up to this particular node, and the gradient
    of the every subsequent node, can be computed by multiplying the *edge weight *with
    the gradient computed at the node just before it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*变量*还存储了标量量（比如损失）相对于它所持有的参数的梯度。可以通过调用***.grad***属性来访问这个梯度。这基本上是计算到当前节点的梯度，而每个后续节点的梯度可以通过将*边权重*与前一个节点计算出的梯度相乘来计算。'
- en: The third attribute a *Variable* holds is a ***grad_fn***, a *Function *object
    which created the variable.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*变量*持有的第三个属性是***grad_fn***，这是创建该变量的*函数*对象。'
- en: '![](../Images/db001b7058cc95475452d2b2f0c97638.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db001b7058cc95475452d2b2f0c97638.png)'
- en: '**NOTE: **PyTorch 0.4 merges the Variable and Tensor class into one, and Tensor
    can be made into a “Variable” by a switch rather than instantiating a new object.
    But since, we’re doing v 0.3 in this tutorial, we’ll go ahead.'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** PyTorch 0.4 将变量和张量类合并为一个，张量可以通过切换而不是实例化新对象来变成“变量”。但由于我们在本教程中使用的是 v
    0.3，我们将继续进行。'
- en: '**Building Block #3.2 : Function**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**构建块 #3.2：函数**'
- en: Did I say *Function* above? It is basically an abstraction for, well, a function.
    Something that takes an input, and returns an output. For example, if we have
    two variables, *a *and *b*, then if,
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我刚才提到的*函数*基本上是对函数的抽象。它接受输入并返回输出。例如，如果我们有两个变量，*a*和*b*，那么如果，
- en: '*c = a + b*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*c = a + b*'
- en: Then *c* is a new variable, and it’s *grad_fn *is something called *AddBackward *(PyTorch’s
    built-in function for adding two variables)*, *the function which took *a* and *b* as
    input, and created *c*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然后*c*是一个新变量，它的*grad_fn*是一个叫做*AddBackward*的东西（PyTorch 用于将两个变量相加的内置函数），这个函数接受了*a*和*b*作为输入，并创建了*c*。
- en: Then, you may ask, why is a need for an entire new class, when python does provide
    a way to define function?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可能会问，为什么需要一个全新的类，而 Python 本身提供了定义函数的方法？
- en: 'While training neural networks, there are two steps: the forward pass, and
    the backward pass. Normally, if you were to implement it using python functions,
    you will have to define two functions. One, to compute the output during forward
    pass, and another, to compute the gradient to be propagated.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，有两个步骤：前向传播和反向传播。通常，如果你使用 Python 函数来实现，你将需要定义两个函数：一个用于计算前向传播过程中的输出，另一个用于计算需要传播的梯度。
- en: '**PyTorch abstracts the need to write two separate functions (for forward,
    and for backward pass), into two member of functions of a single class called *torch.autograd.Function.***'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**PyTorch 将需要编写两个单独的函数（用于前向传播和反向传播）的需求抽象成一个名为 *torch.autograd.Function* 的单一类中的两个成员函数。**'
- en: PyTorch combines *Variables* and *Functions* to create a computation graph.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 结合了 *Variables* 和 *Functions* 来创建计算图。
- en: '**Building Block #3.3 : Autograd**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**构建块 #3.3 : 自动求导**'
- en: Let us now dig into how PyTorch creates a computation graph. First, we define
    our variables.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解 PyTorch 如何创建计算图。首先，我们定义我们的变量。
- en: The result of the above lines of code is,
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码行的结果是，
- en: Now, let’s dissect what the hell just happened here. If you look at the source
    code, here is how things go.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们剖析一下刚才发生了什么。如果你查看源代码，事情是这样发展的。
- en: '**Define the *leaf *variables of the graph (Lines 5–9). **We start by defining
    a bunch of “variables” (Normal, python usage of language, not pytorch *Variables*).
    If you notice, the values we defined are the leaf nodes in the our computation
    graph. It only makes sense that we have to define them since these nodes aren’t
    result of any computation. At this point, these guys now occupy memory in our
    Python namespace. Means, they are hundred percent real. We **must** set the ***requires_grad***attribute
    to True, otherwise, these Variables won’t be included in the computation graph,
    and no gradients would be computed for them (and other variables, that depend
    on these particular variables for gradient flow).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义图的*叶子*变量（第 5–9 行）。** 我们开始定义一堆“变量”（普通的 Python 语言使用，而非 pytorch 的 *Variables*）。如果你注意到，我们定义的值是计算图中的叶子节点。因为这些节点不是任何计算的结果，所以我们必须定义它们。在这一点上，这些变量现在占据了我们
    Python 命名空间中的内存。这意味着它们是真实存在的。我们**必须**将 ***requires_grad*** 属性设置为 True，否则这些 Variables
    不会被包含在计算图中，也不会为它们（以及依赖于这些特定变量以进行梯度流动的其他变量）计算梯度。'
- en: '**Create the graph (Lines 12–15)**. Till now, there is nothing such as computation
    graph in our memory. Only the leaf nodes, but as soon as you write lines 12–15,
    a graph is being generated **ON THE FLY. REALLY IMPORTANT TO NAIL THIS DETAIL.
    ON THE FLY. **When you write *b =w1*a*, it’s when the graph creation kicks in,
    and continues until line 15\. This is precisely the forward pass of our model,
    when the output is being calculated from inputs. The *forward* function of each
    variable may cache some input values to be used while computing the gradient on
    the backward pass. (For example, if our forward function computes *W*x*, then *d(W*x)/d(W)* is *x*,
    the input that needs to be cached)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建图（第 12–15 行）**。到目前为止，我们的内存中还没有计算图。只有叶子节点，但一旦你编写第 12–15 行，图就会**即时生成。非常重要的是要掌握这一细节。即时生成。**
    当你写 *b =w1*a* 时，就是图创建开始的时刻，并持续到第 15 行。这正是我们模型的前向传播阶段，当输出从输入中计算出来时。每个变量的 *forward* 函数可能会缓存一些输入值以在反向传播时计算梯度。（例如，如果我们的前向函数计算 *W*x*，那么 *d(W*x)/d(W)* 就是 *x*，需要缓存的输入）'
- en: Now, the reason I told you the graph I drew earlier wasn’t exactly accurate?
    Because when PyTorch makes a graph, it’s not the *Variable *objects that are the
    nodes of the graph. It’s a *Function *object, precisely, the *grad_fn* of each *Variable *that
    forms the nodes of the graph. So, the PyTorch graph would look like.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，我告诉你我之前绘制的图不完全准确的原因是什么？因为当 PyTorch 创建图时，图中的节点不是 *Variable* 对象。实际上，是每个 *Variable* 的 *grad_fn* 构成了图中的节点。所以，PyTorch
    图的样子会是这样。
- en: '![](../Images/bd1ab5ae2e471579c37576784c3b1341.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd1ab5ae2e471579c37576784c3b1341.png)'
- en: Each Function is a node in the PyTorch computation graph.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Function 是 PyTorch 计算图中的一个节点。
- en: I’ve represented the leaf nodes, by their names, but they too have their *grad_fn’*s
    (which return a None value . It makes sense, as you can’t backpropagate beyond
    leaf nodes). The rest of nodes are now replaced by their *grad_fn’*s. We see that
    the single node *d* is replaced by three Functions, two multiplications, and an
    addition, while loss, is replaced by a *minus* Function.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我已经通过它们的名称表示了叶子节点，但它们也有自己的 *grad_fn* （返回 None 值）。这很有意义，因为你不能在叶子节点之外进行反向传播。其余的节点现在被它们的 *grad_fn* 替换。我们看到单个节点 *d* 被三个
    Functions、两个乘法和一个加法替换，而损失则被一个 *minus* Function 替换。
- en: '**Compute the Gradients (Line 18). **We now compute the gradients by calling
    the *.backward() *function on *L*. What exactly is going on here? First, the gradient
    at L, is simply 1 (*dL / dL*). **Then, we invoke it’s *backward*function, which
    basically has a job of computing the gradients of the output of the *Function *object,
    w.r.t to the inputs of the *Function*object. **Here, L is the result of 10 — d,
    which means, backwards function will compute the gradient (*dL/dd) *as -1.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算梯度（第18行）。**我们现在通过在*L*上调用`*.backward()*`函数来计算梯度。这里到底发生了什么？首先，L的梯度是1（*dL
    / dL*）。**然后，我们调用它的*backward*函数，这个函数的任务基本上是计算*Function*对象的输出相对于*Function*对象输入的梯度。**在这里，L是10
    - d的结果，这意味着，反向函数将计算梯度（*dL/dd*）为-1。'
- en: Now, this computed gradient is multiplied by the accumulated gradient (Stored
    in the *grad* attribute of the* Variable* corresponding to the current node, which
    is *dL/dL = 1 *in our case), and then sent to input node, to be stored in the ***grad *attribute
    of the Variable corresponding to input node.** Technically, what we have done
    is apply the chain rule (*dL/dL*) * (*dL/dd*) = *dL/dd.*
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，这个计算出的梯度与累计梯度相乘（存储在与当前节点对应的*Variable*的*grad*属性中，在我们的例子中是*dL/dL = 1*），然后发送到输入节点，存储在与输入节点对应的*Variable*的***grad*属性中。**从技术上讲，我们所做的就是应用链式法则（*dL/dL*）
    × （*dL/dd*） = *dL/dd*。
- en: Now, let us understand how gradient is propagated for the *Variable* *d. d *is
    calculated from it’s inputs (w3, w4, b, c). In our graph, it consists of 3 nodes,
    2 multiplications and 1 addition.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，让我们理解梯度是如何在*Variable* *d*上进行传播的。*d*是从它的输入（w3, w4, b, c）计算出来的。在我们的图中，它包含3个节点，2次乘法和1次加法。
- en: First, the function *AddBackward (*representing addition operation of node *d* in
    our graph) computes the gradient of it’s output (*w3*b + w4*c*) w.r.t it’s inputs
    (*w3*b and w4*c *), which is (1 for both). Now, these *local*gradients are multiplied
    by accumulated gradient (*dL/dd *x *1* = -1 for both), and the results are saved
    in the *grad* attribute of the respective input nodes.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，函数*AddBackward*（代表我们图中节点*d*的加法操作）计算其输出（*w3*b + w4*c*）相对于其输入（*w3*b和w4*c*）的梯度（两者都是1）。现在，这些*local*梯度与累计梯度（*dL/dd*
    × *1* = -1）相乘，结果保存在各自输入节点的*grad*属性中。
- en: Then, the Function *MulBackward (*representing multiplication operation of *w3*c*)
    computes the gradient of it’s input output w.r.t to it’s inputs (*w3 and c) *a*s
    (c *and *w3) *respectively. The local gradients are multiplied by accumulated
    gradient (*dL/d(w3*c)* = -1). The resultant value (*-1 *x* c *and -1 x *w3*) is
    then stored in *grad* attribute of *Variables* *w3* and *c *respectively.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，函数*MulBackward*（代表*w3*c*的乘法操作）计算其输入输出相对于其输入（*w3和c*）的梯度。局部梯度与累计梯度（*dL/d(w3*c)*
    = -1）相乘。结果值（*-1 × c* 和 *-1 × w3*）然后存储在*Variables*的*w3*和*c*的*grad*属性中。
- en: Gradients for all the nodes are computed in a similar fashion.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有节点的梯度都是以类似的方式计算的。
- en: The gradient of *L* w.r.t any node can be accessed by calling .*grad* on the
    Variable corresponding to that node, **given it’s a leaf node **(PyTorch’s default
    behavior doesn’t allow you to access gradients of non-leaf nodes. More on that
    in a while). Now that we have got our gradients, we can update our weights using
    SGD or whatever optimization algorithm you like.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何节点的*L*的梯度可以通过调用*grad*来访问，该节点对应的Variable，**前提是它是一个叶节点**（PyTorch的默认行为不允许访问非叶节点的梯度。稍后会详细说明）。现在我们得到了梯度，我们可以使用SGD或任何你喜欢的优化算法来更新权重。
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: and so forth.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如此等等。
- en: Some Nifty Details of Autograd
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动微分的一些巧妙细节
- en: So, didn’t I tell you you can’t access the *grad *attribute of non-leaf *Variables*.
    Yeah, that’s the default behavior. You can override it by calling .*retain_grad()*on
    the *Variable* just after defining it and then you’d be able to access it’s *grad *attribute.
    But really, what the heck is going on under the wraps.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我不是告诉过你不能访问非叶节点的*grad*属性吗？是的，这是默认行为。你可以通过在定义后调用`*.retain_grad()*`来覆盖这一行为，然后你就能够访问它的*grad*属性。但真的，底下究竟发生了什么呢？
- en: '**Dynamic Computation Graphs**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态计算图**'
- en: PyTorch creates something called a **Dynamic Computation Graph, **which means
    that the graph is generated on the fly. **Until the *forward* function of a Variable
    is called, there exists no node for the *Variable (it’s grad_fn)* in the graph.** The
    graph is created as a result of *forward* function of many *Variables* being invoked.
    Only then, the buffers are allocated for the graph and intermediate values (used
    for computing gradients later). When you call *backward()*, as the gradients are
    computed, these buffers are essentially freed, and the graph is destroyed. You
    can try calling *backward*() more than once on a graph, and you’ll see PyTorch
    will give you an error. This is because the graph gets destroyed the first time *backward()* is
    called and hence, there’s no graph to call backward upon the second time.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 创建了一种叫做 **动态计算图** 的东西，这意味着图是在运行时动态生成的。**在调用 *Variable* 的 *forward* 函数之前，图中不存在
    *Variable（它的 grad_fn）* 的节点。** 图是由于调用了多个 *Variables* 的 *forward* 函数而创建的。仅在此时，图和中间值（用于后续计算梯度）才会被分配缓冲区。当你调用
    *backward()* 时，随着梯度的计算，这些缓冲区基本上会被释放，图也会被销毁。你可以尝试在图上多次调用 *backward()*，你会看到 PyTorch
    会给你一个错误。这是因为图在第一次调用 *backward()* 时被销毁了，因此在第二次调用时没有图可供反向传播。
- en: If you call *forward* again, an entirely new graph is generated. With new memory
    allocated to it.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你再次调用 *forward*，将会生成一个全新的图，并为其分配新的内存。
- en: '**By default, only the gradients (*grad* attribute) for leaf nodes are saved,
    and the gradients for non-leaf nodes are destroyed.** But this behavior can be
    changed as described above.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**默认情况下，只有叶子节点的梯度（*grad* 属性）被保存，而非叶子节点的梯度则会被销毁。** 但这种行为可以根据上述描述进行更改。'
- en: This is in contrast to the **Static Computation Graphs**, used by TensorFlow
    where the graph is declared ***before*** running the program. The dynamic graph
    paradigm allows you to make changes to your network architecture *during*runtime,
    as a graph is created only when a piece of code is run. This means a graph may
    be redefined during the lifetime for a program. This, however, is not possible
    with static graphs where graphs are created before running the program, and merely
    executed later. Dynamic graphs also make debugging way easier as the source of
    error is easily traceable.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这与 **静态计算图** 相对，静态计算图用于 TensorFlow，其中图是在运行程序 ***之前*** 声明的。动态图范式允许你在运行时对网络架构进行更改，因为图仅在代码片段运行时才会创建。这意味着图可能在程序的生命周期内被重新定义。然而，这在静态图中是不可能的，因为静态图是在运行程序之前创建的，仅在后续执行。动态图还使得调试变得更加容易，因为错误的源头更容易追踪。
- en: Some Tricks of Trade
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一些行业技巧
- en: '**requires_grad**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**requires_grad**'
- en: This is an attribute of the *Variable* class. By default, it’s False. It comes
    handy when you have to freeze some layers, and stop them from updating parameters
    while training. You can simply set the *requires_grad* to False, and these *Variable*s
    won’t be included in the computation graph. Thus, no gradient would be propagated
    to them, or to those layers which depend upon these layers for gradient flow. *requires_grad*, **when
    set to True is** **contagious**, meaning even if one operand of an operation has *requires_grad* set
    to True, so will the result.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 *Variable* 类的一个属性。默认情况下，它是 False。当你需要冻结某些层并在训练时阻止它们更新参数时，它非常方便。你可以简单地将 *requires_grad*
    设置为 False，这些 *Variables* 将不会被包含在计算图中。因此，没有梯度会传播到它们，或者到那些依赖于这些层进行梯度流动的层。*requires_grad*，**当设置为
    True 时是** **传染性的**，这意味着即使一个操作的一个操作数具有 *requires_grad* 设置为 True，结果也将是如此。
- en: '![](../Images/df9e5f91cec8821b5f0aff2c1a35d7ae.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df9e5f91cec8821b5f0aff2c1a35d7ae.png)'
- en: '**b **is not included in the graph. No gradient is backpropagated through ***b*** now. **a** only
    gets gradients from **c **now. Even if **w1 **has requires_grad = True, there
    is no way it can receive gradients.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**b** 不包括在图中。现在没有梯度通过 ***b*** 进行反向传播。**a** 现在仅从 **c** 获取梯度。即使 **w1** 的 requires_grad
    = True，也无法接收梯度。'
- en: '**volatile**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**volatile**'
- en: This again is a attribute of a *Variable* class, which causes a *Variable* to
    be excluded from the computation graph when it is set to True. It might seem quite
    similar to *requires_grad*, given it’s also **contagious when set True**. But
    it has a higher precedence than *requires_grad***. A variable with *requires_grad*equals
    to True and *volatile* equals to True, would not be included in the computation
    graph.**
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这同样是 *Variable* 类的一个属性，它使得当它被设置为 True 时，*Variable* 被排除在计算图之外。它可能看起来与 *requires_grad*
    非常相似，因为它也是 **当设置为 True 时是传染性的**。但它的优先级高于 *requires_grad*。一个 *requires_grad* 等于
    True 和 *volatile* 等于 True 的变量，将不会被包含在计算图中。
- en: You might think, what’s the need of having another switch to override *requires_grad*,
    when we can simply set *requires_grad* to False? Let me digress for a while.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么还需要另一个开关来覆盖*requires_grad*，而我们可以简单地将*requires_grad*设置为 False？让我稍微岔开一下。
- en: Not creating a graph is extremely useful when we are doing inference, and don’t
    need gradients. First, overhead to create a computation graph is eliminated, and
    the speed is boosted. Second, if we create a graph, and since there is no *backward *being
    called afterwords, the buffers used to cache values are never freed and may lead
    to you running out of memory.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在推断时不创建图形非常有用，因为我们不需要梯度。首先，消除了创建计算图形的开销，速度得到了提升。其次，如果我们创建了图形，并且没有在之后调用*backward*，则用于缓存值的缓冲区将永远不会释放，可能导致内存耗尽。
- en: Generally, we have many layers in the a neural network, for which we might have
    set *requires_grad* to True while training. To prevent a graph from being made
    at inference, we can do either of two things. Set r*equires_grad* False on **all** the
    layers (maybe, 152 of them?). **Or, set *volatile* True only on the input, and
    we’re assured no resultant operation will result in a graph being made. **Your
    choice.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们在神经网络中有许多层，可能在训练时已将*requires_grad*设置为 True。为了防止在推断时创建图形，我们可以采取以下两种方法之一。将*requires_grad*设置为**False**（可能是152层？）。**或者，只在输入上设置*volatile*为
    True，我们可以确保不会创建图形。**你可以选择。
- en: '![](../Images/91704abbdd0a52fd54d325cb261738ee.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91704abbdd0a52fd54d325cb261738ee.png)'
- en: No graph is created for **b or **any node that depends on** b.**
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**b或任何依赖于b的节点**，不会创建图形。
- en: '**NOTE: **PyTorch 0.4 has no volatile argument for a combined Tensor/Variable
    class. Instead, the inference code should be put in a torch.no_grad() context
    manager.'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** PyTorch 0.4 没有针对合并 Tensor/Variable 类的 volatile 参数。相反，推断代码应放在 torch.no_grad()
    上下文管理器中。'
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Conclusion
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: So, that was *Autograd* for you. Understanding how *Autograd* works can save
    you a lot of headache when you’re stuck somewhere, or dealing with errors when
    you’re starting out. Thanks for reading so far. I intend to write more tutorials
    on PyTorch, dealing with how to use inbuilt functions to quickly create complex
    architectures (or, maybe not so quickly, but faster than coding block by block).
    So, stay tuned!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是*Autograd*。了解*Autograd*的工作原理可以在你卡住或处理错误时节省很多麻烦。感谢你阅读到这里。我打算写更多关于 PyTorch
    的教程，介绍如何使用内置函数快速创建复杂架构（或者，可能不是那么快，但比逐块编码更快）。敬请期待！
- en: '**Further Reading**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: '[Understanding Backpropagation](http://neuralnetworksanddeeplearning.com/chap2.html)'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[理解反向传播](http://neuralnetworksanddeeplearning.com/chap2.html)'
- en: '[Understanding the Chain Rule](https://www.youtube.com/watch?v=MKWBx78L7Qg)'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[理解链式法则](https://www.youtube.com/watch?v=MKWBx78L7Qg)'
- en: Classes in Python [Part 1](https://www.hackerearth.com/practice/python/object-oriented-programming/classes-and-objects-i/tutorial/) and [Part
    2](https://www.hackerearth.com/practice/python/object-oriented-programming/classes-and-objects-ii-inheritance-and-composition/tutorial/)
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Python 类 [第 1 部分](https://www.hackerearth.com/practice/python/object-oriented-programming/classes-and-objects-i/tutorial/)
    和 [第 2 部分](https://www.hackerearth.com/practice/python/object-oriented-programming/classes-and-objects-ii-inheritance-and-composition/tutorial/)
- en: '[PyTorch’s Official Tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[PyTorch 官方教程](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)'
- en: '**Bio: [Ayoosh Kathuria](https://www.linkedin.com/in/ayoosh-kathuria-44a319132/)**
    is passionate about computer vision, and teaching machines how to extract meaningful
    information from their surroundings. He is currently working on improving object
    detection by leveraging context.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**简历：[Ayoosh Kathuria](https://www.linkedin.com/in/ayoosh-kathuria-44a319132/)**
    对计算机视觉充满热情，致力于教机器如何从环境中提取有意义的信息。他目前正在通过利用上下文来改进目标检测。'
- en: '[Original](https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec).
    Reposted with permission.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec)。经许可转载。'
- en: '**Related:**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[A Simple Starter Guide to Build a Neural Network](/2018/02/simple-starter-guide-build-neural-network.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建神经网络的简单入门指南](/2018/02/simple-starter-guide-build-neural-network.html)'
- en: '[Comparing Deep Learning Frameworks: A Rosetta Stone Approach](/2018/03/deep-learning-frameworks.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[比较深度学习框架：罗塞塔石碑方法](/2018/03/deep-learning-frameworks.html)'
- en: '[Ranking Popular Deep Learning Libraries for Data Science](/2017/10/ranking-popular-deep-learning-libraries-data-science.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[排名前列的数据科学深度学习库](/2017/10/ranking-popular-deep-learning-libraries-data-science.html)'
- en: '* * *'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT 工作'
- en: '* * *'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Getting Started with PyTorch Lightning](https://www.kdnuggets.com/2022/12/getting-started-pytorch-lightning.html)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[入门 PyTorch Lightning](https://www.kdnuggets.com/2022/12/getting-started-pytorch-lightning.html)'
- en: '[Getting Started with PyTorch in 5 Steps](https://www.kdnuggets.com/5-steps-getting-started-pytorch)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 5 个步骤入门 PyTorch](https://www.kdnuggets.com/5-steps-getting-started-pytorch)'
- en: '[How to Ace Data Science Assessment Test by Using Automatic EDA Tools](https://www.kdnuggets.com/2022/04/ace-data-science-assessment-test-automatic-eda-tools.html)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何通过使用自动 EDA 工具来应对数据科学评估测试](https://www.kdnuggets.com/2022/04/ace-data-science-assessment-test-automatic-eda-tools.html)'
- en: '[How I Did Automatic Image Labeling Using Grounding DINO](https://www.kdnuggets.com/2023/05/automatic-image-labeling-grounding-dino.html)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我如何使用 Grounding DINO 实现自动图像标注](https://www.kdnuggets.com/2023/05/automatic-image-labeling-grounding-dino.html)'
- en: '[How ChatGPT Works: The Model Behind The Bot](https://www.kdnuggets.com/2023/04/chatgpt-works-model-behind-bot.html)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT 如何工作：模型背后的秘密](https://www.kdnuggets.com/2023/04/chatgpt-works-model-behind-bot.html)'
- en: '[The Burtch Works 2023 Data Science & AI Professionals Salary Report…](https://www.kdnuggets.com/2023/08/burtch-works-2023-data-science-ai-professionals-salary-report.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Burtch Works 2023 数据科学与 AI 专业人士薪资报告…](https://www.kdnuggets.com/2023/08/burtch-works-2023-data-science-ai-professionals-salary-report.html)'
