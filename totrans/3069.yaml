- en: Recent Advances for a Better Understanding of Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最近为了更好理解深度学习的进展
- en: 原文：[https://www.kdnuggets.com/2018/10/recent-advances-deep-learning.html](https://www.kdnuggets.com/2018/10/recent-advances-deep-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/10/recent-advances-deep-learning.html](https://www.kdnuggets.com/2018/10/recent-advances-deep-learning.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Arthur Pesah](https://twitter.com/artix41?lang=en)**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由[Arthur Pesah](https://twitter.com/artix41?lang=en)**。'
- en: '![](../Images/ca89e4b8b2a89235e376dbea42ef3019.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca89e4b8b2a89235e376dbea42ef3019.png)'
- en: '*I would like to live in a world whose systems are build on **rigorous, reliable,
    verifiable knowledge**, and not on alchemy. […] Simple experiments and simple
    theorems are the **building blocks** that help understand complicated larger phenomena.*'
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我希望生活在一个建立在**严谨、可靠、可验证的知识**基础上的世界，而不是炼金术的世界。[...] 简单的实验和简单的定理是帮助理解复杂现象的**基石**。*'
- en: This call for a better **understanding** of deep learning was the core of Ali
    Rahimi’s [Test-of-Time Award presentation](http://www.argmin.net/2017/12/05/kitchen-sinks/) at
    NIPS in December 2017\. By comparing deep learning with alchemy, the goal of Ali
    was not to dismiss the entire field, but [“to open a conversation”](http://www.argmin.net/2017/12/11/alchemy-addendum/).
    This goal [has definitely been achieved](https://syncedreview.com/2017/12/12/lecun-vs-rahimi-has-machine-learning-become-alchemy/) and
    people [are still debating](https://twitter.com/RandomlyWalking/status/1017899452378550273) whether
    our current practice of deep learning should be considered as alchemy, engineering
    or science.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对深度学习的更好**理解**的呼吁是Ali Rahimi在2017年12月NIPS上[Test-of-Time Award presentation](http://www.argmin.net/2017/12/05/kitchen-sinks/)的核心。通过将深度学习与炼金术进行比较，Ali的目标不是要抛弃整个领域，而是[“开启对话”](http://www.argmin.net/2017/12/11/alchemy-addendum/)。这个目标[确实已实现](https://syncedreview.com/2017/12/12/lecun-vs-rahimi-has-machine-learning-become-alchemy/)且人们[仍在争论](https://twitter.com/RandomlyWalking/status/1017899452378550273)我们当前的深度学习实践是否应被视为炼金术、工程还是科学。
- en: Seven months later, the machine learning community gathered again, this time
    in Stockholm for the International Conference on Machine Learning (ICML). With
    more than 5,000 participants and 629 papers published, it was one of the most
    important events regarding fundamental machine learning research. And **deep learning
    theory** has become one of the biggest subjects of the conference.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 七个月后，机器学习社区再次聚集，这次是在斯德哥尔摩举行的国际机器学习会议（ICML）。会议吸引了超过5000名参与者和629篇论文，是关于基础机器学习研究的最重要事件之一。而**深度学习理论**已成为会议的最大主题之一。
- en: '![](../Images/6db4aa119b366573dc74fd6d3abe85ce.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6db4aa119b366573dc74fd6d3abe85ce.png)'
- en: 'This renew interest was revealed on the first day, with one of the biggest
    rooms of the conference full of machine learning practitioners ready to listen
    to the tutorial [Towards Theoretical Understanding of Deep Learning](http://unsupervised.cs.princeton.edu/deeplearningtutorial.html) by
    Sanjeev Arora. In his talk, the Professor of computer science at Princeton summarized
    the current areas of deep learning theory research, by dividing them into four
    branches:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这种重新引起的兴趣在第一天就显露出来，会议中最大的一个房间里充满了准备听取[Sanjeev Arora的深度学习理论理解](http://unsupervised.cs.princeton.edu/deeplearningtutorial.html)讲座的机器学习从业者。在他的演讲中，普林斯顿大学计算机科学教授总结了当前深度学习理论研究的领域，将其分为四个分支：
- en: '**Non Convex Optimization**: How can we understand the highly non-convex loss
    function associated with deep neural networks? Why does stochastic gradient descent
    even converge?'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非凸优化**：我们如何理解与深度神经网络相关的高度非凸损失函数？为什么随机梯度下降算法甚至能够收敛？'
- en: '**Overparametrization and Generalization**: In classical statistical theory,
    generalization depends on the number of parameters but not in deep learning. Why?
    Can we find another good measure of generalization?'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过度参数化与泛化**：在经典统计理论中，泛化依赖于参数的数量，但在深度学习中却不是这样。为什么？我们能否找到其他好的泛化衡量指标？'
- en: '**Role of Depth**: How does depth help a neural network to converge? What is
    the link between depth and generalization?'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度的作用**：深度如何帮助神经网络收敛？深度与泛化之间有什么联系？'
- en: '**Generative Models**: Why do Generative Adversarial Networks (GANs) work so
    well? What theoretical properties could we use to stabilize them or avoid mode
    collapse?'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成模型**：为什么生成对抗网络（GANs）效果如此好？我们可以利用哪些理论属性来稳定它们或避免模式崩溃？'
- en: In this series of articles, we will try to build intuition in those four areas
    based on the most recent papers, with a particular focus on ICML 2018.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这系列文章中，我们将基于最新的论文尝试在这四个领域建立直觉，特别关注ICML 2018。
- en: This first article will focus on the mysteries of non-convex optimization for
    deep networks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章将集中于深度网络中非凸优化的奥秘。
- en: '**Non-Convex Optimization**'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**非凸优化**'
- en: '![](../Images/1430371fdf266542925477c2b632559b.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1430371fdf266542925477c2b632559b.png)'
- en: '*I bet a lot of you have tried training a deep net of your own from scratch
    and walked away feeling bad about yourself because you couldn’t get it to perform.
    I don’t think it’s your fault. I think it’s gradient descent’s fault.*'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我敢打赌，你们很多人都尝试过从头开始训练一个深度网络，却因为无法使其表现良好而感到沮丧。我认为这不是你的错，而是梯度下降的问题。*'
- en: 'Stated Ali Rahimi with a provocative tone in his talk at NIPS. Stochastic Gradient
    Descent (SGD) is indeed the cornerstone of deep learning. It is supposed to find
    a solution of a highly non-convex optimization problem, and understanding when
    it works or not, and why, is one the most fundamental questions we would have
    to address in a general theory of deep learning. More specifically, the study
    of non-convex optimization for deep neural networks can be divided into two questions:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Ali Rahimi在NIPS的讲座中以挑衅的语气说道。随机梯度下降（SGD）确实是深度学习的基石。它应该能够找到一个高度非凸优化问题的解决方案，理解它何时有效或无效以及原因，是我们在深度学习的一般理论中必须解决的最根本的问题之一。更具体地说，非凸优化在深度神经网络中的研究可以分为两个问题：
- en: What does the loss function look like?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数的样子是什么？
- en: Why does SGD converge?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么SGD会收敛？
- en: '**What does the loss function look like?**'
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**损失函数的样子是什么？**'
- en: 'If I ask you to visualize a global minimum, it’s very likely that the first
    representation that will come to your mind will look something like this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我让你可视化一个全局最小值，很可能你脑海中第一个出现的表示方式会像这样：
- en: '![](../Images/3a8d0340d2e9a822ed4b2df3968d8cf0.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a8d0340d2e9a822ed4b2df3968d8cf0.png)'
- en: And it’s normal. In a 2D-world, it’s not rare to find problems, where around
    a global minimum, your function will be **strictly** convex (which means that
    the two eigenvalues of the hessian matrix at this point will be both strictly
    positive). But in a world with billions of parameters, as it is the case in deep
    learning, what are the odds that none of the directions around a global minimum
    are flat? Or equivalently that the hessian contains not a single zero (or almost
    zero) eigenvalue?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这很正常。在二维世界中，围绕全局最小值找到一个**严格**凸的问题并不罕见（这意味着此点的Hessian矩阵的两个特征值都将严格为正）。但在拥有数十亿参数的世界中，例如深度学习中，围绕全局最小值的方向没有一个是平坦的几率是多少？或者等效地，Hessian矩阵中不含单个零（或接近零）特征值的几率是多少？
- en: One of the first comment of Sanjeev Arora in his tutorial was that the number
    of possible directions that you can take on a loss function grows exponentially
    with the dimension.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Sanjeev Arora在他的教程中提到的第一个评论是，损失函数上可以采取的可能方向的数量随着维度的增加而呈指数增长。
- en: '![](../Images/e7de79b3f62f234d2f3db8df36fdf302.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7de79b3f62f234d2f3db8df36fdf302.png)'
- en: Then, intuitively, it seems likely that a global minimum will not be a point,
    but a **connected manifold**. Which means that if you’ve reached a global minimum,
    you should be able to walk around on a flat path where all the points are also
    minima. This has been experimentally proven on large networks by a team at Heidelberg
    University, in their paper[ Essentially No Barriers in Neural Network Energy Landscape](https://icml.cc/Conferences/2018/Schedule?showEvent=2780) [1].
    They argue an even more general statement, namely that any two global minima can
    be connected through a flat path.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，直观上，全局最小值似乎不可能是一个点，而是一个**连接的流形**。这意味着如果你达到了全局最小值，你应该能够在一个平坦的路径上行走，其中所有的点也都是最小值。这一点已经在大型网络中被海德堡大学的一组团队在他们的论文[ 神经网络能量景观中的基本无障碍](https://icml.cc/Conferences/2018/Schedule?showEvent=2780) [1]中通过实验验证。他们提出了一个更为普遍的论断，即任何两个全局最小值可以通过平坦路径连接。
- en: '![](../Images/1b816f84237cd9e48df1d7886083b0a0.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b816f84237cd9e48df1d7886083b0a0.png)'
- en: It was already known to be the case for a CNN on MNIST or an RNN on PTB [2],
    but this work extended that knowledge to much bigger networks (some DenseNets
    and ResNets) trained on more advanced datasets (CIFAR10 and CIFAR100). To find
    this path, they used a heuristic coming from molecular statistical mechanics,
    called AutoNEB. The idea is to create an initial path (for instance linear) between
    your two minima, and to place pivots on that path. You then iteratively modify
    the positions of the pivots, such that it minimizes the loss of each pivot and
    make sure the distances between pivots stay about the same (by modelling the space
    between pivots by springs).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 已经知道这对于 MNIST 上的 CNN 或 PTB 上的 RNN [2] 是成立的，但这项工作将这一知识扩展到更大的网络（一些 DenseNets 和
    ResNets），这些网络在更高级的数据集（CIFAR10 和 CIFAR100）上进行训练。为了找到这条路径，他们使用了来自分子统计力学的启发式方法，称为
    AutoNEB。其思想是创建一个初始路径（例如线性路径）在两个最小值之间，并在该路径上放置支点。然后，迭代地修改支点的位置，以最小化每个支点的损失，并确保支点之间的距离保持大致相同（通过用弹簧建模支点之间的空间）。
- en: 'If they didn’t prove that result theoretically, they gave some intuitive explanations
    on why such path exists:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果他们没有理论上证明这一结果，他们给出了一些关于为什么存在这样的路径的直观解释：
- en: '*If we perturb a single parameter, say by adding a small constant, but leave
    the others free to adapt to this change to still minimise the loss, it may be
    argued that by adjusting somewhat, the myriad other parameters can “make up” for
    the change imposed on only one of them*'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*如果我们扰动一个参数，比如通过添加一个小常数，但让其他参数自由适应这种变化以仍然最小化损失，可以认为通过某种调整，其他无数参数可以“弥补”对仅一个参数施加的变化*'
- en: Thus, the results of this paper can help us seeing minima in a different way,
    through the lens of overparametrization and high-dimensional spaces.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这篇论文的结果可以帮助我们通过过参数化和高维空间的视角以不同的方式看待最小值。
- en: 'More generally, when thinking about the loss function of neural network, you
    should always have in mind that the number of possible directions at a given point
    is huge. Another consequence of that is the fact that saddle points must be much
    more abundant than local minima: at a given (critical) point, among the billions
    of possible directions, it’s very likely to find one that goes down (if you’re
    not in a global minimum). This intuition was formalized rigorously and proved
    empirically in a paper published at NIPS 2014: [Identifying and attacking the
    saddle point problem in high-dimensional non-convex optimization](https://arxiv.org/abs/1406.2572) [6]'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，当考虑神经网络的损失函数时，你应该始终记住，在某一点上的可能方向数是巨大的。另一个后果是鞍点必须比局部最小值更加丰富：在给定（关键）点上，在数十亿个可能方向中，很可能会找到一个向下的方向（如果你不在全局最小值中）。这种直觉在一篇发表于
    NIPS 2014 的论文中被严谨地形式化和实证证明： [高维非凸优化中的鞍点问题识别与攻击](https://arxiv.org/abs/1406.2572)
    [6]
- en: Why does SGD converge (or not)?
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么 SGD 会收敛（或不收敛）？
- en: The second important question in optimization of deep neural networks is related
    to the convergence properties of SGD. While this algorithm has long been seen
    as a faster but approximate version of gradient descent, we now have evidence
    that SGD actually converges to better, more general, minima [3]. But can we formalize
    it and explain quantitatively the capacity of SGD to escape from local minima
    or saddle points?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 优化深度神经网络的第二个重要问题与 SGD 的收敛性质有关。虽然这个算法长期以来被视为梯度下降的更快但近似版本，但我们现在有证据表明 SGD 实际上收敛到更好、更通用的最小值
    [3]。但我们能否形式化它并定量解释 SGD 逃离局部最小值或鞍点的能力？
- en: SGD modifies the loss function
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SGD 修改了损失函数
- en: 'The paper [An Alternative View: When Does SGD Escape Local Minima?](https://arxiv.org/abs/1802.06175) [4]
    showed that performing SGD is equivalent to doing regular gradient descent on
    a convolved (thus smoothed) loss function. With that point of view and under certain
    assumptions (shown by the authors to be often true in practice), they prove that
    SGD will manage to escape local minima and converge to a small region around a
    global minimum.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 论文 [一种替代视角：SGD 何时逃离局部最小值？](https://arxiv.org/abs/1802.06175) [4] 显示，执行 SGD 等同于对卷积（因此平滑的）损失函数进行常规梯度下降。从这一观点出发，在某些假设（作者证明在实践中通常是正确的）下，他们证明了
    SGD 能够逃离局部最小值，并收敛到全局最小值附近的一个小区域。
- en: '![](../Images/e212da62e5ec63afb09a64285c93e114.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e212da62e5ec63afb09a64285c93e114.png)'
- en: SGD is governed by stochastic differential equations
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SGD 由随机微分方程支配
- en: Another approach to SGD that has really changed my vision of this algorithm
    is *continuous SGD*. The idea was presented by Yoshua Bengio during his talk [On
    stochastic gradient descent, flatness and generalization](http://www.iro.umontreal.ca/~bengioy/talks/ICMLW-nonconvex-14july2018.pptx.pdf),
    given at the ICML Workshop on Non-Convex Optimization. SGD does not move a point
    on a loss function, but a **cloud of points**, or in other words,** a distribution**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种真正改变了我对这一算法看法的方法是*连续 SGD*。这个想法是由 Yoshua Bengio 在他的讲座[关于随机梯度下降、平坦性和泛化](http://www.iro.umontreal.ca/~bengioy/talks/ICMLW-nonconvex-14july2018.pptx.pdf)中提出的，该讲座在
    ICML 非凸优化研讨会上进行。SGD 并不是在损失函数上移动一个点，而是移动一个**点云**，或者换句话说，**一个分布**。
- en: '![](../Images/65ec7412c52399c41e49c8b904527a45.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65ec7412c52399c41e49c8b904527a45.png)'
- en: Slide extracted from the presentation [On stochastic gradient descent, flatness
    and generalization](http://www.iro.umontreal.ca/~bengioy/talks/ICMLW-nonconvex-14july2018.pptx.pdf)*,
    by Y. Bengio, at ICML 2018\. He presented an alternative way to see SGD, where
    you replace points by distributions (clouds of points)*
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 幻灯片摘自讲座[关于随机梯度下降、平坦性和泛化](http://www.iro.umontreal.ca/~bengioy/talks/ICMLW-nonconvex-14july2018.pptx.pdf)*，由
    Y. Bengio 在 ICML 2018 上进行。他展示了看待 SGD 的一种替代方式，在这种方式中，你将点替换为分布（点云）*
- en: 'The size of this cloud of point (i.e. the variance of the associated distribution)
    is proportional to the factor *learning_rate / batch_size*. A proof of this is
    given in the amazing paper by Pratik Chaudhari and Stefano Soatto, [Stochastic
    gradient descent performs variational inference, converges to limit cycles for
    deep networks](https://arxiv.org/pdf/1710.11029.pdf) [5], that they presented
    during the Workshop on Geometry in Machine Learning. This formula is quite intuitive:
    a low batch size means a very noisy gradient (because computed on a very small
    subset of the dataset), and a high learning rate means noisy steps.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个点云的大小（即相关分布的方差）与因子*learning_rate / batch_size* 成正比。Pratik Chaudhari 和 Stefano
    Soatto 在研讨会上展示的精彩论文[随机梯度下降执行变分推断，收敛到深度网络的极限周期](https://arxiv.org/pdf/1710.11029.pdf)
    [5]中给出了这个证明。这个公式相当直观：较低的批量大小意味着梯度噪声很大（因为是基于数据集的一个非常小的子集计算的），而较高的学习率意味着步伐噪声较大。
- en: The consequence of seeing SGD as a distribution moving over time is that the
    equations governing the descent are now [stochastic partial differential equations](https://en.wikipedia.org/wiki/Stochastic_partial_differential_equation).
    More precisely, under certain assumptions, [5] showed that the governing equation
    is actually a [**Fokker-Planck equation**](https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 看到 SGD 作为随时间变化的分布的结果是，控制下降的方程现在是[随机偏微分方程](https://en.wikipedia.org/wiki/Stochastic_partial_differential_equation)。更准确地说，在某些假设下，[5]
    表明控制方程实际上是一个[**Fokker-Planck 方程**](https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation)。
- en: '![](../Images/93d91dd1f555979c4622afaac3b2e92f.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93d91dd1f555979c4622afaac3b2e92f.png)'
- en: Slide extracted from the presentation *High-dimensional Geometry and Dynamics
    of Stochastic Gradient Descent for Deep Networks, by P. Chaudhari and S. Soatto,
    at ICML 2018\. They showed how to pass from a discrete system to a continuous
    one described by the Fokker-Plank equation*
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 幻灯片摘自讲座*高维几何与深度网络的随机梯度下降动力学，由 P. Chaudhari 和 S. Soatto 在 ICML 2018 上进行。他们展示了如何将离散系统转化为由
    Fokker-Planck 方程描述的连续系统*
- en: In statistical physics, this type of equations describes the evolution of particles
    exposed to a drag force (that drifts the distribution, i.e. moves its mean) and
    to random forces (that diffuse the distribution, i.e. increase its variance).
    In SGD, the drag force is modeled by the true gradient while the random forces
    correspond to noise inherent to the algorithm. As you can see in the slide above,
    the diffusion term is proportional to a temperature term *T=1/β=learning_rate/(2*batch_size)*,
    which shows once again the importance of this ratio!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计物理中，这类方程描述了暴露于拖曳力（使分布漂移，即移动其均值）和随机力（使分布扩散，即增加其方差）的粒子的演化。在 SGD 中，拖曳力由真实梯度建模，而随机力对应于算法固有的噪声。正如上面的幻灯片所示，扩散项与温度项*T=1/β=learning_rate/(2*batch_size)*成正比，这再次显示了这个比率的重要性！
- en: '![](../Images/21dced20774a445c7dd54d7eba2b23d0.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21dced20774a445c7dd54d7eba2b23d0.png)'
- en: Evolution of a distribution under the Fokker-Planck equation. It drifts on the
    left and diffuses with time. Source: [Wikipedia](https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Fokker-Planck方程，分布的演化。它向左漂移并随时间扩散。来源：[Wikipedia](https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation)
- en: 'Using this framework, Chaudhari and Soatto proved that our distribution will
    monotonically converge to a certain steady distribution (in the sense of the [KL-divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这个框架，Chaudhari和Soatto证明了我们的分布将单调收敛到某个稳态分布（从[KL散度](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)的角度）：
- en: '![](../Images/dd47899de189419903eb88f049d45719.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd47899de189419903eb88f049d45719.png)'
- en: One of the main theorems of [5], proving monotonic convergence of the distribution
    to a steady state (in the sense of the KL divergence). The second equation shows
    that minimizing F is equivalent to minimizing a certain potential ϕ as well as
    maximizing the entropy of the distribution (trade-off controlled by the temperature
    1/β)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[5]中的主要定理之一，证明了分布对稳态的单调收敛（从KL散度的角度）。第二个方程表明，最小化F等同于最小化某个潜力ϕ，并最大化分布的熵（由温度1/β控制的权衡）。'
- en: 'There are several interesting points to comment in the theorem above:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述定理中有几个有趣的点需要评论：
- en: 'The functional that is minimized by SGD can be rewritten as a sum of two terms
    (Eq. 11): the expectancy of a potential Φ, and the entropy of the distribution.
    The temperature *1/β *controls the trade-off between those two terms.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SGD最小化的泛函可以重写为两个项的和（方程11）：一个潜力Φ的期望值和分布的熵。温度*1/β*控制这两个项之间的权衡。
- en: The potential Φ depends only on the data and the architecture of the network
    (and not the optimization process). If it is equal to the loss function, SGD will
    converge to a global minimum. However, the paper shows that it’s rarely the case,
    and knowing how far Φ is from the loss function will tell you how likely your
    SGD will converge.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜力Φ仅取决于数据和网络的架构（而不是优化过程）。如果它等于损失函数，SGD将收敛到全局最小值。然而，论文表明，这种情况很少发生，了解Φ离损失函数的距离将告诉你SGD收敛的可能性。
- en: The entropy of the final distribution depends on the ratio *learning_rate/batch_size*(the
    temperature). Intuitively, the entropy is related to the size of a distribution
    and having a high temperature often comes down to having a distribution with high
    variance, which usually means a flat minimum. Since flat minima are often considered
    to generalize better, it’s consistent with the empirical finding that high learning
    and low batch size often lead to better minima.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终分布的熵取决于*learning_rate/batch_size*（温度）。直观上，熵与分布的大小相关，而高温通常意味着具有高方差的分布，这通常表示一个平坦的最小值。由于平坦的最小值通常被认为具有更好的泛化性，这与高学习率和低批量大小通常导致更好最小值的经验发现是一致的。
- en: Therefore, seeing SGD as a distribution moving over time showed us that *learning_rate/batch_size *is
    more meaningful than each hyperparameter separated regarding convergence and generalization.
    Moreover, it enabled the introduction of the potentiel of a network, related to
    convergence and that could give a good metric for architecture search.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将SGD视为一个随时间变化的分布使我们发现*learning_rate/batch_size*比每个超参数单独考虑在收敛性和泛化性方面更有意义。此外，这使得引入与收敛相关的网络潜力成为可能，这可以为架构搜索提供一个良好的度量。
- en: '**Conclusion**'
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**结论**'
- en: 'The quest of finding a deep learning theory can be broken down into two parts:
    first, building intuitions on how and why it works, through toy models and experiments,
    then expressing those intuitions into a mathematical form that can help us explaining
    our current results and making new ones.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找深度学习理论的过程可以分为两部分：首先，通过玩具模型和实验建立对其如何以及为何有效的直觉，其次，将这些直觉表达为可以帮助解释我们当前结果并取得新成果的数学形式。
- en: In this first article, we tried to convey more intuition of both the high-dimensional
    loss function of neural networks and the interpretations of SGD, while showing
    that new kinds of formalism are being built in the objective of having a real
    mathematical theory of deep neural networks optimization.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们试图传达关于神经网络的高维损失函数和SGD的更多直觉，同时展示了为了拥有真正的深度神经网络优化数学理论，新的形式主义正在被构建。
- en: However, while non-convex optimization is the cornerstone of deep learning,
    its success comes mostly from its ability to generalize well despite a huge number
    of layers and parameters. This will be the object of the next part.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管非凸优化是深度学习的基石，其成功主要来自于其能够在层数和参数数量庞大的情况下仍能很好地泛化。这将是下一部分的内容。
- en: '**References**'
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[1] Felix Draxler, Kambis Veschgini, Manfred Salmhofer, Fred Hamprecht. [Essentially
    No Barriers in Neural Network Energy Landscape](https://icml.cc/Conferences/2018/Schedule?showEvent=2780), *ICML
    2018.*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Felix Draxler, Kambis Veschgini, Manfred Salmhofer, Fred Hamprecht. [神经网络能量景观中的实质性无障碍](https://icml.cc/Conferences/2018/Schedule?showEvent=2780), *ICML
    2018.*'
- en: '[2] C. Daniel Freeman, Joan Bruna. [Topology and Geometry of Half-Rectified
    Network Optimization](https://arxiv.org/abs/1611.01540), a*rXiv:1611.01540*, 2016.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] C. Daniel Freeman, Joan Bruna. [半矩形网络优化的拓扑与几何](https://arxiv.org/abs/1611.01540),
    a*rXiv:1611.01540*, 2016.'
- en: '[3] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
    Ping Tak Peter Tang. [On large-batch training for deep learning: Generalization
    gap and sharp minima](https://arxiv.org/pdf/1609.04836.pdf), *ICLR 2017*.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
    Ping Tak Peter Tang. [关于深度学习的大批量训练：泛化差距与尖锐极小值](https://arxiv.org/pdf/1609.04836.pdf), *ICLR
    2017*'
- en: '[4] Robert Kleinberg, Yuanzhi Li, Yang Yuan. [An Alternative View: When Does
    SGD Escape Local Minima?](https://arxiv.org/abs/1802.06175), *ICML 2018*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Robert Kleinberg, Yuanzhi Li, Yang Yuan. [另一种观点：SGD 何时逃离局部最小值？](https://arxiv.org/abs/1802.06175), *ICML
    2018*'
- en: '[5] Pratik Chaudhari, Stefano Soatto. [Stochastic gradient descent performs
    variational inference](https://arxiv.org/pdf/1710.11029.pdf), converges to limit
    cycles for deep network, *ICLR 2018*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Pratik Chaudhari, Stefano Soatto. [随机梯度下降进行变分推断](https://arxiv.org/pdf/1710.11029.pdf)，收敛到深度网络的极限周期， *ICLR
    2018*'
- en: '[6] Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli,
    Yoshua Bengio. [Identifying and attacking the saddle point problem in high-dimensional
    non-convex optimization](https://arxiv.org/abs/1406.2572), *NIPS 2014*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli,
    Yoshua Bengio. [识别并解决高维非凸优化中的鞍点问题](https://arxiv.org/abs/1406.2572), *NIPS 2014*'
- en: '**Bio**: [Arthur Pesah](https://twitter.com/artix41?lang=en) is a Master''s
    student in theoretical physics at KTH. A Machine learning enthusiast, physics
    and maths lover, computer geek.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介**: [Arthur Pesah](https://twitter.com/artix41?lang=en) 是 KTH 理论物理学的硕士研究生。机器学习爱好者，物理和数学爱好者，计算机极客。'
- en: '[Original](https://towardsdatascience.com/recent-advances-for-a-better-understanding-of-deep-learning-part-i-5ce34d1cc914).
    Reposted with permission.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[原创](https://towardsdatascience.com/recent-advances-for-a-better-understanding-of-deep-learning-part-i-5ce34d1cc914)。经许可转载。'
- en: '**Related:**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Power Laws in Deep Learning 2: Universality](https://www.kdnuggets.com/2018/09/power-laws-deep-learning-2-universality.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习中的幂律 2：普遍性](https://www.kdnuggets.com/2018/09/power-laws-deep-learning-2-universality.html)'
- en: '[“Auto-What?” –  A Taxonomy of Automated Machine Learning](https://www.kdnuggets.com/2018/09/auto-what-taxonomy-automated-machine-learning.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“自动化什么？”——自动化机器学习的分类](https://www.kdnuggets.com/2018/09/auto-what-taxonomy-automated-machine-learning.html)'
- en: '[Deep Learning Framework Power Scores 2018](https://www.kdnuggets.com/2018/09/deep-learning-framework-power-scores-2018.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习框架能力评分 2018](https://www.kdnuggets.com/2018/09/deep-learning-framework-power-scores-2018.html)'
- en: '* * *'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻求目标，并寻求目标…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Top Resources for Learning Statistics for Data Science](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习数据科学统计的顶级资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)'
- en: '[A $9B AI Failure, Examined](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9亿美元AI失败的检讨](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)'
- en: '[The 5 Characteristics of a Successful Data Scientist](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功的数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)'
- en: '[What Makes Python An Ideal Programming Language For Startups](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[是什么让Python成为创业公司的理想编程语言](https://www.kdnuggets.com/2021/12/makes-python-ideal-programming-language-startups.html)'
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家都应该知道的三个R库（即使你使用Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
