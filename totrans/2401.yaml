- en: When Would Ensemble Techniques be a Good Choice?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![When Would Ensemble Techniques be a Good Choice?](../Images/ee94621a31d8295616fa92d91a805cad.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Brett Jordan](https://unsplash.com/@brett_jordan) via Unsplash'
  prefs: []
  type: TYPE_NORMAL
- en: In any decision making process, you need to gather all the different types of
    data you have, all the information, pull it apart, learn more about it, get experts
    in, and more before making a solid decision.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This is similar in the Machine Learning process with Ensemble techniques. Ensemble
    models combine a variety of models together to help the prediction (decision making)
    process. A single model may not have the capabilities of producing the right prediction
    for a specific data set - this raises the chance of high variance, low accuracy
    and noise and bias. By combining multiple models, we effectively have a higher
    chance to improve the level of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest example is Decision Trees - a probability tree-like structure model
    that continuously splits data to make predictions based on the previous set of
    questions that were answered.
  prefs: []
  type: TYPE_NORMAL
- en: Why Would You Use An Ensemble Technique?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To answer the question of this article, “When would ensemble techniques be a
    good choice?” When you want to improve the performance of machine learning models
    - it’s that simple.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you’re working on a classification task and you wish to increase
    the accuracy of your model - use ensemble techniques. If you want to reduce your
    mean error for your regression task - use ensemble techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main 2 reasons for using an ensemble learning algorithms is:'
  prefs: []
  type: TYPE_NORMAL
- en: Improve predictions - you will achieve better predictive skill rather than just
    using a single model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve robustness - you will achieve better stable predictions rather than
    just using a single model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your overall aim when using ensemble techniques should be to reduce the generalization
    error of the prediction. Therefore, using a variety of base models which are diverse
    will automatically decrease your prediction error.
  prefs: []
  type: TYPE_NORMAL
- en: It is essentially building a more stable, reliable and accurate model that you
    trust.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 3 types of ensemble modeling techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stacking
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Short for Bootstrap Aggregation, as the ensemble modeling technique combines
    Bootstrapping and Aggregation to form one ensemble model. It is based on creating
    multiple sets of the original training data, creating tree-like structure probability
    models which then aggregate to conclude to a final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Each model learns about the errors produced in the previous model and uses a
    different subset of the training data set. Bagging aims to avoid overfitting of
    data and reduce the variance in the predictions and can be used for both regression
    and classification models.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Random Forest is an algorithm of Bagging but with a slight difference. It uses
    a subset of samples of the training data and a subset of features to build multiple
    trees that split. You can see it as multiple decision trees which fit each training
    set in a random mode.
  prefs: []
  type: TYPE_NORMAL
- en: The decision on the split is based on a random selection of features causing
    a differentiation between each tree. This produces a more accurate aggregated
    result and final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other example algorithm are:'
  prefs: []
  type: TYPE_NORMAL
- en: Bagged Decision Trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extra Trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom Bagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Boosting is the act of converting weak learners to strong learners. A weak learner
    fails to make accurate predictions due to their capabilities. A new weak prediction
    rule is generated by applying base learning algorithms. This is done by taking
    a random sample of data which is then inputted into a model and then trained sequentially
    which aims to train the weak learners and try to correct its predecessor.
  prefs: []
  type: TYPE_NORMAL
- en: An example of Boosting is AdaBoost and XGBoost
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AdaBoost is short for Adaptive Boosting and is used as a technique to boost
    the performance of a machine learning algorithm. It takes the notion of Random
    Forests weak learners and builds models on top of several weak learners.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: XGBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: XGboosts stands for Extreme Gradient Boosting and is one of the most popular
    boosting algorithms that can be used for both regression and classification tasks.
    It is a type of supervised learning machine learning algorithm which aims to accurately
    predict a target variable by combining a set of weaker models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other example algorithms are:'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Boosting Machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stochastic Gradient Boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LightGBM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When should I use Bagging vs Boosting?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simplest way to determine when to use bagging or boosting is:'
  prefs: []
  type: TYPE_NORMAL
- en: If the classifier is unstable and has high variance - use Bagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the classifier is stable, however has high bias - use Boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stacking is short for Stacked Generalization and is similar to boosting; with
    the aim to produce more robust predictors. This is done by taking the predictions
    from weak learners and using that to create a strong model. It does this by figuring
    out how to best combine the predictions from multiple models on the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: It is basically asking you ‘If you had a variety of machine learning models
    that perform well on a specific problem, how do you choose which model is the
    best to trust?’
  prefs: []
  type: TYPE_NORMAL
- en: Voting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Voting is an example of Stacking, however it is different for both classification
    and regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For regression, the prediction is made based on the average of other regression
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For classification, there can either be hard voting or soft voting. Hard voting
    is essentially picking the prediction with the highest number of votes, whereas
    soft voting is combining the probabilities of each prediction in each of the models
    and then picking the prediction with the highest total probability.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Other example algorithms are:'
  prefs: []
  type: TYPE_NORMAL
- en: Weighted Average
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blending
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Super Learner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What’s the Difference Between Stacking and Bagging/Boosting?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bagging uses decision trees, where stacking uses different models. Bagging takes
    samples from the training dataset, where stacking fits on the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting uses a sequence of models that converts weak learners to strong learners
    to correct the prior models predicting, whereas stacking uses a single model to
    learn how to combine the predictions from the contributing models in the best
    way.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating everything
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will always need to understand what you’re trying to achieve before you
    attempt to solve a task. Once you do that, you will be able to determine if your
    task is a classification or regression task - in which you can then choose which
    ensemble algorithm will be the best to use to improve your models predictions
    and robustness.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[If I Had To Start Learning Data Science Again, How Would I Do It?](https://www.kdnuggets.com/2020/08/start-learning-data-science-again.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ensemble Learning with Examples](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What makes a visualization good?](https://www.kdnuggets.com/2022/10/sphere-makes-visualization-good.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Your Features Are Important? It Doesn’t Mean They Are Good](https://www.kdnuggets.com/your-features-are-important-it-doesnt-mean-they-are-good)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Quality: The Good, The Bad, and The Ugly](https://www.kdnuggets.com/2022/01/data-quality-good-bad-ugly.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
