- en: 7 SMOTE Variations for Oversampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/01/7-smote-variations-oversampling.html](https://www.kdnuggets.com/2023/01/7-smote-variations-oversampling.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![7 SMOTE Variations for Oversampling](../Images/51405691ec8c00074ee722553f91a123.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The imbalanced dataset is a problem in data science. The problem happens because
    imbalance often leads to modeling performance issues. To mitigate the imbalance
    problem, we can use the oversampling method. Oversampling is the minority resampling
    data to balance out the data.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to oversample, and of them is by using SMOTE1\. Let’s explore
    many SMOTE implementations to learn further about oversampling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: SMOTE Variations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we continue further, we will use the churn dataset from Kaggle2 to represent
    the imbalanced dataset. The dataset target is the  ‘exited’ variable, and we would
    see how the SMOTE would oversample the data based on the minority target.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![7 SMOTE Variations for Oversampling](../Images/5b16aed043c0b0d2ee7e7c9512c2c565.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that our churn dataset is faced with an imbalance problem. Let’s
    try the SMOTE to oversample the data.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. SMOTE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SMOTE is commonly used to oversample continuous data for ML problems by developing
    artificial or synthetic data. We are using continuous data because the model for
    developing the sample only accepts continuous data1.
  prefs: []
  type: TYPE_NORMAL
- en: For our example, we would use two continuous variables from the dataset example;
    ‘EstimatedSalary’ and ‘Age’. Let’s see how both variables spread compared to the
    data target.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![7 SMOTE Variations for Oversampling](../Images/247ba76232da66be0420332cbb7251af.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see the minority class mostly spread on the middle part of the plot.
    Let’s try to oversample the data with SMOTE and see how the differences were made.
    To facilitate the SMOTE oversampling, we would use the imblearn Python package.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: With imblearn, we would oversample our churn data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Imblearn package is based on the scikit-learn API, which was easy to use. In
    the example above, we have oversampled the dataset with SMOTE. Let’s see the ‘Exited’
    variable distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![7 SMOTE Variations for Oversampling](../Images/d6afa0e2c70679f864ee48eaf4395334.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see from the output above, the target variable now has similar proportions.
    Let’s see how the continuous variable spread with the new SMOTE oversampled data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![7 SMOTE Variations for Oversampling](../Images/5c4060008e7281ac205d75fdfc340fe2.png)'
  prefs: []
  type: TYPE_IMG
- en: The above image shows the minority data is now spread more than before we oversample
    the data. If we see the output in more detail, we can see that the minority data
    spread is still close to the core and has spread wider than before.  This happens
    because the sample was based on the neighbor model, which estimated the sample
    based on the nearest neighbor.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. SMOTE-NC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SMOTE-NC is SMOTE for the categorical data. As I mentioned above, SMOTE only
    works for continuous data.
  prefs: []
  type: TYPE_NORMAL
- en: Why don’t we just encode the categorical variable into the continuous variable?
  prefs: []
  type: TYPE_NORMAL
- en: The problem is the SMOTE creates a sample based on the nearest neighbor. If
    you encode the categorical data, say the ‘HasCrCard’ variable, which contains
    classes 0 and 1, the sample result could be 0.8 or 0.34, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: From the data standpoint, it doesn’t make sense. That is why we could use SMOTE-NC
    to ensure that the categorical data oversampling would make sense.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try with example data. For this specific sample, we would use the variables
    ‘HasCrCard’ and ‘Age’. First, I want to show the initial ‘HasCrCard’ variable
    spread.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![7 SMOTE Variations for Oversampling](../Images/d39cbb30be66ffae4d58de33496ac4a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Then let’s see the differences after the oversampling process with SMOTE-NC.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Notice in the above code, the categorical variable position is based on the
    variable position in the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how the ‘HasCrCard’ spread after the oversampling.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![7 SMOTE Variations for Oversampling](../Images/fc7ae07f005dacdeb4f240e3e0ee78ff.png)'
  prefs: []
  type: TYPE_IMG
- en: See that the data oversampling almost have the same proportions. You could try
    with another categorical variable to see how SMOTE-NC works.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Borderline-SMOTE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Borderline-SMOTE is a SMOTE that is based on the classifier borderline. Borderline-SMOTE
    would oversample the data that was close to the classifier borderline. This is
    because the closer the sample from the borderline is expected to be prone to misclassified
    and thus more important to oversampled.3
  prefs: []
  type: TYPE_NORMAL
- en: There are two kinds of Borderline-SMOTE; Borderline-SMOTE1 and Borderline-SMOTE2\.
    The differences are Borderline-SMOTE1 would oversample both classes that are close
    to the borderline. In comparison, Borderline-SMOTE2 would only oversample the
    minority class.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try the Borderline-SMOTE with a dataset example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see how the spread after we initiate the Borderline-SMOTE.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![7 SMOTE Variations for Oversampling](../Images/c5c02737968d67533432aa0896917e02.png)'
  prefs: []
  type: TYPE_IMG
- en: If we see the result above, the output is similar to the SMOTE output, but Borderline-SMOTE
    oversampling results slightly closer to the borderline.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. SMOTE-Tomek
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SMOTE-Tomek uses a combination of both SMOTE and the undersampling Tomek link.
    Tomek link is a cleaning data way to remove the majority class that was overlapping
    with the minority class4.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try SMOTE-TOMEK to the sample dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Let’s take a look at the target variable result after we use SMOTE-Tomek.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![7 SMOTE Variations for Oversampling](../Images/624fe93e93594333095629bcdc4c3bb8.png)'
  prefs: []
  type: TYPE_IMG
- en: The 'Exited' class 0 number is now around 6000 compared to the original dataset,
    which is close to 8000\. This happens because SMOTE-Tomek undersampled the class
    0 while oversampling the minority class.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how the data spread after we oversample the data with SMOTE-Tomek.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![7 SMOTE Variations for Oversampling](../Images/781cc7836229a363b2150612220cb883.png)'
  prefs: []
  type: TYPE_IMG
- en: The resulting spread is still similar to before. But if we see more detail,
    less minority oversampled is produced the further the data is.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. SMOTE-ENN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the SMOTE-Tomek, SMOTE-ENN (Edited Nearest Neighbour) combines oversampling
    and undersampling. The SMOTE did the oversampling, while the ENN did the undersampling.
  prefs: []
  type: TYPE_NORMAL
- en: The Edited Nearest Neighbour is a way to remove majority class samples in both
    original and sample result datasets where the nearest class minority samples5
    misclassifies it. It will remove the majority class close to the border where
    it was misclassified.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try the SMOTE-ENN with an example dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see the SMOTE-ENN result. First, we would take a look at the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![7 SMOTE Variations for Oversampling](../Images/3cd38b1285804752b1707c7b62035c6f.png)'
  prefs: []
  type: TYPE_IMG
- en: The undersampling process of the SMOTE-ENN is much more strict compared to the
    SMOTE-Tomek. From the result above, more than half of the original 'Exited' class
    0 was undersampled, and only a slight increase of the minority class.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see the data spread after the SMOTE-ENN is applied.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![7 SMOTE Variations for Oversampling](../Images/ada4df12952cd5b1987ab8a11b9b45c4.png)'
  prefs: []
  type: TYPE_IMG
- en: The data spread is much larger between the classes than before. However, we
    need to remember that the result data is smaller.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. SMOTE-CUT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SMOTE-CUT or SMOTE-Clustered Undersampling Technique combines oversampling,
    clustering, and undersampling.
  prefs: []
  type: TYPE_NORMAL
- en: SMOTE-CUT implements oversampling with SMOTE, clustering both the original and
    result and removing the class majority samples from clusters.
  prefs: []
  type: TYPE_NORMAL
- en: SMOTE-CUT clustering is based on the EM or Expectation Maximization algorithm,
    which would assign each data with a probability of belonging to clusters. The
    clustering result would guide the algorithm to oversample or undersample, so the
    dataset distribution becomes balanced6.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try using a dataset example. For this example, we would use the crucio
    Python package.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: With the crucio package, we oversample the dataset using the following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see the target data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![7 SMOTE Variations for Oversampling](../Images/13e2592695372363fc26e179816038d8.png)'
  prefs: []
  type: TYPE_IMG
- en: The 'Exited' class is equal, although the undersampling process is quite strict.
    Many of the 'Exited' class 0 were removed due to the undersampling.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see the data spread after SMOTE-CUT was implemented.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![7 SMOTE Variations for Oversampling](../Images/cf9e04ca34b10324a8f6e29428a6dfe5.png)'
  prefs: []
  type: TYPE_IMG
- en: The data spread is more spread but less than the SMOTE-ENN.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. ADASYN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ADASYN or Adaptive Synthetic Sampling is a SMOTE that tries to oversample the
    minority data based on the data density. ADASYN would assign a weighted distribution
    to each of the minority samples and prioritize oversampling to the minority samples
    that are harder to learn7.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try ADASYN with the example dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see the target distribution result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![7 SMOTE Variations for Oversampling](../Images/9e569e33feaf9fb3bab6a836986dacdc.png)'
  prefs: []
  type: TYPE_IMG
- en: As the ADASYN would focus on the data that is harder to learn or less dense,
    the oversampling result was less than the other.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how the data spread was.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![7 SMOTE Variations for Oversampling](../Images/f1ab93711ff7a2c1413dbcbfea31b28e.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see from the image above that the spread is closer to the core but
    closer to the low-density minority data.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data imbalance is a problem in the data field. One way to mitigate the imbalance
    problem is by oversampling the dataset with SMOTE. With research development,
    many SMOTE methods have been created that we can use.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we go through 7 different SMOTE techniques, including
  prefs: []
  type: TYPE_NORMAL
- en: SMOTE
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SMOTE-NC
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Borderline-SMOTE
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SMOTE-TOMEK
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SMOTE-ENN
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SMOTE-CUT
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ADASYN
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SMOTE: Synthetic Minority Over-sampling Technique - [Arxiv.org](https://arxiv.org/abs/1106.1813)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Churn Modelling](https://www.kaggle.com/datasets/shubh0799/churn-modelling)
    dataset from Kaggle licenses under CC0: Public Domain.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Borderline-SMOTE: A New Over-Sampling Method in Imbalanced Data Sets Learning
    - [Semanticscholar.org](https://www.semanticscholar.org/paper/Borderline-SMOTE:-A-New-Over-Sampling-Method-in-Han-Wang/b618f88ebaab51c4d38182e773419478abe44cf8)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Balancing Training Data for Automated Annotation of Keywords: a Case Study
    - [inf.ufrgs.br](https://www.inf.ufrgs.br/maslab/pergamus/pubs/balancing-training-data-for.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Improving Risk Identification of Adverse Outcomes in Chronic Heart Failure Using
    SMOTE+ENN and Machine Learning - [dovepress.com](https://www.dovepress.com/improving-risk-identification-of-adverse-outcomes-in-chronic-heart-fai-peer-reviewed-fulltext-article-RMHP#cit0022)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using Crucio SMOTE and Clustered Undersampling Technique for unbalanced datasets
    - [sigmoid.ai](https://www.sigmoidai.org/using-crucio-smote-and-clustered-undersampling-technique-for-unbalanced-datasets/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ADASYN: Adaptive Synthetic Sampling Approach for ImbalancedLearning - [ResearchGate](https://www.researchgate.net/publication/224330873_ADASYN_Adaptive_Synthetic_Sampling_Approach_for_Imbalanced_Learning)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**[Cornellius Yudha Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**
    is a data science assistant manager and data writer. While working full-time at
    Allianz Indonesia, he loves to share Python and Data tips via social media and
    writing media.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[An Introduction to SMOTE](https://www.kdnuggets.com/2022/11/introduction-smote.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
