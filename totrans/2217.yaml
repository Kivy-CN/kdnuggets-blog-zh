- en: 'The Quest for Model Confidence: Can You Trust a Black Box?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/the-quest-for-model-confidence-can-you-trust-a-black-box](https://www.kdnuggets.com/the-quest-for-model-confidence-can-you-trust-a-black-box)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![The Quest for Model Confidence: Can You Trust a Black Box?](../Images/894e9ac14c8ab3392f26660526c9f14a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) like GPT-4 and LLaMA2 have entered the [data labeling]
    chat. LLMs have come a long way and can now label data and take on tasks historically
    conducted by humans. While obtaining data labels with an LLM is incredibly quick
    and relatively cheap, there’s still one big issue, these models are the ultimate
    black boxes. So the burning question is: how much trust should we put in the labels
    these LLMs generate? In today’s post, we break down this conundrum to establish
    some fundamental guidelines for gauging the confidence we can have in LLM-labeled
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The results presented below are from an experiment conducted by [Toloka](http://toloka.ai)
    using popular models and a dataset in Turkish. This is not a scientific report
    but rather a short overview of possible approaches to the problem and some suggestions
    for  how to determine which method works best for your application.
  prefs: []
  type: TYPE_NORMAL
- en: The Big Question
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we get into the details, here’s the big question: When can we trust
    a label generated by an LLM, and when should we be skeptical? Knowing this can
    help us in automated data labeling and can also be useful in other applied tasks
    like customer support, content generation, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Current State of Affairs**'
  prefs: []
  type: TYPE_NORMAL
- en: So, how are people tackling this issue now? Some directly ask the model to spit
    out a confidence score, some look at the consistency of the model’s answers over
    multiple runs, while others examine the model’s log probabilities. But are any
    of these approaches reliable? Let’s find out.
  prefs: []
  type: TYPE_NORMAL
- en: The Rule of Thumb
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What makes a “good” confidence measure? One simple rule to follow is that there
    should be a positive correlation between the confidence score and the accuracy
    of the label. In other words, a higher confidence score should mean a higher likelihood
    of being correct. You can visualize this relationship using a calibration plot,
    where the X and Y axes represent confidence and accuracy, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Experiments and Their Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Approach 1: Self-Confidence'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The self-confidence approach involves asking the model about its confidence
    directly. And guess what? The results weren’t half bad! While the LLMs we tested
    struggled with the non-English dataset, the correlation between self-reported
    confidence and actual accuracy was pretty solid, meaning models are well aware
    of their limitations. We got similar results for GPT-3.5 and GPT-4 here.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Quest for Model Confidence: Can You Trust a Black Box?](../Images/5a50406ba0dbb94a4a94a29db976aa15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Approach 2: Consistency'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Set a high temperature (~0.7–1.0), label the same item several times, and analyze
    the consistency of the answers, for more details, see this [paper](https://arxiv.org/abs/2305.19187).
    We tried this with GPT-3.5 and it was, to put it lightly, a dumpster fire. We
    prompted the model to answer the same question multiple times and the results
    were consistently erratic. This approach is as reliable as asking a Magic 8-Ball
    for life advice and should not be trusted.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Quest for Model Confidence: Can You Trust a Black Box?](../Images/0aa9e9852008a4f2896db47a07eb52ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Approach 3: Log Probabilities'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Log probabilities offered a pleasant surprise. Davinci-003 returns logprobs
    of the tokens in the completion mode. Examining this output, we got a surprisingly
    decent confidence score that correlated well with accuracy. This method offers
    a promising approach to determining a reliable confidence score.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Quest for Model Confidence: Can You Trust a Black Box?](../Images/bf034f50dc67f3b061d140b9d75c8b81.png)'
  prefs: []
  type: TYPE_IMG
- en: The Takeaway
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, what did we learn? Here it is, no sugar-coating:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-Confidence: Useful, but handle with care. Biases are reported widely.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Consistency: Just don’t. Unless you enjoy chaos.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Log Probabilities: A surprisingly good bet for now if the model allows you
    to access them.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The exciting part? Log probabilities appear to be quite robust even without
    fine-tuning the model, despite this [paper](https://arxiv.org/abs/2305.14975)
    reporting this method to be overconfident. There is room for further exploration.
  prefs: []
  type: TYPE_NORMAL
- en: Future Avenues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A logical next step could be to find a golden formula that combines the best
    parts of each of these three approaches, or explores new ones. So, if you’re up
    for a challenge, this could be your next weekend project!
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alright, ML aficionados and newbies, that’s a wrap. Remember, whether you’re
    working on data labeling or building the next big conversational agent - understanding
    model confidence is key. Don’t take those confidence scores at face value and
    make sure you do your homework!
  prefs: []
  type: TYPE_NORMAL
- en: Hope you found this insightful. Until next time, keep crunching those numbers
    and questioning those models.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Ivan Yamshchikov](https://www.linkedin.com/in/kroniker/?originalSubdomain=de)**
    is a professor of Semantic Data Processing and Cognitive Computing at the Center
    for AI and Robotics, Technical University of Applied Sciences Würzburg-Schweinfurt.
    He also leads the Data Advocates team at Toloka AI. His research interests include
    computational creativity, semantic data processing and generative models.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Sky''s the Limit: Learn how JetBlue uses Monte Carlo and Snowflake…](https://www.kdnuggets.com/2022/12/monte-carlo-jetblue-snowflake-build-trust-improve-model-accuracy.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Working with Confidence Intervals](https://www.kdnuggets.com/2023/04/working-confidence-intervals.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Trust in AI is Priceless](https://www.kdnuggets.com/2022/08/trust-ai-priceless.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[In Data We Trust: Data Centric AI](https://www.kdnuggets.com/2022/10/data-trust-data-centric-ai.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Bounding Box Deep Learning: The Future of Video Annotation](https://www.kdnuggets.com/2022/07/bounding-box-deep-learning-future-video-annotation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Black Friday Deal - Master Machine Learning for Less with DataCamp](https://www.kdnuggets.com/2022/11/datacamp-black-friday-deal-master-machine-learning-less-datacamp.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
