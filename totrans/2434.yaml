- en: 'Naïve Bayes Algorithm: Everything You Need to Know'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html](https://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Figure](../Images/15178d1596b6428dba58b29de1bb24dc.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image credit](https://blogs.oracle.com/datascience/introduction-to-bayesian-inference)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the Naïve Bayes Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simplest solutions are usually the most powerful ones, and [Naïve Bayes](https://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html)
    is a good example of that. Despite the advances in Machine Learning in the last
    years, it has proven to not only be simple but also fast, accurate, and reliable.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: It has been successfully used for many purposes, but it works particularly well
    with natural language processing (NLP) problems.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes is a probabilistic machine learning algorithm based on the **Bayes
    Theorem**, used in a wide variety of classification tasks. In this article, we
    will understand the Naïve Bayes algorithm and all essential concepts so that there
    is no room for doubts in understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes Theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayes’ Theorem is a simple mathematical formula used for calculating conditional
    probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conditional probability** is a measure of the probability of an event occurring
    given that another event has (by assumption, presumption, assertion, or evidence)
    occurred.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula is: —'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38d8516f4eb2af2be4891493f2440dd7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Which tells us: how often A happens *given that B happens*, written **P(A|B) **also
    called posterior probability, When we know: how often B happens *given that A
    happens*, written **P(B|A)** and how likely A is on its own, written **P(A)** and
    how likely B is on its own, written **P(B).**'
  prefs: []
  type: TYPE_NORMAL
- en: In simpler terms, Bayes’ Theorem is a way of finding a probability when we know
    certain other probabilities.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Assumptions Made by Naïve Bayes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The fundamental Naïve Bayes assumption is that each feature makes an:'
  prefs: []
  type: TYPE_NORMAL
- en: independent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: equal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: contribution to the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Let us take an example to get some better intuition. Consider the car theft
    problem with attributes Color, Type, Origin, and the target, Stolen can be either
    Yes or No.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset is represented as below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f61877acf2f7db8957d71371a5dab39e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Concerning our dataset, the concept of assumptions made by the algorithm can
    be understood as:'
  prefs: []
  type: TYPE_NORMAL
- en: We assume that no pair of features are dependent. For example, the color being
    ‘Red’ has nothing to do with the Type or the Origin of the car. Hence, the features
    are assumed to be **Independent**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, each feature is given the same influence(or importance). For example,
    knowing the only Color and Type alone can’t predict the outcome perfectly. So
    none of the attributes are irrelevant and assumed to be contributing **Equally** to
    the outcome.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note:** The assumptions made by Naïve Bayes are generally not correct in
    real-world situations. The independence assumption is never correct but often
    works well in practice. **Hence the name ‘Naï>ve’.**'
  prefs: []
  type: TYPE_NORMAL
- en: Here in our dataset, **we need to classify whether the car is stolen, given
    the features of the car**. The columns represent these features and the rows represent
    individual entries. If we take the first row of the dataset, we can observe that
    the car is stolen if the Color is Red, the Type is Sports and Origin is Domestic.
    So we want to classify a Red Domestic SUV is getting stolen or not. Note that
    there is no example of a Red Domestic SUV in our data set.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to this example, Bayes theorem can be rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/491bf150482fb467a687e989d17a48e0.png)'
  prefs: []
  type: TYPE_IMG
- en: The variable **y** is the class variable(stolen?), which represents if the car
    is stolen or not given the conditions. Variable **X **represents the parameters/features.
  prefs: []
  type: TYPE_NORMAL
- en: '**X** is given as,'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5098e278c044fca5b820da7d14036542.png)'
  prefs: []
  type: TYPE_IMG
- en: Here `x1, x2…, xn` represent the features, i.e they can be mapped to Color,
    Type, and Origin. By substituting for **X **and expanding using the chain rule
    we get,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05da7419b18bc6aaa32873d8bf82d70c.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, you can obtain the values for each by looking at the dataset and substitute
    them into the equation. For all entries in the dataset, the denominator does not
    change, it remains static. Therefore, the denominator can be removed and proportionality
    can be injected.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aba56792a913b70e32e66da097911256.png)'
  prefs: []
  type: TYPE_IMG
- en: In our case, the class variable(**y**) has only two outcomes, yes or no. There
    could be cases where the classification could be multivariate. Therefore, we have
    to find the class variable(**y)** with maximum probability.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0dd539563839c7e2949953bac013f521.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the above function, we can obtain the class, given the predictors/features.
  prefs: []
  type: TYPE_NORMAL
- en: The posterior probability **P(y|X)** can be calculated by first, creating a **Frequency
    Table** for each attribute against the target. Then, molding the frequency tables
    to **Likelihood Tables** and finally, use the Naïve Bayesian equation to calculate
    the posterior probability for each class. The class with the highest posterior
    probability is the outcome of the prediction. Below are the Frequency and likelihood
    tables for all three predictors.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c4a78458a94dff92bd51ac51e0a03b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Frequency and Likelihood tables of ‘Color’
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c047cfd6ed5737207bb0b6e33027e3b.png)'
  prefs: []
  type: TYPE_IMG
- en: Frequency and Likelihood tables of ‘Type’
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ab1a4288ebc8d118b5ef013dac9b024.png)'
  prefs: []
  type: TYPE_IMG
- en: Frequency and Likelihood tables of ‘Origin’
  prefs: []
  type: TYPE_NORMAL
- en: So in our example, we have 3 predictors **X**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ed65c1aa22a95c968245d0c7cae56ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As per the equations discussed above, we can calculate the posterior probability
    P(Yes | X) as :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a8c6366be8f30f46f0d21dce86937fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and, P( No | X ):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3ef76ae68f128b5a95d1a296e75d03f.png)'
  prefs: []
  type: TYPE_IMG
- en: Since 0.144 > 0.048, Which means given the features RED SUV and Domestic, our
    example gets classified as ’NO’ the car is not stolen.
  prefs: []
  type: TYPE_NORMAL
- en: The Zero-Frequency Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the disadvantages of Naïve-Bayes is that if you have no occurrences of
    a class label and a certain attribute value together then the frequency-based
    probability estimate will be zero. And this will get a zero when all the probabilities
    are multiplied.
  prefs: []
  type: TYPE_NORMAL
- en: An approach to overcome this ‘zero-frequency problem’ in a Bayesian environment
    is to add one to the count for every attribute value-class combination when an
    attribute value doesn’t occur with every class value.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, say your training data looked like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3f20887326fed468dc3fb94785fc6a2.png)'
  prefs: []
  type: TYPE_IMG
- en: ????(TimeZone=????????|Spam=????????????)=10/10=1
  prefs: []
  type: TYPE_NORMAL
- en: ????(TimeZone=????????|Spam=????????????)=0/10=0
  prefs: []
  type: TYPE_NORMAL
- en: 'Then you should add one to every value in this table when you’re using it to
    calculate probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b3e31ab9e54ae8594d2327b289cf2be.png)'
  prefs: []
  type: TYPE_IMG
- en: ????(TimeZone=????????|Spam=????????????)=11/12
  prefs: []
  type: TYPE_NORMAL
- en: ????(TimeZone=????????|Spam=????????????)=1/12
  prefs: []
  type: TYPE_NORMAL
- en: This is how we’ll get rid of getting a zero probability.
  prefs: []
  type: TYPE_NORMAL
- en: '![XXXXX](../Images/a99e5ff71310dc4c74783a85d0df96b2.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image credits](https://medium.com/@sanjeethboddi/naive-bayes-algorithm-from-scratch-715d7cc0de53)'
  prefs: []
  type: TYPE_NORMAL
- en: Types of Naïve Bayes Classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**1\. Multinomial Naïve Bayes Classifier**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Feature vectors represent the frequencies with which certain events have been
    generated by a **multinomial distribution**. This is the event model typically
    used for document classification.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Bernoulli Naïve Bayes Classifier**:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the multivariate Bernoulli event model, features are independent booleans
    (binary variables) describing inputs. Like the multinomial model, this model is
    popular for document classification tasks, where binary term occurrence (i.e.
    a word occurs in a document or not) features are used rather than term frequencies
    (i.e. frequency of a word in the document).
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Gaussian Naïve Bayes Classifier: **'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Gaussian Naïve Bayes, continuous values associated with each feature are
    assumed to be distributed according to a **Gaussian distribution (**[Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution)**)**.
    When plotted, it gives a bell-shaped curve which is symmetric about the mean of
    the feature values as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ad7a683c8b444baa0bd2551070994ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The likelihood of the features is assumed to be Gaussian, hence, conditional
    probability is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21e23e25b324679a49299d08c41b1973.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, what if any feature contains numerical values instead of categories i.e.
    Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: One option is to transform the numerical values to their categorical counterparts
    before creating their frequency tables. The other option, as shown above, could
    be using the distribution of the numerical variable to have a good guess of the
    frequency. For example, one common method is to assume normal or gaussian distributions
    for numerical variables.
  prefs: []
  type: TYPE_NORMAL
- en: The probability density function for the normal distribution is defined by two
    parameters (mean and standard deviation).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/a8f7e607f21f5691c12f91cffe63eb4d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image credit](https://www.saedsayad.com/naive_bayesian.htm)'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the problem of playing golf, here the only predictor is `Humidity` and `Play
    Golf?` is the target. Using the above formula we can calculate posterior probability
    if we know the mean and standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/82b9ee1aa4c68b8e7060314dbe13bdb0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Case Study: Naïve Bayes Classifier From Scratch Using Python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An existing problem for any major website today is how to handle virulent and
    divisive content. Quora wants to tackle this problem to keep its platform a place
    where users can feel safe sharing their knowledge with the world.
  prefs: []
  type: TYPE_NORMAL
- en: '[Quora](https://www.quora.com/) is a platform that empowers people to learn
    from each other. On Quora, people can ask questions and connect with others who
    contribute unique insights and quality answers. A key challenge is to weed out
    insincere questions — those founded upon false premises, or that intend to make
    a statement rather than look for helpful answers.'
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to develop a Naïve Bayes classification model that identifies and
    flags insincere questions.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset can be downloaded from [here](https://www.kaggle.com/c/quora-insincere-questions-classification/data).
    Once you have downloaded the train and test data, load it and check.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure](../Images/3122d0c3be8f767d27374ffbdf4e08ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Training dataset
  prefs: []
  type: TYPE_NORMAL
- en: Let us see how are sincere questions look like.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/9f480898c8936c834ae51541304a0480.png)'
  prefs: []
  type: TYPE_IMG
- en: Sincere questions
  prefs: []
  type: TYPE_NORMAL
- en: we see how are insincere questions look like.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/c956e19bcba69b1c8709e12ab7ca9520.png)'
  prefs: []
  type: TYPE_IMG
- en: Insincere questions
  prefs: []
  type: TYPE_NORMAL
- en: '**Text Preprocessing**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next step is to preprocess text before splitting the dataset into a train
    and test set. The preprocessing steps involve: Removing Numbers, Removing Punctuations
    in a string, Removing Stop Words, Stemming of Words and Lemmatization of Words.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Constructing a Naive Bayes Classifier**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Combine all the preprocessing techniques and create a dictionary of words and
    each word’s count in training data.
  prefs: []
  type: TYPE_NORMAL
- en: Calculate probability for each word in a text and filter the words which have
    a probability less than threshold probability. Words with probability less than
    threshold probability are irrelevant.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then for each word in the dictionary, create a probability of that word being
    in insincere questions and its probability insincere questions. Then finding the
    conditional probability to use in naive Bayes classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prediction using conditional probabilities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Naïve Bayes algorithms are often used in sentiment analysis, spam filtering,
    recommendation systems, etc. They are quick and easy to implement but their biggest
    disadvantage is that the requirement of predictors to be independent.
  prefs: []
  type: TYPE_NORMAL
- en: '**Thanks for Reading!**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nagesh Singh Chauhan](https://www.linkedin.com/in/nagesh-singh-chauhan-6936bb13b/)**
    is a Big data developer at CirrusLabs. He has over 4 years of working experience
    in various sectors like Telecom, Analytics, Sales, Data Science having specialisation
    in various Big data components.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://levelup.gitconnected.com/na%C3%AFve-bayes-algorithm-everything-you-need-to-know-9bf3104b78e5).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[KDnuggets News, April 13: Python Libraries Data Scientists Should…](https://www.kdnuggets.com/2022/n15.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gaussian Naive Bayes, Explained](https://www.kdnuggets.com/2023/03/gaussian-naive-bayes-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Everything You Need to Know About Tensors](https://www.kdnuggets.com/2022/05/everything-need-know-tensors.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Everything You Need to Know About Data Lakehouses](https://www.kdnuggets.com/2022/09/everything-need-know-data-lakehouses.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Everything You Need to Know About MLOps: A KDnuggets Tech Brief](https://www.kdnuggets.com/tech-brief-everything-you-need-to-know-about-mlops)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ChatGPT: Everything You Need to Know](https://www.kdnuggets.com/2023/01/chatgpt-everything-need-know.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
