# 走进神经网络的内部，带有Tensorflow的互动代码

> 原文：[https://www.kdnuggets.com/2018/06/inside-mind-neural-network-interactive-code-tensorflow.html](https://www.kdnuggets.com/2018/06/inside-mind-neural-network-interactive-code-tensorflow.html)

![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](/2018/06/inside-mind-neural-network-interactive-code-tensorflow.html?page=2#comments)

**作者 [Jae Duk Seo](https://jaedukseo.me/)，瑞尔森大学**

![图片](../Images/a4da8f275f7ca7b1878ead38e8e6fccb.png)

GIF来自这个[网站](https://giphy.com/gifs/trippy-9lRBSGg6l68Hm)

我一直想了解我的模型的内部工作原理，已经很久了。从今天开始，我希望学习与这个主题相关的内容。对于这篇文章，我想覆盖三个主题：权重直方图、神经元激活的可视化，[内部/积分梯度](https://arxiv.org/pdf/1703.01365.pdf)。

> **请注意，这篇文章是为了我未来自己复习这些材料。**

### **在继续阅读之前**

原视频来自TechTalksTV ([https://vimeo.com/user72337760](https://vimeo.com/user72337760)) 如果出现任何问题，我会尽快删除视频。原视频链接： [https://vimeo.com/238242575](https://vimeo.com/238242575)

这个视频超出了本文的范围，但它确实帮助我理解了内部和积分梯度，以及如何了解神经网络的内部工作原理。

### **数据集 / 网络架构 / 准确性 / 类别数量**

![](../Images/af8e4afbaff1d7d1dfa91156b7a15eee.png)![](../Images/e33ace5e093d9f0b2ca6f6542c2527be.png)

图像来自这个网站

**红色矩形** → 输入图像（32*32*3）

**黑色矩形** → 使用ELU()进行卷积，有/没有均值池化

**橙色矩形** → 分类的Softmax

一如既往，我们将使用CIFAR 10数据集来训练我们的[全卷积网络](https://towardsdatascience.com/iclr-2015-striving-for-simplicity-the-all-convolutional-net-with-interactive-code-manual-b4976e206760)，并尝试了解网络为何将某些图像预测为特定类别。

需要注意的一点是，由于这篇文章更多的是为了了解网络的内部工作原理。我只会使用测试集中50张图像来测量准确性。

![](../Images/b24a63688c6bb3253076ba8f6cd87ccf.png)![](../Images/8c6d61d1bc614a70916de7465f04a928.png)

**左图** → 测试图像（50张图像）的准确性/成本随时间变化

**右图** → 训练图像（50000张图像）的准确性/成本随时间变化

![](../Images/f0ec4efc84c79de081b0168401a6e4ad.png)

如上所示，模型在第7个周期时的最终准确率为81%。（如果你希望查看完整的训练日志，[请点击这里](https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/Understanding_Concepts/COUNTERFACTUALS/viz/z_viz.txt)。）最后，让我们看看每个数字代表了每个类别的什么。

![](../Images/a731e3ab8e9e13dd818a977c4656f3f1.png)

图片来源于这个 [网站](https://github.com/EN10/CIFAR)

### **权重的直方图（训练前 / 训练后）**

![](../Images/601705b2077ada4a42dd471c99e0157c.png)

训练前权重的直方图

上图是每层权重的直方图，为了方便可视化，我将每个直方图分为三层。在最左边，我们可以观察到权重的均值通常为 0，标准差（stddev）值在 0.04 到 0.06 之间。这是预期中的情况，因为我们为每一层声明了不同的标准差值。某些曲线比其他曲线小的原因是由于每层的权重数量不同。（例如，第 0 层只有 3 * 3 * 128 个权重，而第 2 层有 3 * 128 * 128 个权重。）

![](../Images/e6a2ac82dc52da96d1cf867bcb4c8978.png)

不同的标准差值![](../Images/9a9a8c0bbc3af2032a4463163434f23f.png)

训练后权重的直方图

一开始，我们可以观察到明显的差异，尤其是对于前三层。分布的范围从 -5 增加到 5。然而，似乎大多数权重存在于 -1 和 1 之间（或接近零）。对于第 4 层到第 6 层，均值似乎也发生了变化，最后三层也是如此。

### **可视化特定层的激活值**

![](../Images/a232b4af38654cb6efe3556e31821e77.png)

网络的测试输入

使用 [Yosinski 和他的同事们](https://arxiv.org/pdf/1506.06579.pdf) 采用的技术，让我们可视化上述图像在第 3 层、第 6 层和第 9 层之后的变化。（请注意，我最初是在 [Arthur Juliani](https://medium.com/@awjuliani?source=post_header_lockup) 的 [博客文章](https://medium.com/@awjuliani/visualizing-neural-network-layer-activation-tensorflow-tutorial-d45f8bf7bbc4) 中发现该方法的。）

![](../Images/9e203f792f2e37e543c7279c8a815ec2.png)

第 3 层后的激活

**绿色框** → 捕捉绿色值的通道

**蓝色框** → 捕捉蓝色值的通道

现在有 128 个通道，所以我不会全部可视化。而是像上面那样可视化前 27 个通道。我们可以看到，在第 3 层之后，网络中捕捉到了一些颜色值。

![](../Images/ad2a1aa223f7176144fb0bd7e1b9bb34.png)

第 6 层的激活

**红色框** → 捕捉红色的通道

但是，在第六层之后，似乎某些过滤器能够比绿色或蓝色更好地捕捉红色。

<cener>![](../Images/9bbc0ff32982fe294cfc501dd22f3ee6.png)

第 9 层后的激活</cener>

最后，在第九层（全局平均池化之前），我们可以可视化每个通道的深度为1（因此它看起来像灰度图像）。然而（至少对我而言），它似乎不容易被人类理解。所有图像可以在[这里找到](https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/tree/master/Understanding_Concepts/COUNTERFACTUALS/viz)，我还创建了一个GIF，汇集了所有变化。

![](../Images/903cfcf07afc8b305a6892c902b1f520.png)

**GIF顺序**→ 输入图像，第3层后的激活，第6层后的激活，以及第9层后的激活

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业道路

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织进行IT支持

* * *

### 更多相关内容

+   [使用TensorFlow和Keras构建并训练你的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)

+   [6个让人惊叹的ChatGPT扩展插件](https://www.kdnuggets.com/2023/04/6-chatgpt-mindblowing-extensions-anywhere.html)

+   [选择下一个数据科学职位前要牢记的5件事](https://www.kdnuggets.com/2022/01/5-things-keep-mind-selecting-next-job.html)

+   [数据管理：如何保持在客户心中？](https://www.kdnuggets.com/2022/04/data-management-stay-top-customer-mind.html)

+   [深入了解DeepMind利用深度学习推动数学发展的新努力](https://www.kdnuggets.com/2021/12/inside-deepmind-new-efforts-deep-learning-advance-mathematics.html)

+   [AIMET进行神经网络优化](https://www.kdnuggets.com/2022/04/qualcomm-neural-network-optimization-aimet.html)
