- en: PySpark for Data Science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/02/pyspark-data-science.html](https://www.kdnuggets.com/2023/02/pyspark-data-science.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![PySpark for Data Science](../Images/2ebe106d056da2fc385c3f18101024ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '[PySpark](https://spark.apache.org/docs/latest/api/python/index.html) is an
    Python interference for Apache Spark. It is an open-source library that allows
    you to build Spark applications and analyze the data in a distributed environment
    using a PySpark shell. You can use PySpark for batch processing, running SQL queries,
    Dataframes, real-time analytics, machine learning, and graph processing.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '**Advantages of using Spark:**'
  prefs: []
  type: TYPE_NORMAL
- en: In-memory caching allows real-time computation and low latency.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It can be deployed using multiple ways: Spark’s cluster manager, Mesos, and
    Hadoop via Yarn.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: User-friendly API is available for all popular languages that hide the complexity
    of running distributed systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is 100x faster than Hadoop MapReduce in memory and 10x faster on disk.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this code-based tutorial, we will learn how to initial spark session, load
    the data, change the schema, run SQL queries, visualize the data, and train the
    machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you want to use PySpark on a local machine, you need to install Python, Java,
    Apache Spark, and PySpark. If you want to avoid all of that, you can use [Google
    Colab](https://colab.research.google.com/) or [Kaggle](https://www.kaggle.com/).
    Both platforms come with pre-installed libraries, and you can start coding within
    seconds.
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Colab Notebook, we will start by installing `pyspark` and `py4j`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: After that, we will need to provide the session name to initialize the Spark
    session.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Importing the Data using PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this tutorial, we will be using [Global Spotify Weekly Chart](https://www.kaggle.com/datasets/kabhishm/global-spotify-weekly-chart)
    from Kaggle. It contains information about the artist and the songs on the Spotify
    global weekly chart.
  prefs: []
  type: TYPE_NORMAL
- en: Just like Pandas, we can load the data from CSV to dataframe using `spark.read.csv`
    function and display Schema using `printSchema()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe, PySpark has loaded all of the columns as a string. To perform
    exploratory data analysis, we need to change the Schema.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Updating the Schema
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To change the schema, we need to create a new data schema that we will add to
    `StructType` function. You need to make sure that each column field is getting
    the right data type.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Then, we will load the CSV files using extra argument `schema`. After that,
    we will print the schema to check if the correct changes were made.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, we have different data types for the columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Exploring the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can explore your data as a dataframe by using `toPandas()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** we have used `limit` to display the first five rows. It is similar
    to SQL commands. This means that we can use PySpark Python API for SQL command
    to run queries.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![PySpark for Data Science](../Images/edeb0c2579ecd4916d885888acf5f4e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Just like pandas, we can use `describe()` function to display a summary of data
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![PySpark for Data Science](../Images/53998c47fa57c853807d74126e467011.png)'
  prefs: []
  type: TYPE_IMG
- en: The `count()` function used for displaying number of rows. Read [Pandas API
    on Spark](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/index.html)
    to learn about similar APIs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Column Manipulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can rename your column by using `withColumnRenamed` function. It requires
    an old name and a new name as string.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![PySpark for Data Science](../Images/3d14a8dd2bf66d7283cfb5810f0020e9.png)'
  prefs: []
  type: TYPE_IMG
- en: To drop single or multiple columns, you can use `drop()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![PySpark for Data Science](../Images/6d555820029802ccae6b87b18c37cf10.png)'
  prefs: []
  type: TYPE_IMG
- en: You can use `.na` for dealing with missing valuse. In our case, we are dropping
    all missing values rows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Data Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For data analysis, we will be using PySpark API to translate SQL commands. In
    the first example, we are selecting three columns and display the top 5 rows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![PySpark for Data Science](../Images/068bdd7d061c006700e66d9a39e063b1.png)'
  prefs: []
  type: TYPE_IMG
- en: For more complex queries, we will filter values where ‘Total’ is greater than
    or equal to 600 million to 700 million.
  prefs: []
  type: TYPE_NORMAL
- en: <fo not="" size="+1">**Note:** you can also use `df.Total.between(600000000,
    700000000)` to filter out records.</fo>
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![PySpark for Data Science](../Images/7858ab674eb36cec9d98c63941350995.png)'
  prefs: []
  type: TYPE_IMG
- en: Write if/else statement to create a categorical column using `when` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![PySpark for Data Science](../Images/ad64ce3b32231f59d3f35b64409056f1.png)'
  prefs: []
  type: TYPE_IMG
- en: You can use all of the SQL commands as Python API to run a complete query.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![PySpark for Data Science](../Images/6b9760cfc2a485cf69d86f84929f0699.png)'
  prefs: []
  type: TYPE_IMG
- en: Data Visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s take above query and try to display it as a bar chart. We are plotting
    “artists v.s average song streams” and we are only displaying the top seven artists.
  prefs: []
  type: TYPE_NORMAL
- en: It is awesome, if you ask me.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![PySpark for Data Science](../Images/53998c47fa57c853807d74126e467011.png)'
  prefs: []
  type: TYPE_IMG
- en: Saving the Result
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After processing the data and running analysis, it is the time for saving the
    results.
  prefs: []
  type: TYPE_NORMAL
- en: You can save the results in all of the popular file types, such as CSV, JSON,
    and Parquet.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Data Pre-Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are preparing the data for the machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Categorical Encoding**: converting the categorical columns into integers
    using StringIndexer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Assembling Features**: assembling important features into one vector column.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Scaling**: scaling the data using the StandardScaler scaling function.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![PySpark for Data Science](../Images/1df15b21db2ceea33176c94650c35653.png)'
  prefs: []
  type: TYPE_IMG
- en: KMeans Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have already run the Kmean elbow method to find k. If you want to see all
    of the code sources with the output, you can check out my [notebook](https://colab.research.google.com/drive/1O3XKjoqEk_r08nMyVqjRm2ZalL8le4C5?usp=sharing).
  prefs: []
  type: TYPE_NORMAL
- en: Just like scikit-learn, we will provide a number of clusters and train the Kmeans
    clustering model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![PySpark for Data Science](../Images/833dff07fe7525d2deb604b2a1a1419f.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing Predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we will be using a `matplotlib.pyplot.barplot` to display the
    distribution of 4 clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![PySpark for Data Science](../Images/73b5ed95db0eec08514c9c4aa78ee445.png)![PySpark
    for Data Science](../Images/a30a4966f0e14616d98c81c26286af2e.png)'
  prefs: []
  type: TYPE_IMG
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this tutorial, I have given an overview of what you can do using PySpark
    API. The API allows you to perform SQL-like queries, run pandas functions, and
    training models similar to sci-kit learn. You get the best of all worlds with
    distributed computing.
  prefs: []
  type: TYPE_NORMAL
- en: It outshines a lot of Python packages when dealing with large datasets (>1GB).
    If you are a programmer and just interested in Python code, check our Google Colab
    [notebook](https://colab.research.google.com/drive/1O3XKjoqEk_r08nMyVqjRm2ZalL8le4C5#scrollTo=bWopwzVg1ih0).
    You just have to download and add the data from [Kaggle](https://www.kaggle.com/datasets/kabhishm/global-spotify-weekly-chart)
    to start working on it.
  prefs: []
  type: TYPE_NORMAL
- en: Do let me know in the comments, if you want me to keep writing code based-tutorials
    for other Python libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Abid Ali Awan](https://www.polywork.com/kingabzpro)** ([@1abidaliawan](https://twitter.com/1abidaliawan))
    is a certified data scientist professional who loves building machine learning
    models. Currently, he is focusing on content creation and writing technical blogs
    on machine learning and data science technologies. Abid holds a Master''s degree
    in Technology Management and a bachelor''s degree in Telecommunication Engineering.
    His vision is to build an AI product using a graph neural network for students
    struggling with mental illness.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Data Validation for PySpark Applications using Pandera](https://www.kdnuggets.com/2023/08/data-validation-pyspark-applications-pandera.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science Minimum: 10 Essential Skills You Need to Know to Start…](https://www.kdnuggets.com/2020/10/data-science-minimum-10-essential-skills.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets™ News 22:n06, Feb 9: Data Science Programming…](https://www.kdnuggets.com/2022/n06.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Science Definition Humor: A Collection of Quirky Quotes…](https://www.kdnuggets.com/2022/02/data-science-definition-humor.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Data Science Projects to Learn 5 Critical Data Science Skills](https://www.kdnuggets.com/2022/03/5-data-science-projects-learn-5-critical-data-science-skills.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
