["```py\n#Define Sequential Model\nmodel = Sequential() \n#Create input layer\nmodel.add(Dense(32, input_dim=784))\n#Create hidden layer \nmodel.add(Activation('relu')) \n#Create Output layer\nmodel.add(Activation('sigmoid'))  \n```", "```py\n#Compiling the model with a mean squared error loss and RMSProp #optimizer\nmodel.compile(optimizer='rmsprop',loss='mse')\n```", "```py\n*# Train the model, iterating on the data in batches of 32 samples* model.fit(data, labels, epochs=10, batch_size=32)\n```", "```py\nFACT: Sandra went back to the hallway. Sandra moved to the office.  QUESTION: Is Sandra in the office?ANSWER: yes\n```", "```py\nVOCABULARY:\n'.', '?', 'Daniel', 'Is', 'John', 'Mary', 'Sandra', 'apple','back','bathroom', 'bedroom', 'discarded', 'down','dropped','football', 'garden', 'got', 'grabbed', 'hallway','in', 'journeyed', 'kitchen', 'left', 'milk', 'moved','no', 'office', 'picked', 'put', 'the', 'there', 'to','took', 'travelled', 'up', 'went', 'yes'\n```", "```py\nVECTORIZATION INDEX:\n'the': 1, 'bedroom': 2, 'bathroom': 3, 'took': 4, 'no': 5, 'hallway': 6, '.': 7, 'went': 8, 'is': 9, 'picked': 10, 'yes': 11, 'journeyed': 12, 'back': 13, 'down': 14, 'discarded': 15, 'office': 16, 'football': 17, 'daniel': 18, 'travelled': 19, 'mary': 20, 'sandra': 21, 'up': 22, 'dropped': 23, 'to': 24, '?': 25, 'milk': 26, 'got': 27, 'in': 28, 'there': 29, 'moved': 30, 'garden': 31, 'apple': 32, 'grabbed': 33, 'kitchen': 34, 'put': 35, 'left': 36, 'john': 37}\n```", "```py\nFIRST TRAINING SENTENCE VECTORIZED AND PADDED:\narray([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0, 20, 30, 24,  1,  3,  7, 21, 12, 24, 1,  2,  7])\n```", "```py\nFIRST TRAINING SENTENCE: \n\"Mary moved to the bathroom . Sandra journeyed to the bedroom .\"\n```", "```py\n*#Example of a Placeholder\nquestion = Input((max_question_len,batch_size))*\n```", "```py\n#Create input encoder A:\ninput_encoder_m = Sequential()\ninput_encoder_m.add(Embedding(input_dim=vocab_len,output_dim = 64)) \ninput_encoder_m.add(Dropout(0.3))#Outputs: (Samples, story_maxlen,embedding_dim) -- Gives a list of #the lenght of the samples where each item has the\n#lenght of the max story lenght and every word is embedded in the embbeding dimension\n```", "```py\nmatch = dot([input_encoded_m,question_encoded], axes = (2,2))\nmatch = Activation('softmax')(match)\n```", "```py\nresponse = add([match,input_encoded_c])\nresponse = Permute((2,1))(response)\nanswer = concatenate([response, question_encoded])\n```", "```py\nanswer = LSTM(32)(answer)\nanswer = Dropout(0.5)(answer)\n#Output layer:\nanswer = Dense(vocab_len)(answer) \n#Output shape: (Samples, Vocab_size) #Yes or no and all 0s\nanswer = Activation('softmax')(answer)\n```", "```py\nmodel = Model([input_sequence,question], answer)\nmodel.compile(optimizer='rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n```", "```py\nfilename = 'medium_chatbot_1000_epochs.h5'\nmodel.save(filename)\n```", "```py\nmodel.load_weights('medium_chatbot_1000_epochs.h5')\n```", "```py\npred_results = model.predict(([inputs_test,questions_test]))\n```", "```py\nmy_story = 'Sandra picked up the milk . Mary travelled left . '\nmy_story.split()\nmy_question = 'Sandra got the milk ?'\nmy_question.split()\n```", "```py\nmy_story = 'Medium is cool . Jaime really likes it. '\nmy_story.split()\nmy_question = 'Does Jaime like Medium ?'\nmy_question.split()\n```"]