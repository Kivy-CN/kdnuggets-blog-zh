- en: High Performance Deep Learning, Part 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高性能深度学习，第一部分
- en: 原文：[https://www.kdnuggets.com/2021/06/efficiency-deep-learning-part1.html](https://www.kdnuggets.com/2021/06/efficiency-deep-learning-part1.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/06/efficiency-deep-learning-part1.html](https://www.kdnuggets.com/2021/06/efficiency-deep-learning-part1.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '![](../Images/9970edb2850289adc136ca465d25bc5e.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9970edb2850289adc136ca465d25bc5e.png)'
- en: Machine Learning is being used in countless applications today. It is a natural
    fit in domains where there is no single algorithm that works perfectly, and there
    is a large amount of unseen data that the algorithm needs to do a good job predicting
    the right output. Unlike traditional algorithm problems where we expect exact
    optimal answers, machine learning applications can tolerate approximate answers.
    Deep Learning with neural networks has been the dominant methodology of training
    new machine learning models for the past decade. Its rise to prominence is often
    attributed to the ImageNet [1] competition in 2012\. That year, a University of
    Toronto team submitted a deep convolutional network (AlexNet [2], named after
    the lead developer Alex Krizhevsky), performing 41% better than the next best
    submission.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习在今天的应用中无处不在。它在没有单一算法完美解决问题的领域中天然适用，并且在算法需要很好地预测正确输出的情况下，有大量未见过的数据。与我们期望确切最优解的传统算法问题不同，机器学习应用可以容忍近似答案。深度学习与神经网络在过去十年里一直是训练新机器学习模型的主流方法。它的崛起通常归因于
    2012 年的 ImageNet [1] 竞赛。那一年，多伦多大学的一个团队提交了一个深度卷积网络（AlexNet [2]，以首席开发者 Alex Krizhevsky
    命名），表现比下一个最佳提交好 41%。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业的捷径。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Deep and convolutional networks had been tried prior to this but somehow never
    delivered on the promise. Convolutional Layers were first proposed by LeCun et
    al. in the 90s [3]. Likewise, several neural networks had been proposed in the
    80s, 90s, and so on. What took so long for deep networks to outperform hand-tuned
    feature-engineered models?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度和卷积网络之前已经被尝试过，但总是未能兑现承诺。卷积层最早由 LeCun 等人在 90 年代提出 [3]。同样，80 年代、90 年代等也提出了几种神经网络。深度网络为什么花了这么长时间才超越手工调整特征工程的模型？
- en: 'What was different this time around was a combination of multiple things:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这次的不同之处在于多个因素的结合：
- en: '**Compute**: AlexNet was one of the earlier models to rely on Graphics Processing
    Units (GPUs) for training.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算**：AlexNet 是早期依赖图形处理单元（GPU）进行训练的模型之一。'
- en: '**Algorithms**: A critical fix was that the activation function used ReLU.
    This allows the gradient to back-propagate deeper. Previous iterations of deep
    networks used sigmoid or the tanh activation functions, which saturate at either
    1.0 or -1.0 except a very small range of input. As a result, changing the input
    variable leads to a very tiny gradient (if any), and when there are a large number
    of layers, the gradient essentially vanishes.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**算法**：一个关键的修复是激活函数使用了 ReLU。这使得梯度可以更深地反向传播。之前的深度网络使用了 sigmoid 或 tanh 激活函数，这些函数在非常小的输入范围内饱和到
    1.0 或 -1.0。因此，输入变量的变化导致梯度非常微小（如果有的话），当层数很多时，梯度实际上消失了。'
- en: '**Data**: ImageNet has > 1M images over 1000 classes. With the advent of internet-based
    products, collecting labeled data from user actions also became cheaper.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据**：ImageNet 拥有超过 1000 个类别的 100 万张以上的图像。随着互联网产品的出现，从用户行为中收集标注数据也变得更加便宜。'
- en: Rapid Growth of Deep Learning Models
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习模型的快速增长
- en: As a result of this trailblazing work, there was a race to create deeper networks
    with an ever-larger number of parameters. Several model architectures such as
    VGGNet, Inception, ResNet etc., successively beat previous records at ImageNet
    competitions in the subsequent years. These models have also been deployed in
    the real world.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这项开创性的工作，出现了创建更深网络、参数数量越来越大的竞赛。几种模型架构如 VGGNet、Inception、ResNet 等，在随后的几年中相继打破了
    ImageNet 比赛的记录。这些模型也已在实际应用中投入使用。
- en: '![](../Images/26bfdb65847643ab0246ee3cd5782b3a.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26bfdb65847643ab0246ee3cd5782b3a.png)'
- en: '![](../Images/f6cf23304bf80faa7cd8a27f1bc8e8ac.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6cf23304bf80faa7cd8a27f1bc8e8ac.png)'
- en: '*Figure 1: Growth of the number of parameters in popular computer vision and
    natural language deep learning models over time [4].*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1：流行的计算机视觉和自然语言深度学习模型参数数量的增长趋势 [4]。*'
- en: We have seen a similar effect in the world of natural language understanding
    (NLU), where the Transformer architecture significantly beat previous benchmarks
    on the GLUE tasks. Subsequently, BERT and GPT models have both demonstrated improvements
    on NLP-related tasks. BERT spawned several related model architectures optimizing
    its various aspects. GPT-3 has captured the attention by generating realistic
    text accompanying given prompts. Both have been deployed in production. BERT is
    used in Google Search to improve the relevance of results, and GPT-3 is available
    as an API for interested users to consume.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在自然语言理解（NLU）领域看到了类似的效果，其中 Transformer 架构显著超过了 GLUE 任务的之前基准。随后，BERT 和 GPT 模型在
    NLP 相关任务上均显示出了改进。BERT 产生了几种优化其各种方面的相关模型架构。GPT-3 通过生成与给定提示相匹配的逼真文本引起了关注。这两者都已投入生产。BERT
    用于 Google 搜索以提高结果的相关性，而 GPT-3 作为 API 供感兴趣的用户使用。
- en: As it can be inferred, **deep learning research has been focused on improving
    on state of the art**, and as a result, we have seen progressive improvements
    on benchmarks like image classification, text classification, etc. **Each new
    breakthrough in neural networks has led to an increase in the network complexity,
    the number of parameters**, the amount of training resources required to train
    the network, prediction latency, etc.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如所推测，**深度学习研究一直集中于提高最先进的技术**，因此，我们在图像分类、文本分类等基准测试中看到了持续的进步。**每一次神经网络的新突破都导致了网络复杂度的增加、参数数量的增加**、训练网络所需的资源量、预测延迟等。
- en: Natural language models such as GPT-3 now cost millions of dollars [5] to train
    just one iteration. This does not include the cost of trying combinations of different
    hyper-parameters (tuning) or experimenting with the architecture manually or automatically.
    These models also often have billions (or trillions) of parameters.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 像 GPT-3 这样的自然语言模型现在训练一次就需要花费数百万美元 [5]。这还不包括尝试不同超参数组合（调优）或手动或自动实验架构的成本。这些模型通常还拥有数十亿（或万亿）个参数。
- en: At the same time, the incredible performance of these models also drives the
    demand for applying them on new tasks that were earlier bottlenecked by the available
    technology. This creates an interesting problem, where **the spread of these models
    is rate-limited by their efficiency**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，这些模型的卓越性能也推动了在之前受限于现有技术的新任务上的应用需求。这就产生了一个有趣的问题，即**这些模型的传播受到其效率的限制**。
- en: 'More concretely, we face the following problems as we go into this new era
    of deep learning where models are becoming larger and are spreading across different
    domains:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们在进入这一深度学习新时代时，面临以下问题，其中模型变得越来越大，并且跨越不同领域：
- en: '**Sustainable Server-Side Scaling**: Training and deploying large deep learning
    models is costly. While training could be a one-time cost (or could be free if
    one is using a pre-trained model), deploying and letting inference run for over
    a long period of time could still turn out to be expensive. There is also a very
    real concern around the carbon footprint of data centers that are used for training
    and deploying these large models. Large organizations like Google, Facebook, Amazon,
    etc., spend several billion dollars each per year in capital expenditure on their
    data centers. Hence, any efficiency gains are very significant.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**可持续的服务器端扩展**：训练和部署大型深度学习模型的成本很高。虽然训练可能是一次性成本（或者使用预训练模型时可能免费），但长时间部署和进行推理仍可能是昂贵的。还有一个非常实际的关注点是用于训练和部署这些大型模型的数据中心的碳足迹。像谷歌、脸书、亚马逊等大型组织每年在数据中心的资本支出上花费数十亿美元。因此，任何效率提升都是非常重要的。'
- en: '**Enabling On-Device Deployment**: With the advent of smartphones, IoT devices,
    and the applications deployed on them have to be real-time. Hence, there is a
    need for *on-device* ML models (where the model inference happens directly on
    the device), which makes it imperative to optimize the models for the device they
    will run on.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**启用设备端部署**：随着智能手机、物联网设备的普及，部署在这些设备上的应用必须是实时的。因此，需要*设备端*机器学习模型（即模型推理直接在设备上进行），这使得优化模型以适应运行设备变得非常重要。'
- en: '**Privacy & Data Sensitivity**: Being able to use as little data for training
    is critical when the user data might be sensitive to handling / subject to various
    restrictions such as the GDPR law in Europe. Hence, efficiently training models
    with a fraction of the data means lesser data-collection required. Similarly,
    enabling on-device models would imply that the model inference can be run completely
    on the user’s device without the need to send the input data to the server-side.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**隐私与数据敏感性**：在用户数据可能涉及敏感处理或受各种限制（如欧洲的GDPR法规）的情况下，使用尽可能少的数据进行训练至关重要。因此，高效地使用少量数据训练模型意味着需要收集的数据更少。类似地，使设备上的模型能够运行意味着模型推理可以完全在用户设备上进行，而无需将输入数据发送到服务器端。'
- en: '**New Applications**: Efficiency would also enable applications that couldn’t
    have otherwise been feasible with the existing resource constraints.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**新应用**：效率还将使得在现有资源约束下无法实现的应用成为可能。'
- en: '**Explosion of Models**: Often, there might be multiple ML models being served
    concurrently on the same device. This further reduces the available resources
    for a single model. This could happen on the server-side where multiple models
    are co-located on the same machine or could be in an app where different models
    are used for different functionalities.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型爆炸**：通常，可能会有多个机器学习模型在同一设备上同时服务。这进一步减少了单个模型的可用资源。这可能发生在服务器端，多个模型共存在同一台机器上，或者在应用中，不同的模型用于不同的功能。'
- en: Efficient Deep Learning
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高效深度学习
- en: The core challenge that we identified above is *efficiency*. While efficiency
    can be an overloaded term, let us investigate two primary aspects.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们上述识别出的核心挑战是*效率*。虽然效率可能是一个过载的术语，但让我们探讨两个主要方面。
- en: '**Inference Efficiency**:  This primarily deals with questions that someone
    deploying a model for *inference* (computing the model outputs for a given input)
    would ask. Is the model small? Is it fast, etc.? More concretely, how many parameters
    does the model have? What is the disk size, RAM consumption during inference,
    inference latency, etc.?'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**推理效率**：这主要涉及部署模型进行*推理*（计算给定输入的模型输出）时需要问的问题。模型是否小巧？是否快速等等？更具体地说，模型有多少参数？磁盘大小、推理期间的内存消耗、推理延迟等是多少？'
- en: '**Training Efficiency**: This involves questions someone training a model would
    ask, such as how long does the model take to train? How many devices are required
    for the training? Can the model fit in memory? It might also include questions
    like, how much data would the model need to achieve the desired performance on
    the given task?'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练效率**：这涉及到训练模型时需要问的一些问题，比如模型需要多长时间训练？需要多少设备进行训练？模型是否可以适应内存？还可能包括诸如，模型需要多少数据才能在给定任务上达到预期的性能？'
- en: If we were to be given two models performing equally well on a given task, we
    might want to choose a model which does better in either one or ideally both of
    the above aspects. If one were to be deploying a model on devices where inference
    is constrained (such as mobile and embedded devices) or expensive (cloud servers),
    it might be worth paying attention to inference efficiency. Similarly, if one
    is training a large model from scratch on either with limited or costly training
    resources, developing models that are designed for training efficiency would help.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有两个模型在给定任务上表现相同，我们可能会选择在上述一个或理想情况下两个方面表现更好的模型。如果在推理受限（如移动和嵌入设备）或昂贵（云服务器）的设备上部署模型，关注推理效率可能是值得的。同样，如果在有限或昂贵的训练资源下从头开始训练大型模型，开发针对训练效率设计的模型将有所帮助。
- en: '![](../Images/e1079ed6677f853b3518c62769b02a27.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1079ed6677f853b3518c62769b02a27.png)'
- en: '*Figure 2: Pareto-optimality: Green dots represent pareto-optimal models (together
    forming the pareto-frontier), where none of the other models (red dots) get better
    accuracy with the same inference latency or the other way around.*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2: 帕累托最优性：绿色点代表帕累托最优模型（共同形成帕累托前沿），其中没有其他模型（红色点）在相同的推理延迟下获得更好的准确性，反之亦然。*'
- en: Regardless of what one might be optimizing for, we want to achieve *pareto-optimality*.
    This implies that any model that we choose is the best for the tradeoffs that
    we care about. As an example in Figure 2, the green dots represent pareto-optimal
    models, where none of the other models (red dots) get better accuracy with the
    same inference latency or the other way around. Together, the pareto-optimal models
    (green dots) form our pareto-frontier. The models in the pareto-frontier are by
    definition more efficient than the other models since they perform the best for
    their given tradeoff. Hence, when we seek efficiency, we should be thinking about
    discovering and improving on the pareto-frontier.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们优化的目标是什么，我们都希望达到*帕累托最优性*。这意味着我们选择的任何模型都是在我们关心的权衡中表现最好的。例如，在图 2 中，绿色点代表帕累托最优模型，其中没有其他模型（红色点）在相同的推理延迟下获得更好的准确性，反之亦然。总之，帕累托最优模型（绿色点）形成了我们的帕累托前沿。帕累托前沿的模型从定义上讲比其他模型更高效，因为它们在给定的权衡中表现最好。因此，当我们寻求效率时，我们应该考虑发现和改进帕累托前沿。
- en: Efficient Deep Learning can, in turn, be defined as a collection of algorithms,
    techniques, tools, and infrastructure that work together to allow users to train
    and deploy *pareto-optimal* models that simply cost lesser resources to train
    and/or deploy while achieving similar results.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的深度学习可以定义为一组算法、技术、工具和基础设施，它们共同作用，使用户能够训练和部署*帕累托最优*模型，这些模型在训练和/或部署时消耗较少的资源，同时达到类似的结果。
- en: Now that we have motivated the problem, in the next post, we will discuss the
    five pillars of efficiency in Deep Learning.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经激发了这个问题，在下一篇文章中，我们将讨论深度学习效率的五个支柱。
- en: References
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
    2009\. ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference
    on Computer Vision and Pattern Recognition. 248–255\. https://doi.org/10.1109/CVPR.2009.5206848'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, 和 Li Fei-Fei. 2009\.
    ImageNet: 一个大规模层次化图像数据库。2009 年 IEEE 计算机视觉与模式识别大会。248–255\. https://doi.org/10.1109/CVPR.2009.5206848'
- en: '[2] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012\. Imagenet
    classification with deep convolutional neural networks. Advances in neural information
    processing systems 25 (2012), 1097–1105.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Alex Krizhevsky, Ilya Sutskever, 和 Geoffrey E Hinton. 2012\. 使用深度卷积神经网络进行
    Imagenet 分类。神经信息处理系统进展 25 (2012), 1097–1105。'
- en: '[3] Convolutional Networks: [http://yann.lecun.com/exdb/lenet/index.html](http://yann.lecun.com/exdb/lenet/index.html)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 卷积网络: [http://yann.lecun.com/exdb/lenet/index.html](http://yann.lecun.com/exdb/lenet/index.html)'
- en: '[4] PapersWithCode: [https://paperswithcode.com/](https://paperswithcode.com/)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] PapersWithCode: [https://paperswithcode.com/](https://paperswithcode.com/)'
- en: '[5] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. 2020\. Language models are few-shot learners. arXiv preprint arXiv:2005.14165
    (2020).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    等. 2020\. 语言模型是少量样本学习者。arXiv 预印本 arXiv:2005.14165 (2020)。'
- en: '**Bio:** [Gaurav Menghani](http://www.gaurav.im/) ([@GauravML](https://twitter.com/GauravML))
    is a Staff Software Engineer at Google Research, where he leads research projects
    geared towards optimizing large machine learning models for efficient training
    and inference on devices ranging from tiny microcontrollers to Tensor Processing
    Unit (TPU)-based servers. His work has positively impacted > 1 billion active
    users across YouTube, Cloud, Ads, Chrome, etc. He is also an author of an upcoming
    book with Manning Publication on Efficient Machine Learning. Before Google, Gaurav
    worked at Facebook for 4.5 years and has contributed significantly to Facebook’s
    Search system and large-scale distributed databases. He has an M.S. in Computer
    Science from Stony Brook University.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：** [Gaurav Menghani](http://www.gaurav.im/)（[@GauravML](https://twitter.com/GauravML)）是Google
    Research的高级软件工程师，他领导着旨在优化大型机器学习模型的研究项目，以实现高效的训练和推理，这些模型可以在从微控制器到基于Tensor Processing
    Unit（TPU）的服务器等各种设备上运行。他的工作对YouTube、Cloud、Ads、Chrome等领域的超过10亿活跃用户产生了积极的影响。他还是即将出版的Manning
    Publication的《高效机器学习》一书的作者。在Google之前，Gaurav在Facebook工作了4.5年，并对Facebook的搜索系统和大规模分布式数据库做出了重要贡献。他拥有纽约州立大学石溪分校的计算机科学硕士学位。'
- en: '**Related:**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Vision Transformers: Natural Language Processing (NLP) Increases Efficiency
    and Model Generality](https://www.kdnuggets.com/2021/02/vision-transformers-nlp-efficiency-model-generality.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[视觉变换器：自然语言处理（NLP）提高效率和模型泛化能力](https://www.kdnuggets.com/2021/02/vision-transformers-nlp-efficiency-model-generality.html)'
- en: '[Deep Learning’s Most Important Ideas](https://www.kdnuggets.com/2020/09/deep-learnings-most-important-ideas.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习的最重要思想](https://www.kdnuggets.com/2020/09/deep-learnings-most-important-ideas.html)'
- en: '[2 Things You Need to Know about Reinforcement Learning – Computational Efficiency
    and Sample Efficiency](https://www.kdnuggets.com/2020/04/2-things-reinforcement-learning.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于强化学习的2件事——计算效率和样本效率](https://www.kdnuggets.com/2020/04/2-things-reinforcement-learning.html)'
- en: More On This Topic
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[The High Paying Side Hustles for Data Scientists](https://www.kdnuggets.com/2022/01/high-paying-side-hustles-data-scientists.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学家的高薪副业](https://www.kdnuggets.com/2022/01/high-paying-side-hustles-data-scientists.html)'
- en: '[KDnuggets™ News 22:n04, Jan 26: The High Paying Side Hustles…](https://www.kdnuggets.com/2022/n04.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets™ 新闻 22:n04, 1月26日：高薪副业…](https://www.kdnuggets.com/2022/n04.html)'
- en: '[People Management for AI: Building High-Velocity AI Teams](https://www.kdnuggets.com/2022/03/people-management-ai-building-highvelocity-ai-teams.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[人工智能人员管理：构建高效能AI团队](https://www.kdnuggets.com/2022/03/people-management-ai-building-highvelocity-ai-teams.html)'
- en: '[7 High Paying Side Hustles for Data Scientists](https://www.kdnuggets.com/7-high-paying-side-hustles-for-data-scientists)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7个高薪的数据科学副业](https://www.kdnuggets.com/7-high-paying-side-hustles-for-data-scientists)'
- en: '[7 Platforms for Getting High Paying Data Science Jobs](https://www.kdnuggets.com/7-platforms-for-getting-high-paying-data-science-jobs)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7个平台获取高薪数据科学职位](https://www.kdnuggets.com/7-platforms-for-getting-high-paying-data-science-jobs)'
- en: '[High Availability SQL Server Docker Containers in Kubernetes](https://www.kdnuggets.com/2022/04/high-availability-sql-server-docker-containers-kubernetes.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kubernetes中的高可用性SQL Server Docker容器](https://www.kdnuggets.com/2022/04/high-availability-sql-server-docker-containers-kubernetes.html)'
