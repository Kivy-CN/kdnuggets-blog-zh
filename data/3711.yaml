- en: Research Papers for NLP Beginners
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP 初学者的研究论文
- en: 原文：[https://www.kdnuggets.com/2022/11/research-papers-nlp-beginners.html](https://www.kdnuggets.com/2022/11/research-papers-nlp-beginners.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/11/research-papers-nlp-beginners.html](https://www.kdnuggets.com/2022/11/research-papers-nlp-beginners.html)
- en: '![Research Papers for NLP Beginners](../Images/06c3080b57d4b24c0cd05a468f8c5022.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![NLP 初学者的研究论文](../Images/06c3080b57d4b24c0cd05a468f8c5022.png)'
- en: '[Sincerely Media](https://unsplash.com/@sincerelymedia) via Unsplash'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sincerely Media](https://unsplash.com/@sincerelymedia) 通过 Unsplash'
- en: If you’re new to the world of data and have a particular interest in NLP (Natural
    Language Processing), you’re probably looking for resources to help grasp a better
    understanding.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是数据世界的新手，并对 NLP（自然语言处理）有特别的兴趣，你可能在寻找资源以帮助你更好地理解。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织 IT 部门'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: You have probably come across so many different research papers and are sitting
    there confused about which one to choose. Because let’s face it, they’re not short
    and they do consume a lot of brain power. So it would be smart to choose the right
    one that will benefit your path to mastering NLP.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经遇到过许多不同的研究论文，并对选择哪一篇感到困惑。因为说实话，它们并不短，而且确实需要大量的脑力。因此，选择一篇对你掌握 NLP 有帮助的论文将是明智的。
- en: I have done some research and have collected a few NLP research papers that
    have been highly recommended for newbies in the NLP area and overall NLP knowledge.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我做了一些研究，并收集了一些在 NLP 领域高度推荐的研究论文，以供初学者和全面的 NLP 知识参考。
- en: I will break it up into sections so you can go find exactly what you want.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我会将其拆分为几个部分，以便你可以找到你想要的内容。
- en: Machine Learning and NLP
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习与 NLP
- en: '**[Text Classification from Labeled and Unlabeled Documents using EM](https://www.ri.cmu.edu/pub_files/pub1/nigam_k_1999_1/nigam_k_1999_1.pdf)
    by Kamal Nigam, 1999**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**[利用 EM 从标记和未标记文档中进行文本分类](https://www.ri.cmu.edu/pub_files/pub1/nigam_k_1999_1/nigam_k_1999_1.pdf)
    作者：Kamal Nigam，1999年**'
- en: This paper is about how you can improve the accuracy of learned text classifiers
    by augmenting a small number of labeled training documents with a large pool of
    unlabeled documents.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本论文讲述了如何通过将少量标记的训练文档与大量未标记的文档结合来提高文本分类器的准确性。
- en: '**[Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://aclanthology.org/2020.acl-main.442.pdf)
    by Marco Tulio Ribeiro et al., 2020**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**[超越准确性：使用 CheckList 对 NLP 模型进行行为测试](https://aclanthology.org/2020.acl-main.442.pdf)
    作者：Marco Tulio Ribeiro 等，2020年**'
- en: In this paper, you will learn more about CheckList, a task-agnostic methodology
    for testing NLP models as unfortunately some of the most used current approaches
    overestimate the performance of NLP models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，你将更多地了解 CheckList，一种测试 NLP 模型的任务无关方法，因为不幸的是，一些目前最常用的方法高估了 NLP 模型的性能。
- en: Neural Models
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经模型
- en: '**[Natural Language Processing (almost) from Scratch](https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf)
    by Ronan Collobert, 2011**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**[自然语言处理（几乎）从零开始](https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf)
    作者：Ronan Collobert，2011年**'
- en: In this paper, you will go through the foundations of NLP - as it states in
    the title, it is ALMOST from scratch. Topics include Named Entity Recognition,
    Semantic role labeling, networks, training, and more.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，你将学习 NLP 的基础知识——正如标题所示，它几乎是从零开始的。主题包括命名实体识别、语义角色标注、网络、训练等。
- en: '**[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    by Christopher Olah, 2015**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**[理解 LSTM 网络](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) 作者：Christopher
    Olah，2015年**'
- en: Neural Networks are a major part of NLP, therefore having a good understanding
    of it will benefit you in the long run. In this paper, there is a focus on LSTM
    networks which are widely used.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是 NLP 的重要组成部分，因此对其有良好的理解将对你长期受益。这篇论文重点关注广泛使用的 LSTM 网络。
- en: Word/Sentence Representation and Embedding
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词/句子表示和嵌入
- en: '**[Distributed Representations of Words and Phrases and their Compositionality](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)
    by Tomas Mikolov, 2013**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**[词汇和短语的分布式表示及其组合性](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)**
    由 Tomas Mikolov 发表，2013年'
- en: Written by Mikolov, who introduced the Skip-gram model for learning high-quality
    vector representations of words from large amounts of unstructured text data -
    this paper will present several extensions of the original Skip-gram model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Mikolov 撰写，他引入了 Skip-gram 模型，用于从大量非结构化文本数据中学习高质量的单词向量表示——这篇论文将介绍原始 Skip-gram
    模型的几个扩展。
- en: '**[Distributed Representations of Sentences and Documents](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)**
    by Quoc Le and Tomas Mikolov, 2014'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**[句子和文档的分布式表示](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)** 由 Quoc
    Le 和 Tomas Mikolov 发表，2014年'
- en: Going into more depth about the two major weaknesses of bag-of-words, the authors
    introduce Paragraph Vector - which is an unsupervised algorithm that learns fixed-length
    feature representations from variable-length pieces of text, such as sentences.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 深入探讨了袋词模型的两个主要弱点，作者介绍了段落向量——这是一种无监督算法，用于从可变长度的文本片段（如句子）中学习固定长度的特征表示。
- en: Language Modelling
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言建模
- en: '**[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    by Alec Radford, 2018**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**[语言模型是无监督的多任务学习者](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)**
    由 Alec Radford 发表，2018年'
- en: Natural language processing tasks are normally approached with supervised learning
    on task-specific datasets. However, Multitask learning is being tested as a promising
    framework for improving general performance in NLP.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理任务通常通过在特定任务的数据集上进行监督学习来解决。然而，多任务学习被测试作为一种有前途的框架，以提高NLP的整体性能。
- en: '**[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)**
    by Andrej Karpathy, 2015'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**[递归神经网络的非凡有效性](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)**
    由 Andrej Karpathy 发表，2015年'
- en: This paper goes back to the start of recurrent neural networks and why they
    are so effective and robust with code examples to give you a better understanding
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文回溯了递归神经网络的起点，并探讨了它们为何如此有效和稳健，并附有代码示例以帮助你更好地理解。
- en: Attention & Transformers
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力与 Transformer
- en: '**[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
    by Jacob Devlin et al., 2019**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**[BERT：深度双向 Transformer 的语言理解预训练](https://arxiv.org/pdf/1810.04805.pdf)**
    由 Jacob Devlin 等人 发表，2019年'
- en: As you’re learning about machine learning, you have probably heard about BERT
    - Bidirectional Encoder Representations from Transformers. It is widely used and
    known for being able to pre-train deep bidirectional representations from unlabeled
    text. In this paper, you will further understand and learn how to improve your
    fine-tuning based on BERT.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习机器学习时，你可能听说过 BERT——来自 Transformer 的双向编码表示。它被广泛使用，并以能够从未标记的文本中进行深度双向表示预训练而著称。在这篇论文中，你将进一步理解并学习如何基于
    BERT 改进你的微调。
- en: '**[Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) by Ashish
    Vaswani et al., 2017**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**[注意力机制才是关键](https://arxiv.org/pdf/1706.03762.pdf)** 由 Ashish Vaswani 等人 发表，2017年'
- en: This paper focuses on the Transformer, solely on attention mechanisms which
    differ from models which are typically based on complex recurrent or convolutional
    neural networks. You will learn how Transformer generalizes well to other tasks
    and may be the better option.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文专注于 Transformer，完全关注注意力机制，这与通常基于复杂递归或卷积神经网络的模型不同。你将学习 Transformer 如何很好地泛化到其他任务，并可能是更好的选择。
- en: '**[HuggingFace''s Transformers: State-of-the-art Natural Language Processing](https://arxiv.org/pdf/1910.03771.pdf)
    by Thomas Wolf et al., 2020**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**[HuggingFace 的 Transformers：最先进的自然语言处理](https://arxiv.org/pdf/1910.03771.pdf)**
    由 Thomas Wolf 等人 发表，2020年'
- en: Want to learn more about Transformers which has become the dominant architecture
    for natural language processing? In this paper, you will learn more about its
    architecture and how it facilitates the distribution of pre-trained models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于已经成为自然语言处理主流架构的Transformer吗？在这篇论文中，你将深入了解它的架构以及它如何促进预训练模型的分发。
- en: Wrapping Up
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Like I said above, I don’t want to overwhelm you with so many different research
    papers - therefore I have kept it at a minimal level.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我上面所说，我不想用这么多不同的研究论文让你感到困扰——因此我保持了最小化的水平。
- en: If you know of any that beginners may benefit from, please drop them in the
    comments so that they can see them. Thank you!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道任何适合初学者的资源，请在评论中分享，以便他们可以看到。谢谢！
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** 是一名数据科学家和自由技术作家。她特别关注提供数据科学职业建议或教程以及数据科学的理论知识。她还希望探索人工智能如何/能如何有助于人类寿命的延续。作为一个热衷于学习的人，她寻求拓宽自己的技术知识和写作技能，同时帮助指导他人。'
- en: More On This Topic
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Must Read NLP Papers from the Last 12 Months](https://www.kdnuggets.com/2023/03/must-read-nlp-papers-last-12-months.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[过去12个月的必读NLP论文](https://www.kdnuggets.com/2023/03/must-read-nlp-papers-last-12-months.html)'
- en: '[8 Innovative BERT Knowledge Distillation Papers That Have Changed…](https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8篇创新的BERT知识蒸馏论文，这些论文改变了…](https://www.kdnuggets.com/2022/09/eight-innovative-bert-knowledge-distillation-papers-changed-nlp-landscape.html)'
- en: '[Generative Agent Research Papers You Should Read](https://www.kdnuggets.com/generative-agent-research-papers-you-should-read)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你应该阅读的生成代理研究论文](https://www.kdnuggets.com/generative-agent-research-papers-you-should-read)'
- en: '[Top 5 NLP Cheat Sheets for Beginners to Professional](https://www.kdnuggets.com/2022/12/top-5-nlp-cheat-sheets-beginners-professional.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学者到专业人士的前5名NLP备忘单](https://www.kdnuggets.com/2022/12/top-5-nlp-cheat-sheets-beginners-professional.html)'
- en: '[A Brief Introduction to Papers With Code](https://www.kdnuggets.com/2022/04/brief-introduction-papers-code.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[带代码的论文简要介绍](https://www.kdnuggets.com/2022/04/brief-introduction-papers-code.html)'
- en: '[KDnuggets News, April 27: A Brief Introduction to Papers With Code;…](https://www.kdnuggets.com/2022/n17.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets新闻，4月27日：带代码的论文简要介绍；…](https://www.kdnuggets.com/2022/n17.html)'
