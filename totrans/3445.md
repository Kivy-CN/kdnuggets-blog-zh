# Tensorflow 中的多任务学习：第1部分

> 原文：[https://www.kdnuggets.com/2016/07/multi-task-learning-tensorflow-part-1.html](https://www.kdnuggets.com/2016/07/multi-task-learning-tensorflow-part-1.html)

**作者：乔纳森·戈德温，伦敦大学学院**。

> 这篇博客文章附带一个 Jupyter notebook。请[下载这里](https://github.com/jg8610/multi-task-part-1-notebook/tree/master)。

### 引言

**为什么选择多任务学习**

当你考虑人们如何学习新事物时，他们通常会利用自己的经验和对世界的知识来加快学习过程。当我学习一种新语言，特别是相关的语言时，我会利用我已经掌握的语言知识来走捷径。这个过程也可以反向进行——学习一种新语言可以帮助你更好地理解和使用自己的语言。

我们的大脑能够同时学习多种不同的任务——无论是将英语翻译成德语还是翻译成法语，我们的大脑架构都是相同的。如果我们使用机器学习算法来完成这两个任务，我们可能会称之为“多任务”学习。

这是未来几年机器学习中最有趣和激动人心的研究领域之一，极大地减少了学习新概念所需的数据量。深度学习的一个重要承诺是，通过模型的强大能力和在任务之间共享参数的简单方法，我们应该能够在多任务学习中取得显著进展。

当我开始在这个领域进行实验时，我遇到了一些障碍——虽然理解实现多任务学习所需的架构更改很容易，但弄清楚如何在 Tensorflow 中实现它却更难。除了标准网络之外的任何操作都需要对 Tensorflow 的工作原理有很好的理解，但大多数现成的示例并未提供有用的指导。我希望下面的教程能够简单地解释一些关键概念，并帮助那些遇到困难的人。

### 我们将要做的

**第1部分**

1.  **通过示例理解 Tensorflow 计算图**。进行 Tensorflow 多任务学习需要理解计算图的工作原理——如果你已经知道了，可以跳过这部分。

1.  **理解我们如何使用图进行多任务学习**。我们将通过一个示例来演示如何调整一个简单的图以进行多任务学习。

**第2部分**

1.  **构建一个用于词性标注和浅层解析的图**。我们将填写一个模板，训练一个用于两个相关语言任务的网络。别担心，你不需要知道它们是什么！

1.  **联合和分开训练一个网络**。我们将以两种不同的方式训练一个模型。你应该能够在你的笔记本电脑上完成这项工作。

### 使用玩具示例理解计算图

计算图是使 Tensorflow（以及其他类似的软件包）快速的关键因素。它是深度学习机器的一个重要组成部分，但可能会让人感到困惑。

图有一些很棒的特性，使得多任务学习变得非常容易，但首先我们会保持简单，解释关键概念。

**定义：计算图**

**计算图**是你将要运行的**计算模板**（参见：算法）。它**不会执行任何计算**，但这意味着你的计算机可以更快地进行反向传播。

如果你请求 Tensorflow 得到一个**计算结果**，它将**只进行工作所需的计算**，**而不是整个图**。

### 玩具示例 - 线性变换：设置图

我们将查看一个简单计算的图 - 对输入进行线性变换，并计算平方损失：

![玩具示例](../Images/52c75dcdf2f4ae57c1cc42de0b09640c.png)

```py
# Import Tensorflow and numpy
import Tensorflow as tf
import numpy as np

# ======================
# Define the Graph
# ======================

# Create Placeholders For X And Y (for feeding in data)
X = tf.placeholder("float",[10, 10],name="X") # Our input is 10x10
Y = tf.placeholder("float", [10, 1],name="Y") # Our output is 10x1

# Create a Trainable Variable, "W", our weights for the linear transformation
initial_W = np.zeros((10,1))
W = tf.Variable(initial_W, name="W", dtype="float32")

# Define Your Loss Function
Loss = tf.pow(tf.add(Y,-tf.matmul(X,W)),2,name="Loss")

```

关于这个图，有几个要点需要强调：

+   **如果我们现在运行这段代码，我们将不会得到任何输出**。记住，计算图只是一个模板 - 它什么也不做。如果我们想要答案，我们必须告诉 Tensorflow 使用**会话**来运行计算。

+   **我们没有显式创建图对象**。你可能会期待我们需要在某个地方创建一个图对象，以便 Tensorflow 知道我们想创建一个图。实际上，通过使用 Tensorflow 操作，我们告诉 Tensorflow 哪些部分的代码在图中。

**提示：保持图的独立**。你通常会在图外进行大量的数据操作和计算，这意味着在 Python 中跟踪哪些内容是可用的会有点混乱。我喜欢把图放在一个单独的文件中，通常还会放在一个单独的类里以保持关注点分离，但这并不是强制要求。

### 玩具示例 - 线性变换：获取结果

在 Tensorflow 的**会话**中进行图上的计算。要从会话中获取结果，你需要提供两个东西：目标结果和输入。

1.  **目标结果或操作**。你告诉 Tensorflow 你想要哪些图的部分返回值，它将**自动确定需要运行哪些计算**。你也可以调用操作，例如初始化变量。

1.  **按需输入（‘Feed Dict’）**。在大多数计算中，你会即时提供输入数据。在这种情况下，你使用**占位符**构建图，并在计算时提供数据。并不是所有的计算或操作都需要输入 - 对于许多情况，所有信息已经包含在图中。

```py
# Import Tensorflow and Numpy
import Tensorflow as tf
import numpy as np

# ======================
# Define the Graph
# ======================

# Create Placeholders For X And Y (for feeding in data)
X = tf.placeholder("float",[10, 10],name="X") # Our input is 10x10
Y = tf.placeholder("float", [10, 1],name="Y") # Our output is 10x1

# Create a Trainable Variable, "W", our weights for the linear transformation
initial_W = np.zeros((10,1))
W = tf.Variable(initial_W, name="W", dtype="float32")

# Define Your Loss Function
Loss = tf.pow(tf.add(Y,-tf.matmul(X,W)),2,name="Loss")

with tf.Session() as sess: # set up the session
    sess.run(tf.initialize_all_variables())
    Model_Loss = sess.run(
                Loss, # the first argument is the name of the Tensorflow variabl you want to return
                { # the second argument is the data for the placeholders
                  X: np.random.rand(10,10),
                  Y: np.random.rand(10).reshape(-1,1)
                })
    print(Model_Loss)

```

### 如何使用图进行多任务学习

当我们创建一个执行多个任务的神经网络时，我们希望网络的一些部分是共享的，而其他部分则特定于每个任务。在训练时，我们希望从每个任务中获取的信息能在网络的共享部分传递。

因此，首先，让我们绘制一个简单的两任务网络图，其中包含一个共享层和每个任务的特定层。我们将把这些输出输入到我们的损失函数中与目标进行比较。我已经标记了我们希望在图中创建占位符的位置。

![基本共享图](../Images/8ae5537858dcd61fd6448f72993af879.png)

```py
#  GRAPH CODE
# ============

# Import Tensorflow
import Tensorflow as tf

# ======================
# Define the Graph
# ======================

# Define the Placeholders
X = tf.placeholder("float", [10, 10], name="X")
Y1 = tf.placeholder("float", [10, 1], name="Y1")
Y2 = tf.placeholder("float", [10, 1], name="Y2")

# Define the weights for the layers
shared_layer_weights = tf.Variable([10,20], name="share_W")
Y1_layer_weights = tf.Variable([20,1], name="share_Y1")
Y2_layer_weights = tf.Variable([20,1], name="share_Y2")

# Construct the Layers with RELU Activations
shared_layer = tf.nn.relu(tf.matmul(X,shared_layer_weights))
Y1_layer = tf.nn.relu(tf.matmul(shared_layer,Y1_layer_weights))
Y2_layer_weights = tf.nn.relu(tf.matmul(shared_layer,Y2_layer_weights))

# Calculate Loss
Y1_Loss = tf.nn.l2_loss(Y1,Y1_layer)
Y2_Loss = tf.nn.l2_loss(Y2,Y2_layer)

```

当我们训练这个网络时，**我们希望任务 1 层的参数不管任务 2 有多么错误都不改变**，但**共享层的参数要随着两个任务的变化而变化**。这可能看起来有点困难——通常图中只有一个优化器，因为你只优化一个损失函数。幸运的是，利用图的属性，这种模型的训练非常简单。

* * *

## 我们的前 3 个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的 IT 部门

* * *

### 更多相关话题

+   [TensorFlow for Computer Vision - Transfer Learning Made Easy](https://www.kdnuggets.com/2022/01/tensorflow-computer-vision-transfer-learning-made-easy.html)

+   [PyTorch 还是 TensorFlow？比较流行的机器学习框架](https://www.kdnuggets.com/2022/02/packt-pytorch-tensorflow-comparing-popular-machine-learning-frameworks.html)

+   [Tensorflow 的 "Hello World"](https://www.kdnuggets.com/2022/05/hello-world-tensorflow.html)

+   [使用 Tensorflow 训练图像分类模型指南](https://www.kdnuggets.com/2022/12/guide-train-image-classification-model-tensorflow.html)

+   [使用 TensorFlow 和 Keras 构建和训练你的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)

+   [免费 TensorFlow 2.0 完整课程](https://www.kdnuggets.com/2023/02/free-tensorflow-20-complete-course.html)
