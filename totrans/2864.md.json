["```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n```", "```py\ndef sigmoid(scores): \n   return 1 / (1 + np.exp(-scores))\n```", "```py\ndef log_likelihood(features, target, weights): \n   scores = np.dot(features, weights)  \n   ll = np.sum(target * scores - np.log(1 + np.exp(scores)))  \n   return ll\n```", "```py\ndef logistic_regression(features, target, num_steps, learning_rate, add_intercept=False):\n    if add_intercept: \n       intercept = np.ones((features.shape[0], 1))  \n       features = np.hstack((intercept, features))     weights = np.zeros(features.shape[1])\n\n    for step in range(num_steps):  \n      scores = np.dot(features, weights) \n      predictions = sigmoid(scores)  \n      output_error_signal = target - predictions      \n      gradient = np.dot(features.T, output_error_signal)    \n      weights += learning_rate * gradient  \n\n      if step % 10000 == 0:           \n        print(log_likelihood(features, target, weights)) \n\nreturn weights\n```", "```py\nnp.random.seed(10)\nnum_observations = 10000 x1 = np.random.multivariate_normal([0, 0], [[1, 0.5], [0.5, 1]], num_observations)\nx2 = np.random.multivariate_normal([1, 4], [[1, 0.5], [0.5, 1]], num_observations)\n```", "```py\nsimulated_separableish_features = np.vstack((x1, x2)).astype(np.float32)simulated_labels = np.hstack((np.zeros(num_observations), np.ones(num_observations)))\n```", "```py\nplt.figure(figsize=(10, 8))plt.scatter(simulated_separableish_features[:, 0],   simulated_separableish_features[:, 1], c=simulated_labels,    alpha=0.3,)\n\nplt.show()\n```"]