- en: 'Essential Math for Data Science: Eigenvectors and Application to PCA'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/06/essential-math-data-science-eigenvectors-application-pca.html](https://www.kdnuggets.com/2022/06/essential-math-data-science-eigenvectors-application-pca.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Matrix decomposition*, also called *matrix factorization* is the process of
    splitting a matrix into multiple pieces. In the context of data science, you can
    for instance use it to select parts of the data, aimed at reducing dimensionality
    without losing much information (as for instance in Principal Component Analysis,
    as you’ll later in this post). Some operations are also more easily computed on
    the matrices resulting from the decomposition.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, you’ll learn about the eigendecomposition of a matrix. One
    way to understand it is to consider it as a special change of basis (more details
    about change of basis in [my last post](https://hadrienj.github.io/posts/Essential-Math-for-Data-Science-Change-of-Basis/)).
    You’ll first learn about eigenvectors and eigenvalues and then you’ll see how
    it can be applied to Principal Component Analysis (PCA). The main idea is to consider
    the eigendecomposition of a matrix ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png)
    as a change of basis where the new basis vectors are the eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Eigenvectors and Eigenvalues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see in Chapter 7 of [Essential Math for Data Science](https://www.essentialmathfordatascience.com/?utm_source=hadrienj&utm_medium=blog&utm_campaign=hadrienj_2021-02-01-Essential-Math-for-Data-Science-Eigendecomposition.md) you
    can consider matrices as linear transformations. This means that if you take any
    vector ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) and apply the
    matrix ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) to it, you
    obtain a transformed vector ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the example of:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/dee23a1453606cccba63318b471db7cd.png)'
  prefs: []
  type: TYPE_IMG
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/ffd25749845bb0ffe9bb83c6cf3555ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you apply ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) to
    the vector ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) (with the
    matrix-vector product), you get a new vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/ee1a6407bd3de954cc6b370389f22b49.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Equation](../Images/6043576e294a8d22f397ca24b1e957fd.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Equation](../Images/20a8b29fa654acfac64968c9d78f3cdc.png)'
  prefs: []
  type: TYPE_IMG
- en: '![Equation](../Images/f61d37b006830115c275a9ef5faa4c42.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s draw the initial and transformed vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Essential Math for Data Science: Eigenvectors and Application to PCA](../Images/d9ef15de870b38c85005ee8a32e0ea9e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Transformation of the vector ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) by
    the matrix ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) into the
    vector ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png).'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, as you can expect, the transformed vector ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png) doesn’t
    run in the same direction as the initial vector ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png).
    This change of direction characterizes most of the vectors you can transform by ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, take the following vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/f2e88aceae6e8e7f7b10526bb8bd06b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s apply the matrix ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) to
    the vector ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) to obtain
    a vector ![Equation](../Images/ea571e13bda6431149977a7da4e30a98.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Essential Math for Data Science: Eigenvectors and Application to PCA](../Images/aaa03eb844fe9221ed7a97da30f230d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Transformation of the special vector ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) by
    the matrix ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see in Figure 2 that the vector ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) has
    a special relationship with the matrix ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png):
    it is rescaled (with a negative value), but both the initial vector ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) and
    the transformed vector ![Equation](../Images/ea571e13bda6431149977a7da4e30a98.png) are
    on the same line.'
  prefs: []
  type: TYPE_NORMAL
- en: The vector ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) is an *eigenvector* of ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png).
    It is only scaled by a value, which is called an *eigenvalue* of the matrix ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png).
    An eigenvector of the matrix ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) is
    a vector that is contracted or elongated when transformed by the matrix. The eigenvalue
    is the scaling factor by which the vector is contracted or elongated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the vector ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png) is
    an eigenvector of ![Equation](../Images/a4101ce2e79e16bd0d29088d759ceed9.png) if:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/ec7ce5865031a7580f2590d4162f88b9.png)'
  prefs: []
  type: TYPE_IMG
- en: with ![Equation](../Images/ef17aed938e5576b0a5e042246b4ac47.png) (pronounced
    “lambda”) being the eigenvalue corresponding to the eigenvector ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png).
  prefs: []
  type: TYPE_NORMAL
- en: '**Eigenvectors**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Eigenvectors of a matrix are nonzero vectors that are only rescaled when the
    matrix is applied to them. If the scaling factor is positive, the directions of
    the initial and the transformed vectors are the same, if it is negative, their
    directions are reversed.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Number of eigenvectors**'
  prefs: []
  type: TYPE_NORMAL
- en: 'An ![Equation](../Images/9a60fefb21d8c288f3b817f86bef589d.png)-by-![Equation](../Images/9a60fefb21d8c288f3b817f86bef589d.png) matrix
    has, at most, ![Equation](../Images/9a60fefb21d8c288f3b817f86bef589d.png) linearly
    independent eigenvectors. However, each eigenvector multiplied by a nonzero scalar
    is also an eigenvector. If you have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/5184f65ed2fd4fc2c8ca10dfc0147dcd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/450cc7ad9f7b7c3b13e31d23d61da926.png)'
  prefs: []
  type: TYPE_IMG
- en: with ![Equation](../Images/d571d7685ad3d9f4acf544844e771d21.png) any nonzero
    value.
  prefs: []
  type: TYPE_NORMAL
- en: This excludes the zero vector as eigenvector, since you would have
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation](../Images/31c9d00058c36ce6f668e66d7d1179a5.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, every scalar would be an eigenvalue and thus would be undefined.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hands-On Project: Principal Component Analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Principal Component Analysis*, or PCA, is an algorithm that you can use to
    reduce the dimensionality of a dataset. It is useful, for instance, to reduce
    computation time, compress data, or avoid what is called *the curse of dimensionality*.
    It is also useful for visualization purposes: high dimensional data is hard to
    visualize and it can be useful to decrease the number of dimensions to plot your
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: In this hands-on project, you’ll use various concepts that you can learn in
    the book [Essential Math for Data Science](https://www.essentialmathfordatascience.com/?utm_source=hadrienj&utm_medium=blog&utm_campaign=hadrienj_2021-02-01-Essential-Math-for-Data-Science-Eigendecomposition.md),
    as change of basis (Sections 7.5 and 9.2, some samples [here](https://hadrienj.github.io/posts/Essential-Math-for-Data-Science-Change-of-Basis/)),
    eigendecomposition (Chapter 9) or covariance matrices (Section 2.1.3) to understand
    how PCA is working.
  prefs: []
  type: TYPE_NORMAL
- en: In the first part, you’ll learn about the relationship between projections,
    explained variance and error minimization, first with a bit of theory, and then
    by coding a PCA on the beer dataset (consumption of beer as a function of temperature).
    Note that you’ll also find another example in [Essential Math for Data Science](https://www.essentialmathfordatascience.com/?utm_source=hadrienj&utm_medium=blog&utm_campaign=hadrienj_2021-02-01-Essential-Math-for-Data-Science-Eigendecomposition.md) where
    you’ll use Sklearn to use PCA on audio data to visualize audio samples according
    to their category, and then to compress these audio samples.
  prefs: []
  type: TYPE_NORMAL
- en: Under the Hood
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Theoretical context
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal of PCA is to project data onto a lower dimensional space while keeping
    as much of the information contained in the data as possible. The problem can
    be seen as a *perpendicular least squares* problem also called *orthogonal regression*.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll see here that the error of the orthogonal projections is minimized when
    the projection line corresponds to the direction where the variance of the data
    is maximal.
  prefs: []
  type: TYPE_NORMAL
- en: Variance and Projections
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is first important to understand that, when the features of your dataset
    are not completely uncorrelated, some directions are associated with a larger
    variance than others.
  prefs: []
  type: TYPE_NORMAL
- en: '![Essential Math for Data Science: Eigenvectors and Application to PCA](../Images/8d12f3afe57864e2400e68d5d2be0c74.png)>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: The variance of the data in the direction of the vector ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) (red)
    is larger than in the direction of the vector ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png) (green).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Projecting data to a lower-dimensional space means that you might lose some
    information. In Figure 3, if you project two-dimensional data onto a line, the
    variance of the projected data tells you how much information you lose. For instance,
    if the variance of the projected data is near zero, it means that the data points
    will be projected to very close positions: you lose a lot of information.'
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, the goal of the PCA is to change the basis of the data matrix
    such that the direction with the maximum variance (![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) in
    Figure 3) becomes the first *principal component*. The second component is the
    direction with the maximum variance which is orthogonal to the first one, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: When you have found the components of the PCA, you change the basis of your
    data such that the components are the new basis vectors. This transformed dataset
    has new features, which are the components and which are linear combinations of
    the initial features. Reducing the dimensionality is done by selecting some of
    the components only.
  prefs: []
  type: TYPE_NORMAL
- en: '![Essential Math for Data Science: Eigenvectors and Application to PCA](../Images/bf56f430ef597ae9d416efcb74b7a827.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Change of basis such that the maximum variance is in the ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png)-axis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an illustration, Figure 4 shows the data after a change of basis: the maximum
    variance is now associated with the ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png)-axis.
    You can for instance keep only this first dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, expressing the PCA in terms of change of basis, its goal is
    to find a new basis (which is a linear combination of the initial basis) in which
    the variance of the data is maximized along the first dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing the Error
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finding the directions that maximize the variance is similar as minimizing the
    error between the data and its projection.
  prefs: []
  type: TYPE_NORMAL
- en: '![Essential Math for Data Science: Eigenvectors and Application to PCA](../Images/0a186ddc1f3ae7e09e131edc60cc8444.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The direction that maximizes the variance is also the one associated
    with the smallest error (represented in gray).'
  prefs: []
  type: TYPE_NORMAL
- en: You can see in Figure 5 that lower errors are shown in the left figure. Since
    projections are orthogonal, the variance associated to the direction of the line
    on which you project doesn’t impact the error.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the Best Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After changing the basis of the dataset, you should have a covariance between
    features close to zero (as for instance in Figure 4). In other terms, you want
    that the transformed dataset has a diagonal covariance matrix: the covariance
    between each pair of principal components is equal to zero.'
  prefs: []
  type: TYPE_NORMAL
- en: You can see in Chapter 9 of [Essential Math for Data Science](https://www.essentialmathfordatascience.com/?utm_source=hadrienj&utm_medium=blog&utm_campaign=hadrienj_2021-02-01-Essential-Math-for-Data-Science-Eigendecomposition.md),
    that you can use eigendecomposition to diagonalize a matrix (make the matrix diagonal).
    Thus, you can calculate the eigenvectors of the covariance matrix of the dataset.
    They will give you the directions of the new basis in which the covariance matrix
    is diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the principal components are calculated as the eigenvectors of
    the covariance matrix of the dataset. In addition, the eigenvalues give you the
    explained variance of the corresponding eigenvector. Thus, by sorting the eigenvectors
    in the decreasing order according to their eigenvalues, you can sort the principal
    components by importance order, and eventually remove the ones associated with
    a small variance.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the PCA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s illustrate how PCA is working with the beer dataset showing the beer consumption
    and the temperature in São Paulo, Brazil for the year 2015.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s load the data and plot the consumption as a function of the temperature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Essential Math for Data Science: Eigenvectors and Application to PCA](../Images/768a9d226e65a2449abd32e1520240cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Consumption of beer as a function of temperature.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s create the data matrix ![Equation](../Images/fad5207092e118bbda7de8e70211a6f4.png) with
    the two variables: temperatures and consumption.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The matrix ![Equation](../Images/fad5207092e118bbda7de8e70211a6f4.png) has 365
    rows and two columns (the two variables).
  prefs: []
  type: TYPE_NORMAL
- en: '**Eigendecomposition of the Covariance Matrix**'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you saw, the first step is to compute the covariance matrix of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that you can read it as follows: the diagonal values are respectively
    the variances of the first and the second variable. The covariance between the
    two variables is around 12.2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you will calculate the eigenvectors and eigenvalues of this covariance
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can store the eigenvectors as two vectors ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) and ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Let’s plot the eigenvectors with the data (note that you should use centered
    data because it is the data used to calculate the covariance matrix).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can scale the eigenvectors by their corresponding eigenvalues, which is
    the explained variance. For visualization purpose, let’s use a vector length of
    three standard deviations (equal to three times the square root of the explained
    variance):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Essential Math for Data Science: Eigenvectors and Application to PCA](../Images/7e866c41618f3917728a398331a0f01a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The eigenvectors ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) (in
    gray) and ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png) (in red)
    scaled according to the explained variance.'
  prefs: []
  type: TYPE_NORMAL
- en: You can see in Figure 7 that the eigenvectors of the covariance matrix give
    you the important directions of the data. The vector ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png) in
    red is associated with the largest eigenvalue and thus corresponds to the direction
    with the largest variance. The vector ![Equation](../Images/a1bb06b27d7ad44981ed7b8aae646595.png) in
    gray is orthogonal to ![Equation](../Images/97ef2f6485b158f4ab2fca48fc9cc55e.png) and
    is the second principal component.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, you just need to change the basis of the data using the eigenvectors
    as the new basis vectors. But first, you can sort the eigenvectors with respect
    to the eigenvalues in decreasing order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that your eigenvectors are sorted, let’s change the basis of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You can plot the transformed data to check that the principal components are
    now uncorrelated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![Essential Math for Data Science: Eigenvectors and Application to PCA](../Images/f9725bfa0d344f604d5cc1d8dc8003fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The dataset in the new basis.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8 shows the data samples in the new basis. You can see that the first
    dimension (the ![Equation](../Images/18d793480e7f7fb6e486026b18729633.png)-axis)
    corresponds to the direction with the largest variance.
  prefs: []
  type: TYPE_NORMAL
- en: You can keep only the first component of the data in this new basis without
    losing too much information.
  prefs: []
  type: TYPE_NORMAL
- en: '**Covariance matrix or Singular Value Decomposition?**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: One caveat of using the covariance matrix to calculate the PCA is that it can
    be hard to compute when there are many features (as with audio data, like in the
    second part of this hands-on). For this reason, it is usually preferred to use
    the Singular Value Decomposition (SVD) to calculate the PCA.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**[Hadrien Jean](https://hadrienj.github.io/)** is a machine learning scientist.
    He owns a Ph.D in cognitive science from the Ecole Normale Superieure, Paris,
    where he did research on auditory perception using behavioral and electrophysiological
    data. He previously worked in industry where he built deep learning pipelines
    for speech processing. At the corner of data science and environment, he works
    on projects about biodiversity assessement using deep learning applied to audio
    recordings. He also periodically creates content and teaches at Le Wagon (data
    science Bootcamp), and writes articles in his blog ([hadrienj.github.io](http://hadrienj.github.io)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://hadrienj.github.io/posts/Essential-Math-for-Data-Science-Eigendecomposition/).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[How To Overcome The Fear of Math and Learn Math For Data Science](https://www.kdnuggets.com/2021/03/overcome-fear-learn-math-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Principal Component Analysis (PCA) with Scikit-Learn](https://www.kdnuggets.com/2023/05/principal-component-analysis-pca-scikitlearn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Essential Math for Data Science: Visual Introduction to Singular…](https://www.kdnuggets.com/2022/06/essential-math-data-science-visual-introduction-singular-value-decomposition.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a Geospatial Application in Python with Google Earth…](https://www.kdnuggets.com/2022/03/building-geospatial-application-python-google-earth-engine-greppo.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Creating a Web Application to Extract Topics from Audio with Python](https://www.kdnuggets.com/2023/01/creating-web-application-extract-topics-audio-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8 Ways to Improve Your Search Application this Week](https://www.kdnuggets.com/2022/09/corise-8-ways-improve-search-application-week.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
