- en: 'Decision Tree Pruning: The Hows and Whys'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/09/decision-tree-pruning-hows-whys.html](https://www.kdnuggets.com/2022/09/decision-tree-pruning-hows-whys.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Decision Tree Pruning: The Hows and Whys](../Images/ad5cf415445386976861e940e07248ed.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Devin H](https://unsplash.com/@devin_photography?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    via Unsplash'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s recap Decision Trees.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees are a non-parametric supervised learning method that can be used
    for classification and regression tasks. The goal is to build a model that can
    make predictions on the value of a target variable by learning simple decision
    rules inferred from the data features.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees are made up of
  prefs: []
  type: TYPE_NORMAL
- en: Root Node - the very top of the decision tree and is the ultimate decision you’re
    trying to make.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Internal Nodes - this branches off from the Root Node, and represent different
    options
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Leaf Nodes - these are attached at the end of the branches and represent possible
    outcomes for each action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Just like any other machine learning algorithm, the most annoying thing that
    can happen is overfitting. And Decision Trees are one of the machine learning
    algorithms that are susceptible to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting is when a model completely fits the training data and struggles
    or fails to generalize the testing data. This happens when the model memorizes
    noise in the training data and fails to pick up essential patterns which can help
    them with the test data.
  prefs: []
  type: TYPE_NORMAL
- en: One of the techniques you can use to reduce overfitting in Decision Trees is
    Pruning.
  prefs: []
  type: TYPE_NORMAL
- en: What is Decision Tree Pruning and Why is it Important?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pruning is a technique that removes the parts of the Decision Tree which prevent
    it from growing to its full depth. The parts that it removes from the tree are
    the parts that do not provide the power to classify instances. A Decision tree
    that is trained to its full depth will highly likely lead to overfitting the training
    data - therefore Pruning is important.
  prefs: []
  type: TYPE_NORMAL
- en: In simpler terms, the aim of Decision Tree Pruning is to construct an algorithm
    that will perform worse on training data but will generalize better on test data.
    Tuning the hyperparameters of your Decision Tree model can do your model a lot
    of justice and save you a lot of time and money.
  prefs: []
  type: TYPE_NORMAL
- en: How do you Prune a Decision Tree?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two types of pruning: Pre-pruning and Post-pruning. I will go through
    both of them and how they work.'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pre-pruning technique of Decision Trees is tuning the hyperparameters prior
    to the training pipeline. It involves the heuristic known as ‘early stopping’
    which stops the growth of the decision tree - preventing it from reaching its
    full depth.
  prefs: []
  type: TYPE_NORMAL
- en: It stops the tree-building process to avoid producing leaves with small samples.
    During each stage of the splitting of the tree, the cross-validation error will
    be monitored. If the value of the error does not decrease anymore - then we stop
    the growth of the decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hyperparameters that can be tuned for early stopping and preventing overfitting
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth`, `min_samples_leaf`, and `min_samples_split`'
  prefs: []
  type: TYPE_NORMAL
- en: These same parameters can also be used to tune to get a robust model. However,
    you should be cautious as early stopping can also lead to underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Post-pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Post-pruning does the opposite of pre-pruning and allows the Decision Tree model
    to grow to its full depth. Once the model grows to its full depth, tree branches
    are removed to prevent the model from overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm will continue to partition data into smaller subsets until the
    final subsets produced are similar in terms of the outcome variable. The final
    subset of the tree will consist of only a few data points allowing the tree to
    have learned the data to the T. However, when a new data point is introduced that
    differs from the learned data - it may not get predicted well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hyperparameter that can be tuned for post-pruning and preventing overfitting
    is: `ccp_alpha`'
  prefs: []
  type: TYPE_NORMAL
- en: '`ccp` stands for Cost Complexity Pruning and can be used as another option
    to control the size of a tree. A higher value of `ccp_alpha` will lead to an increase
    in the number of nodes pruned.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cost complexity pruning (post-pruning) steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Train your Decision Tree model to its full depth
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the ccp_alphas value using `cost_complexity_pruning_path()`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train your Decision Tree model with different `ccp_alphas` values and compute
    train and test performance scores
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the train and test scores for each value of `ccp_alphas` values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This hyperparameter can also be used to tune to get the best fit models.
  prefs: []
  type: TYPE_NORMAL
- en: Tips
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some tips you can apply when Decision Tree Pruning:'
  prefs: []
  type: TYPE_NORMAL
- en: If the node gets very small, do not continue to split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimum error (cross-validation) pruning without early stopping is a good technique
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a full-depth tree and work backward by applying a statistical test during
    each stage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prune an interior node and raise the sub-tree beneath it up one level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I have gone over the two types of pruning techniques and their
    uses. Decision Trees are highly susceptible to overfitting - therefore pruning
    is a vital phase for the algorithm. If you would like to know more about post-pruning
    decision trees with cost complexity pruning, click on this [link](https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#:~:text=Cost%20complexity%20pruning%20provides%20another,the%20number%20of%20nodes%20pruned.).
  prefs: []
  type: TYPE_NORMAL
- en: '**[Nisha Arya](https://www.linkedin.com/in/nisha-arya-ahmed/)** is a Data Scientist
    and Freelance Technical Writer. She is particularly interested in providing Data
    Science career advice or tutorials and theory based knowledge around Data Science.
    She also wishes to explore the different ways Artificial Intelligence is/can benefit
    the longevity of human life. A keen learner, seeking to broaden her tech knowledge
    and writing skills, whilst helping guide others.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Simplifying Decision Tree Interpretability with Python & Scikit-learn](https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Decision Tree Algorithm, Explained](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding by Implementing: Decision Tree](https://www.kdnuggets.com/2023/02/understanding-implementing-decision-tree.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Telling a Great Data Story: A Visualization Decision Tree](https://www.kdnuggets.com/2021/02/telling-great-data-story-visualization-decision-tree.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Random Forest vs Decision Tree: Key Differences](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets™ News 22:n09, Mar 2: Telling a Great Data Story: A…](https://www.kdnuggets.com/2022/n09.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
