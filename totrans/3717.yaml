- en: Getting Started with Automated Text Summarization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始自动文本摘要
- en: 原文：[https://www.kdnuggets.com/2019/11/getting-started-automated-text-summarization.html](https://www.kdnuggets.com/2019/11/getting-started-automated-text-summarization.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/11/getting-started-automated-text-summarization.html](https://www.kdnuggets.com/2019/11/getting-started-automated-text-summarization.html)
- en: '![Figure](../Images/ab8265240781f013c50e69641c7c4132.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/ab8265240781f013c50e69641c7c4132.png)'
- en: 'Source: [SFL Scientific](https://sflscientific.com/data-science-blog/2016/11/17/text-summarization-in-natural-language-processing)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[SFL Scientific](https://sflscientific.com/data-science-blog/2016/11/17/text-summarization-in-natural-language-processing)
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在 IT 领域'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Automated text summarization](https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html)
    refers to performing the summarization of a document or documents using some form
    of heuristics or statistical methods. A summary in this case is a shortened piece
    of text which accurately captures and conveys the most important and relevant
    information contained in the document or documents we want summarized.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[自动文本摘要](https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html)是指使用某种形式的启发式或统计方法对文档或多个文档进行摘要。此处的摘要是指一个简化的文本，准确捕捉并传达我们想要摘要的文档中最重要和相关的信息。'
- en: 'There are 2 categories of summarization techniques: extractive and abstractive.
    We will focus on the use of extractive methods herein, which function by identifying
    the important sentences or excerpts from the text and reproducing them verbatim
    as part of the summary. No new text is generated; only existing text is used in
    the summarization process. This differs from abstractive methods, which employ
    more powerful natural language processing techniques to interpret text and generate
    new summary text.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要技术有两种分类：抽取式和生成式。我们将在此重点讨论抽取式方法，它通过识别文本中重要的句子或摘录并逐字复述这些内容作为摘要。不会生成新的文本；仅使用现有文本进行摘要。这与生成式方法不同，生成式方法使用更强大的自然语言处理技术来解释文本并生成新的摘要文本。
- en: This article will walk through an extractive summarization process, using a
    simple word frequency approach, implemented in Python. Before we begin, note that
    we are not spending much energy on data preprocessing, tokenization, normalization,
    etc. in this article ([similar to last time](/2019/11/create-vocabulary-nlp-tasks-python.html)),
    nor are we introducing any libraries which are able to easily and effectively
    perform these tasks. I want to focus on presenting the text summarization steps,
    mostly glossing over other important concepts. I am planning a number of follow-ups
    to this piece, and we will add increasing complexity to our NLP tasks as we go.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章将演示一个抽取式摘要过程，使用简单的词频方法，在 Python 中实现。在开始之前，请注意我们不会在这篇文章中花费太多精力进行数据预处理、分词、标准化等操作（[类似于上次](/2019/11/create-vocabulary-nlp-tasks-python.html)），也不会介绍任何能够轻松有效执行这些任务的库。我希望重点介绍文本摘要的步骤，略过其他重要的概念。我计划在这篇文章后续进行更多的跟进，并在过程中逐步增加我们自然语言处理任务的复杂性。
- en: Also, and for example, since we are doing some minimal tokenization here, out
    of necessity, you will get a feel for when it is being performed, and doing so
    more effectively can optionally be left as an exercise for the reader.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，例如，由于我们在这里进行了一些最小化的分词，你将会体会到何时进行分词，且更有效的处理方法可以作为读者的练习。
- en: 'Let''s be clear about what we are going to do here:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们明确一下我们要做什么：
- en: Take textual input (a short news article)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理文本输入（如一篇简短的新闻文章）
- en: Perform minimal text preprocessing
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行最小化的文本预处理
- en: Create a data representation
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建数据表示
- en: Perform summarization using this data representation
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这种数据表示法执行摘要
- en: There are a number of ways of performing text summarization, as noted above,
    and we will be using a very basic extractive method to do so which is based on
    word frequencies within the given article.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种文本摘要方法，如上所述，我们将使用一种非常基础的提取方法，这种方法基于给定文章中的词频。
- en: 'As we are not leaning on libraries for almost anything, our imports are few:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们几乎不依赖库，我们的导入语句很少：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We need the `punctuation` and `stop_words` modules in order to identify these
    when we are scoring our words and, ultimately, sentences for their perceived importance,
    and we will deem neither punctuation nor stop words "important" for this task.
    Why so? As opposed to a language modeling task where these would unquestionably
    be useful, or perhaps a text classification task, it should be obvious that including
    frequently occurring stop words or repetitive punctuation would lead to biasing
    towards these tokens, providing no benefit to us. There are all sorts of reasons
    why we would want to **not** exclude stop words (their arbitrary removal should
    be avoided), but this does not seem to be one of them.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要`punctuation`和`stop_words`模块来识别这些元素，以便在对单词和句子进行评分时确定它们的相对重要性。对于这个任务，我们不会认为标点符号或停用词“重要”。为什么？与语言建模任务相比，其中这些元素无疑是有用的，或者与文本分类任务相比，这些元素可能会有用，但很明显，包含频繁出现的停用词或重复的标点符号会导致偏向这些标记，对我们没有益处。我们有各种理由希望**不**排除停用词（任意删除它们应当避免），但这似乎不是其中之一。
- en: 'Next, we need some text to test our summarization technique on. I manually
    copied and pasted this one from CNN, but feel free to find your own:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一些文本来测试我们的摘要技术。我手动从CNN复制并粘贴了这篇文章，但你可以随意寻找自己的：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Did I say we weren''t tokenizing? Well, we are. Poorly. But let''s not focus
    on that right now. We will need 2 simple tokenizing functions: one for tokenizing
    sentences into words, and another for tokenizing documents into sentences:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我说过我们没有进行分词吗？实际上，我们确实进行了分词，只是做得很差。但现在让我们不再关注这个问题。我们需要两个简单的分词函数：一个用于将句子分词成单词，另一个用于将文档分词成句子：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We need individual words in order to determine their relative frequency in the
    document, and assign a corresponding score; we need individual sentences to subsequently
    sum the scores of each word within in order to determine sentence "importance."
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要单个单词来确定它们在文档中的相对频率，并分配相应的分数；我们需要单个句子来随后汇总每个单词的分数，以确定句子的“重要性”。
- en: Note the we are using "importance" here as a synonym for the relative word frequency
    in the document; we will divide the number of occurrences of each word by the
    number of occurrences of the word which occurs most in the document. Does such
    high frequency equal genuine *importance*? It is naive to assume that it does,
    but it's also the simplest way to introduce the concept of text summarization.
    Interested in challenging our assumption of "importance" here? Try something like
    TF-IDF or word embeddings instead.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在这里使用“重要性”作为文档中相对词频的同义词；我们将每个单词的出现次数除以文档中出现频率最高的单词的出现次数。这种高频等于真正的*重要性*吗？假设它等于重要性是幼稚的，但这也是引入文本摘要概念的最简单方法。对我们这里的“重要性”假设感兴趣吗？可以尝试像TF-IDF或词嵌入这样的东西。
- en: 'Okay, let''s tokenize:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们开始分词吧：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Don't look too closely if you are following along at home, or else you will
    see where our simple tokenization approach fails. Moving on...
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在家跟着做，最好不要看得太仔细，否则你会看到我们简单的分词方法哪里失败了。继续...
- en: Now we need to count the occurrences of each word in the document.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要计算文档中每个单词的出现次数。
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Our poor tokenizing shows up again in the final token above. In the next article,
    I'll show you replacement tokenizers you can drop in place to help with this.
    Why not do this from the start? As I said, I want to focus on the text summarization
    steps.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们糟糕的分词再次出现在上面的最终标记中。在下一篇文章中，我会向你展示一些可以替换的分词工具，以帮助解决这个问题。为什么不从一开始就这样做呢？正如我所说，我想专注于文本摘要的步骤。
- en: 'Now that we have our word counts, we can build a word frequency distribution:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了单词计数，我们可以构建一个单词频率分布：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And there we go: we divided the occurrence of each word by the frequency of
    the most occurring word to get our distribution.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样：我们将每个单词的出现次数除以最常出现单词的频率，以获得我们的分布。
- en: Next we want to score our sentences by using the frequency distribution we generated.
    This is simply summing up the scores of each word in a sentence and hanging on
    to the score. Our function takes a `max_len` argument which sets a maximum length
    to sentences which are to be considered for use in the summarization. It should
    be relatively easy to see that, given the way we are scoring our sentences, we
    could be biasing towards long sentences.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们要使用我们生成的频率分布来对句子进行评分。这只是简单地将每个单词在句子中的得分相加，并保留这个得分。我们的函数接受一个 `max_len` 参数，用于设置要考虑用于汇总的句子的最大长度。由于我们对句子的评分方式，很容易看出，我们可能会偏向长句子。
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that we have scored our sentences for their importance, all that's left
    to do is select (i.e. extract, as in "extractive summarization") the top *k* sentences
    to represent the summary of the article. This function will take the sentence
    scores we generated above as well as a value for the top *k* highest scoring sentences
    to sue for summarization. It will return a string summary of the concatenated
    top sentences, as well as the sentence scores of the sentences used in the summarization.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对句子的相对重要性进行了评分，剩下的就是选择（即“提取性汇总”中的“提取”）前 *k* 个句子来代表文章的总结。这个函数将使用我们上面生成的句子得分以及一个值来确定用于汇总的得分最高的
    *k* 个句子。它将返回一个由前句子连接成的字符串总结，以及用于汇总的句子得分。
- en: '[PRE11]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Let's use the function to generate the summary.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个函数生成总结。
- en: '[PRE12]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: And let's check out the summary sentence scores for good measure.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 并且我们来检查一下总结句子的得分，以确保准确性。
- en: '[PRE14]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The summary seems reasonable at a quick pass, given the text of the article.
    Try out this simple method on some other text for further evidence.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在快速浏览下，这个总结看起来很合理，考虑到文章的内容。可以在其他文本上尝试这个简单的方法以获取更多证据。
- en: 'The next summarization article will build on this simple method in a few key
    ways, namely:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 下一篇总结文章将在几个关键方面建立在这个简单的方法之上，即：
- en: proper tokenization approaches
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合适的分词方法
- en: improvement to our baseline approach, using TF-IDF weighting instead of simple
    word frequency
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对我们基线方法的改进，使用 TF-IDF 权重而不是简单的词频
- en: use of an actual dataset for our summarization
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用实际数据集进行汇总
- en: evaluation of our results
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们结果的评估
- en: See you next time.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 下次见。
- en: '[**Matthew Mayo**](https://www.linkedin.com/in/mattmayo13/) ([**@mattmayo13**](https://twitter.com/mattmayo13))
    is a Data Scientist and the Editor-in-Chief of KDnuggets, the seminal online Data
    Science and Machine Learning resource. His interests lie in natural language processing,
    algorithm design and optimization, unsupervised learning, neural networks, and
    automated approaches to machine learning. Matthew holds a Master''s degree in
    computer science and a graduate diploma in data mining. He can be reached at editor1
    at kdnuggets[dot]com.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Matthew Mayo**](https://www.linkedin.com/in/mattmayo13/) ([**@mattmayo13**](https://twitter.com/mattmayo13))
    是一名数据科学家及 KDnuggets 的主编，KDnuggets 是开创性的在线数据科学和机器学习资源。他的兴趣领域包括自然语言处理、算法设计与优化、无监督学习、神经网络以及机器学习的自动化方法。Matthew
    拥有计算机科学硕士学位和数据挖掘研究生文凭。他可以通过 editor1 at kdnuggets[dot]com 联系到。'
- en: More On This Topic
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Approaches to Text Summarization: An Overview](https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本总结方法概述](https://www.kdnuggets.com/2019/01/approaches-text-summarization-overview.html)'
- en: '[Text Summarization Development: A Python Tutorial with GPT-3.5](https://www.kdnuggets.com/2023/04/text-summarization-development-python-tutorial-gpt35.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[文本总结开发：带有 GPT-3.5 的 Python 教程](https://www.kdnuggets.com/2023/04/text-summarization-development-python-tutorial-gpt35.html)'
- en: '[Summarization with GPT-3](https://www.kdnuggets.com/2022/04/packt-summarization-gpt3.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 GPT-3 的总结](https://www.kdnuggets.com/2022/04/packt-summarization-gpt3.html)'
- en: '[Unlocking GPT-4 Summarization with Chain of Density Prompting](https://www.kdnuggets.com/unlocking-gpt-4-summarization-with-chain-of-density-prompting)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[利用链式密度提示解锁 GPT-4 总结功能](https://www.kdnuggets.com/unlocking-gpt-4-summarization-with-chain-of-density-prompting)'
- en: '[Extractive Summarization with LLM using BERT](https://www.kdnuggets.com/extractive-summarization-with-llm-using-bert)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 BERT 的提取性总结](https://www.kdnuggets.com/extractive-summarization-with-llm-using-bert)'
- en: '[How to Use the pivot_table Function for Advanced Data Summarization…](https://www.kdnuggets.com/how-to-use-the-pivot_table-function-for-advanced-data-summarization-in-pandas)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何使用 pivot_table 函数进行高级数据汇总…](https://www.kdnuggets.com/how-to-use-the-pivot_table-function-for-advanced-data-summarization-in-pandas)'
