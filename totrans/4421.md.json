["```py\nimport numpy as np\nfrom matplotlib import pyplot\n\n#creating our data\nX = np.random.rand(10,1)\ny = np.random.rand(10,1)\nm = len(y)\ntheta = np.ones(1)\n\n#applying gradient descent\na = 0.0005\ncost_list = []\nfor i in range(len(y)):\n\n    theta = theta - a*(1/m)*np.transpose(X)@(X@theta - y)\n\n    cost_val = (1/m)*np.transpose(X)@(X@theta - y)\n    cost_list.append(cost_val)\n\n#Predicting our Hypothesis\nb = theta\nyhat = X.dot(b)\n\n#Plotting our results\npyplot.scatter(X, y, color='red')\npyplot.plot(X, yhat, color='blue')\npyplot.show()\n\n```", "```py\nimport numpy as np\nfrom matplotlib import pyplot\n\n#creating our data\nX = np.random.rand(10,1)\ny = np.random.rand(10,1)\n\n#Computing coefficient\nb = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n\n#Predicting our Hypothesis\nyhat = X.dot(b)\n#Plotting our results\npyplot.scatter(X, y, color='red')\npyplot.plot(X, yhat, color='blue')\npyplot.show()\n\n```", "```py\n#Creating the Dummy Data set and importing libraries\nimport math\nimport seaborn as sns\nimport numpy as np \nfrom scipy import stats\nfrom matplotlib import pyplot\nx = np.random.normal(0,1,size=(100,1))\ny = np.random.random(size=(100,1))\n\n```", "```py\nprint(\"Intercept is \" ,stats.mstats.linregress(x,y).intercept)\nprint(\"Slope is \", stats.mstats.linregress(x,y).slope)\n\n```", "```py\npyplot.figure(figsize=(15,8))\nsns.regplot(x,y)\npyplot.show()\n\n```", "```py\nh = lambda theta_0, theta_1, x: theta_0 + np.dot(x,theta_1) #equation of straight lines\n\n# the cost function (for the whole batch. for comparison later)\ndef J(x, y, theta_0, theta_1):\n    m = len(x)\n    returnValue = 0\n    for i in range(m):\n        returnValue += (h(theta_0, theta_1, x[i]) - y[i])**2\n    returnValue = returnValue/(2*m)\n    return returnValue\n\n# finding the gradient per each training example\ndef grad_J(x, y, theta_0, theta_1):\n    returnValue = np.array([0., 0.])\n    returnValue[0] += (h(theta_0, theta_1, x) - y)\n    returnValue[1] += (h(theta_0, theta_1, x) - y)*x\n    return returnValue\n\nclass AdamOptimizer:\n    def __init__(self, weights, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n        self.alpha = alpha\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.m = 0\n        self.v = 0\n        self.t = 0\n        self.theta = weights\n\n    def backward_pass(self, gradient):\n        self.t = self.t + 1\n        self.m = self.beta1*self.m + (1 - self.beta1)*gradient\n        self.v = self.beta2*self.v + (1 - self.beta2)*(gradient**2)\n        m_hat = self.m/(1 - self.beta1**self.t)\n        v_hat = self.v/(1 - self.beta2**self.t)\n        self.theta = self.theta - self.alpha*(m_hat/(np.sqrt(v_hat) - self.epsilon))\n        return self.theta\n\n```", "```py\nepochs = 1500\nprint_interval = 100\nm = len(x)\ninitial_theta = np.array([0., 0.]) # initial value of theta, before gradient descent\ninitial_cost = J(x, y, initial_theta[0], initial_theta[1])\n\ntheta = initial_theta\nadam_optimizer = AdamOptimizer(theta, alpha=0.001)\nadam_history = [] # to plot out path of descent\nadam_history.append(dict({'theta': theta, 'cost': initial_cost})#to check theta and cost function\n\n```", "```py\nfor j in range(epochs):\n    for i in range(m):\n        gradients = grad_J(x[i], y[i], theta[0], theta[1])\n        theta = adam_optimizer.backward_pass(gradients)\n\n    if ((j+1)%print_interval == 0 or j==0):\n        cost = J(x, y, theta[0], theta[1])\n        print ('After {} epochs, Cost = {}, theta = {}'.format(j+1, cost, theta))\n        adam_history.append(dict({'theta': theta, 'cost': cost}))\n\nprint ('\\nFinal theta = {}'.format(theta))\n\n```", "```py\nb = theta\nyhat = b[0] + x.dot(b[1])\npyplot.figure(figsize=(15,8))\npyplot.scatter(x, y, color='red')\npyplot.plot(x, yhat, color='blue')\npyplot.show()\n\n```", "```py\nimport numpy as np\nfrom matplotlib import pyplot\n\n#Creating our data\nX = np.random.rand(10,1)\ny = np.random.rand(10,1)\n\n#Computing coefficient\nb = np.linalg.pinv(X).dot(y)\n\n#Predicting our Hypothesis\nyhat = X.dot(b)\n\n#Plotting our results\npyplot.scatter(X, y, color='red')\npyplot.plot(X, yhat, color='blue')\npyplot.show()\n\n```"]