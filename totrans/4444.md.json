["```py\nimport torch\nfrom torch import nn\n\nclass Model(nn.Module):\n    def __init__(self, dataset):\n        super(Model, self).__init__()\n        self.lstm_size = 128\n        self.embedding_dim = 128\n        self.num_layers = 3\n\n        n_vocab = len(dataset.uniq_words)\n        self.embedding = nn.Embedding(\n            num_embeddings=n_vocab,\n            embedding_dim=self.embedding_dim,\n        )\n        self.lstm = nn.LSTM(\n            input_size=self.lstm_size,\n            hidden_size=self.lstm_size,\n            num_layers=self.num_layers,\n            dropout=0.2,\n        )\n        self.fc = nn.Linear(self.lstm_size, n_vocab)\n\n    def forward(self, x, prev_state):\n        embed = self.embedding(x)\n        output, state = self.lstm(embed, prev_state)\n        logits = self.fc(output)\n        return logits, state\n\n    def init_state(self, sequence_length):\n        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n                torch.zeros(self.num_layers, sequence_length, self.lstm_size))\n```", "```py\nID,Joke\n1,What did the bartender say to the jumper cables? You better not try to start anything.\n2,Don't you hate jokes about German sausage? They're the wurst!\n3,Two artists had an art contest... It ended in a draw\nâ€¦\n```", "```py\nimport torch\nimport pandas as pd\nfrom collections import Counter\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        args,\n    ):\n        self.args = args\n        self.words = self.load_words()\n        self.uniq_words = self.get_uniq_words()\n\n        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n\n        self.words_indexes = [self.word_to_index[w] for w in self.words]\n\n    def load_words(self):\n        train_df = pd.read_csv('data/reddit-cleanjokes.csv')\n        text = train_df['Joke'].str.cat(sep=' ')\n        return text.split(' ')\n\n    def get_uniq_words(self):\n        word_counts = Counter(self.words)\n        return sorted(word_counts, key=word_counts.get, reverse=True)\n\n    def __len__(self):\n        return len(self.words_indexes) - self.args.sequence_length\n\n    def __getitem__(self, index):\n        return (\n            torch.tensor(self.words_indexes[index:index+self.args.sequence_length]),\n            torch.tensor(self.words_indexes[index+1:index+self.args.sequence_length+1]),\n        )\n```", "```py\nimport argparse\nimport torch\nimport numpy as np\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom model import Model\nfrom dataset import Dataset\n\ndef train(dataset, model, args):\n    model.train()\n\n    dataloader = DataLoader(dataset, batch_size=args.batch_size)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    for epoch in range(args.max_epochs):\n        state_h, state_c = model.init_state(args.sequence_length)\n\n        for batch, (x, y) in enumerate(dataloader):\n            optimizer.zero_grad()\n\n            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n            loss = criterion(y_pred.transpose(1, 2), y)\n\n            state_h = state_h.detach()\n            state_c = state_c.detach()\n\n            loss.backward()\n            optimizer.step()\n\n            print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n```", "```py\ndef predict(dataset, model, text, next_words=100):\n    model.eval()\n\n    words = text.split(' ')\n    state_h, state_c = model.init_state(len(words))\n\n    for i in range(0, next_words):\n        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n\n        last_word_logits = y_pred[0][-1]\n        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n        word_index = np.random.choice(len(last_word_logits), p=p)\n        words.append(dataset.index_to_word[word_index])\n\n    return words\n```", "```py\nparser = argparse.ArgumentParser()\nparser.add_argument('--max-epochs', type=int, default=10)\nparser.add_argument('--batch-size', type=int, default=256)\nparser.add_argument('--sequence-length', type=int, default=4)\nargs = parser.parse_args()\n\ndataset = Dataset(args)\nmodel = Model(dataset)\n\ntrain(dataset, model, args)\nprint(predict(dataset, model, text='Knock knock. Whos there?'))\n```", "```py\n{'epoch': 9, 'batch': 91, 'loss': 5.953955173492432}\n{'epoch': 9, 'batch': 92, 'loss': 6.1532487869262695}\n{'epoch': 9, 'batch': 93, 'loss': 5.531163215637207}\n['Knock', 'knock.', 'Whos', 'there?', '3)', 'moostard', 'bird', 'Book,',\n'What', 'when', 'when', 'the', 'Autumn', 'He', 'What', 'did', 'the',\n'psychologist?', 'And', 'look', 'any', 'jokes.', 'Do', 'by', \"Valentine's\",\n'Because', 'I', 'papa', 'could', 'believe', 'had', 'a', 'call', 'decide',\n'elephants', 'it', 'my', 'eyes?', 'Why', 'you', 'different', 'know', 'in',\n'an', 'file', 'of', 'a', 'jungle?', 'Rock', '-', 'and', 'might', \"It's\",\n'every', 'out', 'say', 'when', 'to', 'an', 'ghost', 'however:', 'the', 'sex,',\n'in', 'his', 'hose', 'and', 'because', 'joke', 'the', 'month', '25', 'The',\n'97', 'can', 'eggs.', 'was', 'dead', 'joke', \"I'm\", 'a', 'want', 'is', 'you',\n'out', 'to', 'Sorry,', 'the', 'poet,', 'between', 'clean', 'Words', 'car',\n'his', 'wife', 'would', '1000', 'and', 'Santa', 'oh', 'diving', 'machine?',\n'He', 'was']\n```"]