- en: 'Machine Learning Explainability vs Interpretability: Two concepts that could
    help restore trust in AI'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习可解释性与可理解性：两个概念可能有助于恢复对 AI 的信任
- en: 原文：[https://www.kdnuggets.com/2018/12/machine-learning-explainability-interpretability-ai.html](https://www.kdnuggets.com/2018/12/machine-learning-explainability-interpretability-ai.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/12/machine-learning-explainability-interpretability-ai.html](https://www.kdnuggets.com/2018/12/machine-learning-explainability-interpretability-ai.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Richard Gall](https://www.linkedin.com/in/richard-gall-5a6a235a/?originalSubdomain=uk),
    Packt**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Richard Gall](https://www.linkedin.com/in/richard-gall-5a6a235a/?originalSubdomain=uk)，Packt**'
- en: '![](../Images/812d3cd84e6ad9f407ca723c00c24a22.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/812d3cd84e6ad9f407ca723c00c24a22.png)'
- en: It doesn’t take a data scientist to work out that the machine and deep learning
    algorithms built into automation and artificial intelligence systems lack transparency.
    It also doesn’t take a great deal of detective work to see that many of these
    systems contain an imprint of the unconscious biases of the engineers that helped
    to develop them.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要数据科学家来发现，内置于自动化和人工智能系统中的机器和深度学习算法缺乏透明性。也不需要太多侦探工作就能看到，这些系统中包含了开发这些系统的工程师无意识偏见的印记。
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Arguably, in the midst of what The Economist termed a [*techlash*](https://www.economist.com/international/2017/08/10/internet-firms-face-a-global-techlash),
    this lack of transparency has only (ironically) become more visible. While many
    of the incidents that have contributed towards the techlash are as much issues
    caused by a mixture of corporate self-interest and an alarming absence of governance
    and accountability, there’s no escaping the fact that the practice of data science
    and machine learning engineering naturally find their way hooked onto some of
    the year’s biggest business and politics stories.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，在《经济学人》所称的[*技术反弹*](https://www.economist.com/international/2017/08/10/internet-firms-face-a-global-techlash)的背景下，这种不透明性（具有讽刺意味地）变得更加明显。尽管许多导致技术反弹的事件既是公司自身利益与令人担忧的治理和问责缺失的混合问题，但不可否认的是，数据科学和机器学习工程的实践自然地与今年最大的一些商业和政治故事紧密相关。
- en: It’s in this context that the concepts of explainability and interpretability
    [have taken on new urgency](https://hub.packtpub.com/improve-interpretability-machine-learning-systems/).
    It’s likely that they are only going to become more important in 2019, as discussions
    around the ethics of artificial intelligence continues.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种背景下，可解释性和可理解性的概念[变得更加紧迫](https://hub.packtpub.com/improve-interpretability-machine-learning-systems/)。由于对人工智能伦理的讨论持续进行，它们在
    2019 年可能会变得更加重要。
- en: But what are explainability and interpretability? And how what do they actually
    mean for those of us doing data mining, analysis, science in 2019?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，什么是可解释性和可理解性？它们对我们这些从事数据挖掘、分析、科学的人在 2019 年究竟意味着什么？
- en: The difference between machine learning explainability and interpretability
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习可解释性与可理解性之间的区别
- en: In the context of machine learning and artificial intelligence, explainability
    and interpretability are often used interchangeably. While they are very closely
    related, it’s worth unpicking the differences, if only to see how complicated
    things can get once you start digging deeper into machine learning systems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和人工智能的背景下，可解释性和可理解性通常被交替使用。虽然它们非常密切相关，但值得探讨它们之间的区别，至少可以看到深入了解机器学习系统后事情可能会变得多么复杂。
- en: Interpretability is about the extent to which a cause and effect can be observed
    within a system. Or, to put it another way, it is the extent to which you are
    able to *predict* what is going to happen, given a change in input or algorithmic
    parameters. It’s being able to look at an algorithm and go *yep, I can see what’s
    happening here.*
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性是指在系统中观察因果关系的程度。换句话说，就是在输入或算法参数发生变化时，你能够*预测*会发生什么的程度。它就是能够看着一个算法，*嗯，我可以看到这里发生了什么。*
- en: 'Explainability, meanwhile, is the extent to which the internal mechanics of
    a machine or deep learning system can be explained in human terms. It’s easy to
    miss the subtle difference with interpretability, but consider it like this: interpretability
    is about being able to discern the mechanics without necessarily knowing why.
    Explainability is being able to quite literally explain what is happening.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，可解释性是指机器或深度学习系统的内部机制可以用人类术语解释的程度。理解这个细微的差别可能很困难，但可以这样考虑：可解释性是能够辨别机制而不一定知道原因。可解释性则是能够字面上解释发生了什么。
- en: 'Think of it this way: say you’re doing a science experiment at school. The
    experiment might be interpretable insofar as you can see what you’re doing, but
    it is only really explainable once you dig into the chemistry behind what you
    can see happening.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 可以这样理解：假设你在学校做科学实验。实验可能在你看到自己在做什么时是可以解释的，但只有当你深入了解你所看到的化学反应时，它才真正具备可解释性。
- en: That might be a little crude, but it is nevertheless a good starting point for
    thinking about how the two concepts relate to one another.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能有点粗略，但它仍然是思考这两个概念之间关系的一个很好的起点。
- en: Why are explainability and interpretability important in artificial intelligence
    and machine learning?
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么在人工智能和机器学习中，可解释性和可理解性如此重要？
- en: If 2018’s techlash has taught us anything, it’s that although technology can
    certainly be put to dubious usage, there are plenty of ways in which it can produce
    poor - discriminatory - outcomes with no intention of causing harm.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果2018年的技术反弹教会了我们什么，那就是尽管技术确实可以被用于可疑的用途，但它也有很多方式可能产生不良 - 歧视性 - 结果，而这些结果并非有意造成伤害。
- en: As domains like healthcare look to deploy artificial intelligence and deep learning
    systems, where questions of accountability and transparency are particularly important,
    if we’re unable to properly deliver improved interpretability, and ultimately
    explainability, in our algorithms, we’ll seriously be limiting the potential impact
    of artificial intelligence. Which would be a shame.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 随着医疗保健等领域希望部署人工智能和深度学习系统，其中问责制和透明度的问题尤其重要，如果我们无法在算法中妥善提升可解释性，最终实现可解释性，我们将严重限制人工智能的潜在影响。这将是非常可惜的。
- en: But aside from the legal and professional considerations that need to be made,
    there’s also an argument that improving interpretability and explainability are
    important even in more prosaic business scenarios. Understanding how an algorithm
    is actually working can help to better align the activities of data scientists
    and analysts with the [key questions and needs of their organization](https://hub.packtpub.com/ux-designers-can-teach-machine-learning-engineers-start-model-interpretability/).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 除了需要考虑的法律和专业问题外，还有一个论点认为，在更平凡的商业场景中，提高可解释性和可理解性同样重要。了解算法的实际工作方式可以帮助更好地将数据科学家和分析师的活动与[他们组织的关键问题和需求](https://hub.packtpub.com/ux-designers-can-teach-machine-learning-engineers-start-model-interpretability/)对齐。
- en: Techniques and methods for improving machine learning interpretability
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提高机器学习可解释性的方法和技巧
- en: While questions of transparency and ethics may feel abstract for the data scientist
    on the ground, there are, in fact, a number of practical things that can be done
    to improve an algorithm’s interpretability and explainability.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管透明性和伦理的问题对实际工作中的数据科学家来说可能显得抽象，但实际上有许多实用的方法可以用来提高算法的可解释性和可理解性。
- en: Algorithmic generalization
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 算法的泛化
- en: The first is to improve generalization. This sounds simple, but it isn’t that
    easy. When you think most machine learning engineering is applying algorithms
    in a very specific way to uncover a certain desired outcome, the model itself
    can feel like a secondary element - it’s simply a means to an end. However, by
    shifting this attitude to consider the overall health of the algorithm, and the
    data on which it is running, you can begin to set a solid foundation for improved
    interpretability.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第一点是提高泛化能力。这听起来简单，但实际上并不容易。当你认为大多数机器学习工程是在以非常具体的方式应用算法以揭示某个期望结果时，模型本身可能会显得是一个次要元素——它只是实现目标的手段。然而，通过转变这种态度，考虑算法的整体健康状况及其运行的数据，你可以开始为改善可解释性奠定坚实的基础。
- en: Pay attention to feature importance
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意特征的重要性
- en: This should be obvious, but it’s easily missed. Looking closely at the way the
    various features of your algorithm have been set is a practical way to actually
    engage with a diverse range of questions, from business alignment to ethics. Debate
    and discussion over how each feature should be set might be a little time-consuming,
    but having that tacit awareness that different features have been set in a certain
    way is nevertheless an important step in moving towards interpretability and explainability.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该是显而易见的，但很容易被忽视。仔细查看算法中各种特征的设置方式，是实际解决从业务对齐到伦理等各种问题的有效方法。对每个特征如何设置进行辩论和讨论可能会有些耗时，但尽管如此，意识到不同特征以某种方式被设置仍然是朝着可解释性和透明性迈出的重要一步。
- en: 'LIME: Local Interpretable Model-Agnostic Explanations'
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LIME：局部可解释模型无关解释
- en: While the techniques above offer practical steps that data scientists can take,
    LIME is an actual method developed by researchers to gain greater transparency
    on what’s happening inside an algorithm. The researchers explain that LIME can
    explain “the predictions of any classifier in an interpretable and faithful manner,
    by learning an interpretable model locally around the prediction.”
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上述技术提供了数据科学家可以采取的实际步骤，但LIME是由研究人员开发的一种实际方法，用于获得对算法内部发生的事情的更大透明度。研究人员解释说，LIME可以以“可解释且忠实的方式解释任何分类器的预测，通过在预测周围局部学习一个可解释的模型”。
- en: What this means in practice is that the LIME model develops an approximation
    of the model by testing it out to see what happens when certain aspects within
    the model are changed. Essentially it’s about trying to recreate the output from
    the same input through a process of experimentation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这意味着LIME模型通过测试来开发模型的近似值，以查看当模型中的某些方面发生变化时会发生什么。本质上，这就是通过实验过程尝试从相同的输入重现输出。
- en: DeepLIFT (Deep Learning Important Features)
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DeepLIFT（深度学习重要特征）
- en: 'DeepLIFT is a useful model in the particularly tricky area of deep learning.
    It works through a form of backpropagation: it takes the output, then attempts
    to pull it apart by ‘reading’ the various neurons that have gone into developing
    that original output.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: DeepLIFT是一个在深度学习领域特别棘手的情况中非常有用的模型。它通过一种反向传播形式工作：它获取输出，然后尝试通过“读取”形成该原始输出的各种神经元来分解它。
- en: Essentially, it’s a way of digging back into the feature selection inside of
    the algorithm (as the name indicates).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，这是一种深入挖掘算法内部特征选择的方法（正如名称所示）。
- en: Layer-wise relevance propagation
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 层级相关传播
- en: Layer-wise relevance propagation is similar to DeepLIFT, in that it works backwards
    from the output, identifying the most relevant neurons within the neural network
    until you return to the input (say, for example, an image). If you want to learn
    more about the mathematics behind the concept, [this post by Dan Shiebler is a
    great place to begin](http://danshiebler.com/2017-04-16-deep-taylor-lrp/).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 层级相关传播类似于DeepLIFT，它从输出向后工作，识别神经网络中的最相关神经元，直到你回到输入（例如，一张图片）。如果你想了解更多关于这一概念背后的数学知识，[Dan
    Shiebler的这篇文章是一个很好的起点](http://danshiebler.com/2017-04-16-deep-taylor-lrp/)。
- en: 'Adding complexity to tackle complexity: can it improve transparency?'
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 增加复杂性以应对复杂性：这能提高透明度吗？
- en: The central problem with both explainability and interpretability is that you’re
    adding an additional step in the development process. Indeed, you’re probably
    adding multiple steps. From one perspective, this looks like you’re trying to
    tackle complexity with even greater complexity.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性和透明性的核心问题在于，你在开发过程中增加了一个额外的步骤。实际上，你可能还添加了多个步骤。从某种角度看，这就像是试图用更大的复杂性来应对复杂性。
- en: And to a certain extent, this is true. What this means in practice is that if
    we’re to get really serious about interpretability and explainability, there needs
    to be a broader cultural change in the way in which data science and engineering
    is done, and how people *believe* it should be done.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种程度上，这确实是对的。实际上，这意味着如果我们要真正认真对待解释性和可解释性，就需要在数据科学和工程的方式，以及人们*相信*它应该如何进行的文化上，进行更广泛的变革。
- en: That, perhaps, is the really challenging part.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这或许才是最具挑战性的部分。
- en: '**Bio**: [Richard Gall](https://www.linkedin.com/in/richard-gall-5a6a235a/?originalSubdomain=uk)
    is the Editorial Content Product Manager at Packt. Through quality messaging,
    focused on the needs of the communities and customers Packt helps, Richard works
    to drive both direct and inbound revenues and improve the reach and relevance
    of the Packt brand within today''s tech landscape.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介**： [理查德·加尔](https://www.linkedin.com/in/richard-gall-5a6a235a/?originalSubdomain=uk)
    是 Packt 的编辑内容产品经理。通过优质的信息传递，专注于 Packt 帮助的社区和客户的需求，理查德致力于推动直接和入站收入的增长，并提高 Packt
    品牌在当今技术领域的影响力和相关性。'
- en: '**Resources:**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：**'
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在线和基于网络：分析、数据挖掘、数据科学、机器学习教育](https://www.kdnuggets.com/education/online.html)'
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分析、数据科学、数据挖掘和机器学习的软件](https://www.kdnuggets.com/software/index.html)'
- en: '**Related:**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Four Approaches to Explaining AI and Machine Learning](https://www.kdnuggets.com/2018/12/four-approaches-ai-machine-learning.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[解释 AI 和机器学习的四种方法](https://www.kdnuggets.com/2018/12/four-approaches-ai-machine-learning.html)'
- en: '[Interpretability is crucial for trusting AI and machine learning](https://www.kdnuggets.com/2018/11/interpretability-trust-ai-machine-learning.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[解释性对信任 AI 和机器学习至关重要](https://www.kdnuggets.com/2018/11/interpretability-trust-ai-machine-learning.html)'
- en: '[Using Uncertainty to Interpret your Model](https://www.kdnuggets.com/2018/11/using-uncertainty-interpret-model.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用不确定性来解释你的模型](https://www.kdnuggets.com/2018/11/using-uncertainty-interpret-model.html)'
- en: More On This Topic
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[A Comprehensive Survey on Trustworthy Graph Neural Networks:…](https://www.kdnuggets.com/2022/05/comprehensive-survey-trustworthy-graph-neural-networks-privacy-robustness-fairness-explainability.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于可信赖的图神经网络的全面调查：…](https://www.kdnuggets.com/2022/05/comprehensive-survey-trustworthy-graph-neural-networks-privacy-robustness-fairness-explainability.html)'
- en: '[How ML Model Explainability Accelerates the AI Adoption Journey for…](https://www.kdnuggets.com/2022/07/ml-model-explainability-accelerates-ai-adoption-journey-financial-services.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习模型解释性如何加速 AI 采用过程](https://www.kdnuggets.com/2022/07/ml-model-explainability-accelerates-ai-adoption-journey-financial-services.html)'
- en: '[7 Things You Didn''t Know You Could do with a Low Code Tool](https://www.kdnuggets.com/2022/09/7-things-didnt-know-could-low-code-tool.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[你不知道的低代码工具的 7 种用途](https://www.kdnuggets.com/2022/09/7-things-didnt-know-could-low-code-tool.html)'
- en: '[3 Mistakes That Could Be Affecting the Accuracy of Your Data Analytics](https://www.kdnuggets.com/2023/03/3-mistakes-could-affecting-accuracy-data-analytics.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可能影响数据分析准确性的 3 个错误](https://www.kdnuggets.com/2023/03/3-mistakes-could-affecting-accuracy-data-analytics.html)'
- en: '[Using SHAP Values for Model Interpretability in Machine Learning](https://www.kdnuggets.com/2023/08/shap-values-model-interpretability-machine-learning.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 SHAP 值进行机器学习模型解释性分析](https://www.kdnuggets.com/2023/08/shap-values-model-interpretability-machine-learning.html)'
- en: '[Simplifying Decision Tree Interpretability with Python & Scikit-learn](https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 和 Scikit-learn 简化决策树解释性](https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)'
