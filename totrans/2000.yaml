- en: Schedule & Run ETLs with Jupysql and GitHub Actions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/05/schedule-run-etls-jupysql-github-actions.html](https://www.kdnuggets.com/2023/05/schedule-run-etls-jupysql-github-actions.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Schedule & Run ETLs with Jupysql and GitHub Actions](../Images/4f0d1e0ad6d0308517fba43087bd543c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'In this blog you''ll achieve:'
  prefs: []
  type: TYPE_NORMAL
- en: Have a basic understanding of ETLs and JupySQL
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the public Penguins dataset and perform ETL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Schedule the ETL we've built on GitHub actions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this brief yet informative guide, we aim to provide you with a comprehensive
    understanding of the fundamental concepts of ETL (Extract, Transform, Load) and
    JupySQL, a flexible and versatile tool that allows for seamless SQL-based ETL
    from Jupyter.
  prefs: []
  type: TYPE_NORMAL
- en: Our primary focus will be on demonstrating how to effectively execute ETLs through
    JupySQL, the popular and powerful Python library designed for SQL interaction,
    while also highlighting the benefits of automating the ETL process through scheduling
    a full example ETL notebook via GitHub actions.
  prefs: []
  type: TYPE_NORMAL
- en: But first, what is an ETL?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let's dive into the details. `ETL` (Extract, Transform, Load) crucial process
    in data management that involves the extraction of data from various sources,
    the transformation of the extracted data into a usable format, and loading the
    transformed data into a target database or data warehouse. It is an essential
    process for data analysis, data science, data integration, and data migration,
    among other purposes. On the other hand, JupySQL is a widely-used Python library
    that simplifies the interaction with databases through the power of SQL queries.
    By using JupySQL, data scientists and analysts can easily execute SQL queries,
    manipulate data frames, and interact with databases from their Jupyter notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Why ETLs are important?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ETLs play a significant role in data analytics and business intelligence. They
    help businesses to collect data from various sources, including social media,
    web pages, sensors, and other internal and external systems. By doing this, businesses
    can obtain a holistic view of their operations, customers, and market trends.
  prefs: []
  type: TYPE_NORMAL
- en: After extracting data, ETLs transform it into a structured format, such as a
    relational database, which allows businesses to analyze and manipulate data easily.
    By transforming data, ETLs can clean, validate, and standardize it, making it
    easier to understand and analyze.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, ETLs load the data into a database or data warehouse, where businesses
    can access it easily. By doing this, ETLs enable businesses to access accurate
    and up-to-date information, allowing them to make informed decisions.
  prefs: []
  type: TYPE_NORMAL
- en: What is JupySQL?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[JupySQL](https://github.com/ploomber/jupysql) is an extension for Jupyter
    Notebooks that allows you to interact with databases using SQL queries. It provides
    a convenient way to access databases and data warehouses directly from Jupyter
    Notebooks, allowing you to perform complex data manipulations and analyses.'
  prefs: []
  type: TYPE_NORMAL
- en: JupySQL supports multiple database management systems, including SQLite, MySQL,
    PostgreSQL, DuckDB, Oracle, Snowflake and more (check out our integrations section
    on the left to learn more). You can connect to databases using standard connection
    strings or through the use of environment variables.
  prefs: []
  type: TYPE_NORMAL
- en: Why JupySQL?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JupySQL, a powerful tool, facilitates direct SQL query interaction with databases
    inside Jupyter notebooks. With a view to carrying out efficient and accurate data
    extraction and transformation processes, there are several critical factors to
    consider when performing ETLs via JupySQL. JupySQL provides users with the necessary
    tools to interact with data sources and perform data transformations with ease.
    To save valuable time and effort while guaranteeing consistency and reliability,
    automating the ETL process through scheduling a full ETL notebook via GitHub actions
    can be a game-changer. By utilizing JupySQL, users can achieve the best of both
    worlds, data interactivity (Jupyter) and ease of usage and SQL connectivity (JupySQL),
    thereby streamlining the data management process and allowing data scientists
    and analysts to concentrate on their core competencies - generating valuable insights
    and reports.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with JupySQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To use JupySQL, you need to install it using pip. You can run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once installed, you can load the extension in Jupyter notebooks using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading the extension, you can connect to a database using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, to connect to a local DuckDB database, you can use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Performing ETL using JupySQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To perform ETLs using JupySQL, we will follow the standard ETL process, which
    involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To extract data using JupySQL, we need to connect to the source database and
    execute a query to retrieve the data. For example, to extract data from a MySQL
    database, we can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This command connects to the MySQL database using the specified connection string
    and retrieves all the data from the "mytable" table. The data is stored in the
    "data" variable as a Pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: We can also use `%%sql df <<` to save the data into the df variable'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we''ll be running locally via DuckDB we can simply Extract a public dataset
    and start working immediately. We''re going to get our sample dataset (we will
    work with the Penguins datasets via a csv file):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can get a sample of the data to check we''re connected and we can query
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Transform data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After extracting data, it''s often necessary to transform it into a format
    that''s more suitable for analysis. This step may include cleaning data, filtering
    data, aggregating data, and combining data from multiple sources. Here are some
    common data transformation techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cleaning data**: Data cleaning involves removing or fixing errors, inconsistencies,
    or missing values in the data. For example, you might remove rows with missing
    values, replace missing values with the mean or median value, or fix typos or
    formatting errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filtering data**: Data filtering involves selecting a subset of data that
    meets specific criteria. For example, you might filter data to only include records
    from a specific date range, or records that meet a certain threshold.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregating data**: Data aggregation involves summarizing data by calculating
    statistics such as the sum, mean, median, or count of a particular variable. For
    example, you might aggregate sales data by month or by product category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Combining data**: Data combination involves merging data from multiple sources
    to create a single dataset. For example, you might combine data from different
    tables in a relational database, or combine data from different files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In JupySQL, you can use Pandas DataFrame methods to perform data transformations
    or native SQL. For example, you can use the rename method to rename columns, the
    dropna method to remove missing values, and the astype method to convert data
    types. I'll demonstrate how to do it either with pandas or SQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: You can use either `%sql` or `%%sql`, check out the difference between
    the two [here](https://jupysql.ploomber.io/en/latest/community/developer-guide.html?highlight=%25sql%20vs%20%25%25sql#magics-e-g-sql-sql-etc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s an example of how to use Pandas and the JupySQL alternatives to transform
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In our example we''ll use simple transformations, in a similar manner to the
    above code. We''ll clean our data from NAs and will split a column (species) into
    3 individual columns (named for each species):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Load data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After transforming the data, we need to load it into a destination database
    or data warehouse. We can use `ipython-sql` to connect to the destination database
    and execute SQL queries to load the data. For example, to load data into a PostgreSQL
    database, we can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This command connects to the PostgreSQL database using the specified connection
    string, drops the "mytable" table if it exists, creates a new table with the specified
    columns and data types, and loads the data from the CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: Since our use case is using DuckDB locally we can simply save the newly created
    `transformed_df` into a csv file, but we can also use the snipped above to save
    it into our DB or DWH depending on our use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following step to save the new data as a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can see a new file called `transformed_data.csv` was created for us. In the
    next step we'll see how we can automate this process and consume the final file
    via GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling on GitHub actions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last step in our process is executing the complete notebook via GitHub actions.
    To do that we can use `ploomber-engine` which lets you schedule notebooks, along
    with other notebook capabilities such as profiling, debugging etc. If needed we
    can pass external parameters to our notebook and make it a generic template.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Our notebook file is loading a public dataset and saves it after ETL
    locally, we can easily change it to consume any dataset, and load it to S3, visualize
    the data as a dashboard and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For our example we can use this sample ci.yml file (this is what sets the github
    workflow in your repository), and put it in our repository, the final file should
    be located under `.github/workflows/ci.yml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Content of the `ci.yml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In this example CI, I've also added a scheduled trigger, this job will run nightly
    at 4 am.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ETLs are an essential process for data analytics and business intelligence.
    They help businesses to collect, transform, and load data from various sources,
    making it easier to analyze and make informed decisions. JupySQL is a powerful
    tool that allows you to interact with databases using SQL queries directly in
    Jupyter notebooks. Combined with Github actions we can create powerful workflows
    that can be scheduled and help us get the data to its final stage.
  prefs: []
  type: TYPE_NORMAL
- en: By using JupySQL, you can perform ETLs easily and efficiently, allowing you
    to extract, transform, and load data in a structured format while Github actions
    allocate compute and set the environment.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Ido Michael](https://www.linkedin.com/in/ido-michael/)** co-founded Ploomber
    to help data scientists build faster. He''d been working at AWS leading data engineering/science
    teams. Single handedly he built 100’s of data pipelines during those customer
    engagements together with his team. Originally from Israel, he came to NY for
    his MS at Columbia University. He focused on building Ploomber after he constantly
    found that projects dedicated about 30% of their time just to refactor the dev
    work (prototype) into a production pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[GitHub Actions For Machine Learning Beginners](https://www.kdnuggets.com/github-actions-for-machine-learning-beginners)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Best Practices for Building ETLs for ML](https://www.kdnuggets.com/best-practices-for-building-etls-for-ml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting Started with PyTest: Effortlessly Write and Run Tests in Python](https://www.kdnuggets.com/getting-started-with-pytest-effortlessly-write-and-run-tests-in-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Distribute and Run LLMs with llamafile in 5 Simple Steps](https://www.kdnuggets.com/distribute-and-run-llms-with-llamafile-in-5-simple-steps)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learn How to Run Alpaca-LoRA on Your Device in Just a Few Steps](https://www.kdnuggets.com/2023/05/learn-run-alpacalora-device-steps.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Run an LLM Locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
