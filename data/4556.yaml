- en: Beginners Guide to the Three Types of Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的三种类型入门指南
- en: 原文：[https://www.kdnuggets.com/2019/11/beginners-guide-three-types-machine-learning.html](https://www.kdnuggets.com/2019/11/beginners-guide-three-types-machine-learning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/11/beginners-guide-three-types-machine-learning.html](https://www.kdnuggets.com/2019/11/beginners-guide-three-types-machine-learning.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Rebecca Vickery](https://www.linkedin.com/in/rebecca-vickery-20b94133/),
    Data Scientist**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[Rebecca Vickery](https://www.linkedin.com/in/rebecca-vickery-20b94133/)，数据科学家**'
- en: '![Figure](../Images/97361e3877e513bd6978ee0085f57086.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/97361e3877e513bd6978ee0085f57086.png)'
- en: Visualising KMeans performance with [Yellowbrick](https://www.scikit-yb.org/en/latest/)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[Yellowbrick](https://www.scikit-yb.org/en/latest/)可视化KMeans性能
- en: Machine learning problems can generally be divided into three types. Classification
    and regression, which are known as supervised learning, and unsupervised learning
    which in the context of machine learning applications often refers to clustering.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习问题通常可以分为三种类型。分类和回归被称为监督学习，无监督学习在机器学习应用的上下文中通常指的是聚类。
- en: In the following article, I am going to give a brief introduction to each of
    these three problems and will include a walkthrough in the popular python library [scikit-learn](https://scikit-learn.org/stable/index.html).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的文章中，我将简要介绍这三种问题，并将包括在流行的Python库[scikit-learn](https://scikit-learn.org/stable/index.html)中的演示。
- en: Before I start I’ll give a brief explanation for the meaning behind the terms
    supervised and unsupervised learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我将简要解释监督学习和无监督学习的含义。
- en: '**Supervised Learning:** *In supervised learning, you have a known set of inputs
    (features) and a known set of outputs (labels). Traditionally these are known
    as X and y. The goal of the algorithm is to learn the mapping function that maps
    the input to the output. So that when given new examples of X the machine can
    correctly predict the corresponding y labels.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督学习：** *在监督学习中，你有一组已知的输入（特征）和一组已知的输出（标签）。传统上这些被称为X和y。算法的目标是学习将输入映射到输出的映射函数。这样，当给出新的X示例时，机器可以正确预测相应的y标签。*'
- en: '**Unsupervised Learning:*** In unsupervised learning, you only have a set of
    inputs (X) and no corresponding labels (y). The goal of the algorithm is to find
    previously unknown patterns in the data. Quite often these algorithms are used
    to find meaningful clusters of similar samples of X so in effect finding the categories
    intrinsic to the data.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督学习：** *在无监督学习中，你只有一组输入（X），没有对应的标签（y）。算法的目标是发现数据中之前未知的模式。这些算法通常用于发现相似样本X的有意义的簇，从而在数据中找到固有的类别。*'
- en: Classification
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类
- en: In classification, the outputs (y) are categories. These can be binary, for
    example, if we were classifying spam email vs not spam email. They can also be
    multiple categories such as classifying species of [flowers](https://archive.ics.uci.edu/ml/datasets/iris),
    this is known as multiclass classification.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类中，输出（y）是类别。这些类别可以是二元的，例如，如果我们将垃圾邮件与非垃圾邮件进行分类。它们也可以是多个类别，比如对[花卉](https://archive.ics.uci.edu/ml/datasets/iris)的分类，这被称为多类分类。
- en: Let’s walk through a simple example of classification using scikit-learn. If
    you don’t already have this installed it can be installed either via pip or conda
    as outlined [here](https://scikit-learn.org/stable/install.html).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个使用scikit-learn的简单分类示例来演示。如果你还没有安装，可以通过pip或conda进行安装，安装方法详见[这里](https://scikit-learn.org/stable/install.html)。
- en: Scikit-learn has a number of datasets that can be directly accessed via the
    library. For ease in this article, I will be using these example datasets throughout.
    To illustrate classification I will use the wine dataset which is a multiclass
    classification problem. In the dataset, the inputs (X) consist of 13 features
    relating to various properties of each wine type. The known outputs (y) are wine
    types which in the dataset have been given a number 0, 1 or 2.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn有多个可以通过库直接访问的数据集。为了方便起见，在本文中我将使用这些示例数据集。为了说明分类，我将使用葡萄酒数据集，这是一个多类分类问题。在数据集中，输入（X）包括与每种葡萄酒类型相关的13个特征。已知的输出（y）是葡萄酒类型，在数据集中被赋予了数字0、1或2。
- en: The imports I am using for all the code in this article are shown below.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本文中使用的所有代码的导入如下所示。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the below code I am downloading the data and converting to a pandas data
    frame.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我正在下载数据并转换为pandas数据框。
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The next stage in a supervised learning problem is to split the data into test
    and train sets. The train set can be used by the algorithm to learn the mapping
    between inputs and outputs, and then the reserved test set can be used to evaluate
    how well the model has learned this mapping. In the below code I am using the
    scikit-learn model_selection function `train_test_split` to do this.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习问题的下一阶段是将数据拆分为测试集和训练集。训练集可以被算法用来学习输入和输出之间的映射，然后保留的测试集可以用来评估模型学习这个映射的效果。在下面的代码中，我使用了
    scikit-learn 的 model_selection 函数 `train_test_split` 来完成这一任务。
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the next step, we need to choose the algorithm that will be best suited to
    learn the mapping in your chosen dataset. In scikit-learn there are many different
    algorithms to choose from, all of which use different functions and methods to
    learn the mapping, you can view the full list [here](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们需要选择最适合学习你所选数据集映射的算法。在 scikit-learn 中，有许多不同的算法可以选择，这些算法使用不同的函数和方法来学习映射，你可以在
    [这里](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)
    查看完整列表。
- en: To determine the best model I am running the following code. I am training the
    model using a selection of algorithms and obtaining the F1-score for each one.
    The F1 score is a good indicator of the overall accuracy of a classifier. I have
    written a detailed description of the various metrics that can be used to evaluate
    a classifier [here](https://towardsdatascience.com/understanding-the-confusion-matrix-and-its-business-applications-c4e8aaf37f42).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定最佳模型，我运行了以下代码。我使用一组选定的算法训练模型，并为每个算法获取 F1 分数。F1 分数是分类器总体准确性的良好指标。我已经详细描述了用于评估分类器的各种度量标准
    [这里](https://towardsdatascience.com/understanding-the-confusion-matrix-and-its-business-applications-c4e8aaf37f42)。
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/b228fa9a0ee7af8de72a061aabe63c88.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b228fa9a0ee7af8de72a061aabe63c88.png)'
- en: A perfect F1 score would be 1.0, therefore, the closer the number is to 1.0
    the better the model performance. The results above suggest that the Random Forest
    Classifier is the best model for this dataset.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 完美的 F1 分数是 1.0，因此，数值越接近 1.0，模型性能越好。以上结果表明，随机森林分类器是这个数据集的最佳模型。
- en: Regression
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回归
- en: In regression, the outputs (y) are continuous values rather than categories.
    An example of regression would be predicting how many sales a store may make next
    month, or what the future price of your house might be.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归中，输出（y）是连续值而不是类别。回归的一个例子是预测商店下个月的销售额，或预测你房子的未来价格。
- en: Again to illustrate regression I will use a dataset from scikit-learn known
    as the boston housing dataset. This consists of 13 features (X) which are various
    properties of a house such as the number of rooms, the age and crime rate for
    the location. The output (y) is the price of the house.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明回归，我将使用一个来自 scikit-learn 的数据集，即波士顿房价数据集。该数据集包含 13 个特征（X），这些特征是房子的各种属性，例如房间数量、房龄和位置的犯罪率。输出（y）是房价。
- en: I am loading the data using the code below and splitting it into test and train
    sets using the same method I used for the wine dataset.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在使用下面的代码加载数据，并使用与我对葡萄酒数据集相同的方法将其拆分为测试集和训练集。
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can use this [cheat sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) to
    see the available algorithms suited to regression problems in scikit-learn. We
    will use similar code to the classification problem to loop through a selection
    and print out the scores for each.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个 [备忘单](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)
    查看适用于回归问题的 scikit-learn 可用算法。我们将使用类似的代码来处理分类问题，遍历选择并打印出每个算法的得分。
- en: There are a number of different metrics used to evaluate regression models.
    These are all essentially error metrics and measure the difference between the
    actual and predicted values achieved by the model. I have used the root mean squared
    error (RMSE). For this metric, the closer to zero the value is the better the
    performance of the model. This [article](https://www.dataquest.io/blog/understanding-regression-error-metrics/) gives
    a really good explanation of error metrics for regression problems.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种不同的度量标准用于评估回归模型。这些本质上都是误差度量，衡量模型预测值与实际值之间的差异。我使用了均方根误差（RMSE）。对于这个度量标准，数值越接近零，模型的性能越好。这个
    [文章](https://www.dataquest.io/blog/understanding-regression-error-metrics/) 对回归问题的误差度量进行了很好的解释。
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/112c20479514357f35a9a727672f387f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/112c20479514357f35a9a727672f387f.png)'
- en: The RMSE score suggests that either the linear regression and ridge regression
    algorithms perform best for this dataset.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE分数表明线性回归和岭回归算法在这个数据集中表现最好。
- en: Unsupervised learning
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督学习
- en: There are a number of different types of unsupervised learning but for simplicity
    here I am going to focus on the [clustering methods](https://en.wikipedia.org/wiki/Cluster_analysis).
    There are many different algorithms for clustering all of which use slightly different
    techniques to find clusters of inputs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习有许多不同类型，但为了简化，这里我将重点关注[聚类方法](https://en.wikipedia.org/wiki/Cluster_analysis)。有许多不同的聚类算法，它们使用略微不同的技术来寻找输入的簇。
- en: Probably one of the most widely used methods is Kmeans. This algorithm performs
    an iterative process whereby a specified number of randomly generated means are
    initiated. A distance metric, [Euclidean](https://en.wikipedia.org/wiki/Euclidean_distance) distance
    is calculated for each data point from the centroids, thus creating clusters of
    similar values. The centroid of each cluster then becomes the new mean and this
    process is repeated until the optimum result has been achieved.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最广泛使用的方法之一是K均值（Kmeans）。该算法执行一个迭代过程，其中启动了指定数量的随机生成的均值。计算每个数据点到质心的距离度量，[欧几里得](https://en.wikipedia.org/wiki/Euclidean_distance)距离，从而创建相似值的簇。每个簇的质心随后成为新的均值，这个过程会重复进行，直到达到最佳结果。
- en: Let’s use the wine dataset we used in the classification task, with the y labels
    removed, and see how well the k-means algorithm can identify the wine types from
    the inputs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们在分类任务中使用的葡萄酒数据集，去掉y标签，看看K均值算法能多好地从输入中识别出葡萄酒类型。
- en: As we are only using the inputs for this model I am splitting the data into
    test and train using a slightly different method.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们仅使用输入来建立此模型，我将使用稍微不同的方法将数据拆分为测试集和训练集。
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As Kmeans is reliant on the distance metric to determine the clusters it is
    usually necessary to perform feature scaling (ensuring that all features have
    the same scale) before training the model. In the below code I am using the MinMaxScaler
    to scale the features so that all values fall between 0 and 1.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于K均值依赖于距离度量来确定簇，因此通常在训练模型之前需要进行特征缩放（确保所有特征具有相同的尺度）。在下面的代码中，我使用MinMaxScaler对特征进行缩放，使所有值落在0和1之间。
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With K-means you have to specify the number of clusters the algorithm should
    use. So one of the first steps is to identify the optimum number of clusters.
    This is achieved by iterating through a number of values of k and plotting the
    results on a chart. This is known as the Elbow method as it typically produces
    a plot with a curve that looks a little like the curve of your elbow. The yellowbrick [library](https://www.scikit-yb.org/en/latest/quickstart.html) (which
    is a great library for visualising scikit-learn models and can be pip installed)
    has a really nice plot for this. The code below produces this visualisation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用K均值时，你必须指定算法应使用的簇数量。因此，第一步之一是确定最佳簇数量。这是通过迭代不同的k值并将结果绘制在图表上来实现的。这被称为肘部法则，因为它通常生成一个看起来有点像肘部曲线的图。黄砖（yellowbrick）[库](https://www.scikit-yb.org/en/latest/quickstart.html)（这是一个非常适合可视化scikit-learn模型的库，可以通过pip安装）对此有一个非常好的图。下面的代码生成了这个可视化。
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/a4ff56635ba4125cf527a067392ceba7.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4ff56635ba4125cf527a067392ceba7.png)'
- en: Ordinarily, we wouldn’t already know how many categories we have in a dataset
    where we are using a clustering technique. However, in this case, we know that
    there are three wine types in the data — the curve has correctly selected three
    as the optimum number of clusters to use in the model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们不会知道数据集中使用聚类技术时有多少类别。然而，在这种情况下，我们知道数据中有三种葡萄酒类型——曲线正确地选择了三作为模型中使用的最佳簇数量。
- en: The next step is to initialise the K-means algorithm and fit the model to the
    training data and evaluate how effectively the algorithm has clustered the data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是初始化K均值算法，并将模型拟合到训练数据中，并评估算法对数据的聚类效果。
- en: One method used for this is known as the [silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering)).
    This measures the consistency of values within the clusters. Or in other words
    how similar to each other the values in each cluster are, and how much separation
    there is between the clusters. The silhouette score is calculated for each value
    and will range from -1 to +1\. These values are then plotted to form a silhouette
    plot. Again yellowbrick provides a simple way to construct this type of plot.
    The code below creates this visualisation for the wine dataset.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一种用于此的办法称为 [轮廓系数](https://en.wikipedia.org/wiki/Silhouette_(clustering))。它衡量集群内部值的一致性。换句话说，它测量每个集群内的值彼此有多相似，以及集群之间的分离程度。轮廓系数是对每个值进行计算的，范围从
    -1 到 +1。这些值然后被绘制成轮廓图。再次，yellowbrick 提供了一种简单的方式来构建这种图。下面的代码为酒类数据集创建了这个可视化。
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/97361e3877e513bd6978ee0085f57086.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97361e3877e513bd6978ee0085f57086.png)'
- en: 'A silhouette plot can be interpreted in the following way:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓图可以按以下方式解读：
- en: The closer the mean score (which is the red dotted line in the above) is to
    +1 the better matched the data points are within the cluster.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均分数（上图中的红色虚线）越接近 +1，数据点在集群内的匹配度越好。
- en: Data points with a score of 0 are very close to the decision boundary for another
    cluster (so the separation is low).
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分数为 0 的数据点非常接近另一个集群的决策边界（因此分离度较低）。
- en: Negative values indicate that the data points may have been assigned to the
    wrong cluster.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负值表示数据点可能被分配到了错误的集群。
- en: The width of each cluster should be reasonably uniform if they aren’t then the
    incorrect value of k may have been used.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果每个集群的宽度不均匀，那么可能使用了错误的 k 值。
- en: The plot for the wine data set above shows that cluster 0 may not be as consistent
    as the others due to most data points being below the average score and a few
    data points having a score below 0.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的酒类数据集的图表显示，集群0可能不如其他集群一致，因为大多数数据点低于平均分，而且有少数数据点的分数低于0。
- en: Silhouette scores can be particularly useful in comparing one algorithm against
    another or different values of k.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓系数在比较一个算法与另一个算法或不同的 k 值时特别有用。
- en: In this post, I wanted to give a brief introduction to each of the three types
    of machine learning. There are many other steps involved in all of these processes
    including feature engineering, data processing and hyperparameter optimisation
    to determine both the best data preprocessing techniques and the best models to
    use.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我想简要介绍三种机器学习类型。所有这些过程都涉及许多其他步骤，包括特征工程、数据处理和超参数优化，以确定最佳的数据预处理技术和模型。
- en: Thanks for reading!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: '**Bio: [Rebecca Vickery](https://www.linkedin.com/in/rebecca-vickery-20b94133/)**
    is learning data science through self study. Data Scientist @ Holiday Extras.
    Co-Founder of alGo.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Rebecca Vickery](https://www.linkedin.com/in/rebecca-vickery-20b94133/)**
    通过自学数据科学。数据科学家 @ Holiday Extras。alGo的联合创始人。'
- en: '[Original](https://towardsdatascience.com/beginners-guide-to-the-three-types-of-machine-learning-3141730ef45d).
    Reposted with permission.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/beginners-guide-to-the-three-types-of-machine-learning-3141730ef45d)。经授权转载。'
- en: '**Related:**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Machine Learning Classification: A Dataset-based Pictorial](/2018/11/machine-learning-classification-dataset-based-pictorial.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习分类：基于数据集的图解](/2018/11/machine-learning-classification-dataset-based-pictorial.html)'
- en: '[Python Libraries for Interpretable Machine Learning](/2019/09/python-libraries-interpretable-machine-learning.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可解释的机器学习的 Python 库](/2019/09/python-libraries-interpretable-machine-learning.html)'
- en: '[Five Command Line Tools for Data Science](/2019/07/five-command-line-tools-data-science.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学的五个命令行工具](/2019/07/five-command-line-tools-data-science.html)'
- en: '* * *'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行 IT'
- en: '* * *'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Three R Libraries Every Data Scientist Should Know (Even if You Use Python)](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[每个数据科学家都应该了解的三个 R 库（即使你使用 Python）](https://www.kdnuggets.com/2021/12/three-r-libraries-every-data-scientist-know-even-python.html)'
- en: '[Python Basics: Syntax, Data Types, and Control Structures](https://www.kdnuggets.com/python-basics-syntax-data-types-and-control-structures)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 基础：语法、数据类型和控制结构](https://www.kdnuggets.com/python-basics-syntax-data-types-and-control-structures)'
- en: '[Optimizing Data Storage: Exploring Data Types and Normalization in SQL](https://www.kdnuggets.com/optimizing-data-storage-exploring-data-types-and-normalization-in-sql)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化数据存储：探索 SQL 中的数据类型和规范化](https://www.kdnuggets.com/optimizing-data-storage-exploring-data-types-and-normalization-in-sql)'
- en: '[Types of Visualization Frameworks](https://www.kdnuggets.com/types-of-visualization-frameworks)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可视化框架类型](https://www.kdnuggets.com/types-of-visualization-frameworks)'
- en: '[KDnuggets News, December 14: 3 Free Machine Learning Courses for…](https://www.kdnuggets.com/2022/n48.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，12月14日：3 个免费的机器学习课程……](https://www.kdnuggets.com/2022/n48.html)'
- en: '[OpenAI API for Beginners: Your Easy-to-Follow Starter Guide](https://www.kdnuggets.com/openai-api-for-beginners-your-easy-to-follow-starter-guide)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI API 初学者指南：你的易于跟随的入门指南](https://www.kdnuggets.com/openai-api-for-beginners-your-easy-to-follow-starter-guide)'
