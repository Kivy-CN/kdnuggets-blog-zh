- en: An introduction to explainable AI, and why we need it
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释人工智能简介，以及我们为何需要它
- en: 原文：[https://www.kdnuggets.com/2019/04/introduction-explainable-ai.html](https://www.kdnuggets.com/2019/04/introduction-explainable-ai.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/04/introduction-explainable-ai.html](https://www.kdnuggets.com/2019/04/introduction-explainable-ai.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Patrick Ferris](https://www.patrickferris.me/)**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Patrick Ferris](https://www.patrickferris.me/) 撰写**。'
- en: '![](../Images/9ba3c9c3cc86e9a0e62613fe62bd1f46.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ba3c9c3cc86e9a0e62613fe62bd1f46.png)'
- en: '**The Black Box - a metaphor that represents the unknown inner mechanics of
    functions like neural networks**'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**黑箱 - 代表神经网络等函数的未知内部机制的隐喻**'
- en: Neural networks (and all of their subtypes) are increasingly being used to build
    programs that can predict and classify in a myriad of different settings.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（及其所有子类型）越来越多地用于构建能够在各种不同环境中进行预测和分类的程序。
- en: Examples include [machine translation](https://arxiv.org/pdf/1806.08730.pdf) using
    recurrent neural networks, and [image classification](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) using
    a convolutional neural network. Research published by Google DeepMind has sparked
    interest in [reinforcement learning](https://arxiv.org/pdf/1312.5602.pdf).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 示例包括使用递归神经网络的[机器翻译](https://arxiv.org/pdf/1806.08730.pdf)，以及使用卷积神经网络的[图像分类](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)。谷歌DeepMind发布的研究引发了对[强化学习](https://arxiv.org/pdf/1312.5602.pdf)的兴趣。
- en: All of these approaches have advanced many fields and produced usable models
    that can improve productivity and efficiency.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些方法都推动了许多领域的发展，并产生了可用于提高生产力和效率的模型。
- en: However, **we don’t really know how they work**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，**我们真的不知道它们是如何工作的**。
- en: 'I was fortunate enough to attend the [Knowledge Discovery and Data Mining](http://www.kdd.org/kdd2018/)(KDD)
    conference this year. Of the talks I went to, there were two main areas of research
    that seem to be on a lot of people’s minds:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我有幸参加了今年的[知识发现与数据挖掘](http://www.kdd.org/kdd2018/)（KDD）会议。在我参加的讲座中，有两个主要的研究领域似乎引起了很多人的关注：
- en: Firstly, finding a meaningful representation of graph structures to feed into
    neural networks. [Oriol Vinyals](https://ai.google/research/people/OriolVinyals)from
    DeepMind gave a talk about their [Message Passing Neural Networks](https://arxiv.org/pdf/1704.01212.pdf).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，寻找有意义的图结构表示以供神经网络使用。[Oriol Vinyals](https://ai.google/research/people/OriolVinyals)来自DeepMind做了关于他们的[消息传递神经网络](https://arxiv.org/pdf/1704.01212.pdf)的讲座。
- en: The second area, and the focus of this article, are explainable AI models. As
    we generate newer and more innovative applications for neural networks, the question
    of ‘How do they work?’ becomes more and more important.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个领域，以及本文的重点，是可解释的人工智能模型。随着我们为神经网络生成更新、更具创新性的应用，“它们是如何工作的？”这个问题变得越来越重要。
- en: '**Why the need for Explainable Models?**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**为何需要可解释模型？**'
- en: Neural Networks are not infallible.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络并非万无一失。
- en: Besides the problems of overfitting and underfitting that we’ve developed many
    tools (like Dropout or increasing the data size) to counteract, neural networks
    operate in an opaque way.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们已经开发出许多工具（如 Dropout 或增加数据量）来对抗的过拟合和欠拟合问题之外，神经网络以一种不透明的方式操作。
- en: We don’t really know why they make the choices they do. As models become more
    complex, the task of producing an interpretable version of the model becomes more
    difficult.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上不知道它们为什么做出这些选择。随着模型变得越来越复杂，生成一个可解释的模型变得更加困难。
- en: Take, for example, the [one pixel attack](https://arxiv.org/pdf/1710.08864.pdf) (see
    here for a great [video](https://www.youtube.com/watch?v=SA4YEAWVpbk) on the paper).
    This is carried out by using a sophisticated approach which analyzes the CNNs
    and applies differential evolution (a member of the evolutionary class of algorithms).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以[单像素攻击](https://arxiv.org/pdf/1710.08864.pdf)为例（请查看这里的[视频](https://www.youtube.com/watch?v=SA4YEAWVpbk)）。这是一种通过分析卷积神经网络（CNN）并应用差分进化（进化算法的一个成员）来实施的复杂方法。
- en: Unlike other optimisation strategies which restrict the objective function to
    be differentiable, this approach uses an iterative evolutionary algorithm to produce
    better solutions. Specifically, for this one pixel attack, the only information
    required was the probabilities of the class labels.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与限制目标函数可微的其他优化策略不同，这种方法使用迭代进化算法来生成更好的解决方案。具体来说，对于这个单像素攻击，唯一需要的信息是类别标签的概率。
- en: '![](../Images/fe7fb38ebc0ffaa0856815de932b0b20.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe7fb38ebc0ffaa0856815de932b0b20.png)'
- en: '**From [One pixel attack for fooling deep neural networks](https://arxiv.org/pdf/1710.08864.pdf) by
    Jiawei Su et al.**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**来自[单像素攻击以欺骗深度神经网络](https://arxiv.org/pdf/1710.08864.pdf)的Jiawei Su等人**'
- en: 'The relative ease of fooling these neural networks is worrying. Beyond this
    lies a more systemic problem: trusting a neural network.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 欺骗这些神经网络的相对容易性令人担忧。在这背后存在一个更系统性的问题：信任神经网络。
- en: The best example of this is in the medical domain. Say you are building a neural
    network (or any black-box model) to help predict heart disease given a patient’s
    records.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的例子是在医学领域。假设你正在构建一个神经网络（或任何黑箱模型）来帮助预测根据患者记录的心脏病。
- en: When you train and test your model, you get a good accuracy and a convincing
    positive predictive value. You bring it to a clinician and they agree it seems
    to be a powerful model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当你训练和测试模型时，你会获得良好的准确性和令人信服的正预测值。你把它交给临床医生，他们同意这似乎是一个强大的模型。
- en: 'But they will be hesitant to use it because you (or the model) cannot answer
    the simple question: “Why did you predict this person as more likely to develop
    heart disease?”'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但他们会犹豫使用它，因为你（或模型）不能回答一个简单的问题：“你为什么预测这个人更可能发展成心脏病？”
- en: This lack of transparency is a problem for the clinician who wants to understand
    the way the model works to help them improve their service. It is also a problem
    for the patient who wants a concrete reason for this prediction.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏透明度对希望了解模型工作方式以改善服务的临床医生来说是个问题。对患者来说，他们也希望有一个具体的预测理由。
- en: Ethically, is it right to tell a patient that they have a higher probability
    of a disease if your only reasoning is that “the black-box told me so”? Health
    care is as much about science as it is about empathy for the patient.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从伦理角度来看，如果你的唯一理由是“黑箱告诉我如此”，是否正确告知患者他们有更高的疾病概率？医疗保健不仅关乎科学，也关乎对患者的同情。
- en: The field of explainable AI has grown in recent years, and this trend looks
    set to continue.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释AI领域近年来不断发展，这一趋势似乎将继续。
- en: What follows are some of the interesting and innovative avenues researchers
    and machine learning experts are exploring in their search for models which not
    only perform well, but can tell you why they make the choices they do.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是一些有趣和创新的方向，研究人员和机器学习专家正在探索这些方向，以寻找不仅表现良好，而且能够解释其决策原因的模型。
- en: Reversed Time Attention Model (RETAIN)
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 反向时间注意力模型（RETAIN）
- en: The RETAIN model was developed at Georgia Institute of Technology by [Edward
    Choi et al.](https://arxiv.org/pdf/1608.05745.pdf) It was introduced to help doctors
    understand why a model was predicting patients to be at risk of heart failure.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: RETAIN模型由[Edward Choi et al.](https://arxiv.org/pdf/1608.05745.pdf)在乔治亚理工学院开发。它的推出旨在帮助医生理解为什么模型预测患者有心力衰竭的风险。
- en: '![](../Images/522f5ed8973e9aea1646ffc78ccf6faf.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/522f5ed8973e9aea1646ffc78ccf6faf.png)'
- en: '**The RETAIN Recurrent Neural Network Model makes use of Attention Mechanisms
    to improve interpretability**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**RETAIN递归神经网络模型利用注意力机制提高可解释性**'
- en: The idea is, given a patients’ hospital visits records which also contain the
    events of the visit, they could predict the risk of heart failure.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是，根据患者的住院记录以及访问事件，它们可以预测心力衰竭的风险。
- en: The researchers split the input into two recurrent neural networks. This let
    them use the [attention mechanism](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/) on
    each to understand what the neural network was focusing on.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员将输入分成两个递归神经网络。这使他们能够对每个网络使用[注意力机制](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)以了解神经网络关注的重点。
- en: Once trained, the model could predict a patient’s risk. But it could also make
    use of the alpha and beta parameters to output which hospital visits (and which
    events within a visit) influenced its choice.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，模型可以预测患者的风险。但它也可以利用alpha和beta参数输出哪些住院记录（以及住院中的哪些事件）影响了它的选择。
- en: '**Local Interpretable Model-Agnostic Explanations (LIME)**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**局部可解释模型无关解释（LIME）**'
- en: Another approach that has become fairly common in use is [LIME](https://arxiv.org/pdf/1602.04938.pdf).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种已经变得相当常用的方法是[LIME](https://arxiv.org/pdf/1602.04938.pdf)。
- en: This is a post-hoc model — it provides an explanation of a decision after it
    has been made. This means it isn’t a pure ‘glass-box’, transparent model (like
    decision trees) from start to finish.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种事后模型——它在决策做出后提供解释。这意味着它不是一个从头到尾纯粹的‘玻璃箱’透明模型（如决策树）。
- en: One of the main strengths of this approach is that it’s model agnostic. It can
    be applied to any model in order to produce explanations for its predictions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的主要优势之一是它对模型无关。可以应用于任何模型，以产生对其预测的解释。
- en: The key concept underlying this approach is perturbing the inputs and watching
    how doing so affects the model’s outputs. This lets us build up a picture of which
    inputs the model is focusing on and using to make its predictions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的关键概念是扰动输入，并观察这样做如何影响模型的输出。这使我们能够建立起模型关注和利用哪些输入以进行预测的图像。
- en: 'For instance, imagine some kind of CNN for image classification. There are
    four main steps to using the LIME model to produce an explanation:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设一种用于图像分类的CNN。使用LIME模型生成解释的主要步骤有四个：
- en: Start with a normal image and use the black-box model to produce a probability
    distribution over the classes.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一张正常图像开始，使用黑箱模型生成类别的概率分布。
- en: Then perturb the input in some way. For images, this could be hiding pixels
    by coloring them grey. Now run these through the black-box model to see the how
    the probabilities for the class it originally predicted changed.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后以某种方式扰动输入。对于图像来说，这可能是通过将像素遮挡成灰色来完成的。现在将这些输入通过黑箱模型运行，以观察原本预测的类别概率如何变化。
- en: Use an interpretable (usually linear) model like a decision tree on this dataset
    of perturbations and probabilities to extract the key features which explain the
    changes. The model is locally weighted — meaning that we care more about the perturbations
    that are most similar to the original image we were using.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对这组扰动和概率数据集使用可解释的（通常是线性的）模型，如决策树，以提取解释变化的关键特征。模型是局部加权的——这意味着我们更关心那些与我们使用的原始图像最相似的扰动。
- en: Output the features (in our case, pixels) with the greatest weights as our explanation.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出权重最大的特征（在我们的例子中是像素）作为我们的解释。
- en: '**Layer-wise Relevance Propagation (LRP)**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**层次相关传播（LRP）**'
- en: This [approach](https://arxiv.org/pdf/1604.00825.pdf) uses the idea of relevance
    redistribution and conservation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个[方法](https://arxiv.org/pdf/1604.00825.pdf)利用了相关性重新分配和保留的思想。
- en: We start with an input (say, an image) and its probability of a classification.
    Then, work backwards to redistribute this to all of the inputs (in this case pixels).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个输入（例如，一张图像）及其分类概率开始。然后，向后推导，将这一概率重新分配给所有的输入（在这种情况下是像素）。
- en: The redistribution process is fairly simple from layer to layer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从一层到另一层的重新分配过程相当简单。
- en: '![](../Images/25f23922a4cbb7f3daaa2aa12175a637.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25f23922a4cbb7f3daaa2aa12175a637.png)'
- en: Don’t be scared — this equation is just weighting relevances based on neuron
    activation and weight connnection
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 不要害怕——这个方程只是基于神经元激活和权重连接来加权相关性
- en: 'In the above equation, each term represents the following ideas:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，每一项代表以下思想：
- en: x_j — the activation value for neuron*j *in layer *l*
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: x_j — 第*l*层中神经元*j*的激活值
- en: w_j,k — the weighting of the connection between neuron*j* in layer *l* and neuron *k* in
    layer *l + 1*
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: w_j,k — 第*l*层中神经元*j*与第*l + 1*层中神经元*k*之间的连接权重
- en: R_j — Relevance scores for each neuron in layer*l*
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R_j — 第*l*层中每个神经元的相关性评分
- en: R_k — Relevance scores for each neuron in layer*l+1*
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R_k — 第*l+1*层中每个神经元的相关性评分
- en: The epsilon is just a small value to prevent dividing by zero.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: epsilon只是一个小值，用于防止除以零。
- en: As you can see, we can work our way backwards to determine the relevance of
    individual inputs. Further, we can sort these in order of relevance. This lets
    us extract a meaningful subset of inputs as our most useful or powerful in making
    a prediction.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们可以向后推导以确定各个输入的相关性。此外，我们可以按相关性排序。这使我们能够提取出最有用或最强大的输入子集，以进行预测。
- en: '**What next?**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**接下来做什么？**'
- en: The above methods for producing explainable models are by no means exhaustive.
    They are a sample of some of the approaches researchers have tried using to produce
    interpretable predictions from black-box models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以上生成可解释模型的方法绝非穷尽。这些只是研究人员尝试过的一些方法的样本，用于从黑箱模型中生成可解释的预测。
- en: Hopefully this post also sheds some light onto why it is such an important area
    of research. We need to continue researching these methods, and develop new ones,
    in order for machine learning to benefit as many fields as possible — in a safe
    and trustworthy fashion.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这篇文章也能揭示为什么这是一个如此重要的研究领域。我们需要继续研究这些方法，并开发新的方法，以便机器学习能够以安全可靠的方式惠及尽可能多的领域。
- en: If you find yourself wanting more papers and areas to read about, try some of
    the following.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想阅读更多论文和领域，尝试以下一些。
- en: '[DeepMind](http://proceedings.mlr.press/v80/kim18d/kim18d.pdf)’s research on
    Concept Activation Vectors, as well as the [slides](http://s.interpretable.ml/nips_interpretable_ml_2017_victoria_Krakovna.pdf)from
    Victoria Krakovna’s talk at Neural Information Processing Systems (NIPS) conference.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DeepMind](http://proceedings.mlr.press/v80/kim18d/kim18d.pdf)关于概念激活向量的研究，以及维多利亚·克拉科夫娜在神经信息处理系统（NIPS）会议上的[幻灯片](http://s.interpretable.ml/nips_interpretable_ml_2017_victoria_Krakovna.pdf)。'
- en: A [paper](https://arxiv.org/pdf/1711.07373.pdf) by Dung Huk Park et al. on datasets
    for measuring explainable models.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Dung Huk Park 等人关于解释模型的测量数据集的论文](https://arxiv.org/pdf/1711.07373.pdf)。'
- en: '[Finale Doshi-Velez](https://www.seas.harvard.edu/directory/finale) and [Been
    Kim](https://beenkim.github.io/)’s [paper](https://arxiv.org/pdf/1702.08608.pdf) on
    the field in general'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Finale Doshi-Velez](https://www.seas.harvard.edu/directory/finale) 和 [Been
    Kim](https://beenkim.github.io/) 关于该领域的[论文](https://arxiv.org/pdf/1702.08608.pdf)'
- en: Artificial intelligence should not become a powerful deity which we follow blindly.
    But neither should we forget about it and the beneficial insight it can have.
    Ideally, we will build flexible and interpretable models that can work in collaboration
    with experts and their domain knowledge to provide a brighter future for everyone.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能不应该成为我们盲目追随的强大神明。但我们也不应该忘记它及其可能带来的有益洞察。理想情况下，我们将建立灵活且可解释的模型，与专家及其领域知识合作，为每个人提供一个更光明的未来。
- en: '**Bio**: [Patrick Ferris](https://www.patrickferris.me/) is a nineteen year
    old programmer, blogger and all-round tech enthusiast, and currently Editor in
    Chief of the Hacker''s at Cambridge blog.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介**: [Patrick Ferris](https://www.patrickferris.me/) 是一名十九岁的程序员、博客作者和全方位的技术爱好者，目前是剑桥黑客博客的主编。'
- en: '[Original](https://medium.freecodecamp.org/an-introduction-to-explainable-ai-and-why-we-need-it-a326417dd000).
    Reposted with permission.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.freecodecamp.org/an-introduction-to-explainable-ai-and-why-we-need-it-a326417dd000)。经许可转载。'
- en: '**Resources:**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：**'
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在线和基于网络的：分析、数据挖掘、数据科学、机器学习教育](https://www.kdnuggets.com/education/online.html)'
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分析、数据科学、数据挖掘和机器学习的软件](https://www.kdnuggets.com/software/index.html)'
- en: '**Related:**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[XAI – A Data Scientist’s Mouthpiece](https://www.kdnuggets.com/2019/04/xai-data-scientist.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XAI – 数据科学家的发言人](https://www.kdnuggets.com/2019/04/xai-data-scientist.html)'
- en: '[Explainable AI or Halting Faulty Models ahead of Disaster](https://www.kdnuggets.com/2019/03/explainable-ai.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可解释的 AI 或在灾难前停止故障模型](https://www.kdnuggets.com/2019/03/explainable-ai.html)'
- en: '[The AI Black Box Explanation Problem](https://www.kdnuggets.com/2019/03/ai-black-box-explanation-problem.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AI 黑箱解释问题](https://www.kdnuggets.com/2019/03/ai-black-box-explanation-problem.html)'
- en: '* * *'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在 IT 领域'
- en: '* * *'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Explainable Forecasting and Nowcasting with State-of-the-art Deep…](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[最先进的深度学习技术下的可解释性预测与即时预测](https://www.kdnuggets.com/2021/12/sota-explainable-forecasting-and-nowcasting.html)'
- en: '[A Community for Synthetic Data is Here and This is Why We Need It](https://www.kdnuggets.com/2022/04/community-synthetic-data-need.html)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[合成数据的社区已经到来，这就是我们为何需要它](https://www.kdnuggets.com/2022/04/community-synthetic-data-need.html)'
- en: '[6 Things You Need To Know About Data Management And Why It Matters…](https://www.kdnuggets.com/2022/05/6-things-need-know-data-management-matters-computer-vision.html)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于数据管理你需要知道的 6 件事及其重要性……](https://www.kdnuggets.com/2022/05/6-things-need-know-data-management-matters-computer-vision.html)'
- en: '[We Don''t Need Data Scientists, We Need Data Engineers](https://www.kdnuggets.com/2021/02/dont-need-data-scientists-need-data-engineers.html)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我们不需要数据科学家，我们需要数据工程师](https://www.kdnuggets.com/2021/02/dont-need-data-scientists-need-data-engineers.html)'
- en: '[Closing the Gap Between Human Understanding and Machine Learning:…](https://www.kdnuggets.com/2023/06/closing-gap-human-understanding-machine-learning-explainable-ai-solution.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[弥合人类理解与机器学习之间的差距：可解释 AI 解决方案](https://www.kdnuggets.com/2023/06/closing-gap-human-understanding-machine-learning-explainable-ai-solution.html)'
- en: '[Why You Need To Learn Python In 2022](https://www.kdnuggets.com/2022/04/need-learn-python-2022.html)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么你需要在 2022 年学习 Python](https://www.kdnuggets.com/2022/04/need-learn-python-2022.html)'
