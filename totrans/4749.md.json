["```py\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\ntqdm.pandas(desc=\"progress-bar\")\nfrom gensim.models import Doc2Vec\nfrom sklearn import utils\nfrom sklearn.model_selection import train_test_split\nimport gensim\nfrom sklearn.linear_model import LogisticRegression\nfrom gensim.models.doc2vec import TaggedDocument\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('Consumer_Complaints.csv')\ndf = df[['Consumer complaint narrative','Product']]\ndf = df[pd.notnull(df['Consumer complaint narrative'])]\ndf.rename(columns = {'Consumer complaint narrative':'narrative'}, inplace = True)\ndf.head(10)\n```", "```py\ndf.shape\n```", "```py\ndf.index = range(318718)\ndf['narrative'].apply(lambda x: len(x.split(' '))).sum()\n```", "```py\ncnt_pro = df['Product'].value_counts()\n\nplt.figure(figsize=(12,4))\nsns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Product', fontsize=12)\nplt.xticks(rotation=90)\nplt.show();\n```", "```py\ndef print_complaint(index):\n    example = df[df.index == index][['narrative', 'Product']].values[0]\n    if len(example) > 0:\n        print(example[0])\n        print('Product:', example[1])\n\nprint_complaint(12)\n```", "```py\nprint_complaint(20)\n```", "```py\nfrom bs4 import BeautifulSoup\ndef cleanText(text):\n    text = BeautifulSoup(text, \"lxml\").text\n    text = re.sub(r'\\|\\|\\|', r' ', text) \n    text = re.sub(r'http\\S+', r'<URL>', text)\n    text = text.lower()\n    text = text.replace('x', '')\n    return text\ndf['narrative'] = df['narrative'].apply(cleanText)\n```", "```py\ntrain, test = train_test_split(df, test_size=0.3, random_state=42)\n```", "```py\nimport nltk\nfrom nltk.corpus import stopwords\ndef tokenize_text(text):\n    tokens = []\n    for sent in nltk.sent_tokenize(text):\n        for word in nltk.word_tokenize(sent):\n            if len(word) < 2:\n                continue\n            tokens.append(word.lower())\n    return tokens\n\ntrain_tagged = train.apply(\n    lambda r: TaggedDocument(words=tokenize_text(r['narrative']), tags=[r.Product]), axis=1)\ntest_tagged = test.apply(\n    lambda r: TaggedDocument(words=tokenize_text(r['narrative']), tags=[r.Product]), axis=1)\n```", "```py\ntrain_tagged.values[30]\n```", "```py\nimport multiprocessing\n\ncores = multiprocessing.cpu_count()\n```", "```py\nmodel_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\nmodel_dbow.build_vocab([x for x in tqdm(train_tagged.values)])\n```", "```py\n%%time\nfor epoch in range(30):\n    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n    model_dbow.alpha -= 0.002\n    model_dbow.min_alpha = model_dbow.alpha\n```", "```py\ndef vec_for_learning(model, tagged_docs):\n    sents = tagged_docs.values\n    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n    return targets, regressorsdef vec_for_learning(model, tagged_docs):\n    sents = tagged_docs.values\n    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n    return targets, regressors\n```", "```py\ny_train, X_train = vec_for_learning(model_dbow, train_tagged)\ny_test, X_test = vec_for_learning(model_dbow, test_tagged)\n\nlogreg = LogisticRegression(n_jobs=1, C=1e5)\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score, f1_score\n\nprint('Testing accuracy %s' % accuracy_score(y_test, y_pred))\nprint('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n```", "```py\nmodel_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\nmodel_dmm.build_vocab([x for x in tqdm(train_tagged.values)])\n```", "```py\n%%time\nfor epoch in range(30):\n    model_dmm.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n    model_dmm.alpha -= 0.002\n    model_dmm.min_alpha = model_dmm.alpha\n```", "```py\ny_train, X_train = vec_for_learning(model_dmm, train_tagged)\ny_test, X_test = vec_for_learning(model_dmm, test_tagged)\n\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n\nprint('Testing accuracy %s' % accuracy_score(y_test, y_pred))\nprint('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n```", "```py\nmodel_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\nmodel_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n```", "```py\nfrom gensim.test.test_doc2vec import ConcatenatedDoc2Vec\nnew_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])\n```", "```py\ndef get_vectors(model, tagged_docs):\n    sents = tagged_docs.values\n    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n    return targets, regressors\n```", "```py\ny_train, X_train = get_vectors(new_model, train_tagged)\ny_test, X_test = get_vectors(new_model, test_tagged)\n\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n\nprint('Testing accuracy %s' % accuracy_score(y_test, y_pred))\nprint('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n```"]