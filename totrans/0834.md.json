["```py\ndf.head()\ndf.info()\ndf.isnull().sum()\ndf.duplicated().sum()\ndf.describe([x*0.1 for x in range(10)])\n for c in list(df):\n    print(df[c].value_counts())\n```", "```py\ndf['age'].fillna(df['age'].mean())\ndf['age'].fillna(df['age'].median())\n```", "```py\ndf['price'].fillna(df.group('type_building')['price'].transform('mean'),\ninplace=True)\n```", "```py\ndf['type_building'].fillna(df['type_building'].mode()[0])\n```", "```py\ndf = df.drop_duplicates()\n```", "```py\ndf = df[df.Age<=90]\n```", "```py\nfor c in columns_with_outliers:\n   transform= 'clipped_'+ c\n   lower_limit = df[c].quantile(0.10)\n   upper_limit = df[c].quantile(0.90)\n   df[transform] = df[c].clip(lower_limit, upper_limit, axis = 0)\n```", "```py\nfrom sklearn.preprocessing import OneHotEncoder\n\ndata_to_encode = df[cols_to_encode]\nencoder = OneHotEncoder(dtype='int')\nencoded_data = encoder.fit_transform(data_to_encode)\ndummy_variables = encoder.get_feature_names_out(cols_to_encode)\nencoded_df = pd.DataFrame(encoded_data.toarray(), columns=encoder.get_feature_names_out(cols_to_encode))\n\nfinal_df = pd.concat([df.drop(cols_to_encode, axis=1), encoded_df], axis=1)\n```", "```py\nfrom sklearn.preprocessing import OrdinalEncoder\n\ndata_to_encode = df[cols_to_encode]\nencoder = OrdinalEncoder(dtype='int')\nencoded_data = encoder.fit_transform(data_to_encode)\nencoded_df = pd.DataFrame(encoded_data.toarray(), columns=[\"Income\"])\n\nfinal_df = pd.concat([df.drop(cols_to_encode, axis=1), encoded_df], axis=1)\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nX = final_df.drop(['y'],axis=1)\ny = final_df['y']\n\ntrain_idx, test_idx,_,_ = train_test_split(X.index,y,test_size=0.2,random_state=123)\ntrain_idx, val_idx,_,_ = train_test_split(train_idx,y_train,test_size=0.2,random_state=123)\n\ndf_train = final_df[final_df.index.isin(train_idx)]\ndf_test = final_df[final_df.index.isin(test_idx)]\ndf_val = final_df[final_df.index.isin(val_idx)]\n```", "```py\ntrain_idx, test_idx,y_train,_ = train_test_split(X.index,y,test_size=0.2,stratify=y,random_state=123)\ntrain_idx, val_idx,_,_ = train_test_split(train_idx,y_train,test_size=0.2,stratify=y_train,random_state=123)\n```", "```py\nfrom sklearn.preprocessing import MinMaxScaler\nsc=MinMaxScaler()\ndf_train[numeric_features]=sc.fit_transform(df_train[numeric_features])\ndf_test[numeric_features]=sc.transform(df_test[numeric_features])\ndf_val[numeric_features]=sc.transform(df_val[numeric_features])\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\ndf_train[numeric_features]=sc.fit_transform(df_train[numeric_features])\ndf_test[numeric_features]=sc.transform(df_test[numeric_features])\ndf_val[numeric_features]=sc.transform(df_val[numeric_features])\n```", "```py\nfrom sklearn.preprocessing import RobustScaler\nsc=RobustScaler()\ndf_train[numeric_features]=sc.fit_transform(df_train[numeric_features])\ndf_test[numeric_features]=sc.transform(df_test[numeric_features])\ndf_val[numeric_features]=sc.transform(df_val[numeric_features])\n```", "```py\n# undersampling\nfrom imblearn.over_sampling import RandomUnderSampler,RandomOverSampler\nundersample = RandomUnderSampler(sampling_strategy='majority')\nX_train, y_train = undersample.fit_resample(df_train.drop(['y'],axis=1),df_train['y'])\n# oversampling\noversample = RandomOverSampler(sampling_strategy='minority')\nX_train, y_train = oversample.fit_resample(df_train.drop(['y'],axis=1),df_train['y'])\n```", "```py\nfrom imblearn.over_sampling import SMOTE\nresampler = SMOTE(random_state=123)\nX_train, y_train = resampler.fit_resample(df_train.drop(['y'],axis=1),df_train['y'])\n```"]