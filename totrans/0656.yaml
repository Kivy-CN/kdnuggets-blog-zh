- en: A 2019 Guide to Human Pose Estimation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2019年人体姿态估计指南
- en: 原文：[https://www.kdnuggets.com/2019/08/2019-guide-human-pose-estimation.html](https://www.kdnuggets.com/2019/08/2019-guide-human-pose-estimation.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/08/2019-guide-human-pose-estimation.html](https://www.kdnuggets.com/2019/08/2019-guide-human-pose-estimation.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)![Figure](../Images/55d0d8b6f3e8490e3924480c94501134.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)![图](../Images/55d0d8b6f3e8490e3924480c94501134.png)'
- en: Photo by [David Hofmann](https://unsplash.com/@davidhofmann) on [Unsplash](https://unsplash.com/search/photos/dance)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[David Hofmann](https://unsplash.com/@davidhofmann)拍摄，来源于[Unsplash](https://unsplash.com/search/photos/dance)
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT需求'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Human pose estimation refers to the process of inferring poses in an image.
    Essentially, it entails predicting the positions of a person’s joints in an image
    or video. This problem is also sometimes referred to as the localization of human
    joints. It’s also important to note that pose estimation has various sub-tasks
    such as single pose estimation, estimating poses in an image with many people,
    estimating poses in crowded places, and estimating poses in videos.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人体姿态估计指的是在图像中推断姿态的过程。本质上，它涉及预测图像或视频中人物关节的位置。这个问题有时也被称为人体关节的定位。值得注意的是，姿态估计还有各种子任务，如单一姿态估计、多人图像中的姿态估计、拥挤场所中的姿态估计以及视频中的姿态估计。
- en: 'Pose estimation can be performed in either 3D or 2D. Some of the applications
    of human pose estimation include:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 姿态估计可以在3D或2D中进行。人体姿态估计的一些应用包括：
- en: '[Activity recognition](https://www.researchgate.net/publication/277411291_Real-Time_Human_Pose_Estimation_and_Gesture_Recognition_from_Depth_Images_Using_Superpixels_and_SVM_Classifier)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[活动识别](https://www.researchgate.net/publication/277411291_Real-Time_Human_Pose_Estimation_and_Gesture_Recognition_from_Depth_Images_Using_Superpixels_and_SVM_Classifier)'
- en: '[Animation](https://blog.deepmotion.com/2019/05/21/markerless-augmented-reality-for-ar-avatars/)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[动画](https://blog.deepmotion.com/2019/05/21/markerless-augmented-reality-for-ar-avatars/)'
- en: '[Gaming](https://www.microsoft.com/en-us/research/publication/key-developments-in-human-pose-estimation-for-kinect/)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[游戏](https://www.microsoft.com/en-us/research/publication/key-developments-in-human-pose-estimation-for-kinect/)'
- en: '[Augmented reality](https://arxiv.org/abs/1806.09316)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[增强现实](https://arxiv.org/abs/1806.09316)'
- en: Some of the approaches used in the papers we’ll highlight are **bottom-up**
    and **top-down**. Essentially, in a bottom-up approach, the processing is done
    from high to low resolutions, while in top-down processing is done from low to
    high resolutions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将要重点介绍的论文中使用的一些方法包括**自下而上**和**自上而下**。本质上，自下而上的方法是从高分辨率到低分辨率处理，而自上而下的方法则是从低分辨率到高分辨率处理。
- en: The top-down approach starts by identifying and localizing individual person
    instances using a bounding box [object detector](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3).
    This is then followed by estimating the pose of a single person. The bottom-up
    approach starts by localizing identity-free semantic entities, then grouping them
    into person instances.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 自上而下的方法首先使用边界框[物体检测器](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3)来识别和定位单个人体实例。接下来估计单个人的姿态。自下而上的方法则从定位不带身份的语义实体开始，然后将其分组为人体实例。
- en: 'We’ll now look at some research that’s been conducted in an attempt to solve
    the problem of human pose estimation:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将现在查看一些为解决人体姿态估计问题而进行的研究：
- en: '[DeepPose: Human Pose Estimation via Deep Neural Networks](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#0057)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DeepPose: 通过深度神经网络进行人体姿态估计](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#0057)'
- en: '[Efficient Object Localization Using Convolutional Networks](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#e6c7)'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[使用卷积网络的高效目标定位](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#e6c7)'
- en: '[Human Pose Estimation with Iterative Error Feedback](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#ae35)'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[基于迭代错误反馈的人体姿态估计](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#ae35)'
- en: '[Stacked Hourglass Networks for Human Pose Estimation](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#b8ef)'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[用于人体姿态估计的堆叠沙漏网络](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#b8ef)'
- en: '[Convolutional Pose Machines](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#480f)'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[卷积姿态机器](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#480f)'
- en: '[DeepCut: Joint Subset Partition and Labeling for Multi-Person Pose Estimation](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#aca3)'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DeepCut：用于多人的姿态估计的联合子集分区和标注](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#aca3)'
- en: '[Simple Baselines for Human Pose Estimation and Tracking](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#9ee6)'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[用于人体姿态估计和跟踪的简单基线](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#9ee6)'
- en: '[RMPE: Regional Multi-Person Pose Estimation](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#b177)'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[RMPE：区域性多人姿态估计](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#b177)'
- en: '[OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#7c7f)'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[OpenPose：使用部件亲和场进行实时多人2D姿态估计](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#7c7f)'
- en: '[Human Pose Estimation for Real-World Crowded Scenarios](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#9b96)'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[现实世界拥挤场景的人体姿态估计](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#9b96)'
- en: '[DensePose: Dense Human Pose Estimation In The Wild](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#0c65)'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DensePose：野外密集人体姿态估计](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#0c65)'
- en: '[PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up,
    Part-Based, Geometric Embedding Model](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#36d4)'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[PersonLab：基于自下而上、部件化、几何嵌入模型的人体姿态估计和实例分割](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#36d4)'
- en: 'DeepPose: Human Pose Estimation via Deep Neural Networks (CVPR, 2014)'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DeepPose：通过深度神经网络进行的人体姿态估计（CVPR, 2014）
- en: This paper proposes using deep neural networks(DNNs) to tackle this ML task.
    The authors of this paper are Alexander Toshev and Christian Szegedy from Google.
    The formulation of the pose estimation itself is a DNN-based regression on the
    joints. The authors achieve state of the art results on standard benchmarks such
    as the MPII, LSP, and FLIC datasets. They also analyze the effects of jointly
    training a multi-staged architecture with repeated intermediate supervision.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本文建议使用深度神经网络（DNNs）来解决这一机器学习任务。本文的作者是来自谷歌的亚历山大·托舍夫（Alexander Toshev）和克里斯蒂安·施泽迪（Christian
    Szegedy）。姿态估计的公式化本身是对关节进行基于DNN的回归。作者在标准基准测试集，如MPII、LSP和FLIC数据集上实现了最先进的结果。他们还分析了联合训练具有重复中间监督的多阶段架构的效果。
- en: '**[DeepPose: Human Pose Estimation via Deep Neural Networks](https://arxiv.org/abs/1312.4659?source=post_page-----c10b79b64b73----------------------)**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**[DeepPose：通过深度神经网络进行的人体姿态估计](https://arxiv.org/abs/1312.4659?source=post_page-----c10b79b64b73----------------------)**'
- en: We propose a method for human pose estimation based on Deep Neural Networks
    (DNNs). The pose estimation is formulated…
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种基于深度神经网络（DNNs）的人体姿态估计方法。姿态估计被公式化为…
- en: The DNN is able to capture the content of all the joints and doesn’t require
    the use of graphical models. As seen below, the network is made up of seven layers.
    A pooling layer, a convolution layer, and a fully-connected layer form part of
    these layers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: DNN能够捕捉所有关节的内容，并且不需要使用图形模型。如下面所示，网络由七层组成。这些层包括一个池化层、一个卷积层和一个全连接层。
- en: The convolution layer and fully-connected layer are the only layers that have
    learnable parameters. They both contain linear transformations followed by a rectified
    linear unit. The network takes an input image of size 220 × 220 and the learning
    rate is set to 0.0005\. The dropout regularization for the fully-connected layers
    is set to 0.6\. Some of the datasets used in this model are [Frames Labeled In
    Cinema (FLIC)](https://bensapp.github.io/flic-dataset.html) and [Leeds Sports
    Dataset](http://sam.johnson.io/research/lsp.html).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层和全连接层是唯一具有可学习参数的层。它们都包含线性变换，后跟一个修正线性单元。网络输入图像的大小为220 × 220，学习率设置为0.0005。全连接层的dropout正则化设置为0.6。该模型使用的一些数据集包括[电影标记帧
    (FLIC)](https://bensapp.github.io/flic-dataset.html)和[利兹体育数据集](http://sam.johnson.io/research/lsp.html)。
- en: '![Figure](../Images/ed678246402935f1a64e6189c27dbe0b.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/ed678246402935f1a64e6189c27dbe0b.png)'
- en: '[source](https://arxiv.org/abs/1312.4659)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[源](https://arxiv.org/abs/1312.4659)'
- en: The figure below shows the performance of the model on the Percentage of Correct
    Parts (PCP) metric.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了模型在正确部件百分比 (PCP) 评估指标上的表现。
- en: '![Figure](../Images/09ab5fcd5228837e18a7ae3c8145dbda.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/09ab5fcd5228837e18a7ae3c8145dbda.png)'
- en: '[source](https://arxiv.org/abs/1312.4659)![Figure](../Images/d844ddd9e34ef892417fb3c069ffadd9.png)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[源](https://arxiv.org/abs/1312.4659)![图](../Images/d844ddd9e34ef892417fb3c069ffadd9.png)'
- en: '[source](https://arxiv.org/abs/1312.4659)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[源](https://arxiv.org/abs/1312.4659)'
- en: Efficient Object Localization Using Convolutional Networks (2015)
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高效目标定位使用卷积网络 (2015)
- en: This paper proposes a ConvNet architecture that predicts the location of human
    joints in monocular RGB images. The authors of this paper are from New York University.
    The model allows for increased pooling that improves computational efficiency.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文提出了一种ConvNet架构，用于预测单目RGB图像中的人体关节位置。本文作者来自纽约大学。该模型允许增加池化操作，从而提高计算效率。
- en: '**[Efficient Object Localization Using Convolutional Networks](https://arxiv.org/abs/1411.4280?source=post_page-----c10b79b64b73----------------------)**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**[高效目标定位使用卷积网络](https://arxiv.org/abs/1411.4280?source=post_page-----c10b79b64b73----------------------)**'
- en: Recent state-of-the-art performance on human-body pose estimation has been achieved
    with Deep Convolutional Networks…
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，在人体姿态估计方面取得了深度卷积网络的**最先进**性能…
- en: The network first performs body part localization and outputs a low-resolution
    per-pixel heatmap. This heatmap shows the probability of a joint occurring at
    each spatial location in the image.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 网络首先执行身体部位定位，并输出低分辨率的逐像素热图。该热图显示了关节在图像中每个空间位置出现的概率。
- en: '![Figure](../Images/0c1c9fd0bcd6a1dd095524e4fe56c674.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/0c1c9fd0bcd6a1dd095524e4fe56c674.png)'
- en: '[source](https://arxiv.org/pdf/1411.4280.pdf)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[源](https://arxiv.org/pdf/1411.4280.pdf)'
- en: The paper also introduces a network that uses the features from the hidden layer
    from the heatmap regression model in order to increase localization accuracy.
    The model uses a multi-resolution ConvNet architecture and implements a sliding
    window detector that has overlapping contexts to produce a coarse heatmap output.
    A sliding window is usually a rectangular box that has a fixed height and width.
    The box slides across the image. As the box slides the classifiers try to identify
    whether that section has an object that is of interest to the task at hand.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 论文还介绍了一个网络，该网络利用热图回归模型中的隐藏层特征以提高定位准确性。该模型使用多分辨率ConvNet架构，并实现了具有重叠上下文的滑动窗口检测器，以生成粗略的热图输出。滑动窗口通常是一个具有固定高度和宽度的矩形框。框在图像上滑动。当框滑动时，分类器尝试识别该区域是否存在对当前任务感兴趣的物体。
- en: '![Figure](../Images/09b5c5a0d8d08b47e98115ff5f6c92d8.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/09b5c5a0d8d08b47e98115ff5f6c92d8.png)'
- en: '[source](https://arxiv.org/pdf/1411.4280.pdf)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[源](https://arxiv.org/pdf/1411.4280.pdf)'
- en: The figure below shows the full model architecture proposed in this paper. This
    architecture is implemented with Torch7 and evaluated using FLIC and MPII-Human-Pose
    datasets.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了本文提出的完整模型架构。该架构使用Torch7实现，并通过FLIC和MPII-Human-Pose数据集进行评估。
- en: '![Figure](../Images/4128b5ce22ea37e2f254554d21aea4c4.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/4128b5ce22ea37e2f254554d21aea4c4.png)'
- en: '[source](https://arxiv.org/pdf/1411.4280.pdf)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[源](https://arxiv.org/pdf/1411.4280.pdf)'
- en: The model’s performance is evaluated using the standard PCK (Percentage of Correct
    Keypoints) measure on the FLIC dataset and the PCKh measure on the MPII dataset.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在FLIC数据集上使用标准PCK（关键点正确百分比）度量以及在MPII数据集上使用PCKh度量来评估模型的性能。
- en: '![Figure](../Images/98c7f764c9b6efd5148d3ede2a104914.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/98c7f764c9b6efd5148d3ede2a104914.png)'
- en: '[source](https://arxiv.org/pdf/1411.4280.pdf)![Figure](../Images/5df7c18ed1dfd58cf04526539ce08c06.png)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1411.4280.pdf)![图](../Images/5df7c18ed1dfd58cf04526539ce08c06.png)'
- en: '[source](https://arxiv.org/pdf/1411.4280.pdf)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1411.4280.pdf)'
- en: Human Pose Estimation with Iterative Error Feedback (2016)
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 迭代误差反馈的人体姿态估计（2016）
- en: This paper proposes a framework that extends the expressive power of hierarchical
    feature extractors to include both input and output spaces. It does this by introducing
    top-down feedback. The authors of this paper are from UC Berkeley.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文提出了一个框架，扩展了层级特征提取器的表达能力，包括输入和输出空间。它通过引入自上而下的反馈来实现这一点。本文的作者来自加州大学伯克利分校。
- en: Outputs are not predicted in one go; however, a self-correcting model that feeds
    back the error predictions is used. The authors call this process Iterative Error
    Feedback (IEF). The model produces great results on the task of articulated pose
    estimation on the challenging MPII and LSP benchmarks.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 输出不是一次性预测的；而是使用一个自我修正的模型来反馈误差预测。作者称这个过程为迭代误差反馈（IEF）。该模型在挑战性的MPII和LSP基准测试中对关节姿态估计任务产生了出色的结果。
- en: '**[Human Pose Estimation with Iterative Error Feedback](https://arxiv.org/abs/1507.06550?source=post_page-----c10b79b64b73----------------------)**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**[迭代误差反馈的人体姿态估计](https://arxiv.org/abs/1507.06550?source=post_page-----c10b79b64b73----------------------)**'
- en: Hierarchical feature extractors such as Convolutional Networks (ConvNets) have
    achieved impressive performance on a…
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 层级特征提取器，如卷积网络（ConvNets），在...
- en: The figure below shows an implementation of Iterative Error Feedback (IEF) for
    2D human pose estimation. The left panel shows the input image I and the initial
    guess of keypoints Y0\. The three key points here correspond to the right wrist
    (green), left wrist (blue), and top of the head (red). The function f in this
    architecture is modeled as a convolutional neural network. The function g converts
    each 2D keypoint position into one Gaussian heatmap channel.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了用于2D人体姿态估计的迭代误差反馈（IEF）实现。左侧面板显示了输入图像I和关键点的初步猜测Y0。这里的三个关键点分别对应右腕（绿色）、左腕（蓝色）和头顶（红色）。该架构中的函数f被建模为卷积神经网络。函数g将每个2D关键点位置转换为一个高斯热图通道。
- en: '![Figure](../Images/9c5cc095f61ad5b345ae4f612f998531.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/9c5cc095f61ad5b345ae4f612f998531.png)'
- en: '[source](https://arxiv.org/abs/1507.06550)![Figure](../Images/09eae34ba0923ca93c2f0f6d73b64e93.png)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1507.06550)![图](../Images/09eae34ba0923ca93c2f0f6d73b64e93.png)'
- en: This model can be visualized using this mathematical equation.![Figure](../Images/2a684ba4918beac3a95a03ef33f35278.png)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型可以通过这个数学方程进行可视化。![图](../Images/2a684ba4918beac3a95a03ef33f35278.png)
- en: '[source](https://arxiv.org/abs/1507.06550)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1507.06550)'
- en: The table shown below represents the performance of this model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 下表显示了该模型的性能。
- en: '![Figure](../Images/5b32f85a566d7c1d7b0f4887328cf447.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/5b32f85a566d7c1d7b0f4887328cf447.png)'
- en: '[source](https://arxiv.org/abs/1507.06550)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1507.06550)'
- en: Stacked Hourglass Networks for Human Pose Estimation (2016)
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 堆叠小时玻璃网络用于人体姿态估计（2016）
- en: This paper argues that repeated bottom-up and top-down processing with intermediate
    supervision improves the performance of their proposed network. The network is
    referred to as a “stacked hourglass” because of the successive processes of polling
    and upsampling that are performed to produce the final predictions. The authors
    of this paper are from the University of Michigan.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文认为，反复的自下而上和自上而下的处理加上中间监督可以提高所提出网络的性能。该网络被称为“堆叠小时玻璃”，因为为了生成最终预测，进行了一系列的下采样和上采样处理。本文的作者来自密歇根大学。
- en: '**[Stacked Hourglass Networks for Human Pose Estimation](https://arxiv.org/abs/1603.06937?source=post_page-----c10b79b64b73----------------------)**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**[堆叠小时玻璃网络用于人体姿态估计](https://arxiv.org/abs/1603.06937?source=post_page-----c10b79b64b73----------------------)**'
- en: This work introduces a novel convolutional network architecture for the task
    of human pose estimation. Features are…
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作介绍了一种新型的卷积网络架构，用于人体姿态估计任务。特征...
- en: '![](../Images/dc6abd6307386c4c1ff37cf606af45f8.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc6abd6307386c4c1ff37cf606af45f8.png)'
- en: The network was tested on the FLIC and MPII Human Pose benchmarks. It achieves
    more than 2% improved accuracy on average on MPII across all joints, and between
    4–5% improved accuracy on difficult joints such as ankles and knees.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 网络在FLIC和MPII人体姿态基准上进行了测试。它在MPII上所有关节的平均准确度提高了2%以上，在困难的关节如踝关节和膝关节上提高了4-5%。
- en: '![Figure](../Images/a31b700b1fafcf1ef8e3746964ce9b64.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/a31b700b1fafcf1ef8e3746964ce9b64.png)'
- en: '[source](https://arxiv.org/abs/1603.06937)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1603.06937)'
- en: The hourglass architecture is designed to capture information at every scale.
    The network outputs pixel-wise predictions. The set up of the network has a convolution
    layer and max-pooling layers that are used for feature processing. The network
    outputs heatmaps that predict the occurrence of specific joints at each pixel-level.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 沙漏架构旨在捕捉每个尺度上的信息。网络输出像素级的预测。网络设置包括卷积层和用于特征处理的最大池化层。网络输出热图，预测每个像素级别上特定关节的出现。
- en: '![Figure](../Images/11db08a91e470d495235f4e2e3993281.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/11db08a91e470d495235f4e2e3993281.png)'
- en: '[source](https://arxiv.org/abs/1603.06937)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1603.06937)'
- en: The figure below shows the performance of the model on various body parts.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了模型在各种身体部位上的表现。
- en: '![Figure](../Images/928170938b5018d6ca45f2079330fc9e.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/928170938b5018d6ca45f2079330fc9e.png)'
- en: '[source](https://arxiv.org/abs/1603.06937)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1603.06937)'
- en: '**Convolutional Pose Machines (2016)**'
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**卷积姿态机器（2016）**'
- en: This paper introduces Convolutional Pose Machines (CPMs) for articulated pose
    estimations. CPMs are made up of a sequence of convolution networks that produce
    a 2D belief map for the location of each part. This paper is from the Robotics
    Institute Carnegie Mellon University.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了用于细致姿态估计的卷积姿态机器（CPMs）。CPMs由一系列卷积网络组成，这些网络生成每个部位位置的2D信念图。本文来自卡内基梅隆大学机器人研究所。
- en: '**[Convolutional Pose Machines](https://arxiv.org/abs/1602.00134?source=post_page-----c10b79b64b73----------------------)**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**[卷积姿态机器](https://arxiv.org/abs/1602.00134?source=post_page-----c10b79b64b73----------------------)**'
- en: Pose Machines provide a sequential prediction framework for learning rich implicit
    spatial models. In this work we show…
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 姿态机器提供了一个顺序预测框架，用于学习丰富的隐式空间模型。在这项工作中我们展示了…
- en: At every stage in a CPM, the image features and belief maps produced in the
    preceding stage are used as input.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPM的每个阶段，前一阶段生成的图像特征和信念图用作输入。
- en: '![Figure](../Images/f7a2e25316f2331dfeed679200733639.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/f7a2e25316f2331dfeed679200733639.png)'
- en: '[source](https://arxiv.org/pdf/1602.00134.pdf)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1602.00134.pdf)'
- en: The network learns implicit spatial models through a sequential composition
    of convolutional architectures. It also introduces a systematic approach to designing
    and training such an architecture in order to learn image features and image-dependent
    spatial models for structured prediction tasks. This doesn’t require the use of
    any graphical model style inference.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 网络通过卷积架构的顺序组合来学习隐式空间模型。它还引入了一种系统的方法来设计和训练这样的架构，以便学习图像特征和图像依赖的空间模型用于结构化预测任务。这不需要使用任何图形模型风格的推断。
- en: The network is tested on the MPII, LSP, and FLIC datasets.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 网络在MPII、LSP和FLIC数据集上进行了测试。
- en: '![Figure](../Images/46757ee79bd0744587d3a9cf02b90b5b.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/46757ee79bd0744587d3a9cf02b90b5b.png)'
- en: '[source](https://arxiv.org/pdf/1602.00134.pdf)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1602.00134.pdf)'
- en: The model’s PCKh-0.5 score achieves state-of-the-art results at 87.95% and a
    PCKh0.5 score of 78.28% on the ankle. It has been implemented using Caffe, and [the
    code has been open-sourced](https://github.com/shihenw/convolutional-pose-machines-release).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的PCKh-0.5得分达到了87.95%的前沿结果，而踝部的PCKh0.5得分为78.28%。它已经使用Caffe实现，并且[代码已开源](https://github.com/shihenw/convolutional-pose-machines-release)。
- en: '![Figure](../Images/965e2b2b97fffb78eefd8913c1d2490d.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/965e2b2b97fffb78eefd8913c1d2490d.png)'
- en: '[source](https://arxiv.org/pdf/1602.00134.pdf)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1602.00134.pdf)'
- en: 'DeepCut: Joint Subset Partition and Labeling for Multi-Person Pose Estimation
    (CVPR 2016)'
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'DeepCut: 关节子集划分与标注用于多人人体姿态估计（CVPR 2016）'
- en: This paper proposes a method for detecting poses in images with multiple people.
    The model works by detecting the number of people in an image and then predicting
    the joint locations for each image. This paper is by Max Planck Institute and
    Stanford University.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种用于检测多人人体姿态的方法。该模型通过检测图像中的人数，然后预测每个图像的关节位置。本文由马克斯·普朗克研究所和斯坦福大学联合完成。
- en: '**[DeepCut: Joint Subset Partition and Labeling for Multi-Person Pose Estimation](https://arxiv.org/abs/1511.06645?source=post_page-----c10b79b64b73----------------------)**'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**[DeepCut: 关节子集划分与标注用于多人人体姿态估计](https://arxiv.org/abs/1511.06645?source=post_page-----c10b79b64b73----------------------)**'
- en: This paper considers the task of articulated human pose estimation of multiple
    people in real-world images. We propose…
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 本文考虑了对现实世界图像中多个人体姿态进行细致估计的任务。我们提出了…
- en: '![Figure](../Images/699d93f9201c67d4a4e3081b918006a9.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/699d93f9201c67d4a4e3081b918006a9.png)'
- en: '[source](https://arxiv.org/abs/1511.06645)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1511.06645)'
- en: In order to obtain strong part detectors, the authors adapt FastRCN for the
    task. They alter it in two ways; proposal generation and detection region size.
    The performance of this model in predicting various body parts is shown below.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得强大的部位检测器，作者调整了FastRCN以适应这个任务。他们在两个方面进行了修改：提议生成和检测区域大小。该模型在预测各种身体部位方面的性能如下图所示。
- en: '![Figure](../Images/4bc0448de2b8aea3707c2ff1bbedfa60.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/4bc0448de2b8aea3707c2ff1bbedfa60.png)'
- en: '[source](https://arxiv.org/abs/1511.06645)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1511.06645)'
- en: Since using proposals for body part detection may be suboptimal, the authors
    use a fully-convolutional VGG that has a stride of 32 px and reduce that stride
    to 8 px. They then scale the image input to a standing height of 340 px, and this
    obtains the best results.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用身体部位检测的提议可能效果不佳，作者使用了一个步幅为32像素的全卷积VGG，并将步幅减少到8像素。他们将图像输入缩放到340像素的站立高度，这样可以获得最佳效果。
- en: For the loss function, they first try the softmax that outputs the probabilities
    of different body parts. Later, they implement the sigmoid activation function
    on the output neurons and cross-entropy loss. In the end, they found out that
    the sigmoid activation function obtains better results than the softmax loss function.
    The model is trained and evaluated on Leeds Sports Poses (LSP), LSP Extended (LSPET),
    and MPII Human Pose.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于损失函数，他们首先尝试了输出不同身体部位概率的softmax。后来，他们在输出神经元上实现了sigmoid激活函数和交叉熵损失。最终，他们发现sigmoid激活函数比softmax损失函数获得了更好的结果。该模型在Leeds
    Sports Poses (LSP)、LSP Extended (LSPET)和MPII Human Pose上进行了训练和评估。
- en: '![Figure](../Images/74fb000184fe58032a6c7d35fdaa6ed5.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/74fb000184fe58032a6c7d35fdaa6ed5.png)'
- en: '[source](https://arxiv.org/abs/1511.06645)![Figure](../Images/a39fec1a973acf60777fbbbcdef4ad1e.png)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1511.06645)![图](../Images/a39fec1a973acf60777fbbbcdef4ad1e.png)'
- en: '[source](https://arxiv.org/abs/1511.06645)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1511.06645)'
- en: Simple Baselines for Human Pose Estimation and Tracking (EECV, 2018)
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 《用于人体姿态估计和跟踪的简单基线》（EECV, 2018）
- en: This paper’s pose estimation solution is based on deconvolutional layers added
    on a [ResNet](https://en.wikipedia.org/wiki/Residual_neural_network). The model
    achieves an mAP of 73.7 on a COCO test-dev split. Its pose tracking model achieves
    an mAP score of 74.6 and a MOTA (Multiple Object Tracking Accuracy) score of 57.8\.
    The authors of this paper are from Microsoft Research Asia and the University
    of Electronic Science and Technology of China.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文的姿态估计解决方案基于在[ResNet](https://en.wikipedia.org/wiki/Residual_neural_network)上添加的反卷积层。该模型在COCO测试开发集上达到了73.7的mAP。其姿态跟踪模型达到了74.6的mAP评分和57.8的MOTA（多目标跟踪准确率）评分。本文的作者来自微软亚洲研究院和电子科技大学。
- en: '**[Simple Baselines for Human Pose Estimation and Tracking](https://arxiv.org/abs/1804.06208?source=post_page-----c10b79b64b73----------------------)**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**[用于人体姿态估计和跟踪的简单基线](https://arxiv.org/abs/1804.06208?source=post_page-----c10b79b64b73----------------------)**'
- en: There has been significant progress on pose estimation and increasing interests
    on pose tracking in recent years. At…
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年在姿态估计方面取得了显著进展，并且对姿态跟踪的兴趣不断增加。 在…
- en: The method used in this network adds a few deconvolutional layers over the last
    convolution stage in the ResNet architecture. This structure makes it very easy
    to generate heatmaps from deep- and low-resolution images. Three deconvolutional
    layers with batch normalization and ReLU activation are used by default.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络使用的方法在ResNet架构的最后卷积阶段上添加了几个反卷积层。这种结构使得从深度和低分辨率图像生成热图变得非常容易。默认使用三个带有批量归一化和ReLU激活的反卷积层。
- en: '![Figure](../Images/eb348438c6b5f460b7903dfbfc766bb5.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/eb348438c6b5f460b7903dfbfc766bb5.png)'
- en: '[source](https://arxiv.org/pdf/1804.06208.pdf)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1804.06208.pdf)'
- en: The figure below shows the proposed flow of the pose tracking framework. Pose
    tracking in a video is done by first estimating the human pose, giving it a unique
    identifier, and then tracking it across the frames.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了姿态跟踪框架的提议流程。视频中的姿态跟踪首先通过估计人体姿态来进行，为其分配一个唯一标识符，然后在各帧之间跟踪该标识符。
- en: '![Figure](../Images/db6bc67d352424ee370c1b8a5a4130f3.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/db6bc67d352424ee370c1b8a5a4130f3.png)'
- en: '[source](https://arxiv.org/pdf/1804.06208.pdf)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1804.06208.pdf)'
- en: Below is a comparison of this model to other models.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是该模型与其他模型的比较。
- en: '![Figure](../Images/ef431646692f65135a831fa749b6f869.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/ef431646692f65135a831fa749b6f869.png)'
- en: '[source](https://arxiv.org/pdf/1804.06208.pdf)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1804.06208.pdf)'
- en: 'RMPE: Regional Multi-Person Pose Estimation (2018)'
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'RMPE: 区域多人物体姿态估计（2018）'
- en: 'This paper proposes a regional multi-person pose estimation (RMPE) framework
    for estimation in inaccurate human bounding boxes. The framework consists of three
    components: a Symmetric Spatial Transformer Network (SSTN), Parametric Pose NonMaximum-Suppression
    (NMS), and a Pose-Guided Proposals Generator (PGPG). This framework achieves a
    76.7 mAP on the MPII (multi-person) dataset. The authors of this paper are from
    shanghai Jiao Tong University, China and Tencent YouTu.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一个区域多人物体姿态估计（RMPE）框架，用于在不准确的人体边界框中进行估计。该框架包含三个组件：对称空间变换网络（SSTN）、参数化姿态非极大值抑制（NMS）和姿态引导提议生成器（PGPG）。该框架在MPII（多人物体）数据集上实现了76.7的mAP。本文的作者来自中国上海交通大学和腾讯优图。
- en: '**[RMPE: Regional Multi-person Pose Estimation](https://arxiv.org/abs/1612.00137?source=post_page-----c10b79b64b73----------------------)**'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**[RMPE: 区域多人物体姿态估计](https://arxiv.org/abs/1612.00137?source=post_page-----c10b79b64b73----------------------)**'
- en: Multi-person pose estimation in the wild is challenging. Although state-of-the-art
    human detectors have demonstrated…
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际环境中的多人物体姿态估计具有挑战性。尽管最先进的人类检测器已证明…
- en: '![Figure](../Images/0d27121f2c13c8570ea2ad5d4117a89b.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/0d27121f2c13c8570ea2ad5d4117a89b.png)'
- en: '[source](https://arxiv.org/abs/1612.00137)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1612.00137)'
- en: In this framework, the bounding boxes obtained by the human detector are fed
    into the “Symmetric STN + SPPE” module. The pose proposals are then generated
    automatically. These poses are fine-tuned by parametric pose NMS in order to obtain
    the estimated human poses. At training, “Parallel SPPE” is introduced in order
    to avoid the local minima.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在该框架中，人类检测器获得的边界框被输入到“对称STN + SPPE”模块。然后自动生成姿态提议。这些姿态通过参数化姿态NMS进行微调，以获得估计的人体姿态。在训练中，引入了“并行SPPE”以避免局部最小值。
- en: '![Figure](../Images/096f1ecce3bd5cb97d7f60f73e64689a.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/096f1ecce3bd5cb97d7f60f73e64689a.png)'
- en: '[source](https://arxiv.org/abs/1612.00137)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1612.00137)'
- en: The figure below shows the performance of the model in comparison to other frameworks,
    as well as some of the pose predictions obtained by it.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图展示了模型与其他框架的性能比较，以及一些通过它获得的姿态预测。
- en: '![Figure](../Images/d5759be84d4332212c7b654881a1d4e8.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/d5759be84d4332212c7b654881a1d4e8.png)'
- en: '[source](https://arxiv.org/abs/1612.00137)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1612.00137)'
- en: 'OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields
    (2019)'
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'OpenPose: 实时多人物体2D姿态估计使用部位关联字段（2019）'
- en: OpenPose is an open-source real-time system for multi-person 2D pose detection,
    including body, foot, hand, and facial keypoints. This paper proposes a real-time
    approach for detecting 2D human poses in images and videos. This proposed method
    uses nonparametric representations known as Part Affinity Fields (PAFs). Some
    of the authors of this paper are from IEEE.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: OpenPose是一个开源的实时多人物体2D姿态检测系统，包括身体、脚、手和面部关键点。本文提出了一种实时检测图像和视频中2D人体姿态的方法。该方法使用了称为部位关联字段（PAFs）的非参数表示。本文的一些作者来自IEEE。
- en: '**[OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields](https://arxiv.org/abs/1812.08008?source=post_page-----c10b79b64b73----------------------)**'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**[OpenPose: 实时多人物体2D姿态估计使用部位关联字段](https://arxiv.org/abs/1812.08008?source=post_page-----c10b79b64b73----------------------)**'
- en: Realtime multi-person 2D pose estimation is a key component in enabling machines
    to have an understanding of people in…
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 实时多人物体2D姿态估计是让机器理解人类的关键组成部分…
- en: '![Figure](../Images/507abb2d77ebcbd20fb97a2ac46e9386.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/507abb2d77ebcbd20fb97a2ac46e9386.png)'
- en: '[source](https://arxiv.org/abs/1812.08008)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1812.08008)'
- en: As shown below, this method takes an image as the input for a CNN and predicts
    confidence maps for detecting body parts and PAFs for part association. This paper
    also presents an annotated foot dataset with 15K human foot instances. The dataset
    has been publicly released.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，该方法以图像作为CNN的输入，并预测用于检测身体部位的置信度图和用于部位关联的PAFs。本文还展示了一个注释脚数据集，包含15K个人脚实例。该数据集已公开发布。
- en: '![](../Images/0ab5f986ea08fcd12822cf88d1aa56fa.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ab5f986ea08fcd12822cf88d1aa56fa.png)'
- en: The network architecture iteratively predicts affinity fields that encode part-to-part
    association (shown in blue) and detection confidence maps (shown in beige).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 网络架构迭代地预测编码部位间关联（以蓝色显示）和检测置信度图（以米色显示）的关联字段。
- en: '![Figure](../Images/3319378587ca10cea5ff768783a76e5a.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/3319378587ca10cea5ff768783a76e5a.png)'
- en: '[source](https://arxiv.org/abs/1812.08008)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[source](https://arxiv.org/abs/1812.08008)'
- en: OpenPose is also the surrounding software and API that has the ability to fetch
    images from a variety of sources. For example, one can select the input as a camera
    feed, webcam, video, or image. It runs on different platforms such as Ubuntu,
    Windows, Mac OS X, and embedded systems (e.g., Nvidia Tegra TX2). It also provides
    support for different hardware, such as CUDA GPUs, OpenCL GPUs, and CPU-only devices.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: OpenPose 也是一个周边软件和 API，能够从各种来源中获取图像。例如，可以选择输入为相机视频、网络摄像头、视频或图像。它运行在不同的平台上，如
    Ubuntu、Windows、Mac OS X 和嵌入式系统（例如，Nvidia Tegra TX2）。它还支持不同的硬件，如 CUDA GPU、OpenCL
    GPU 和仅 CPU 的设备。
- en: OpenPose has three blocks; body+foot detection, hand detection, and face detection.
    It has been evaluated on the MPII human multi-person dataset, COCO keypoint challenge
    dataset, and the paper’s proposed foot dataset. The figure below shows the results
    obtained by OpenPose in comparison to other models.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: OpenPose 包含三个模块：身体+脚检测、手部检测和面部检测。它已在 MPII 人体多人物数据集、COCO 关键点挑战数据集以及论文提出的脚数据集上进行了评估。下图展示了
    OpenPose 与其他模型的比较结果。
- en: '![Figure](../Images/19fe9ad7f2cfa40147ffbff4d7271a02.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/19fe9ad7f2cfa40147ffbff4d7271a02.png)'
- en: '[source](https://arxiv.org/abs/1812.08008)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[source](https://arxiv.org/abs/1812.08008)'
- en: Human Pose Estimation for Real-World Crowded Scenarios (AVSS, 2019)
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 现实世界拥挤场景的人体姿态估计 (AVSS, 2019)
- en: This paper proposes methods for estimating pose estimation for human crowds.
    The challenges of estimating poses in such densely populated areas include people
    in close proximity to each other, mutual occlusions, and partial visibility. The
    authors of this paper are from the Fraunhofer Institute for Optronics and Karlsruhe
    Institute of Technology KIT.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了用于估计人群姿态的多种方法。在这些密集人群区域中估计姿态的挑战包括人员彼此接近、相互遮挡以及部分可见性。本文作者来自弗劳恩霍夫光电研究所和卡尔斯鲁厄理工学院
    KIT。
- en: '**[Human Pose Estimation for Real-World Crowded Scenarios](https://arxiv.org/abs/1907.06922?source=post_page-----c10b79b64b73----------------------)**'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**[现实世界拥挤场景的人体姿态估计](https://arxiv.org/abs/1907.06922?source=post_page-----c10b79b64b73----------------------)**'
- en: Human pose estimation has recently made significant progress with the adoption
    of deep convolutional neural networks…
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 人体姿态估计最近在采用深度卷积神经网络方面取得了显著进展…
- en: '![Figure](../Images/69bc1ea9e8b85e61a7ce6e20f9911d9d.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/69bc1ea9e8b85e61a7ce6e20f9911d9d.png)'
- en: '[source](https://arxiv.org/abs/1907.06922)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[source](https://arxiv.org/abs/1907.06922)'
- en: One of the methods used to optimize pose estimation for crowded images is a
    single-person pose estimator using the ResNet50 network as the backbone. The method
    is a two-stage, top-down approach that localizes each person and then performs
    a single-person pose estimation for every prediction.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 用于优化拥挤图像的姿态估计的方法之一是使用 ResNet50 网络作为骨干的单人姿态估计器。该方法是一种两阶段的自上而下的方法，首先定位每个人，然后对每个预测进行单人姿态估计。
- en: The paper also introduces two occlusion detection networks; Occlusion Net and
    Occlusion Net Cross Branch. Occlusion Net splits after two transposed convolutions
    so a joint representation can be learned in the previous layers. The Occlusion
    Net Cross Branch splits after one transposed convolution. The Occlusion Detection
    Networks output two sets of heatmaps per pose. One heatmap for the visible keypoints,
    and the other for occluded keypoints.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 论文还介绍了两个遮挡检测网络；Occlusion Net 和 Occlusion Net Cross Branch。Occlusion Net 在两个转置卷积后进行拆分，以便在之前的层中学习联合表示。Occlusion
    Net Cross Branch 在一个转置卷积后进行拆分。遮挡检测网络输出每个姿态两个热图集。一个热图用于可见关键点，另一个用于遮挡关键点。
- en: '![Figure](../Images/586cd342ebef504cdcc1125b88332296.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/586cd342ebef504cdcc1125b88332296.png)'
- en: '[source](https://arxiv.org/abs/1907.06922)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[source](https://arxiv.org/abs/1907.06922)'
- en: The table below shows the performance of this model compared to other models.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了该模型与其他模型的性能比较。
- en: '![Figure](../Images/d4250b836d4064589ba4fb90a3c4b027.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![Figure](../Images/d4250b836d4064589ba4fb90a3c4b027.png)'
- en: '[source](https://arxiv.org/abs/1907.06922)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[source](https://arxiv.org/abs/1907.06922)'
- en: 'DensePose: Dense Human Pose Estimation In The Wild (2018)'
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'DensePose: Dense Human Pose Estimation In The Wild (2018)'
- en: This is a paper from INRIA-CentraleSupelec and Facebook AI Research, whose objective
    is to map all human pixels of an RGB image to the 3D surface of the human body.
    It also introduces a DensePose-COCO dataset. This is a dataset with image-to-surface
    correspondence of 50K COCO images that have been manually annotated. The authors
    then use this dataset to train CNN-based systems that deliver dense correspondence
    in the presence of background, occlusions, and scale variations. Correspondence
    is basically a representation of how images in one image correspond to the pixels
    in another image.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一篇来自INRIA-CentraleSupelec和Facebook AI Research的论文，其目标是将RGB图像的所有人体像素映射到人体的3D表面。论文还介绍了一个DensePose-COCO数据集。该数据集包含50000张经过人工注释的COCO图像的图像与表面对应关系。作者随后使用该数据集训练基于CNN的系统，以在背景、遮挡和尺度变化的情况下提供密集的对应关系。对应关系基本上表示了一个图像中的图像如何与另一个图像中的像素对应。
- en: '![Figure](../Images/418b249d40b2e6d48af2b3e32b4a5c1c.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/418b249d40b2e6d48af2b3e32b4a5c1c.png)'
- en: '[source](https://arxiv.org/pdf/1802.00434.pdf)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1802.00434.pdf)'
- en: '**[DensePose: Dense Human Pose Estimation In The Wild](https://arxiv.org/abs/1802.00434?source=post_page-----c10b79b64b73----------------------)**'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**[DensePose: Dense Human Pose Estimation In The Wild](https://arxiv.org/abs/1802.00434?source=post_page-----c10b79b64b73----------------------)**'
- en: In this work, we establish dense correspondences between RGB image and a surface-based
    representation of the human…
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们建立了RGB图像与基于表面的人的表示之间的密集对应关系……
- en: In this model, a single RGB image is taken as input and used to establish a
    correspondence between surface points and image pixels.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，单张RGB图像被作为输入，用于建立表面点和图像像素之间的对应关系。
- en: '![Figure](../Images/58d6a82539aa873ea2748c4e3fd5ba66.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/58d6a82539aa873ea2748c4e3fd5ba66.png)'
- en: '[source](https://arxiv.org/abs/1802.00434)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1802.00434)'
- en: The approach in this model has been built by combining with the Mask-RCNN system.
    The model operates at 20–26 frames per second on a GTX 1080 GPU for a 240 × 320
    image and 4–5 frames per second on a 240 × 320 image.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的这种方法是通过与Mask-RCNN系统结合建立的。该模型在GTX 1080 GPU上对240 × 320图像的运行速度为20–26帧每秒，对240
    × 320图像的运行速度为4–5帧每秒。
- en: '![Figure](../Images/ceffce83779d1920fc62ffcd494adb6c.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/ceffce83779d1920fc62ffcd494adb6c.png)'
- en: '[source](https://arxiv.org/abs/1802.00434)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1802.00434)'
- en: The authors have combined the Dense Regression (DenseReg) system with the Mask-RCNN
    architecture to come up with the DensePose-RCNN system.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 作者将Dense Regression（DenseReg）系统与Mask-RCNN架构相结合，提出了DensePose-RCNN系统。
- en: '![Figure](../Images/72f6e710b2c95542388340e3ef4f0568.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/72f6e710b2c95542388340e3ef4f0568.png)'
- en: '[source](https://arxiv.org/abs/1802.00434)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1802.00434)'
- en: The model uses a fully-convolutional network that’s dedicated to generating
    a classification and a regression head for assignment and coordinate predictions.
    The authors use the same architecture used in the keypoint branch of MaskRCNN.
    It consists of a stack of 8 alternating 3×3 fully convolutional and ReLU layers
    with 512 channels.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用一个全卷积网络，专门用于生成分类和回归头，以进行分配和坐标预测。作者使用了MaskRCNN关键点分支中使用的相同架构。它由8层交替的3×3全卷积层和ReLU层堆叠而成，具有512个通道。
- en: '![Figure](../Images/5acc00760af4216e598c4db272924d01.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/5acc00760af4216e598c4db272924d01.png)'
- en: '[source](https://arxiv.org/abs/1802.00434)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1802.00434)'
- en: The authors run experiments on a test set of 1.5k images containing 2.3k humans
    and a training set of 48K humans. The figure below is a comparison of its performance
    in relation to other methods.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在一个包含2300个人的1500张图像的测试集上进行了实验，并在一个包含48000人训练集上进行了实验。下图展示了其与其他方法的性能比较。
- en: '![Figure](../Images/e7c18974e7ebfd363aaf7334b0ffd599.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/e7c18974e7ebfd363aaf7334b0ffd599.png)'
- en: '[source](https://arxiv.org/abs/1802.00434)![Figure](../Images/ab30fc8ee857fa55f8549e28ea20f8e9.png)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1802.00434)![图](../Images/ab30fc8ee857fa55f8549e28ea20f8e9.png)'
- en: '[source](https://arxiv.org/abs/1802.00434)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/1802.00434)'
- en: 'PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up,
    Part-Based, Geometric Embedding Model (2018)'
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'PersonLab: 基于自下而上的部分模型进行姿态估计和实例分割（2018）'
- en: The authors of this paper are from Google. They present a box-free bottom-up
    approach for pose estimation and instance segmentation for images with multiple
    people.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文的作者来自Google。他们提出了一种无框的自下而上的姿态估计和实例分割方法，适用于多人的图像。
- en: '![Figure](../Images/45b18cdd4e6d34b5143b3111719d91b9.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/45b18cdd4e6d34b5143b3111719d91b9.png)'
- en: '[source](https://arxiv.org/pdf/1803.08225.pdf)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1803.08225.pdf)'
- en: This means that the authors first detect body parts and then group these parts
    into human instances. The approach achieves COCO test-dev keypoint average precision
    of 0.665 using single-scale inference and 0.687 using multi-scale inference.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着作者首先检测身体部位，然后将这些部位归类为人类实例。该方法在COCO测试集上的关键点平均精度为0.665（单尺度推理）和0.687（多尺度推理）。
- en: '**[PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up,
    Part-Based, Geometric…](https://arxiv.org/abs/1803.08225?source=post_page-----c10b79b64b73----------------------)**'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**[PersonLab: 基于自下而上的部分基础几何模型的人物姿态估计与实例分割](https://arxiv.org/abs/1803.08225?source=post_page-----c10b79b64b73----------------------)**'
- en: We present a box-free bottom-up approach for the tasks of pose estimation and
    instance segmentation of people in…
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种无框的自下而上的方法，用于姿态估计和实例分割任务。
- en: The model proposesd in this paper is a box-free, fully convolutional system
    that first predicts all the key points for every individual in an image. The model
    is trained on the COCO keypoint dataset.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出的模型是一个无框的完全卷积系统，该系统首先预测图像中每个人的所有关键点。该模型在COCO关键点数据集上进行训练。
- en: At the keypoint detection stage, the model detects visible key points of a person
    in the image. The PersonLab system was evaluated on the standard COCO keypoints
    task and on COCO instance segmentation for the person class only.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在关键点检测阶段，该模型检测图像中可见的关键点。PersonLab系统在标准COCO关键点任务以及仅针对人员类别的COCO实例分割任务上进行了评估。
- en: '![Figure](../Images/5d51faa167fc8433d13f5db55a8e06da.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/5d51faa167fc8433d13f5db55a8e06da.png)'
- en: '[source](https://arxiv.org/pdf/1803.08225.pdf)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1803.08225.pdf)'
- en: The figure below shows its performance on the COCO keypoints test-dev split.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了其在COCO关键点测试集上的表现。
- en: '![Figure](../Images/98580f66ab64fccb46db21736e4f1155.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/98580f66ab64fccb46db21736e4f1155.png)'
- en: '[source](https://arxiv.org/pdf/1803.08225.pdf)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1803.08225.pdf)'
- en: Conclusion
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结论
- en: We should now be up to speed on some of the most common — and a couple of very
    recent — techniques for performing human pose estimation in a variety of contexts.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在应该对一些最常见的——以及一些非常新的——人类姿态估计技术有了了解，这些技术适用于各种情境。
- en: The papers/abstracts mentioned and linked to above also contain links to their
    code implementations. We’d be happy to see the results you obtain after testing
    them.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 上述提到并链接到的论文/摘要也包含了其代码实现的链接。我们很乐意看到你在测试后获得的结果。
- en: '**Bio: [Derrick Mwiti](https://derrickmwiti.com/)** is a data analyst, a writer,
    and a mentor. He is driven by delivering great results in every task, and is a
    mentor at Lapid Leaders Africa.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人简介: [Derrick Mwiti](https://derrickmwiti.com/)** 是一名数据分析师、作家和导师。他致力于在每项任务中取得优异成果，并且是Lapid
    Leaders Africa的导师。'
- en: '[Original](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73).
    Reposted with permission.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73)。转载已获许可。'
- en: '**Related:**'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关:**'
- en: '[2019 Guide to Semantic Segmentation](/2019/08/2019-guide-semantic-segmentation.html)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2019年语义分割指南](/2019/08/2019-guide-semantic-segmentation.html)'
- en: '[A 2019 Guide to Object Detection](/2019/08/2019-guide-object-detection.html)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2019年目标检测指南](/2019/08/2019-guide-object-detection.html)'
- en: '[Object Detection with Luminoth](/2019/03/object-detection-luminoth.html)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Luminoth进行目标检测](/2019/03/object-detection-luminoth.html)'
- en: More On This Topic
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[R vs Python (Again): A Human Factor Perspective](https://www.kdnuggets.com/2022/01/r-python-human-factor-perspective.html)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[R与Python（再次）：人因因素视角](https://www.kdnuggets.com/2022/01/r-python-human-factor-perspective.html)'
- en: '[The Gap Between Deep Learning and Human Cognitive Abilities](https://www.kdnuggets.com/2022/10/gap-deep-learning-human-cognitive-abilities.html)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习与人类认知能力之间的差距](https://www.kdnuggets.com/2022/10/gap-deep-learning-human-cognitive-abilities.html)'
- en: '[Closing the Gap Between Human Understanding and Machine Learning:…](https://www.kdnuggets.com/2023/06/closing-gap-human-understanding-machine-learning-explainable-ai-solution.html)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[缩小人类理解与机器学习之间的差距：…](https://www.kdnuggets.com/2023/06/closing-gap-human-understanding-machine-learning-explainable-ai-solution.html)'
- en: '[Beyond Human Boundaries: The Rise of SuperIntelligence](https://www.kdnuggets.com/beyond-human-boundaries-the-rise-of-superintelligence)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[超越人类边界：超智能的崛起](https://www.kdnuggets.com/beyond-human-boundaries-the-rise-of-superintelligence)'
- en: '[Natural Language Processing: Bridging Human Communication with AI](https://www.kdnuggets.com/natural-language-processing-bridging-human-communication-with-ai)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自然语言处理：用AI架起人类沟通的桥梁](https://www.kdnuggets.com/natural-language-processing-bridging-human-communication-with-ai)'
- en: '[Beyond Coding: Why The Human Touch Matters](https://www.kdnuggets.com/beyond-coding-why-the-human-touch-matters)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[超越编程：为何人情味至关重要](https://www.kdnuggets.com/beyond-coding-why-the-human-touch-matters)'
