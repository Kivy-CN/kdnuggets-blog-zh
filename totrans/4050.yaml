- en: Mastering the Art of Data Cleaning in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/mastering-the-art-of-data-cleaning-in-python](https://www.kdnuggets.com/mastering-the-art-of-data-cleaning-in-python)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Mastering the Art of Data Cleaning in Python](../Images/8962cd78b382626bafaea7ad8d538500.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning is a critical part of any data analysis process. It's the step
    where you remove errors, handle missing data, and make sure that your data is
    in a format that you can work with. Without a well-cleaned dataset, any subsequent
    analyses can be skewed or incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This article introduces you to several key techniques for data cleaning in Python,
    using powerful libraries like pandas, numpy, seaborn, and matplotlib.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Importance of Data Cleaning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving into the mechanics of data cleaning, let's understand its importance.
    Real-world data is often messy. It can contain duplicate entries, incorrect or
    inconsistent data types, missing values, irrelevant features, and outliers. All
    these factors can lead to misleading conclusions when analyzing data. This makes
    data cleaning an indispensable part of the data science lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll cover the following data cleaning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Mastering the Art of Data Cleaning in Python](../Images/9705a754315fcc1b9d070e034988a2f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Setup for Data Cleaning in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before getting started, let's import the necessary libraries. We'll be using
    pandas for data manipulation, and seaborn and matplotlib for visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll also import the datetime Python module for manipulating the dates.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Loading and Inspecting Your Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we'll need to load our data. In this example, we're going to load a CSV
    file using pandas. We also add the delimiter argument.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, it's important to inspect the data to understand its structure, what kind
    of variables we're working with, and whether there are any missing values. Since
    the data we imported is not huge, let’s have a look at the whole dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here’s how the dataset looks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Mastering the Art of Data Cleaning in Python](../Images/14b819ad4461b06f0037afbd0fb31cc0.png)'
  prefs: []
  type: TYPE_IMG
- en: You can immediately see there are some missing values. Also, the date formats
    are inconsistent.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at the DataFrame summary using the info() method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here’s the code output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Mastering the Art of Data Cleaning in Python](../Images/e02bf810733535c1ba1e9252f7922309.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that only the column square_feet doesn’t have any NULL values, so
    we’ll somehow have to handle this. Also, the columns advertisement_date, and sale_date
    are the object data type, even though this should be a date.
  prefs: []
  type: TYPE_NORMAL
- en: The column location is completely empty. Do we need it?
  prefs: []
  type: TYPE_NORMAL
- en: We’ll show you how to handle these issues. We’ll start by learning how to delete
    unnecessary columns.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting Unnecessary Columns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two columns in the dataset that we don’t need in our data analysis,
    so we’ll remove them.
  prefs: []
  type: TYPE_NORMAL
- en: The first column is buyer. We don’t need it, as the buyer’s name doesn’t impact
    the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: We’re using the drop() method with the specified column name. We set the axis
    to 1 to specify that we want to delete a column. Also, the inplace argument is
    set to True so that we modify the existing DataFrame, and not create a new DataFrame
    without the removed column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The second column we want to remove is location. While it might be useful to
    have this information, this is a completely empty column, so let’s just remove
    it.
  prefs: []
  type: TYPE_NORMAL
- en: We take the same approach as with the first column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Of course, you can remove these two columns simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Both approaches return the following dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: '![Mastering the Art of Data Cleaning in Python](../Images/d4b19f806cfe3564dd619a9df95ffe74.png)'
  prefs: []
  type: TYPE_IMG
- en: Handling Duplicate Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Duplicate data can occur in your dataset for various reasons and can skew your
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s detect the duplicates in our dataset. Here’s how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: The below code uses the method **duplicated()** to consider duplicates in the
    whole dataset. Its default setting is to consider the first occurrence of a value
    as unique and the subsequent occurrences as duplicates. You can modify this behavior
    using the **keep** parameter. For instance, df.duplicated(keep=False) would mark
    all duplicates as True, including the first occurrence.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here’s the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Mastering the Art of Data Cleaning in Python](../Images/d4fe70038e3767a4460653a2ef241088.png)'
  prefs: []
  type: TYPE_IMG
- en: The row with index 3 has been marked as duplicate because row 2 with the same
    values is its first occurrence.
  prefs: []
  type: TYPE_NORMAL
- en: Now we need to remove duplicates, which we do with the following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The **drop_duplicates()** function considers all columns while identifying
    duplicates. If you want to consider only certain columns, you can pass them as
    a list to this function like this: df.drop_duplicates(subset=[''column1'', ''column2'']).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mastering the Art of Data Cleaning in Python](../Images/0c802373e36ec7124b65ee9e9cb0cc8b.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the duplicate row has been dropped. However, the indexing stayed
    the same, with index 3 missing. We’ll tidy this up by resetting indices.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This task is performed by using the **reset_index()** function.  The drop=True
    argument is used to discard the original index. If you do not include this argument,
    the old index will be added as a new column in your DataFrame. By setting drop=True,
    you are telling pandas to forget the old index and reset it to the default integer
    index.
  prefs: []
  type: TYPE_NORMAL
- en: For practice, try to [remove duplicates from this Microsoft dataset](https://platform.stratascratch.com/coding/9849-find-the-duplicate-records-in-the-dataset?code_type=2&utm_source=blog&utm_medium=click&utm_campaign=kdn+data+cleaning).
  prefs: []
  type: TYPE_NORMAL
- en: Data Type Conversion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, data types might be incorrectly set. For example, a date column might
    be interpreted as strings. You need to convert these to their appropriate types.
  prefs: []
  type: TYPE_NORMAL
- en: In our dataset, we’ll do that for the columns advertisement_date and sale_date,
    as they are shown as the object data type. Also, the date dates are formatted
    differently across the rows. We need to make it consistent, along with converting
    it to date.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way is to use the **to_datetime()** method. Again, you can do that
    column by column, as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: When doing that, we set the dayfirst argument to True because some dates start
    with the day first.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You can also convert both columns at the same time by using the **apply()**
    method with **to_datetime()**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Both approaches give you the same result.
  prefs: []
  type: TYPE_NORMAL
- en: '![Mastering the Art of Data Cleaning in Python](../Images/19dc91cbaf4dbb488a3593d95ad7a444.png)'
  prefs: []
  type: TYPE_IMG
- en: Now the dates are in a consistent format. We see that not all data has been
    converted. There’s one NaT value in advertisement_date and two in sale_date. This
    means the date is missing.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s check if the columns are converted to dates by using the **info()** method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![Mastering the Art of Data Cleaning in Python](../Images/e6ec0eb9378e4f56d55550dcb47a8a4d.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, both columns are not in datetime64[ns] format.
  prefs: []
  type: TYPE_NORMAL
- en: Now, try to convert the data from TEXT to NUMERIC in this [Airbnb dataset](https://platform.stratascratch.com/coding/9634-host-response-rates-with-cleaning-fees?code_type=2&utm_source=blog&utm_medium=click&utm_campaign=kdn+data+cleaning).
  prefs: []
  type: TYPE_NORMAL
- en: Handling Missing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Real-world datasets often have missing values. Handling missing data is vital,
    as certain algorithms cannot handle such values.
  prefs: []
  type: TYPE_NORMAL
- en: Our example also has some missing values, so let’s take a look at the two most
    usual approaches to handling missing data.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting Rows With Missing Values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the number of rows with missing data is insignificant compared to the total
    number of observations, you might consider deleting these rows.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, the last row has no values except the square feet and advertisement
    date. We can’t use such data, so let’s remove this row.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s the code where we indicate the row’s index.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The DataFrame now looks like this.
  prefs: []
  type: TYPE_NORMAL
- en: '![Mastering the Art of Data Cleaning in Python](../Images/9f2a0e5598824db43db0c0347be9f221.png)'
  prefs: []
  type: TYPE_IMG
- en: The last row has been deleted, and our DataFrame now looks better. However,
    there are still some missing data which we’ll handle using another approach.
  prefs: []
  type: TYPE_NORMAL
- en: Imputing Missing Values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have significant missing data, a better strategy than deleting could
    be imputation. This process involves filling in missing values based on other
    data. For numerical data, common imputation methods involve using a measure of
    central tendency (mean, median, mode).
  prefs: []
  type: TYPE_NORMAL
- en: In our already changed DataFrame, we have NaT (Not a Time) values in the columns
    advertisement_date and sale_date. We’ll impute these missing values using the
    **mean()** method.
  prefs: []
  type: TYPE_NORMAL
- en: The code uses the **fillna()** method to find and fill the null values with
    the mean value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You can also do the same thing in one line of code. We use the **apply()** to
    apply the function defined using **lambda**. Same as above, this function uses
    the **fillna()** and **mean()** methods to fill in the missing values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The output in both cases looks like this.
  prefs: []
  type: TYPE_NORMAL
- en: '![Mastering the Art of Data Cleaning in Python](../Images/96cc85a9edbd5b5dc67ab39ab9774fab.png)'
  prefs: []
  type: TYPE_IMG
- en: Our sale_date column now has times which we don’t need. Let’s remove them.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use the **strftime()** method, which converts the dates to their string
    representation and a specific format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![Mastering the Art of Data Cleaning in Python](../Images/e2b413dcdf2794b2dfce981d3964e274.png)'
  prefs: []
  type: TYPE_IMG
- en: The dates now look all tidy.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to use **strftime()** on multiple columns, you can again use **lambda**
    the following way.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s see how we can impute missing categorical values.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical data is a type of data that is used to group information with similar
    characteristics. Each of these groups is a category. Categorical data can take
    on numerical values (such as "1" indicating "male" and "2" indicating "female"),
    but those numbers do not have mathematical meaning. You can't add them together,
    for instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Categorical data is typically divided into two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nominal data:** This is when the categories are only labeled and cannot be
    arranged in any particular order. Examples include gender (male, female), blood
    type (A, B, AB, O), or color (red, green, blue).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Ordinal data:** This is when the categories can be ordered or ranked. While
    the intervals between the categories are not equally spaced, the order of the
    categories has a meaning. Examples include rating scales (1 to 5 rating of a movie),
    an education level (high school, undergraduate, graduate), or stages of cancer
    (Stage I, Stage II, Stage III).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For imputing missing categorical data, the mode is typically used. In our example,
    the column property_category is categorical (nominal) data, and there’s data missing
    in two rows.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s replace the missing values with mode.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This code uses the **fillna()** function to replace all the NaN values in the
    property_category column. It replaces it with mode.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the [0] part is used to extract the first value from this Series.
    If there are multiple modes, this will select the first one. If there's only one
    mode, it still works fine.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Mastering the Art of Data Cleaning in Python](../Images/d5a4dc6cfbcae664ee2ba044bfc1cf82.png)'
  prefs: []
  type: TYPE_IMG
- en: The data now looks pretty good. The only thing that’s remaining is to see if
    there are outliers.
  prefs: []
  type: TYPE_NORMAL
- en: You can practice dealing with nulls on this [Meta interview question](https://platform.stratascratch.com/coding/9790-find-the-number-of-processed-and-not-processed-complaints-of-each-type?code_type=2&utm_source=blog&utm_medium=click&utm_campaign=kdn+data+cleaning),
    where you’ll have to replace NULLs with zeros.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with Outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Outliers are data points in a dataset that are distinctly different from the
    other observations. They may lie exceptionally far from the other values in the
    data set, residing outside an overall pattern. They're considered unusual due
    to their values either being significantly higher or lower compared to the rest
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Outliers can arise due to various reasons such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Measurement or input errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data corruption
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: True statistical anomalies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outliers can significantly impact the results of your data analysis and statistical
    modeling. They can lead to a skewed distribution, bias, or invalidate the underlying
    statistical assumptions, distort the estimated model fit, reduce the predictive
    accuracy of predictive models, and lead to incorrect conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: Some commonly used methods to detect outliers are Z-score, IQR (Interquartile
    Range), box plots, scatter plots, and data visualization techniques. In some advanced
    cases, machine learning methods are used as well.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing data can help identify outliers. Seaborn's boxplot is handy for
    this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We use the plt.figure() to set the width and height of the figure in inches.
  prefs: []
  type: TYPE_NORMAL
- en: Then we create the boxplot for the columns advertised_price and sale_price,
    which looks like this.
  prefs: []
  type: TYPE_NORMAL
- en: '![Mastering the Art of Data Cleaning in Python](../Images/eab301802d2a19402b4de84b5a41005f.png)'
  prefs: []
  type: TYPE_IMG
- en: The plot can be improved for easier use by adding this to the above code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We use the above code to set the labels for both axes. We also notice that the
    values on the y-axis are in the scientific notation, and we can’t use that for
    the price values. So we change this to plain style using the plt.ticklabel_format()
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Then we create the formatter that will show the values on the y-axis with commas
    as thousand separators and decimal dots. The last code line applies this to the
    axis.
  prefs: []
  type: TYPE_NORMAL
- en: The output now looks like this.
  prefs: []
  type: TYPE_NORMAL
- en: '![Mastering the Art of Data Cleaning in Python](../Images/484a574270a0fdf86aa68c9650010659.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, how do we identify and remove the outlier?
  prefs: []
  type: TYPE_NORMAL
- en: One of the ways is to use the IQR method.
  prefs: []
  type: TYPE_NORMAL
- en: IQR, or Interquartile Range, is a statistical method used to measure variability
    by dividing a data set into quartiles. Quartiles divide a rank-ordered data set
    into four equal parts, and values within the range of the first quartile (25th
    percentile) and the third quartile (75th percentile) make up the interquartile
    range.
  prefs: []
  type: TYPE_NORMAL
- en: 'The interquartile range is used to identify outliers in the data. Here''s how
    it works:'
  prefs: []
  type: TYPE_NORMAL
- en: First, calculate the first quartile (Q1), the third quartile (Q3), and then
    determine the IQR. The IQR is computed as Q3 - Q1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Any value below Q1 - 1.5IQR or above Q3 + 1.5IQR is considered an outlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On our boxplot, the box actually represents the IQR. The line inside the box
    is the median (or second quartile). The 'whiskers' of the boxplot represent the
    range within 1.5*IQR from Q1 and Q3.
  prefs: []
  type: TYPE_NORMAL
- en: Any data points outside these whiskers can be considered outliers. In our case,
    it’s the value of $12,000,000\. If you look at the boxplot, you’ll see how clearly
    this is represented, which shows why data visualization is important in detecting
    outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s remove the outliers by using the IQR method in Python code. First,
    we’ll remove the advertised price outliers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We first calculate the first quartile (or the 25th percentile) using the **quantile()**
    function. We do the same for the third quartile or the 75th percentile.
  prefs: []
  type: TYPE_NORMAL
- en: They show the values below which 25% and 75% of the data fall, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Then we calculate the difference between the quartiles. Everything so far is
    just translating the IQR steps into Python code.
  prefs: []
  type: TYPE_NORMAL
- en: As a final step, we remove the outliers. In other words, all data less than
    Q1 - 1.5 * IQR or more than Q3 + 1.5 * IQR.
  prefs: []
  type: TYPE_NORMAL
- en: The '~' operator negates the condition, so we are left with only the data that
    are not outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Then we can do the same with the sale price.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Of course, you can do it in a more succinct way using the **for loop**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The loop iterates of the two columns. For each column, it calculates the IQR
    and then removes the rows in the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that this operation is done sequentially, first for advertised_price
    and then for sale_price. As a result, the DataFrame is modified in-place for each
    column, and rows can be removed due to being an outlier in either column. Therefore,
    this operation might result in fewer rows than if outliers for advertised_price
    and sale_price were removed independently and the results were combined afterward.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, the output will be the same in both cases. To see how the box
    plot changed, we need to plot it again using the same code as earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here’s the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Mastering the Art of Data Cleaning in Python](../Images/02a67feb20614804d7f213806b8ba6c4.png)'
  prefs: []
  type: TYPE_IMG
- en: You can practice calculating percentiles in Python by solving the [General Assembly
    interview question](https://platform.stratascratch.com/coding/9611-find-the-80th-percentile-of-hours-studied?code_type=2&utm_source=blog&utm_medium=click&utm_campaign=kdn+data+cleaning).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data cleaning is a crucial step in the data analysis process. Though it can
    be time-consuming, it's essential to ensure the accuracy of your findings.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, Python's rich ecosystem of libraries makes this process more manageable.
    We learned how to remove unnecessary rows and columns, reformat data, and deal
    with missing values and outliers. These are the usual steps that have to be performed
    on most any data. However, you’ll also sometimes need to [combine two columns
    into one](https://platform.stratascratch.com/coding/9834-combine-the-first-and-last-names-of-workers?code_type=2&utm_source=blog&utm_medium=click&utm_campaign=kdn+data+cleaning),
    [verify the existing data](https://platform.stratascratch.com/coding/9737-verify-that-the-first-4-digits-are-equal-to-1415-for-all-phone-numbers?code_type=2&utm_source=blog&utm_medium=click&utm_campaign=kdn+data+cleaning),
    [assign labels to it](https://platform.stratascratch.com/coding/9628-reviews-bins-on-reviews-number?code_type=2&utm_source=blog&utm_medium=click&utm_campaign=kdn+data+cleaning),
    or [get rid of the white spaces](https://platform.stratascratch.com/coding/9818-file-contents-shuffle?code_type=2&utm_source=blog&utm_medium=click&utm_campaign=kdn+data+cleaning).
  prefs: []
  type: TYPE_NORMAL
- en: All this is data cleaning, as it allows you to turn messy, real-world data into
    a well-structured dataset that you can analyze with confidence. Just compare the
    dataset we started with to the one we ended up with.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t see the satisfaction in this result and the clean data doesn’t
    make you strangely excited, what in the world are you doing in data science!?
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://twitter.com/StrataScratch)****[Nate Rosidi](https://twitter.com/StrataScratch)****
    is a data scientist and in product strategy. He''s also an adjunct professor teaching
    analytics, and is the founder of StrataScratch, a platform helping data scientists
    prepare for their interviews with real interview questions from top companies.
    Nate writes on the latest trends in the career market, gives interview advice,
    shares data science projects, and covers everything SQL.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Mastering the Art of Data Storytelling: A Guide for Data Scientists](https://www.kdnuggets.com/2023/06/mastering-art-data-storytelling-guide-data-scientists.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Collection of Guides on Mastering SQL, Python, Data Cleaning, Data…](https://www.kdnuggets.com/collection-of-guides-on-mastering-sql-python-data-cleaning-data-wrangling-and-exploratory-data-analysis)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Steps to Mastering Data Cleaning with Python and Pandas](https://www.kdnuggets.com/7-steps-to-mastering-data-cleaning-with-python-and-pandas)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Steps to Mastering Data Cleaning and Preprocessing Techniques](https://www.kdnuggets.com/2023/08/7-steps-mastering-data-cleaning-preprocessing-techniques.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data storytelling - the art of telling stories through data](https://www.kdnuggets.com/2023/07/manning-data-storytelling-the-art-telling-stories-data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7 Steps to Master the Art of Data Storytelling](https://www.kdnuggets.com/7-steps-to-master-the-art-of-data-storytelling)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
