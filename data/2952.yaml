- en: “Please, explain.” Interpretability of machine learning models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “请解释一下。” 机器学习模型的可解释性
- en: 原文：[https://www.kdnuggets.com/2019/05/interpretability-machine-learning-models.html](https://www.kdnuggets.com/2019/05/interpretability-machine-learning-models.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/05/interpretability-machine-learning-models.html](https://www.kdnuggets.com/2019/05/interpretability-machine-learning-models.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Olga Mierzwa-Sulima](https://www.linkedin.com/in/olga-mierzwa-sulima-84843539/),
    Appsilon**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[Olga Mierzwa-Sulima](https://www.linkedin.com/in/olga-mierzwa-sulima-84843539/)，Appsilon**。'
- en: '[Original](https://appsilon.com/please-explain-black-box/). Reposted with permission.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://appsilon.com/please-explain-black-box/)。经许可转载。'
- en: In February 2019 Polish government added an amendment to a banking law that
    gives a customer a right to receive an explanation in case of a negative credit
    decision. It’s one of the direct consequences of implementing GDPR in EU. This
    means that a bank needs to be able to explain why the loan wasn’t granted if the
    decision process was automatic.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年2月，波兰政府在银行法中新增了一项修正案，赋予客户在遭遇负面信用决策时获得解释的权利。这是实施GDPR在欧盟的直接后果之一。这意味着，如果决策过程是自动化的，银行需要能够解释为什么贷款没有被批准。
- en: In October 2018 world headlines reported about [Amazon AI recruiting tool](https://www.theguardian.com/technology/2018/oct/10/amazon-hiring-ai-gender-bias-recruiting-engine) that
    favored men. Amazon’s model was trained on biased data that were skewed towards
    male candidates. It has built rules that penalized resumes that included the word
    “women’s”.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年10月，全球头条报道了一个[亚马逊AI招聘工具](https://www.theguardian.com/technology/2018/oct/10/amazon-hiring-ai-gender-bias-recruiting-engine)偏爱男性。亚马逊的模型在有偏的数据上进行训练，这些数据倾向于男性候选人。它建立了惩罚包含“女性”一词的简历的规则。
- en: '**Consequences of not understanding models’ predictions**'
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**不了解模型预测的后果**'
- en: What is common for the two examples above is that both models in the banking
    industry and the one built by Amazon are very complex tools, so-called black-box
    classifiers, that don’t offer straightforward and human-interpretable decision
    rules.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 上述两个例子的共同点是，银行业的模型和亚马逊构建的模型都是非常复杂的工具，所谓的黑箱分类器，这些工具没有提供直接且易于人类理解的决策规则。
- en: Financial institutions will have to invest in model interpretability research
    if they want to continue using ML-based solutions. And they probably will, because
    such algorithms are more accurate in predicting credit risk. Amazon on the other
    hand, could have saved a lot of money and bad press if the model was properly
    validated and understood.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果金融机构希望继续使用基于机器学习的解决方案，它们将不得不投资于模型可解释性研究。而且它们很可能会这样做，因为这种算法在预测信用风险方面更为准确。另一方面，亚马逊如果对模型进行适当的验证和理解，可能会节省大量金钱和负面新闻。
- en: '**Why now? Trends in data modeling.**'
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**为什么现在？数据建模的趋势。**'
- en: Machine learning has continued to stay on the top of Gartner’s Hype Cycle since
    2014, to be replaced by the Deep Learning (a form of ML) in 2018 suggesting the
    adoption hasn’t reached its peak yet.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自2014年以来，机器学习一直保持在Gartner的炒作周期榜单上，2018年被深度学习（机器学习的一种形式）取代，这表明机器学习的采用尚未达到顶峰。
- en: '![](../Images/57e65fff7ae21083675f67d428b48500.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57e65fff7ae21083675f67d428b48500.png)'
- en: '[**Source**](https://www.gartner.com/smarterwithgartner/5-trends-emerge-in-gartner-hype-cycle-for-emerging-technologies-2018/)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[**来源**](https://www.gartner.com/smarterwithgartner/5-trends-emerge-in-gartner-hype-cycle-for-emerging-technologies-2018/)'
- en: Machine learning growth is predicted to further accelerate. Based on the [report](http://www.univa.com/resources/univa-machine-learning-survey.php) by
    Univa 96% of the companies are expected to use ML in production in the next 2
    years.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 预测机器学习的增长将进一步加速。根据[Univa的报告](http://www.univa.com/resources/univa-machine-learning-survey.php)，预计在接下来的两年内，96%的公司将会在生产中使用机器学习。
- en: 'The reasons behind this are: widespread data collection, availability of vast
    computation resources and active open-source community. ML adoption growth is
    accompanied by the increase in ML-interpretability research driven by regulations
    like GDPR, EU’s “right to explain”, concerns about safety (medicine, autonomous
    vehicles), reproducibility and bias or end-users expectations (debug the model
    to improve it or learn something new about the studied subject).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 造成这种情况的原因有：数据收集的广泛性、庞大的计算资源的可用性和活跃的开源社区。机器学习的采用增长伴随着机器学习可解释性研究的增加，这种研究受到如GDPR、欧盟的“解释权”、安全（医学、自动驾驶车辆）、可重复性、偏见或最终用户期望（调试模型以改进或了解有关研究主题的新信息）等法规的推动。
- en: '![](../Images/d41678e8798ddf49f1a5a07c61346c62.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d41678e8798ddf49f1a5a07c61346c62.png)'
- en: '**[Source](http://people.csail.mit.edu/beenkim/papers/BeenK_FinaleDV_ICML2017_tutorial.pdf)**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**[来源](http://people.csail.mit.edu/beenkim/papers/BeenK_FinaleDV_ICML2017_tutorial.pdf)**'
- en: '**Black-box algorithms interpretability possibilities**'
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**黑盒算法的可解释性可能性**'
- en: As data scientists, we should be able to provide an explanation to end users
    about how a model works. However, this not necessarily means understanding every
    piece of the model or generating a set of decision rules.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家，我们应该能够向最终用户解释模型的工作原理。然而，这并不一定意味着要理解模型的每一个部分或生成一套决策规则。
- en: 'There could also be a case where this is not required:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 也有可能存在这种情况：这不是必需的：
- en: problem is well studied,
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题已经被很好地研究，
- en: model results has no consequences,
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型结果没有后果，
- en: understanding the model by the end-user could pose a risk of gaming the system.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 终端用户对模型的理解可能会带来系统被操控的风险。
- en: If we look at the results from the [Kaggle’s Machine Learning and Data Science
    Survey](https://www.kaggle.com/sudhirnl7/data-science-survey-2018) from 2018,
     around 60% of respondents think they could explain most of machine learning models
    (some models were still hard to explain for them). The most common approach used
    to ML understanding is analyzing model features by looking at feature importance
    and feature correlations.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看2018年[Kaggle的机器学习和数据科学调查](https://www.kaggle.com/sudhirnl7/data-science-survey-2018)的结果，大约60%的受访者认为他们能够解释大多数机器学习模型（虽然一些模型仍然难以解释）。最常用的机器学习理解方法是通过查看特征重要性和特征相关性来分析模型特征。
- en: '**Feature importance analysis** offers first good insights into what the model
    is learning and what factors might be important. However, this technique can be
    unreliable if features are correlated. It can provide good insights only if model
    variables are interpretable. For many [GBMs](https://towardsdatascience.com/boosting-algorithm-gbm-97737c63daa3) libraries
    it’s fairly easy to generate [feature importance plots](https://www.r-bloggers.com/variable-importance-plot-and-variable-selection/).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征重要性分析**提供了对模型学习内容和可能重要因素的初步良好见解。然而，如果特征之间存在相关性，这种技术可能不可靠。只有当模型变量是可解释的时，它才会提供良好的见解。对于许多[GBM](https://towardsdatascience.com/boosting-algorithm-gbm-97737c63daa3)库，生成[特征重要性图](https://www.r-bloggers.com/variable-importance-plot-and-variable-selection/)相对容易。'
- en: In the case of **Deep Learning** situation is much more complicated. When using
    neural networks you could look at weights, as they contain the information about
    the input, but the information is compressed. What’s more, you can only analyze
    the connections on the first level, since on further levels it’s too complicated.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在**深度学习**的情况下，情况要复杂得多。当使用神经网络时，你可以查看权重，因为它们包含关于输入的信息，但这些信息是压缩的。而且，你只能分析第一层的连接，因为在更深层次上太复杂了。
- en: No wonder that when in 2016 [**LIME **(Local Interpretable Model-Interpretable
    Explanations) ](https://arxiv.org/abs/1602.04938)paper was presented at NIPS conference
    it had a huge impact. The idea behind LIME is to locally approximate a black-box
    model with an easier to understand white-box model constructed on interpretable
    input data. It has proven great results providing [interpretation for image classification](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime) and [text](https://christophm.github.io/interpretable-ml-book/lime.html#lime-for-text).
    However, for tabular data, it’s difficult to find interpretable features and their
    local interpretation might be misleading.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 难怪在2016年，[**LIME**（局部可解释模型-可解释性解释）](https://arxiv.org/abs/1602.04938)论文在NIPS会议上发布时产生了巨大影响。LIME的理念是通过在可解释的输入数据上构建一个更易理解的白盒模型，来局部逼近一个黑盒模型。它在[图像分类的解释](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime)和[文本](https://christophm.github.io/interpretable-ml-book/lime.html#lime-for-text)方面取得了良好的结果。然而，对于表格数据来说，找到可解释的特征非常困难，而且它们的局部解释可能会产生误导。
- en: LIME is implemented in Python ([lime](https://github.com/marcotcr/lime) and [Skater](https://github.com/datascienceinc/Skater))
    and R ([lime package](https://cran.r-project.org/web/packages/lime/index.html) and [iml
    package](https://cran.r-project.org/web/packages/iml/index.html), [live package](https://cloud.r-project.org/web/packages/live/index.html))
    and is very easy to use.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: LIME在Python中实现（[lime](https://github.com/marcotcr/lime)和[Skater](https://github.com/datascienceinc/Skater)）以及R中（[lime包](https://cran.r-project.org/web/packages/lime/index.html)和[iml包](https://cran.r-project.org/web/packages/iml/index.html)，[live包](https://cloud.r-project.org/web/packages/live/index.html)），使用起来非常方便。
- en: Another promising idea is [SHAP (Shapley Additive Explanations)](https://arxiv.org/abs/1705.07874).
    It’s based on game theory. It assumes that features are players, models are coalitions
    and Shapley values tell how to fairly distribute the “payout” among the features.
    This technique distributes the effects fairly, is easy to use and offers visually
    compelling implementation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有前景的想法是 [SHAP (Shapley Additive Explanations)](https://arxiv.org/abs/1705.07874)。它基于博弈论。它假设特征是参与者，模型是联盟，而Shapley值则告诉我们如何公平地分配“收益”。这种技术公平地分配影响，使用简单，并提供了视觉上令人信服的实现。
- en: '[**DALEX** package](https://github.com/pbiecek/DALEX) (Descriptive Machine
    Learning Explanations) available in R offers a set of tools that help to understand
    how complex models are working. Using DALEX you can create model explainer and
    inspect it visually e.g. breakdown plots. You might also be interested in [DrWhy.Ai](https://github.com/ModelOriented/DrWhy/blob/master/README.md) which
    is developed by the same group of researchers as DALEX.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[**DALEX** 包](https://github.com/pbiecek/DALEX)（描述性机器学习解释）在R中提供了一套工具，帮助理解复杂模型的工作原理。使用DALEX，你可以创建模型解释器并进行视觉检查，例如分解图。你可能还会对
    [DrWhy.Ai](https://github.com/ModelOriented/DrWhy/blob/master/README.md) 感兴趣，它是由与DALEX相同的研究团队开发的。'
- en: '**Practical use cases**'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**实际应用案例**'
- en: '**Detecting objects on the pictures**'
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**检测图片中的对象**'
- en: '**Image recognition** is already widely used, among others in autonomous cars
    to detect if cars, traffic lights etc. are on the picture, in wildlife conservation
    to detect if a certain animal is in the picture or in the insurance to detect
    flooding of crops.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**图像识别** 已经被广泛使用，例如在自动驾驶汽车中检测图片中的汽车、交通灯等，在野生动物保护中检测图片中的特定动物，或在保险中检测作物的洪水情况。'
- en: We will use the “Husky vs Wolf example” from the original LIME paper to illustrate
    the importance of model interpretation. The classifier task was to identify if
    a wolf was on the picture or not. It falsely misclassified Siberian Husky as a
    wolf. Thanks to LIME researchers were able to identify what areas of the pictures
    were important for the model. It turned out that if the picture contains snow
    it is classified as a wolf.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用原始LIME论文中的“哈士奇与狼的例子”来说明模型解释的重要性。分类任务是识别图片中是否有狼。它错误地将西伯利亚哈士奇误判为狼。多亏了LIME，研究人员能够确定图片中哪些区域对模型的重要性。结果发现，如果图片中有雪，它会被分类为狼。
- en: '![](../Images/9f4faf0e9878b6d177fc2001ead3de57.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f4faf0e9878b6d177fc2001ead3de57.png)'
- en: The algorithm was using the background of the picture and totally ignoring animal
    characteristics. The model should look at the animal eyes instead. Thanks to this
    discovery it was possible to fix the model and extend the training examples to
    prevent the reasoning snow = wolf.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法使用了图片的背景，完全忽略了动物的特征。模型应该关注动物的眼睛。得益于这一发现，模型得以修正，并扩展了训练样本，以防止推理中的雪=狼问题。
- en: '**Classification as decision support system**'
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**分类作为决策支持系统**'
- en: Intensive Care Unit of Amsterdam UMC [wants predict the probabilities of patient’s
    readmission and/or mortality at the moment of discharge](https://medium.com/@Pacmedhealth/ai-for-health-care-tackling-the-issue-of-interpretability-868be42aaf50).
    The goal is to help doctors pick the right moment to move the patient from ICU.
    If the doctor understands what the model is doing is more likely to use it’s recommendation
    in making the final judgement.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 阿姆斯特丹UMC的重症监护病房 [希望预测患者出院时的再入院和/或死亡概率](https://medium.com/@Pacmedhealth/ai-for-health-care-tackling-the-issue-of-interpretability-868be42aaf50)。目标是帮助医生选择合适的时机将患者从ICU转移。如果医生理解模型的工作原理，他们更可能在做出最终判断时使用其建议。
- en: In order to demonstrate how such model can be interpreted using LIME, we can
    have a look at the example from [another study](https://www.researchgate.net/publication/309551203_Machine_Learning_Model_Interpretability_for_Precision_Medicine) that
    aims to do early prediction of the mortality at the ICU. Random Forest model (a
    black-box model) is used to predict mortality status and lime package is used
    to locally explain the prediction score for every patient.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示如何使用LIME解释模型，我们可以查看 [另一项研究](https://www.researchgate.net/publication/309551203_Machine_Learning_Model_Interpretability_for_Precision_Medicine)
    的例子，该研究旨在进行ICU中的早期死亡预测。使用随机森林模型（黑箱模型）来预测死亡状态，并使用lime包来局部解释每位患者的预测分数。
- en: '![](../Images/f78b78b2133323e495c6249e5d6381b4.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f78b78b2133323e495c6249e5d6381b4.png)'
- en: '[**Source**](https://www.researchgate.net/publication/309551203_Machine_Learning_Model_Interpretability_for_Precision_Medicine)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[**来源**](https://www.researchgate.net/publication/309551203_Machine_Learning_Model_Interpretability_for_Precision_Medicine)'
- en: A patient from the selected example has high death probability (78%). The model
    features that contribute to mortality are higher counts of atrial fibrillation
    and higher lactate level, which is consistent with current medical understanding.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 选定示例中的一位患者死亡概率很高（78%）。模型特征中对死亡率有贡献的因素是较高的心房颤动次数和较高的乳酸水平，这与当前医学理解一致。
- en: '**Humans and machines – a perfect match**'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**人类与机器——完美的匹配**'
- en: In order to achieve success in building an interpretable AI we need to combine
    data science knowledge, algorithms and end users expertise. Data science work
    doesn’t finish after creating the model. It’s an iterative, usually long process
    with feedback loops provided by the experts, making sure the outcome is solid
    and understandable by humans.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了成功构建可解释的 AI，我们需要结合数据科学知识、算法和终端用户的专业知识。数据科学的工作并不会在创建模型后结束。这是一个迭代的、通常较长的过程，需要专家提供反馈，以确保结果是稳健的并且易于人类理解。
- en: 'We strongly believe that by combining humans expertise and machines performance
    we can obtain the best conclusion: improve machine results and overcome human
    gut-feel bias.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们坚信，通过结合人类的专业知识和机器的性能，我们可以得出最佳结论：提升机器结果，克服人类直觉偏差。
- en: '**Bio**: [Olga Mierzwa-Sulima](https://www.linkedin.com/in/olga-mierzwa-sulima-84843539/) is
    a Senior Data Scientist and Project Leader at Appsilon.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**简历**： [Olga Mierzwa-Sulima](https://www.linkedin.com/in/olga-mierzwa-sulima-84843539/)
    是 Appsilon 的高级数据科学家和项目负责人。'
- en: '**Resources:**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：**'
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在线和基于网页的：分析、数据挖掘、数据科学、机器学习教育](https://www.kdnuggets.com/education/online.html)'
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分析、数据科学、数据挖掘和机器学习的软件](https://www.kdnuggets.com/software/index.html)'
- en: '**Related:**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Are BERT Features InterBERTible?](https://www.kdnuggets.com/2019/02/bert-features-interbertible.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BERT 特征是否可以互换？](https://www.kdnuggets.com/2019/02/bert-features-interbertible.html)'
- en: '[Artificial Intelligence and Data Science Advances in 2018 and Trends for 2019](https://www.kdnuggets.com/2019/02/ai-data-science-advances-trends.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2018 年人工智能和数据科学进展及 2019 年趋势](https://www.kdnuggets.com/2019/02/ai-data-science-advances-trends.html)'
- en: '[The year in AI/Machine Learning advances: Xavier Amatriain 2018 Roundup](https://www.kdnuggets.com/2019/01/xamat-ai-machine-learning-roundup.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2018 年 AI/机器学习进展：Xavier Amatriain 总结](https://www.kdnuggets.com/2019/01/xamat-ai-machine-learning-roundup.html)'
- en: '* * *'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Explain NLP Models with LIME](https://www.kdnuggets.com/2022/01/explain-nlp-models-lime.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 LIME 解释 NLP 模型](https://www.kdnuggets.com/2022/01/explain-nlp-models-lime.html)'
- en: '[Using SHAP Values for Model Interpretability in Machine Learning](https://www.kdnuggets.com/2023/08/shap-values-model-interpretability-machine-learning.html)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 SHAP 值进行机器学习模型解释性](https://www.kdnuggets.com/2023/08/shap-values-model-interpretability-machine-learning.html)'
- en: '[Simplifying Decision Tree Interpretability with Python & Scikit-learn](https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 和 Scikit-learn 简化决策树解释](https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)'
- en: '[SHAP: Explain Any Machine Learning Model in Python](https://www.kdnuggets.com/2022/11/shap-explain-machine-learning-model-python.html)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SHAP：在 Python 中解释任何机器学习模型](https://www.kdnuggets.com/2022/11/shap-explain-machine-learning-model-python.html)'
- en: '[Why Do Machine Learning Models Die In Silence?](https://www.kdnuggets.com/2022/01/machine-learning-models-die-silence.html)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么机器学习模型在沉默中消亡？](https://www.kdnuggets.com/2022/01/machine-learning-models-die-silence.html)'
- en: '[Working With Sparse Features In Machine Learning Models](https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在机器学习模型中处理稀疏特征](https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html)'
