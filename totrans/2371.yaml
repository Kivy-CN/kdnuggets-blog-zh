- en: Choosing the Right Clustering Algorithm for Your Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/10/right-clustering-algorithm.html](https://www.kdnuggets.com/2019/10/right-clustering-algorithm.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Data clustering is an essential step in the arrangement of a correct and throughout
    data model. To fulfill an analysis, the volume of information should be sorted
    out according to the commonalities. The main question is, what commonality parameter
    provides the best results – and what is implicated under “the best” definition
    at all.
  prefs: []
  type: TYPE_NORMAL
- en: This article should be useful for the beginning data scientists, or for experts
    who want to refresh their memories on the topic. It includes the most widespread
    clustering algorithms, as well as their insightful review. Depending on the particularities
    of each method, the recommendations considering their application are provided.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Four Basic Algorithms And How To Choose One
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on the clusterization models, four common classes of algorithms are
    differentiated. There are no less than 100 algorithms in general, but their popularity
    is rather moderate, as well as their field of application.
  prefs: []
  type: TYPE_NORMAL
- en: Connectivity-based Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clusterization, based on the computation of distances between the objects of
    the whole dataset, is called **connectivity-based**, or hierarchical. Depending
    on the “direction” of the algorithm, it can unite or, inversely, divide the array
    of information – the names *agglomerative* and *divisive* appeared from this exact
    variation. The most popular and reasonable type is the agglomerative one, where
    you start by inputting the number of data points, that then are subsequently united
    into larger and larger clusters, until the limit is reached.
  prefs: []
  type: TYPE_NORMAL
- en: The most prominent example of connectivity-based clusterization is the classification
    of plants. The “tree” of dataset starts with a particular species and ends with
    a few kingdoms of plants, each consisting of even smaller clusters (phyla, classes,
    orders, etc.)
  prefs: []
  type: TYPE_NORMAL
- en: 'After applying one of the connectivity-based algorithms you receive a dendrogram
    of data, that presents you the structure of the information rather than its distinct
    separation on clusters. Such a feature may possess both the benefit or the harm:
    the complexity of the algorithm may turn out to be excessive or simply inapplicable
    for datasets with little to no hierarchy. It also shows poor performance: due
    to the abundance of iterations, complete processing will take an unreasonable
    amount of time. On top of that, you won’t get a precise structure using the hierarchical
    algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/751b46e72c63821f823138179b1ba200.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image source](https://genomicsclass.github.io/book/pages/figure/clustering_and_heatmaps-color_dendrogram-1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, the incoming data required from the counter comes down to
    the number of data points, that doesn’t influence the final result substantially,
    or the preset distance metric, that is coarse-measured and approximate as well.
  prefs: []
  type: TYPE_NORMAL
- en: Centroid-based Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Centroid-based** clustering, from my experience, is the most frequently occurred
    model thanks to its comparative simplicity. The model is aimed at classifying
    each object of the dataset to the particular cluster. The number of clusters (*k*)
    is chosen randomly, which is probably the greatest “weakness” of the method. This
    *k-means* algorithm is especially popular in machine learning thanks to the alikeness
    with [k-nearest neighbors](https://www.kaggle.com/chavesfm/tuning-parameters-for-k-nearest-neighbors-iris)
    (kNN) method.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a44928d1bd4107472b395c5b0f895bf.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image source](https://www.mathworks.com/help/examples/stats/win64/AssignNewDataToExistingClustersAndGenerateCodeExample_01.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The process of calculation consists of multiple steps. Firstly, the incoming
    data is chosen, which is the rough number of the clusters the dataset should be
    divided into. The centers of clusters should be situated as far as possible from
    each other – that will increase the accuracy of the result.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, the algorithm finds distances between each object of the dataset and
    every cluster. The smallest coordinate (if we’re talking about graphical representation)
    determines to which cluster the object is moved.
  prefs: []
  type: TYPE_NORMAL
- en: After that, the center of the cluster is recalculated according to the means
    of all objects’ coordinates. The first step of the algorithm repeats, but with
    a new center of the cluster that was recomputed. Such iterations continue unless
    certain conditions are reached. For example, the algorithm may end when the center
    of the cluster hasn’t moved or moved insignificantly from the previous iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the simplicity – both mathematical and coding – k-means has some drawbacks
    that don’t allow me to use it everywhere possible. That includes:'
  prefs: []
  type: TYPE_NORMAL
- en: a negligent edge of each cluster, because the priorities are set on the center
    of the cluster, not on its borders;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an inability to create a structure of a dataset with objects that can be classified
    to multiple clusters in equal measure;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a need to guess the optimal *k* number, or a need to make preliminary calculations
    to specify this gauge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expectation-maximization Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Expectation-maximization** algorithm, at the same time, allows avoiding those
    complications while providing an even higher level of accuracy. Simply put, it
    calculates the relation probability of each dataset point to all the clusters
    we’ve specified. The main “tool” that is used for this clusterization model is
    **Gaussian Mixture Models (GMM)** – the assumption that the points of the dataset
    generally follow the [Gaussian distribution](https://www.encyclopedia.com/science-and-technology/mathematics/mathematics/normal-distribution#3).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The k-means algorithm is, basically, a simplified version of the EM principle.
    They both require manual input of clusters number, and that’s the main intricacy
    the methods bear. Apart from that, the principles of computing (either for GMM
    or k-means) are simple: the approximate range of cluster is specified gradually
    with each new iteration.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike the centroid-based models, the EM algorithm allows the points to classify
    for two or more clusters – it simply presents you the possibility of each event,
    using which you’re able to conduct further analysis. More to that, the borders
    of each cluster compose ellipsoids of different measures unlike k-means, where
    the cluster is visually represented as a circle. However, the algorithm simply
    would not work for datasets where objects do not follow the Gaussian distribution.
    That is the main disadvantage of the method: it is more applicable to theoretical
    problems rather than the actual measurements or observations.'
  prefs: []
  type: TYPE_NORMAL
- en: Density-based Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, the unofficial favorite of [data scientists’ hearts](http://www.mastersindatascience.org/careers/data-scientist/),
    **density-based clustering** comes. The name comprises the main point of the model
    – to divide the dataset into clusters the counter inputs the ε parameter, the
    “neighborhood” distance. If the object is located within the circle (sphere) of
    the ε radius, it, therefore, relates to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/564c5cbe622f373106b85a379f77f1e0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image source](https://www.datanovia.com/en/wp-content/uploads/dn-tutorials/005-advanced-clustering/figures/023-dbscan-density-based-clustering-density-based-clustering-1.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Step by step, DBSCAN (Density-Based Spatial Clustering of Applications with
    Noise) algorithm checks every object, changes its status to “viewed,” classifies
    it to the cluster OR noise, until finally the whole dataset is processed. The
    clusters determined with DBSCAN can have arbitrary shapes, thereby are extremely
    accurate. Besides, the algorithm doesn’t make you calculate the number of clusters
    – it is determined automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Still, even such a masterpiece as DBSCAN has a drawback. If the dataset consists
    of variable density clusters, the method shows poor results. It also might not
    be your choice if the placement of objects is too close, and the ε parameter can’t
    be estimated easily.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Summing up, there is no such thing as a wrong-chosen algorithm – some of them
    are just more suitable for the particular dataset structures. In order to always
    pick up the best (read – more suitable) algorithm, you need to have a throughout
    understanding of their advantages, disadvantages, and peculiarities.
  prefs: []
  type: TYPE_NORMAL
- en: Some algorithms could be excluded from the very beginning if they, for example,
    do not correspond with the dataset specifications. To avoid odd work, you can
    spend a little time memorizing the information instead of choosing the path of
    trial and error and learning from your own mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: We wish you to always pick the best algorithm at first. Keep up the great job!
  prefs: []
  type: TYPE_NORMAL
- en: '**[Josh Thompson](https://blog.powertofly.com/u/joshthompson)** is a Lead Editor
    at [Masters in Data Science](https://www.mastersindatascience.org). He is in charge
    of writing case studies, how-tos, and blog posts on AI, ML, Big Data, and hard
    work of data specialists. In his leisure time, he adores swimming and motorcycling.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Unlock the Secrets to Choosing the Perfect Machine Learning Algorithm!](https://www.kdnuggets.com/2023/07/ml-algorithm-choose.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Clustering Unleashed: Understanding K-Means Clustering](https://www.kdnuggets.com/2023/07/clustering-unleashed-understanding-kmeans-clustering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[DBSCAN Clustering Algorithm in Machine Learning](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What is K-Means Clustering and How Does its Algorithm Work?](https://www.kdnuggets.com/2023/05/kmeans-clustering-algorithm-work.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ChatGPT-Powered Data Exploration: Unlock Hidden Insights in Your Dataset](https://www.kdnuggets.com/2023/07/chatgptpowered-data-exploration-unlock-hidden-insights-dataset.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An Easy Guide to Choose the Right Machine Learning Algorithm](https://www.kdnuggets.com/2020/05/guide-choose-right-machine-learning-algorithm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
