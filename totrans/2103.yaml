- en: The Easiest Way of Running Llama 3 Locally
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/easiest-way-of-running-llama-3-locally](https://www.kdnuggets.com/easiest-way-of-running-llama-3-locally)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Most Easiest Way of Running Llama 3 Locally](../Images/39060dd5295993a6ff5cdf6171b3a640.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Running LLMs (Large Language Models) locally has become popular as it provides
    security, privacy, and more control over model outputs. In this mini tutorial,
    we learn the easiest way of downloading and using the Llama 3 model.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Llama 3 is Meta AI's latest family of LLMs. It is open-source, comes with advanced
    AI capabilities, and improves response generation compared to Gemma, Gemini, and
    Claud 3.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: What is Ollama?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Ollama/ollama](https://github.com/ollama/ollama) is an open-source tool for
    using LLMs like Llama 3 on your local machine. With new research and development,
    these large language models do not require large VRam, computing, or storage.
    Instead, they are optimized for use in laptops.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple tools and frameworks available for you to use LLMs locally,
    but Ollama is the easiest to set up and use. It lets you use LLMs directly from
    a terminal or Powershell. It is fast and comes with core features that will make
    you start using it immediately.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: The best part of Ollama is that it integrates with all kinds of software, extensions,
    and applications. For example, you can use the CodeGPT extension in VScode and
    connect Ollama to start using Llama 3 as your AI code assistant.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Installing Ollama
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Download and Install Ollama by going to the GitHub repository [Ollama/ollama](https://github.com/ollama/ollama),
    scrolling down, and clicking the download link for your operating system.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![Download option for various operating systems of Ollama](../Images/6e8b4e26d87d3166233ef73014ddd816.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: Image from [ollama/ollama](https://github.com/ollama/ollama) | Download option
    for various operating systems
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: After Ollama is successfully installed it will show in the system tray as shown
    below.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![Ollama in system tray](../Images/2f89a1204d44f371b2c87bcdc44c667b.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: Downloading and Using Llama 3
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To download the Llama 3 model and start using it, you have to type the following
    command in your terminal/shell.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Depending on your internet speed, it will take almost 30 minutes to download
    the 4.7GB model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![PowerShell: downloading the Llama 3 using Ollama](../Images/5723d8bfd58789ad1627b12f584e983f.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: Apart from the Llama 3 model, you can also install other LLMs by typing the
    commands below.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![Running other LLMs using Ollama](../Images/ed95b36b560e7818b171650123da7288.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: Image from [ollama/ollama](https://github.com/ollama/ollama) | Running other
    LLMs using Ollama
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: As soon as downloading is completed, you will be able to use the LLama 3 locally
    as if you are using it online.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt:** *"Describe a day in the life of a Data Scientist."*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![Using Llama 3 in Ollama](../Images/0988f34affd21f6b2648f83ad2ac06e2.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: To demonstrate how fast the response generation is, I have attached the GIF
    of Ollama generating Python code and then explaining it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** If you have Nvidia GPU on your laptop and CUDA installed, Ollama
    will automatically use GPU instead of CPU to generate a response. Which is 10
    better.'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Prompt:** *"Write a Python code for building the digital clock."*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![Checking the speed of Llama 3 response generation on GPU using Ollama](../Images/413943f3b41ecb21cc7e968434a65a19.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: You can exit the chat by typing `/bye` and then start again by typing `ollama
    run llama3`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Open-source frameworks and models have made AI and LLMs accessible to everyone.
    Instead of being controlled by a few corporations, these locally run tools like
    Ollama make AI available to anyone with a laptop.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Using LLMs locally provides privacy, security, and more control over response
    generation. Moreover, you don't have to pay to use any service. You can even create
    your own AI-powered coding assistant and use it in VSCode.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn about other applications to run LLMs locally, then you
    should read [5 Ways To Use LLMs On Your Laptop](/5-ways-to-use-llms-on-your-laptop).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.polywork.com/kingabzpro)****[Abid Ali Awan](https://www.polywork.com/kingabzpro)****
    ([@1abidaliawan](https://www.linkedin.com/in/1abidaliawan)) is a certified data
    scientist professional who loves building machine learning models. Currently,
    he is focusing on content creation and writing technical blogs on machine learning
    and data science technologies. Abid holds a Master''s degree in technology management
    and a bachelor''s degree in telecommunication engineering. His vision is to build
    an AI product using a graph neural network for students struggling with mental
    illness.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[A Simple Guide to Running LlaMA 2 Locally](https://www.kdnuggets.com/a-simple-guide-to-running-llama-2-locally)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Easiest Way to Make Beautiful Interactive Visualizations With Pandas](https://www.kdnuggets.com/2021/12/easiest-way-make-beautiful-interactive-visualizations-pandas.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Llama, Llama, Llama: 3 Simple Steps to Local RAG with Your Content](https://www.kdnuggets.com/3-simple-steps-to-local-rag-with-your-content)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Ollama Tutorial: Running LLMs Locally Made Super Simple](https://www.kdnuggets.com/ollama-tutorial-running-llms-locally-made-super-simple)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using Groq Llama 3 70B Locally: Step by Step Guide](https://www.kdnuggets.com/using-groq-llama-3-70b-locally-step-by-step-guide)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[本地使用 Groq Llama 3 70B：逐步指南](https://www.kdnuggets.com/using-groq-llama-3-70b-locally-step-by-step-guide)'
- en: '[Run an LLM Locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在本地使用 LM Studio 运行 LLM](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio)'
