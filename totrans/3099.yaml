- en: An Intuitive Introduction to Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/06/intuitive-introduction-gradient-descent.html](https://www.kdnuggets.com/2018/06/intuitive-introduction-gradient-descent.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Keshav Dhandhania](https://www.linkedin.com/in/keshav-dhandhania-b3246028/)
    and [Savan Visalpara](https://www.linkedin.com/in/savanvisalpara/)**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient Descent** is one of the most popular and widely used **optimization
    algorithms**. Given a [machine learning](https://www.commonlounge.com/discussion/9325cf512f514e21815ec4c2e2e6e0e3) model
    with parameters (weights and biases) and a cost function to evaluate how good
    a particular model is, our learning problem reduces to that of finding a good
    set of weights for our model which minimizes the cost function.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc63f6509182c24acc1f8db752d95b28.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient descent is an **iterative method**. We start with some set of values
    for our model parameters (weights and biases), and improve them slowly. To improve
    a given set of weights, we try to get a sense of the value of the cost function
    for weights similar to the current weights (by calculating the gradient) and move
    in the direction in which the cost function reduces. By repeating this step thousands
    of times we’ll continually minimize our cost function.
  prefs: []
  type: TYPE_NORMAL
- en: Pseudocode for Gradient Descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gradient descent is used to minimize a cost function *J(w)* parametrized by
    model parameters ***w***. The gradient (or derivative) tells us the incline or
    slope of the cost function. Hence, to minimize the cost function, we move in the
    direction opposite to the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: '**initialize**the weights *w* randomly'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**calculate the gradients** G of cost function w.r.t parameters [1]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**update the weights** by an amount proportional to G, i.e. *w* = *w - *ηG'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: repeat till cost *J*(*w*) stops reducing or other pre-defined **termination
    criteria**is met
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In step 3, η is the **learning rate** which determines the size of the steps
    we take to reach a minimum. We need to be very careful about this parameter since
    high values of η may overshoot the minimum and very low value will reach minimum
    very slowly.
  prefs: []
  type: TYPE_NORMAL
- en: A popular sensible choice for the termination criteria is that the cost *J*(*w*)
    stops reducing on the *validation* dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Intuition for Gradient Descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine you’re blind folded in a rough terrain, and your objective is to reach
    the lowest altitude. One of the simplest strategies you can use, is to feel the
    ground in every direction, and take a step in the direction where the ground is
    descending the fastest. If you keep repeating this process, you might end up at
    the lake, or even better, somewhere in the huge valley.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/59bb9127f3f9f358f23b88646516e848.png)'
  prefs: []
  type: TYPE_IMG
- en: The rough terrain is analogous to the cost function. Minimizing the cost function
    is analogous to trying to reach lower altitudes. You are blind folded, since we
    don’t have the luxury of evaluating (seeing) the value of the function for every
    possible set of parameters. Feeling the slope of the terrain around you is analogous
    to calculating the gradient, and taking a step is analogous to one iteration of
    update to the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: By the way — as a small aside — this tutorial is part of the [free Data Science
    Course](https://www.commonlounge.com/discussion/367fb21455e04c7c896e9cac25b11b47) and [free
    Machine Learning Course](https://www.commonlounge.com/discussion/33a9cce246d343dd85acce5c3c505009%2Fmain) on [Commonlounge](https://www.commonlounge.com/).
    The courses include many hands-on assignments and projects. If you’re interested
    in learning Data Science / ML, definitely recommend checking it out.
  prefs: []
  type: TYPE_NORMAL
- en: Variants of Gradient Descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are multiple variants of gradient descent depending on how much of the
    data is being used to calculate the gradient. The main reason behind these variations
    is computational efficiency. A dataset may have millions of data points, and calculating
    the gradient over the entire dataset can be computationally expensive.
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch gradient descent** computes the gradient of the cost function w.r.t
    to parameter w for **entire training data**. Since we need to calculate the gradients
    for the whole dataset to perform one parameter update, batch gradient descent
    can be very slow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic gradient descent (SGD)** computes the gradient for each update
    using a **single training data point ***x_i* (chosen at random). The idea is that
    the gradient calculated this way is a *stochastic approximation* to the gradient
    calculated using the entire training data. Each update is now much faster to calculate
    than in batch gradient descent, and over many updates, we will head in the same
    general direction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In **mini-batch gradient descent**, we calculate the gradient for each small
    mini-batch of training data. That is, we first divide the training data into small
    batches (say M samples / batch). We perform one update per mini-batch. M is usually
    in the range 30–500, depending on the problem. Usually mini-batch GD is used because
    computing infrastructure - compilers, CPUs, GPUs - are often optimized for performing
    vector additions and vector multiplications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of these, SGD and mini-batch GD are most popular. In a typical scenario, we
    do several passes over the training data before the termination criteria is met.
    Each pass is called an *epoch*. Also note that since the update step is much more
    computationally efficient in SGD and mini-batch GD, we typically perform 100s-1000s
    of updates in between checks for termination criteria being met.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the learning rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Typically, the value of the learning rate is chosen manually. We usually start
    with a small value such as 0.1, 0.01 or 0.001 and adapt it based on whether the
    cost function is reducing very slowly (increase learning rate) or is exploding
    / being erratic (decrease learning rate).
  prefs: []
  type: TYPE_NORMAL
- en: Although manually choosing a learning rate is still the most common practice,
    several methods such as Adam optimizer, AdaGrad and RMSProp have been proposed
    to automatically choose a suitable learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: Footnote
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The value of the gradient G depends on the inputs, the current values of the
    model parameters, and the cost function. You might need to revisit the topic of
    differentiation if you are calculating the gradient by hand.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Originally published as part of the [free Machine Learning Course](https://www.commonlounge.com/discussion/33a9cce246d343dd85acce5c3c505009/main)
    and [free Data Science Course](https://www.commonlounge.com/discussion/367fb21455e04c7c896e9cac25b11b47)
    on [www.commonlounge.com](https://www.commonlounge.com/discussion/69a34ad6029549f39087d00d052607ab/main).
    Reposted with permission.
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Deep Learning in H2O using R](https://www.kdnuggets.com/2018/01/deep-learning-h2o-using-r.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The 10 Deep Learning Methods AI Practitioners Need to Apply](https://www.kdnuggets.com/2017/12/10-deep-learning-methods-ai-practitioners-need-apply.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Objective Functions in Neural Networks](https://www.kdnuggets.com/2017/11/understanding-objective-functions-neural-networks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Concepts You Should Know About Gradient Descent and Cost Function](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Back To Basics, Part Dos: Gradient Descent](https://www.kdnuggets.com/2023/03/back-basics-part-dos-gradient-descent.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gradient Descent: The Mountain Trekker''s Guide to Optimization with…](https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Support Vector Machines: An Intuitive Approach](https://www.kdnuggets.com/2022/08/support-vector-machines-intuitive-approach.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An Intuitive Explanation of Collaborative Filtering](https://www.kdnuggets.com/2022/09/intuitive-explanation-collaborative-filtering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Vanishing Gradient Problem: Causes, Consequences, and Solutions](https://www.kdnuggets.com/2022/02/vanishing-gradient-problem.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
