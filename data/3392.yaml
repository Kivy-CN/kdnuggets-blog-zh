- en: How to Rank 10% in Your First Kaggle Competition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在第一次Kaggle竞赛中排名前10%
- en: 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2](https://www.kdnuggets.com/2016/11/rank-ten-precent-first-kaggle-competition.html/2)
- en: '**Outlier**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**异常值**'
- en: '[![Outlier Example](../Images/70eadff4905dbdee3a23ff64ef3219a8.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example_outlier.png)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[![异常值示例](../Images/70eadff4905dbdee3a23ff64ef3219a8.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example_outlier.png)'
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织IT'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The plot shows some scaled coordinates data. We can see that there are some
    outliers in the top-right corner. Exclude them and the distribution looks good.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了一些缩放坐标数据。我们可以看到在右上角有一些异常值。排除它们后，分布看起来很好。
- en: '**Dummy Variables**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**虚拟变量**'
- en: For categorical variables, a common practice is **[One-hot Encoding](https://en.wikipedia.org/wiki/One-hot)**.
    For a categorical variable with `n` possible values, we create a group of `n` dummy
    variables. Suppose a record in the data takes one value for this variable, then
    the corresponding dummy variable is set to `1` while other dummies in the same
    group are all set to `0`.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类变量，常见的做法是**[独热编码](https://en.wikipedia.org/wiki/One-hot)**。对于一个有`n`种可能值的分类变量，我们创建一组`n`个虚拟变量。假设数据中的一条记录对这个变量的取值是某个值，则对应的虚拟变量被设置为`1`，而同组中的其他虚拟变量都设置为`0`。
- en: '[![Dummies Example](../Images/f7891877228b4993c0d084ec1c553387.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example-dummies.png)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[![虚拟变量示例](../Images/f7891877228b4993c0d084ec1c553387.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-example-dummies.png)'
- en: In this example, we transform `DayOfWeek` into 7 dummy variables.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将`DayOfWeek`转换成7个虚拟变量。
- en: Note that when the categorical variable can take many values (hundreds or more),
    this might not work well. It’s difficult to find a general solution to that, but
    I’ll discuss one scenario in the next section.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当分类变量可以取许多值（数百个或更多）时，这可能效果不好。很难找到一种通用的解决方案，但我会在下一节中讨论一个场景。
- en: '**Feature Engineering**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征工程**'
- en: Some describe the essence of Kaggle competitions as **feature engineering supplemented
    by model tuning and ensemble learning**. Yes, that makes a lot of sense. **Feature
    engineering gets your very far.** Yet it is how well you know about the domain
    of given data that decides how far you can go. For example, in a competition where
    data is mainly consisted of texts, Natural Language Processing teachniques are
    a must. The approach of constructing useful features is something we all have
    to continuously learn in order to do better.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人将Kaggle竞赛的本质描述为**特征工程补充模型调优和集成学习**。是的，这确实很有道理。**特征工程能让你走得很远。** 然而，决定你能走多远的是你对给定数据领域的了解程度。例如，在一个数据主要由文本组成的竞赛中，自然语言处理技术是必不可少的。构建有用特征的方法是我们必须不断学习以做得更好的。
- en: 'Basically, **when you feel that a variable is intuitively useful for the task,
    you can include it as a feature**. But how do you know it actually works? The
    simplest way is to plot it against the target variable like this:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，**当你觉得一个变量对任务直观上有用时，你可以将其作为特征包括在内**。但你怎么知道它实际上有效？最简单的方法是像这样将其与目标变量绘制出来：
- en: '[![Checking Feature Validity](../Images/d9b85e951f68e16d26e5899549ae2247.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-vis_check_feat.png)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[![检查特征有效性](../Images/d9b85e951f68e16d26e5899549ae2247.png)](http://7xlo8f.com1.z0.glb.clouddn.com/blog-vis_check_feat.png)'
- en: '**Feature Selection**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征选择**'
- en: 'Generally speaking, **we should try to craft as many features as we can and
    have faith in the model’s ability to pick up the most significant features**.
    Yet there’s still something to gain from feature selection beforehand:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，**我们应该尽可能多地创建特征，并相信模型能够挑选出最重要的特征**。然而，在此之前进行特征选择仍然有一些收获：
- en: Less features mean faster training
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征少意味着训练更快
- en: Some features are linearly related to others. This might put a strain on the
    model.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些特征与其他特征线性相关。这可能会给模型带来压力。
- en: By picking up the most important features, we can use interactions between them
    as new features. Sometimes this gives surprising improvement.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过挑选最重要的特征，我们可以将它们之间的交互用作新特征。有时这会带来意想不到的改善。
- en: The simplest way to inspect feature importance is by fitting a random forest
    model. There are more robust feature selection algorithms (e.g. [this](http://jmlr.org/papers/volume10/tuv09a/tuv09a.pdf))
    which are theoretically superior but not practicable due to the absence of efficient
    implementation. You can combat noisy data (to an extent) simply by increasing
    number of trees used in a random forest.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 检查特征重要性的最简单方法是拟合一个随机森林模型。还有更稳健的特征选择算法（例如，[这个](http://jmlr.org/papers/volume10/tuv09a/tuv09a.pdf)），虽然在理论上更优越，但由于缺乏高效实现而不具实用性。你可以通过增加随机森林中使用的树的数量来对抗噪声数据（在一定程度上）。
- en: This is important for competitions in which data is **[anonymized](https://en.wikipedia.org/wiki/Data_anonymization)** because
    you won’t waste time trying to figure out the meaning of a variable that’s of
    no significance.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这在数据**[匿名化](https://en.wikipedia.org/wiki/Data_anonymization)**的竞赛中很重要，因为你不会浪费时间去弄清楚没有意义的变量的含义。
- en: '**Feature Encoding**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征编码**'
- en: Sometimes raw features have to be converted to some other formats for them to
    work properly.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有时原始特征必须转换为其他格式才能正常工作。
- en: For example, suppose we have a categorical variable which can take more than
    10K different values. Then naively creating dummy variables is not a feasible
    option. An acceptable solution is to create dummy variables for only a subset
    of the values (e.g. values that constitute 95% of the feature importance) and
    assign everything else to an ‘others’ class.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个可以取超过10K不同值的分类变量。然后，简单地创建虚拟变量并不是一个可行的选择。一个可接受的解决方案是仅为值的一个子集（例如，占特征重要性95%的值）创建虚拟变量，其余的分配给‘其他’类别。
- en: '**Updated on Oct 28th, 2016: **For the scenario described above, another possible
    solution is to use **Factorized Machines**. Please refer to [this post](https://www.kaggle.com/c/expedia-hotel-recommendations/forums/t/21607/1st-place-solution-summary) by
    Kaggle user “idle_speculation” for details.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**更新于 2016年10月28日：** 对于上述场景，另一个可能的解决方案是使用**因子化机器**。有关详细信息，请参阅Kaggle用户“idle_speculation”的[这篇帖子](https://www.kaggle.com/c/expedia-hotel-recommendations/forums/t/21607/1st-place-solution-summary)。'
- en: '**Model Selection**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型选择**'
- en: 'When the features are set, we can start training models. Kaggle competitions
    usually favor**tree-based models**:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当特征设置好后，我们可以开始训练模型。Kaggle 比赛通常偏爱**基于树的模型**：
- en: '**Gradient Boosted Trees**'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度提升树**'
- en: Random Forest
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Extra Randomized Trees
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 额外的随机树
- en: 'The following models are slightly worse in terms of general performance, but
    are suitable as base models in ensemble learning (will be discussed later):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下模型在总体表现上略逊一筹，但作为集成学习中的基础模型是合适的（稍后会讨论）：
- en: SVM
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Linear Regression
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Logistic Regression
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Neural Networks
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络
- en: Note that this does not apply to computer vision competitions which are pretty
    much dominated by neural network models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这不适用于计算机视觉竞赛，这些竞赛几乎完全由神经网络模型主导。
- en: All these models are implemented in **[Sklearn](http://scikit-learn.org/)**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些模型都在**[Sklearn](http://scikit-learn.org/)**中实现。
- en: Here I want to emphasize the greatness of **[Xgboost](https://github.com/dmlc/xgboost)**.
    The outstanding performance of gradient boosted trees and Xgboost’s efficient
    implementation makes it very popular in Kaggle competitions. Nowadays almost every
    winner uses Xgboost in one way or another.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我想强调**[Xgboost](https://github.com/dmlc/xgboost)**的伟大。梯度提升树的出色表现和Xgboost的高效实现使其在Kaggle比赛中非常受欢迎。如今，几乎每个赢家都以某种方式使用Xgboost。
- en: '**Updated on Oct 28th, 2016: **Recently Microsoft open sourced **[LightGBM](https://github.com/Microsoft/LightGBM)**,
    a potentially better library than Xgboost for gradient boosting.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**更新于 2016年10月28日：** 最近微软开源了**[LightGBM](https://github.com/Microsoft/LightGBM)**，这是一个可能比Xgboost更好的梯度提升库。'
- en: By the way, for Windows users installing Xgboost could be a painstaking process.
    You can refer to [this post](https://dnc1994.com/2016/03/installing-xgboost-on-windows/) by
    me if you run into problems.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，对于Windows用户来说，安装Xgboost可能是一个痛苦的过程。如果你遇到问题，可以参考我写的[这篇文章](https://dnc1994.com/2016/03/installing-xgboost-on-windows/)。
- en: More On This Topic
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关话题更多内容
- en: '[How LinkedIn Uses Machine Learning To Rank Your Feed](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LinkedIn如何利用机器学习来排名你的动态](https://www.kdnuggets.com/2022/11/linkedin-uses-machine-learning-rank-feed.html)'
- en: '[How to Get Your First Job in Data Science without Any Work Experience](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在没有任何工作经验的情况下获得你的第一份数据科学工作](https://www.kdnuggets.com/2021/02/first-job-data-science-without-work-experience.html)'
- en: '[Building and Training Your First Neural Network with TensorFlow and Keras](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用TensorFlow和Keras构建并训练你的第一个神经网络](https://www.kdnuggets.com/2023/05/building-training-first-neural-network-tensorflow-keras.html)'
- en: '[It''s alive! Build your first robots with Python and some cheap,…](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用Python和一些便宜的组件制作你的第一个机器人！](https://www.kdnuggets.com/2023/06/manning-build-first-robots-python-cheap-basic-components.html)'
- en: '[From Zero to Hero: Create Your First ML Model with PyTorch](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[从零到英雄：使用PyTorch创建你的第一个机器学习模型](https://www.kdnuggets.com/from-zero-to-hero-create-your-first-ml-model-with-pytorch)'
- en: '[Deploying Your First Machine Learning Model](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[部署你的第一个机器学习模型](https://www.kdnuggets.com/deploying-your-first-machine-learning-model)'
