- en: 'Reinforcement Learning: Teaching Computers to Make Optimal Decisions'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习：教计算机做出最佳决策
- en: 原文：[https://www.kdnuggets.com/2023/07/reinforcement-learning-teaching-computers-make-optimal-decisions.html](https://www.kdnuggets.com/2023/07/reinforcement-learning-teaching-computers-make-optimal-decisions.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/07/reinforcement-learning-teaching-computers-make-optimal-decisions.html](https://www.kdnuggets.com/2023/07/reinforcement-learning-teaching-computers-make-optimal-decisions.html)
- en: What Is Reinforcement Learning?
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是强化学习？
- en: Reinforcement learning is a branch of machine learning that deals with an agent
    learning—through experience—how to interact with a complex environment.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是机器学习的一个分支，它涉及智能体通过经验学习如何与复杂环境互动。
- en: From AI agents that play and surpass human performance in complex board games
    such as chess and Go to autonomous navigation, reinforcement learning has a suite
    of interesting and diverse applications.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 从在复杂棋盘游戏（如象棋和围棋）中超越人类表现的AI智能体，到自主导航，强化学习具有一系列有趣且多样的应用。
- en: Remarkable breakthroughs in the field of reinforcement learning include DeepMind’s
    agent [AlphaGo Zero](https://www.deepmind.com/blog/alphago-zero-starting-from-scratch)
    that can defeat even human champions in the game of Go and [AlphaFold](https://www.deepmind.com/research/highlighted-research/alphafold)
    that can predict complex 3D protein structure.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习领域的显著突破包括DeepMind的智能体[AlphaGo Zero](https://www.deepmind.com/blog/alphago-zero-starting-from-scratch)，它可以击败甚至人类围棋冠军，以及[AlphaFold](https://www.deepmind.com/research/highlighted-research/alphafold)，它可以预测复杂的3D蛋白质结构。
- en: This guide will introduce you to the reinforcement learning paradigm. We’ll
    take a simple yet motivating real-world example to understand the reinforcement
    learning framework.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本指南将介绍强化学习范式。我们将通过一个简单但有启发性的现实世界例子来理解强化学习框架。
- en: The Reinforcement Learning Framework
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习框架
- en: Let's start by defining the components of a reinforcement learning framework.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先来定义强化学习框架的组成部分。
- en: '![Reinforcement Learning: Teaching Computers to Make Optimal Decisions](../Images/d1344fd46c3456a8c995f0c2d76e076a.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习：教计算机做出最佳决策](../Images/d1344fd46c3456a8c995f0c2d76e076a.png)'
- en: Reinforcement Learning Framework | Image by Author
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习框架 | 图片由作者提供
- en: 'In a typical reinforcement learning framework:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的强化学习框架中：
- en: There is an **agent** learning to interact with the **environment**.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个**智能体**在学习如何与**环境**互动。
- en: The agent can measure its **state**, take **actions**, and occasionally gets
    a **reward**.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体可以测量其**状态**，采取**行动**，并偶尔获得**奖励**。
- en: 'Practical examples of this setting: the agent can play against an opponent
    (say, a game of chess) or try to navigate a complex environment.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置的实际例子：智能体可以与对手对弈（比如，一场象棋比赛）或尝试在复杂环境中导航。
- en: As a super simplified example, consider a mouse in a maze. Here, the agent is
    *not* playing against an opponent but rather trying to figure out a path to the
    exit. If there are more than one paths leading to the exit, we may prefer the
    shortest path out of the maze.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个超级简化的例子，考虑一个在迷宫中的鼠标。这里，智能体*不是*与对手对战，而是试图找出通向出口的路径。如果有多个路径通向出口，我们可能会选择最短的路径。
- en: '![Reinforcement Learning: Teaching Computers to Make Optimal Decisions](../Images/343faadea35883d9687ff5a3a9df4e82.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习：教计算机做出最佳决策](../Images/343faadea35883d9687ff5a3a9df4e82.png)'
- en: Mouse in a Maze | Image by Author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 迷宫中的鼠标 | 图片由作者提供
- en: In this example, the *mouse* is the *agent* trying to navigate the *environment*
    which is the *maze*. The *action* here is the movement of the mouse within the
    maze. When it successfully navigates the maze to the exit—it gets a *piece of
    cheese* as a *reward*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，*鼠标*是试图在*迷宫*中导航的*智能体*。这里的*行动*是鼠标在迷宫中的移动。当它成功找到出口时，它会获得一块*奶酪*作为*奖励*。
- en: '![Reinforcement Learning: Teaching Computers to Make Optimal Decisions](../Images/5ed063fccd92b8c4942e9a76efda7ece.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习：教计算机做出最佳决策](../Images/5ed063fccd92b8c4942e9a76efda7ece.png)'
- en: Example | Image by Author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 | 图片由作者提供
- en: The sequence of actions happens in discrete time steps (say, t = 1, 2, 3,...).
    At any time step **t**, the mouse can only measure its current state in the maze.
    It doesn’t know the whole maze yet.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 行动序列发生在离散的时间步骤中（比如，t = 1, 2, 3,...）。在任何时间步骤**t**，鼠标只能测量其在迷宫中的当前状态。它还不知道整个迷宫的情况。
- en: So the agent (the mouse) measures its state ![Equation](../Images/50d4c306c3da2ab64f5bca9a582b48ce.png)
    in the environment at time step **t**, takes a valid action ![Equation](../Images/8df11873ed817574f0269f21bf1c8037.png)
    and moves to state ![Equation](../Images/1978948a8f905005c08aff7cb743dab0.png).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以代理（小鼠）在时间步**t**测量其环境状态 ![方程式](../Images/50d4c306c3da2ab64f5bca9a582b48ce.png)，采取一个有效的行动
    ![方程式](../Images/8df11873ed817574f0269f21bf1c8037.png)并移动到状态 ![方程式](../Images/1978948a8f905005c08aff7cb743dab0.png)。
- en: '![Reinforcement Learning: Teaching Computers to Make Optimal Decisions](../Images/9e636b521640ce1e36e2998cedaa0bab.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习：教计算机做出最佳决策](../Images/9e636b521640ce1e36e2998cedaa0bab.png)'
- en: State | Image by Author
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 状态 | 作者提供的图像
- en: How Is Reinforcement Learning Different?
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习有什么不同？
- en: Notice how the mouse (the agent) has to figure its way out of the maze through
    **trial and error**. Now if the mouse hits one of the walls of the maze, it has
    to try to find its way back and etch a different route to the exit.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意观察小鼠（代理）如何通过**试错法**找出迷宫的出口。如果小鼠碰到迷宫的墙壁，它必须尝试找到回去的路，并绘制一条不同的路线到达出口。
- en: If this were a supervised learning setting, after every move, the agent would
    get to know whether or not that action—was correct—and would lead to a reward.
    Supervised learning is like learning from a teacher.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是一个监督学习环境，那么在每次移动后，代理会知道该行动是否正确，并是否会导致奖励。监督学习就像是从教师那里学习。
- en: While a teacher tells you ahead of time, a critic always tells you—after the
    performance is over— how good or bad your performance was. For this reason, reinforcement
    learning is also called learning in the *presence of a critic*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当教师事先告诉你时，批评者总是在表演结束后告诉你—你的表现有多好或多坏。因此，强化学习也被称为在*批评者存在下的学习*。
- en: Terminal State and Episode
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 终端状态和回合
- en: When the mouse has reached the exit, it reaches the **terminal state**. Meaning
    it cannot explore any further.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当小鼠到达出口时，它达到了**终端状态**。这意味着它不能再进一步探索。
- en: And the sequence of actions—from the initial state to the terminal state—is
    called an episode. For any learning problem, we need multiple episodes for the
    agent to learn to navigate. Here, for our agent (the mouse) to learn the sequence
    of actions that would lead it to the exit, and subsequently, receive the piece
    of cheese, we’d need many episodes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从初始状态到终端状态的一系列行动称为一个回合。对于任何学习问题，我们需要多个回合让代理学习导航。在这里，为了让我们的代理（小鼠）学习到达出口的行动序列，并随之获得奶酪，我们需要很多回合。
- en: Dense and Sparse Rewards
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 密集奖励和稀疏奖励
- en: Whenever the agent takes a correct action or a sequence of actions that is correct,
    it gets a **reward**. In this case, the mouse receives a piece of cheese as a
    reward for etching a valid route—through the maze(the environment)—to the exit.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 每当代理采取正确的行动或一系列正确的行动时，它就会获得一个**奖励**。在这种情况下，小鼠因绘制有效路径—穿越迷宫（环境）—到达出口而获得奶酪作为奖励。
- en: In this example, the mouse receives a piece of cheese only at the very end—when
    it reaches the exit.This is an example of a *sparse* and delayed reward.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，小鼠只有在最后—即到达出口时—才会得到一块奶酪。这是一个*稀疏*且延迟的奖励示例。
- en: If the rewards are more frequent, then we will have a *dense* reward system.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果奖励更频繁，那么我们将拥有一个*密集*的奖励系统。
- en: Looking back we need to figure out (it’s not trivial) which action or sequence
    of actions caused the agent to get the reward; this is commonly called the [credit
    assignment problem](https://ai.stackexchange.com/questions/12908/what-is-the-credit-assignment-problem).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾过去，我们需要弄清楚（这并不简单）是哪一个行动或一系列行动使代理获得了奖励；这通常被称为[信用分配问题](https://ai.stackexchange.com/questions/12908/what-is-the-credit-assignment-problem)。
- en: Policy, Value Function, and the Optimization Problem
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略、价值函数和优化问题
- en: The environment is often not deterministic but probabilistic and so is the policy.
    Given a state ![Equation](../Images/50d4c306c3da2ab64f5bca9a582b48ce.png), the
    agent takes an action and goes to another state ![Equation](../Images/991b31a62368366eaf20405d8514c526.png)
    with a certain probability.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 环境通常不是确定性的，而是概率性的，策略也是如此。给定一个状态 ![方程式](../Images/50d4c306c3da2ab64f5bca9a582b48ce.png)，代理会采取一个行动并以一定的概率转移到另一个状态
    ![方程式](../Images/991b31a62368366eaf20405d8514c526.png)。
- en: 'The policy helps define a mapping from the set of possible states to the actions.
    It helps answer questions like:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 策略有助于定义从可能状态集合到行动的映射。它有助于回答诸如以下问题：
- en: What actions to take to maximize the expected reward?
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应采取什么行动以最大化期望奖励？
- en: 'Or better yet: Given a state, what is the best possible action that the agent
    can take so as to maximize expected reward?'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者更好地说：给定一个状态，代理能采取的最佳行动是什么，以最大化期望奖励？
- en: 'So you can think of the agent as *enacting a policy* π:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以把代理看作是*执行策略* π：
- en: '![Reinforcement Learning: Teaching Computers to Make Optimal Decisions](../Images/c29258389a41a8438d777e35214799e3.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习：教计算机做出最优决策](../Images/c29258389a41a8438d777e35214799e3.png)'
- en: 'Another related and helpful concept is the value function. The **value function**
    is given by:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个相关且有帮助的概念是价值函数。**价值函数**定义为：
- en: '![Reinforcement Learning: Teaching Computers to Make Optimal Decisions](../Images/0eb789ff1281ec7c694c4f52ee9f8b5b.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习：教计算机做出最优决策](../Images/0eb789ff1281ec7c694c4f52ee9f8b5b.png)'
- en: This signifies the value of being in a state given a policy π. The quantity
    denotes the expected reward in the future if the agent starts at state and enacts
    the policy π thereafter.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示在给定策略π的情况下处于某状态的价值。这个量表示如果代理从某状态开始并随后执行策略π，那么未来的期望奖励。
- en: 'To sum up: the goal of reinforcement learning is to optimize the policy so
    as to maximize the expected future rewards. Therefore, we can think of it as an
    optimization problem to solve for π.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下：强化学习的目标是优化策略，以最大化期望的未来奖励。因此，我们可以将其视为一个优化问题，求解π。
- en: Discount Factor
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 折扣因子
- en: 'Notice that we have a new quantity ɣ. What does it stand for? ɣ is called the
    **discount factor,** a quantity between 0 and 1\. Meaning future rewards are discounted
    (read: because now is greater than much later).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到我们有了一个新的量ɣ。它代表什么？ɣ被称为**折扣因子**，一个介于0和1之间的量。意味着未来的奖励会被折扣（即：现在的价值大于未来）。
- en: Exploration vs. Exploitation Tradeoff
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索与利用的权衡
- en: 'Going back to the food loop example of mouse in a maze: If the mouse is able
    to figure out a route to exit A with a small piece of cheese, it can keep repeating
    it and collecting the cheese piece. But what if the maze also had another exit
    B with a bigger cheese piece (greater reward)?'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 回到老鼠在迷宫中的食物循环示例：如果老鼠能够找到通向出口A的路线，并获得一小块奶酪，它可以不断重复这一过程并收集奶酪。但如果迷宫还有另一个出口B，那里有一块更大的奶酪（更大的奖励）呢？
- en: So long as the mouse keeps *exploiting* this current strategy without *exploring*
    new strategies, it is not going to get the much greater reward of a bigger cheese
    piece at exit B.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 只要老鼠继续*利用*当前策略而不*探索*新策略，它就无法获得出口B处更大奶酪的更大奖励。
- en: '![Reinforcement Learning: Teaching Computers to Make Optimal Decisions](../Images/4fd7b3a8aa6aa8366c8d3ed3166d7cf5.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习：教计算机做出最优决策](../Images/4fd7b3a8aa6aa8366c8d3ed3166d7cf5.png)'
- en: Exploration vs. Exploitation | Image by Author
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 探索与利用 | 图片作者
- en: But the *uncertainty* associated with exploring new strategies and future rewards
    is greater. So how do we exploit and explore? This tradeoff between exploiting
    the current strategy and exploring new ones with potentially better rewards is
    called the **exploration vs exploitation tradeoff**.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 但探索新策略和未来奖励的*不确定性*更大。那么我们如何在利用和探索之间取得平衡呢？这种在利用当前策略和探索具有潜在更好奖励的新策略之间的权衡被称为**探索与利用的权衡**。
- en: One possible approach is the *ε-greedy search*. Given a set of all possible
    actions , the *ε-greedy search* explores one of the possible actions with the
    probability *ε* while exploiting the current strategy with the probability 1 -
    *ε*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的方法是*ε-贪婪搜索*。给定所有可能的行动，*ε-贪婪搜索*以概率*ε*探索可能的行动之一，同时以概率1 - *ε*利用当前策略。
- en: Wrap-up and Next Steps
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结和下一步
- en: 'Let''s summarize what we’ve covered so far. We learned about the components
    of the reinforcement learning framework:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下我们到目前为止学到的内容。我们学习了强化学习框架的组成部分：
- en: The agent interacts with the environment, gets to measure its current state,
    takes actions, and receives rewards as positive reinforcement. The framework is
    probabilistic.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理与环境互动，测量其当前状态，采取行动，并获得作为正向强化的奖励。该框架是概率性的。
- en: We then went over value functions and policy, and how the optimization problem
    often boils down to finding the optimal policies that maximize the expected future
    awards.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们讨论了价值函数和策略，以及优化问题如何通常归结为找到最大化期望未来奖励的最优策略。
- en: 'You’ve now learned just enough to navigate the reinforcement learning landscape.
    Where to go from here? We did not talk about reinforcement learning algorithms
    in this guide, so you can explore some basic algorithms:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经学到了足够的知识来导航强化学习领域。接下来该怎么做？我们在本指南中没有讨论强化学习算法，你可以探索一些基本算法：
- en: If we know everything about the environment (and can model it completely), we
    can use model-based algorithms like [policy iteration and value iteration](http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-f01/www/handouts/111301.pdf).
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们对环境了解一切（并且可以完全建模），我们可以使用基于模型的算法，如 [策略迭代和价值迭代](http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15381-f01/www/handouts/111301.pdf)。
- en: However, in most cases, we may not be able to model the environment completely.
    In this case, you can look at model-free algorithms such as [Q-learning](https://deeplizard.com/learn/video/qhRNvCVVJaA)
    which optimizes state-action pairs.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，在大多数情况下，我们可能无法完全建模环境。在这种情况下，你可以查看无模型算法，如 [Q-learning](https://deeplizard.com/learn/video/qhRNvCVVJaA)，它优化状态-动作对。
- en: If you’re looking to further your understanding of reinforcement learning, [David
    Silver’s reinforcement learning lectures](https://www.youtube.com/watch?v=2pWv7GOvuf0)
    on YouTube and [Hugging Face’s Deep Reinforcement Learning Course](https://huggingface.co/learn/deep-rl-course/unit0/introduction)
    are some good resources to look at.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想进一步了解强化学习，YouTube 上的 [David Silver 强化学习讲座](https://www.youtube.com/watch?v=2pWv7GOvuf0)
    和 [Hugging Face 深度强化学习课程](https://huggingface.co/learn/deep-rl-course/unit0/introduction)
    是一些不错的资源。
- en: '**[Bala Priya C](https://www.linkedin.com/in/bala-priya/)** is a developer
    and technical writer from India. She likes working at the intersection of math,
    programming, data science, and content creation. Her areas of interest and expertise
    include DevOps, data science, and natural language processing. She enjoys reading,
    writing, coding, and coffee! Currently, she''s working on learning and sharing
    her knowledge with the developer community by authoring tutorials, how-to guides,
    opinion pieces, and more.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Bala Priya C](https://www.linkedin.com/in/bala-priya/)** 是一位来自印度的开发者和技术作者。她喜欢在数学、编程、数据科学和内容创作的交集上工作。她的兴趣和专长领域包括
    DevOps、数据科学和自然语言处理。她喜欢阅读、写作、编程和咖啡！目前，她正在通过编写教程、操作指南、评论文章等与开发者社区分享她的知识。'
- en: '* * *'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速入门网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织在 IT 方面'
- en: '* * *'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[The Optimal Way to Input Missing Data with Pandas fillna()](https://www.kdnuggets.com/2023/02/optimal-way-input-missing-data-pandas-fillna.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Pandas fillna() 输入缺失数据的最佳方法](https://www.kdnuggets.com/2023/02/optimal-way-input-missing-data-pandas-fillna.html)'
- en: '[Generalized and Scalable Optimal Sparse Decision Trees(GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通用且可扩展的最优稀疏决策树(GOSDT)](https://www.kdnuggets.com/2023/02/generalized-scalable-optimal-sparse-decision-treesgosdt.html)'
- en: '[Driving Better Business Decisions](https://www.kdnuggets.com/2022/04/informs-driving-better-business-decisions.html)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[驱动更好的商业决策](https://www.kdnuggets.com/2022/04/informs-driving-better-business-decisions.html)'
- en: '[Explainable AI: 10 Python Libraries for Demystifying Your Model''s Decisions](https://www.kdnuggets.com/2023/01/explainable-ai-10-python-libraries-demystifying-decisions.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可解释的 AI：揭示模型决策的 10 个 Python 库](https://www.kdnuggets.com/2023/01/explainable-ai-10-python-libraries-demystifying-decisions.html)'
- en: '[Hands-on Reinforcement Learning Course Part 3: SARSA](https://www.kdnuggets.com/2022/01/handson-reinforcement-learning-course-part-3-sarsa.html)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[实践强化学习课程第三部分：SARSA](https://www.kdnuggets.com/2022/01/handson-reinforcement-learning-course-part-3-sarsa.html)'
- en: '[Hands-On Reinforcement Learning Course, Part 1](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-course-part-1.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[动手实践强化学习课程，第 1 部分](https://www.kdnuggets.com/2021/12/hands-on-reinforcement-learning-course-part-1.html)'
