# 机器学习研究人员需要了解的8种神经网络架构

> 原文：[https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html](https://www.kdnuggets.com/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html)

[评论](/2018/02/8-neural-network-architectures-machine-learning-researchers-need-learn.html?page=2#comments)

![头图](../Images/ce396eda700c6c69f296bdb37596e40f.png)

**我们为什么需要机器学习？**

* * *

## 我们的前三个课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业的轨道。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织的 IT

* * *

机器学习适用于那些人类无法直接编码的任务。有些任务复杂到人类无法全面解决所有细节并明确编码。因此，我们将大量数据提供给机器学习算法，让算法通过探索数据和寻找能够实现程序员设定目标的模型来解决问题。

让我们来看这两个例子：

+   编写解决诸如在混乱场景中的新光照条件下从新视角识别三维物体这样的问题的程序是非常困难的。我们不知道该写什么程序，因为我们不知道大脑是如何处理的。即使我们对如何做到这一点有一个好的想法，程序也可能复杂得令人恐惧。

+   编写程序来计算信用卡交易是否欺诈的概率是很困难的。可能没有既简单又可靠的规则。我们需要结合大量的弱规则。欺诈是一个不断变化的目标，但程序需要不断改变。

然后是**机器学习方法**：我们不是为每个特定任务手动编写程序，而是收集大量示例，指定给定输入的正确输出。机器学习算法然后利用这些示例生成一个能完成任务的程序。学习算法生成的程序可能与典型的手写程序大相径庭，可能包含数百万个数字。如果我们做对了，程序不仅在我们训练的情况下有效，也能在新案例中有效。如果数据发生变化，程序也可以通过在新数据上进行训练而改变。你应该注意到，大量计算现在比支付人们编写任务特定程序更便宜。

鉴于此，一些最适合由机器学习解决的任务包括：

+   模式识别：真实场景中的物体，面部身份或面部表情，语音

+   异常检测：不寻常的信用卡交易序列，核电站传感器读数的异常模式

+   预测：未来的股票价格或货币汇率，某个人可能喜欢哪些电影

**什么是神经网络？**

神经网络是广义机器学习文献中的一类模型。例如，如果你参加了Coursera上的机器学习课程，神经网络很可能会被涵盖。神经网络是一组已彻底改变机器学习领域的特定算法。它们受到生物神经网络的启发，目前所谓的深度神经网络已经证明效果非常好。神经网络本身是一般的函数近似，因此它们几乎可以应用于任何机器学习问题，只要问题涉及从输入到输出空间的复杂映射学习。

这里有3个理由说服你学习神经计算：

+   了解大脑实际如何运作：它非常庞大且复杂，由一些在你碰触时会死亡的物质构成。因此，我们需要使用计算机模拟。

+   了解一种受神经元及其自适应连接启发的并行计算风格：这是一种与顺序计算截然不同的风格。

+   通过使用受大脑启发的新颖学习算法来解决实际问题：即使这些学习算法的工作方式与大脑实际运作不完全相同，它们也可以非常有用。

在完成了著名的[Andrew Ng的机器学习Coursera课程](https://github.com/khanhnamle1994/machine-learning)之后，我开始对神经网络和深度学习产生了兴趣。因此，我开始寻找最好的在线资源来学习这些主题，并发现了[Geoffrey Hinton的机器学习神经网络课程](https://github.com/khanhnamle1994/neural-nets)。如果你是深度学习从业者或希望进入深度学习/机器学习领域的人，你真的应该参加这门课程。Geoffrey Hinton无疑是深度学习领域的奠基人。他在这门课程中确实提供了非凡的内容。在这篇博客文章中，我想分享来自这门课程的**8种神经网络架构**，我认为任何机器学习研究人员都应该熟悉这些架构，以推动他们的工作。

![](../Images/39c93b981f680576756eed859830f9e8.png)

通常，这些架构可以分为3个特定类别：

**1 — 前馈神经网络**

这些是实际应用中最常见的神经网络类型。第一层是输入层，最后一层是输出层。如果有多个隐藏层，我们称之为“深度”神经网络。它们计算一系列变换，以改变案例之间的相似性。每一层神经元的活动是下面一层活动的非线性函数。

**2 — 递归网络**

这些在其连接图中具有有向循环。这意味着你有时可以通过沿着箭头回到起点。它们可能具有复杂的动态，这使得训练它们非常困难。它们在生物学上更为真实。

当前对寻找有效训练递归网络的方法非常感兴趣。递归神经网络是一种非常自然的建模序列数据的方式。它们等同于每个时间片有一个隐藏层的非常深的网络；只是它们在每个时间片使用相同的权重，并且在每个时间片获取输入。它们能够在隐藏状态中记住信息很长时间，但训练它们利用这种潜力非常困难。

**3 — 对称连接网络**

这些类似于递归网络，但单元之间的连接是对称的（它们在两个方向上具有相同的权重）。对称网络比递归网络更容易分析。由于它们遵循一个能量函数，因此它们在功能上也更加受限。没有隐藏单元的对称连接网络称为“霍普菲尔德网络”。具有隐藏单元的对称连接网络称为“玻尔兹曼机”。

### 1 — 感知器

被认为是第一代神经网络的**感知器**，仅仅是单个神经元的计算模型。它们在1960年代初由[弗兰克·罗森布拉特](https://blogs.umass.edu/comphon/2017/06/15/did-frank-rosenblatt-invent-deep-learning-in-1962/)推广。它们似乎具有非常强大的学习算法，并且对它们能够学习做什么做出了许多宏伟的声明。1969年，明斯基和帕普斯出版了一本名为[《感知器》](https://mitpress.mit.edu/books/perceptrons)的书，分析了它们的功能并展示了它们的局限性。许多人认为这些局限性适用于所有神经网络模型。然而，感知器学习程序今天仍广泛用于具有包含数百万特征的大型特征向量的任务。

![](../Images/4d94d59e6e4b8eae6bd7be64cc25f945.png)

在统计模式识别的标准范式中，我们首先将原始输入向量转换为特征激活向量。然后，我们使用基于常识的手写程序来定义特征。接下来，我们学习如何加权每个特征激活，以得到一个标量量。如果这个量高于某个阈值，我们决定输入向量是目标类别的正例。

标准的感知机结构遵循前馈模型，这意味着输入被送入神经元，经过处理后产生输出。在下面的图示中，这意味着网络从下往上读取：输入从底部进入，输出从顶部出去。

![](../Images/2b2a45c42c8d6cd62cb8bcdb0870fd89.png)

然而，感知机确实存在限制：如果允许手动选择特征，并且使用足够的特征，你几乎可以做任何事情。对于二进制输入向量，我们可以为每一个指数级的二进制向量设置一个单独的特征单元，因此我们可以在二进制输入向量上做出任何可能的区分。但是一旦手工编码的特征确定后，感知机能够学习的内容就会受到很大的限制。

这一结果对感知机是毁灭性的，因为模式识别的核心是能够识别尽管有像平移这样的变换。明斯基和帕珀特的“群不变性定理”指出，如果变换形成一个群，感知机的学习部分无法学会应对这些变换。为了应对这些变换，感知机需要使用多个特征单元来识别信息子模式的变换。因此，模式识别中复杂的部分必须通过手工编码的特征检测器解决，而不是学习过程。

没有隐藏单元的网络在输入输出映射的建模能力上非常有限。更多的线性单元层并没有帮助，它仍然是线性的。固定输出的非线性不够。因此，我们需要多层自适应的非线性隐藏单元。但是我们如何训练这样的网络？我们需要一种有效的方式来调整所有的权重，而不仅仅是最后一层。这很困难。学习进入隐藏单元的权重等同于学习特征。这很困难，因为没有人直接告诉我们隐藏单元应该做什么。

### 2 — 卷积神经网络

机器学习研究长期以来重点关注物体检测问题。各种因素使得识别物体变得困难：

+   分割：真实场景中物体常常被其他物体遮挡。很难判断哪些部分属于同一个物体。物体的部分可能被其他物体遮挡。

+   光照：像素的强度受到光照和物体本身的共同影响。

+   变形：物体可以以各种非仿射方式变形。例如，手写的“2”可能有一个大圈或只是一个尖点。

+   功能性：物体类别通常由它们的使用方式来定义。例如，椅子是设计用来坐的，因此它们具有多种不同的物理形状。

+   视角：视角的变化会导致图像变化，而标准学习方法无法应对这些变化。信息在输入维度（即像素）之间跳跃。

+   想象一个医疗数据库，其中患者的年龄有时会跳到通常编码体重的输入维度！为了应用机器学习，我们首先需要消除这种维度跳跃。

![](../Images/5b43460563dfae211f0ceeee551229b6.png)

复制特征方法目前是解决对象检测问题的主流方法。它使用许多不同位置的相同特征检测器的副本。它还可以在尺度和方向上进行复制，这既棘手又昂贵。复制大大减少了需要学习的自由参数数量。它使用几种不同的特征类型，每种特征都有其自己的复制检测器映射。它还允许每个图像补丁以多种方式进行表示。

那么复制特征检测器能实现什么呢？

+   等效活动：复制特征并未使神经活动对平移不变。活动是等变的。

+   不变知识：如果一个特征在训练期间在某些位置有用，那么在测试期间，这个特征的检测器将在所有位置都可用。

1998年，Yann LeCun及其合作者开发了一个非常好的手写数字识别器，名为[LeNet](http://yann.lecun.com/exdb/lenet/)。它在一个前馈网络中使用反向传播，具有多个隐藏层，每层有多个复制单元映射，邻近复制单元输出的池化，一个宽网络可以同时处理几个字符，即使它们重叠，以及一种聪明的方式来训练一个完整的系统，而不仅仅是一个识别器。后来它在**卷积神经网络**的名字下被正式化。有趣的是：这个网络曾用于阅读北美约10%的支票。

![](../Images/5d555581239934e2d31a32647a7003a6.png)

卷积神经网络可以用于所有与对象识别相关的工作，从手写数字到三维物体。然而，识别从网络下载的彩色照片中的真实物体比识别手写数字要复杂得多。分类的数量多了百倍（1000对比10），像素的数量多了百倍（256 x 256彩色对比28 x 28灰度），三维场景的二维图像，要求分割的杂乱场景，以及每张图像中的多个物体。相同类型的卷积神经网络能否有效呢？

然后来了[ILSVRC-2012竞赛](http://www.image-net.org/challenges/LSVRC/2012/)，这是一个包含约120万张高分辨率训练图像的**ImageNet**数据集。测试图像将以没有初始注释（没有分割或标签）的形式呈现，算法需要生成标注以指定图像中存在的物体。一些领先的计算机视觉团队，如牛津大学、INRIA、XRCE等，在这个数据集上尝试了现有的一些最佳计算机视觉方法。通常，计算机视觉系统使用复杂的多阶段系统，早期阶段通常通过优化一些参数进行手动调整。

![](../Images/10ef5959d0e91a8e208616591820caa9.png)

竞赛的获胜者，[Alex Krizhevsky (NIPS 2012)](http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf)，开发了一种由Yann LeCun首创的非常深的卷积神经网络。这种架构包括7个隐藏层，不包括一些最大池化层。早期层是卷积层，而最后2层是全球连接层。每个隐藏层的激活函数都是修正线性单元。这些激活函数比逻辑单元训练得更快且更具表达能力。此外，它还使用了竞争性归一化来抑制在附近单元活动更强时的隐藏活动。这有助于处理强度的变化。

有一些技术技巧可以显著提高神经网络的泛化能力：

1.  在256 x 256图像上训练随机224 x 224的图像补丁以获取更多数据，并使用图像的左右反射。在测试时，结合来自10个不同补丁的意见：四个224 x 224的角落补丁加上中央的224 x 224补丁以及这些5个补丁的反射。

1.  使用“dropout”来规范全球连接层中的权重（这些层包含大部分参数）。Dropout意味着在每个训练样本中，层中的一半隐藏单元会被随机移除。这防止了隐藏单元过度依赖其他隐藏单元。

![](../Images/adcd19bf34e1806f4d14c72e8c826e70.png)

在硬件需求方面，Alex在2块Nvidia GTX 580 GPU（超过1000个快速小核心）上使用了非常高效的卷积网络实现。这些GPU非常适合矩阵-矩阵乘法，并且具有非常高的内存带宽。这使得他能够在一周内训练网络，并使得在测试时快速结合来自10个补丁的结果。如果我们能够足够快速地通信状态，我们可以将网络分布在许多核心上。随着核心成本的降低和数据集的增大，大型神经网络将比传统计算机视觉系统进步更快。

### 3 — 递归神经网络

![](../Images/165765dffd0351593bc95483a6eccae1.png)

要理解RNN，我们需要对序列建模有一个简要的概述。在将机器学习应用于序列时，我们通常希望将输入序列转换为一个不同领域的输出序列；例如，将一系列声音压力转换为一系列单词身份。当没有单独的目标序列时，我们可以通过尝试预测输入序列中的下一个项来获得教学信号。目标输出序列是输入序列向前移动1步。这似乎比尝试从图像中的其他像素预测一个像素，或从图像的其余部分预测一个图像块要自然得多。预测序列中的下一个项模糊了监督学习和无监督学习之间的区别。它使用了为监督学习设计的方法，但不需要单独的教学信号。

**无记忆模型**是解决此任务的标准方法。特别是，自回归模型可以从固定数量的前项预测序列中的下一个项，使用“延迟抽头”；前馈神经网络是广义的自回归模型，使用一个或多个层的非线性隐藏单元。然而，如果我们给我们的生成模型一些隐藏状态，并且如果我们给这个隐藏状态其自身的内部动态，我们就会得到一种更有趣的模型：它可以在其隐藏状态中存储信息很长时间。如果这些动态是嘈杂的，并且它们从隐藏状态中生成输出的方式是嘈杂的，我们就无法知道其确切的隐藏状态。我们能做的最好的是推断隐藏状态向量空间的概率分布。这种推断仅对两种类型的隐藏状态模型是可行的。

**递归神经网络**非常强大，因为它们结合了两种特性：1）分布式隐藏状态，使它们能够高效地存储大量过去的信息，2）非线性动态，使它们能够以复杂的方式更新其隐藏状态。拥有足够的神经元和时间，RNN可以计算出计算机能够计算的任何内容。那么，RNN可以表现出什么样的行为？它们可以振荡，可以收敛到点吸引子，可以表现得混乱。它们可能会学习实现许多小程序，每个程序捕获一个知识点，并并行运行，相互作用以产生非常复杂的效果。

![](../Images/e6043e3e88d5c61a2b88f6a281218d60.png)

然而，RNN 的计算能力使得训练非常困难。由于梯度爆炸或消失问题，训练 RNN 是相当困难的。当我们通过多个层进行反向传播时，梯度的大小会发生什么变化？如果权重很小，梯度会指数级缩小。如果权重很大，梯度会指数级增长。典型的前馈神经网络可以应对这些指数效应，因为它们只有少数几个隐藏层。另一方面，在对长序列进行训练的 RNN 中，梯度容易爆炸或消失。即使有良好的初始权重，仍然很难检测到当前目标输出依赖于很多时间步之前的输入，因此 RNN 处理长程依赖关系时困难重重。

本质上，有4种有效的方式来学习 RNN：

+   **长短期记忆**：将 RNN 制作成小模块，这些模块设计用于长时间记忆值。

+   **Hessian 自由优化**：使用一种可以检测到极小梯度但更小曲率的高级优化器来解决梯度消失问题。

+   **回声状态网络**：非常小心地初始化输入 -> 隐藏层和隐藏层 -> 隐藏层以及输出 -> 隐藏层的连接，以便隐藏状态具有一个巨大的弱耦合振荡器库，这些振荡器可以通过输入进行选择性驱动。

+   **良好的动量初始化**：像在回声状态网络中那样初始化，然后使用动量学习所有的连接。

### 4 — 长短期记忆网络

![](../Images/d6dbe07ea1caf153d300783588d959d4.png)

[Hochreiter & Schmidhuber (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf) 通过构建被称为**长短期记忆网络**的模型，解决了让 RNN 记住长时间信息（如数百个时间步）的问题。他们设计了一个使用逻辑单元和线性单元的记忆单元，并具有乘法交互。当“写入”门打开时，信息进入单元。只要“保持”门打开，信息就会留在单元中。通过打开“读取”门，可以从单元中读取信息。

阅读草书是一项自然的 RNN 任务。输入是一系列（x，y，p）坐标，其中 p 表示笔是否抬起或放下。输出是一系列字符。[Graves & Schmidhuber (2009)](http://people.idsia.ch/~juergen/nips2009.pdf) 证明了带有 LSTM 的 RNN 目前是阅读草书的最佳系统。简而言之，他们使用了一系列小图像作为输入，而不是笔坐标。

![](../Images/0de05190c7dbf55ac81c791d617a527e.png)

### 相关主题

+   [成为伟大数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)

+   [每个初学者数据科学家应掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)

+   [2021年最佳ETL工具](https://www.kdnuggets.com/2021/12/mozart-best-etl-tools-2021.html)

+   [停止学习数据科学以寻找目标，并以寻找目标来学习数据科学](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [学习数据科学统计学的最佳资源](https://www.kdnuggets.com/2021/12/springboard-top-resources-learn-data-science-statistics.html)

+   [成功数据科学家的5个特征](https://www.kdnuggets.com/2021/12/5-characteristics-successful-data-scientist.html)
