- en: Maximize Performance in Edge AI Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在边缘AI应用中最大化性能
- en: 原文：[https://www.kdnuggets.com/maximize-performance-in-edge-ai-applications](https://www.kdnuggets.com/maximize-performance-in-edge-ai-applications)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://www.kdnuggets.com/maximize-performance-in-edge-ai-applications](https://www.kdnuggets.com/maximize-performance-in-edge-ai-applications)'
- en: As AI migrates from the cloud to the Edge, we see the technology being used
    in an ever-expanding variety of use cases – ranging from anomaly detection to
    applications including smart shopping, surveillance, robotics, and factory automation.
    Hence, there is no one-size-fits-all solution. But with the rapid growth of camera-enabled
    devices, AI has been most widely adopted for analyzing real-time video data to
    automate video monitoring to enhance safety, improve operational efficiencies,
    and provide better customer experiences, ultimately gaining a competitive edge
    in their industries. To better support video analysis, you must understand the
    strategies for optimizing system performance in edge AI deployments.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着AI从云端迁移到边缘，我们看到这项技术在不断扩展的应用场景中得到使用——从异常检测到包括智能购物、监控、机器人技术和工厂自动化的应用。因此，没有一种通用的解决方案。但随着摄像头启用设备的快速增长，AI已被广泛应用于分析实时视频数据，以自动化视频监控，提升安全性，提高操作效率，并提供更好的客户体验，最终在其行业中获得竞争优势。为了更好地支持视频分析，你必须了解优化边缘AI部署系统性能的策略。
- en: Strategies for Optimizing AI System Performance Include
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化AI系统性能的策略包括
- en: Selecting the right-sized compute engines to meet or exceed the required performance
    levels. For an AI application, these compute engines must perform the functions
    of the entire vision pipeline (i.e., video pre- and post-processing, neural network
    inferencing).
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择合适大小的计算引擎，以满足或超越所需的性能水平。对于AI应用，这些计算引擎必须执行整个视觉管道的功能（即，视频的前处理和后处理、神经网络推理）。
- en: A dedicated AI accelerator, whether it be discrete or integrated into an SoC
    (as opposed to running the AI inferencing on a CPU or GPU) may be required.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要一个专用的AI加速器，无论是独立的还是集成在SoC中（而不是在CPU或GPU上运行AI推理）。
- en: Comprehending the difference between throughput and latency; whereby throughput
    is the rate that data can be processed in a system and latency measures the data
    processing delay through the system and is often associated with real-time responsiveness.
    For example, a system can generate image data at 100 frames per second (throughput)
    but it takes 100ms (latency) for an image to go through the system.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解吞吐量和延迟之间的区别；其中吞吐量是系统中数据处理的速率，而延迟则衡量数据通过系统的处理延迟，并且通常与实时响应性相关。例如，系统可以以每秒100帧的速度生成图像数据（吞吐量），但图像通过系统的时间是100毫秒（延迟）。
- en: Considering the ability to easily scale AI performance in the future to accommodate
    growing needs, changing requirements, and evolving technologies (e.g., more advanced
    AI models for increased functionality and accuracy). You can accomplish performance
    scaling using AI accelerators in module format or with additional AI accelerator
    chips.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑未来轻松扩展AI性能以适应不断增长的需求、变化的要求和不断发展的技术（例如，更先进的AI模型以提高功能性和准确性）。你可以通过使用模块化格式的AI加速器或额外的AI加速器芯片来实现性能扩展。
- en: Understanding Variable AI Performance Requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解可变AI性能要求
- en: The actual performance requirements are application dependent. Typically, one
    can expect that for video analytics, the system must process data streams coming
    in from cameras at 30-60 frames per second and with a resolution of 1080p or 4k.
    An AI-enabled camera would process a single stream; an edge appliance would process
    multiple streams in parallel. In either case, the edge AI system must support
    the pre-processing functions to transform the camera’s sensor data into a format
    that matches the input requirements of the AI inferencing section (Figure 1).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的性能要求依赖于应用。通常，可以预期对于视频分析，系统必须处理来自摄像头的数据流，速率为每秒30-60帧，分辨率为1080p或4k。一台AI启用的摄像头将处理单个流；一个边缘设备将并行处理多个流。在任何情况下，边缘AI系统必须支持预处理功能，将摄像头的传感器数据转换为符合AI推理部分输入要求的格式（见图1）。
- en: Pre-processing functions take in the raw data and perform tasks such as resize,
    normalization, and color space conversion, before feeding the input into the model
    running on the AI accelerator. Pre-processing can use efficient image processing
    libraries like OpenCV to reduce the preprocessing times. Postprocessing involves
    analyzing the output of the inference. It uses tasks such as non-maximum suppression
    (NMS interprets the output of most object detection models) and image display
    to generate actionable insights, such as bounding boxes, class labels, or confidence
    scores.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理功能接收原始数据，执行如调整大小、归一化和颜色空间转换等任务，然后将输入数据提供给运行在AI加速器上的模型。预处理可以使用高效的图像处理库，如OpenCV，以减少预处理时间。后处理涉及分析推理的输出。它使用如非极大值抑制（NMS解释大多数目标检测模型的输出）和图像显示等任务生成可操作的见解，如边界框、类别标签或置信度分数。
- en: '![Maximize Performance in Edge AI Applications](../Images/00e9b9c92eb05e73b077e71869076f03.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![最大化边缘AI应用的性能](../Images/00e9b9c92eb05e73b077e71869076f03.png)'
- en: Figure 1\. For AI model inferencing, the pre- and post-processing functions
    are typically performed on an applications processor.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 对于AI模型推理，预处理和后处理功能通常在应用处理器上执行。
- en: The AI model inferencing can have the additional challenge of processing multiple
    neural network models per frame, depending on the application’s capabilities.
    Computer vision applications usually involve multiple AI tasks requiring a pipeline
    of multiple models. Furthermore, one model’s output is often the next model’s
    input. In other words, models in an application often depend on each other and
    must be executed sequentially. The exact set of models to execute may not be static
    and could vary dynamically, even on a frame-by-frame basis.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: AI模型推理可能面临额外的挑战，即每帧处理多个神经网络模型，具体取决于应用的能力。计算机视觉应用通常涉及多个AI任务，需要多个模型的流水线。此外，一个模型的输出通常是下一个模型的输入。换句话说，应用中的模型通常彼此依赖，并且必须按顺序执行。要执行的模型集合可能不是静态的，甚至可能在每帧之间动态变化。
- en: The challenge of running multiple models dynamically requires an external AI
    accelerator with dedicated and sufficiently large memory to store the models.
    Often the integrated AI accelerator inside an SoC is unable to manage the multi-model
    workload due to constraints imposed by shared memory subsystem and other resources
    in the SoC.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 运行多个模型动态的挑战需要一个外部AI加速器，该加速器需配备专用且足够大的内存来存储这些模型。由于SoC中共享内存子系统和其他资源的限制，SoC内部集成的AI加速器通常无法管理多模型工作负载。
- en: For example, motion prediction-based object tracking relies on continuous detections
    to determine a vector which is used to identify the tracked object at a future
    position. The effectiveness of this approach is limited because it lacks true
    reidentification capability. With motion prediction, an object’s track can be
    lost due to missed detections, occlusions, or the object leaving the field of
    view, even momentarily.  Once lost, there is no way to re-associate the object’s
    track. Adding reidentification solves this limitation but requires a visual appearance
    embedding (i.e., an image fingerprint). Appearance embeddings require a second
    network to generate a feature vector by processing the image contained inside
    the bounding box of the object detected by the first network. This embedding can
    be used to reidentify the object again, irrespective of time or space. Since embeddings
    must be generated for each object detected in the field of view, processing requirements
    increase as the scene becomes busier. Object tracking with reidentification requires
    careful consideration between performing high-accuracy / high resolution / high-frame
    rate detection and reserving sufficient overhead for embeddings scalability. One
    way to solve the processing requirement is to use a dedicated AI accelerator. 
    As mentioned earlier, the SoC’s AI engine can suffer from the lack of shared memory
    resources.  Model optimization can also be used to lower the processing requirement,
    but it could impact performance and/or accuracy.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，基于运动预测的物体跟踪依赖于连续检测来确定一个向量，这个向量用于识别未来位置的跟踪物体。由于缺乏真正的重识别能力，这种方法的效果有限。使用运动预测时，物体的轨迹可能因错过检测、遮挡或物体暂时离开视野而丢失。一旦丢失，就无法重新关联物体的轨迹。增加重识别功能可以解决这个限制，但需要一个视觉外观嵌入（即图像指纹）。外观嵌入需要第二个网络来通过处理包含在第一个网络检测到的物体边界框内的图像来生成特征向量。这个嵌入可以用来重新识别物体，无论时间或空间如何。由于必须为视野中的每个检测到的物体生成嵌入，因此随着场景变得更加繁忙，处理要求也会增加。带有重识别的物体跟踪需要在执行高精度/高分辨率/高帧率检测和保留足够的开销以便嵌入可扩展性之间进行仔细权衡。解决处理需求的一种方法是使用专用的AI加速器。如前所述，SoC的AI引擎可能会受到共享内存资源不足的影响。模型优化也可以用来降低处理需求，但可能会影响性能和/或精度。
- en: Don’t limit AI Performance with System-level Overhead
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不要让系统级开销限制AI性能
- en: In a smart camera or edge appliance, the integrated SoC (i.e., host processor)
    acquires the video frames and performs the pre-processing steps we described earlier.
    These functions can be performed with the SoC’s CPU cores or GPU (if one is available),
    but they can also be performed by dedicated hardware accelerators in the SoC (e.g.,
    image signal processor). After these pre-processing steps are completed, the AI
    accelerator that is integrated into the SoC can then directly access this quantized
    input from system memory, or in the case of a discrete AI accelerator, the input
    is then delivered for inference, typically over the USB or PCIe interface.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在智能摄像头或边缘设备中，集成的SoC（即主处理器）获取视频帧并执行我们之前描述的预处理步骤。这些功能可以由SoC的CPU核心或GPU（如果有的话）来执行，也可以由SoC中的专用硬件加速器（例如图像信号处理器）来完成。在这些预处理步骤完成后，集成在SoC中的AI加速器可以直接访问系统内存中的量化输入，或者在离散AI加速器的情况下，输入则通过USB或PCIe接口传递进行推理。
- en: An integrated SoC can contain a range of computation units, including CPUs,
    GPUs, AI accelerator, vision processors, video encoders/decoders, image signal
    processor (ISP), and more. These computation units all share the same memory bus
    and consequently access to the same memory. Furthermore, the CPU and GPU might
    also have to play a role in the inference and these units will be busy running
    other tasks in a deployed system. This is what we mean by system-level overhead
    (Figure 2).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 集成SoC可以包含一系列计算单元，包括CPU、GPU、AI加速器、视觉处理器、视频编码器/解码器、图像信号处理器（ISP）等。这些计算单元共享相同的内存总线，从而访问相同的内存。此外，CPU和GPU可能还需要在推理中发挥作用，并且这些单元在部署系统中将忙于运行其他任务。这就是我们所说的系统级开销（见图2）。
- en: Many developers mistakenly evaluate the performance of the built-in AI accelerator
    in the SoC without considering the effect of system-level overhead on total performance. 
    As an example, consider running a YOLO benchmark on a 50 TOPS AI accelerator integrated
    in an SoC, which mightobtain a benchmark result of 100 inferences/second (IPS).
    But in a deployed system with all its other computational units active, those
    50 TOPS could reduce to something like 12 TOPS and the overall performance would
    only yield 25 IPS, assuming a generous 25% utilization factor. System overhead
    is always a factor if the platform is continuously processing video streams. Alternatively,
    with a discrete AI accelerator (e.g., Kinara Ara-1, Hailo-8, Intel Myriad X),
    the system-level utilization could be greater than 90% because once the host SoC
    initiates the inferencing function and transfers the AI model’s input data, the
    accelerator runs autonomously utilizing its dedicated memory for accessing model
    weights and parameters.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 许多开发人员错误地评估了 SoC 内置 AI 加速器的性能，而没有考虑系统级开销对整体性能的影响。例如，考虑在 SoC 中运行一个 50 TOPS 的
    AI 加速器上的 YOLO 基准测试，可能会得到 100 次推断/秒 (IPS) 的基准结果。但在实际部署的系统中，所有其他计算单元都在活动时，这 50 TOPS
    可能会减少到约 12 TOPS，而整体性能只会得到 25 IPS，假设使用了宽松的 25% 利用率因素。系统开销始终是一个因素，特别是当平台持续处理视频流时。相反，使用离散
    AI 加速器（例如，Kinara Ara-1、Hailo-8、Intel Myriad X），系统级利用率可能会超过 90%，因为一旦主 SoC 启动推断功能并传输
    AI 模型的输入数据，加速器会自主运行，利用其专用内存访问模型权重和参数。
- en: '![Maximize Performance in Edge AI Applications](../Images/169f84e82b6cf6d3a162e4e1d5f12db8.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![在边缘 AI 应用中最大化性能](../Images/169f84e82b6cf6d3a162e4e1d5f12db8.png)'
- en: Figure 2\. The shared memory bus will govern the system-level performance, shown
    here with estimated values. Real values will vary based on your application usage
    model and the SoC’s compute unit configuration.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 共享内存总线将决定系统级性能，此处显示了估计值。实际值将根据你的应用使用模型和 SoC 的计算单元配置有所不同。
- en: Video Analytics at the Edge Require Low-latency
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 边缘的视频分析需要低延迟
- en: Until this point, we’ve discussed AI performance in terms of frames per second
    and TOPS. But low latency is another important requirement to deliver a system’s
    real-time responsiveness. For example, in gaming, low latency is critical for
    a seamless and responsive gaming experience, particularly in motion-controlled
    games and virtual reality (VR) systems. In autonomous driving systems, low latency
    is vital for real-time object detection, pedestrian recognition, lane detection,
    and traffic sign recognition to avoid compromising safety. Autonomous driving
    systems typically require end-to-end latency of less than 150ms from detection
    to the actual action. Similarly, in manufacturing, low latency is essential for
    real-time defect detection, anomaly recognition, and robotic guidance depend on
    low-latency video analytics to ensure efficient operation and minimize production
    downtime.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了以每秒帧数和 TOPS 作为衡量标准的 AI 性能。但低延迟是提供系统实时响应的另一个重要要求。例如，在游戏中，低延迟对于无缝和响应迅速的游戏体验至关重要，尤其是在动作控制游戏和虚拟现实
    (VR) 系统中。在自动驾驶系统中，低延迟对于实时物体检测、行人识别、车道检测和交通标志识别至关重要，以避免影响安全。自动驾驶系统通常要求从检测到实际动作的端到端延迟小于
    150 毫秒。类似地，在制造业中，低延迟对实时缺陷检测、异常识别和机器人指导至关重要，依赖于低延迟视频分析以确保高效操作并减少生产停机时间。
- en: 'In general, there are three components of latency in a video analytics application
    (Figure 3):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，视频分析应用中的延迟有三个组成部分（图 3）：
- en: Data capture latency is the time from the camera sensor capturing a video frame
    to the frame’s availability to the analytics system for processing. You can optimize
    this latency by choosing a camera with a fast sensor and low latency processor,
    selecting optimal frame rates, and using efficient video compression formats.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据捕获延迟是指从相机传感器捕获视频帧到帧可用于分析系统处理的时间。你可以通过选择具有快速传感器和低延迟处理器的相机、选择最佳帧率和使用高效的视频压缩格式来优化这个延迟。
- en: Data transfer latency is the time for captured and compressed video data to
    travel from the camera to the edge devices or local servers. This includes network
    processing delays that occur at each end point.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据传输延迟是指从相机捕获并压缩的视频数据传输到边缘设备或本地服务器的时间。这包括每个端点发生的网络处理延迟。
- en: Data processing latency refers to the time for the edge devices to perform video
    processing tasks such as frame decompression and analytics algorithms (e.g., motion
    prediction-based object tracking, face recognition). As pointed out earlier, processing
    latency is even more important for applications that must run multiple AI models
    for each video frame.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据处理延迟是指边缘设备执行视频处理任务的时间，如帧解压和分析算法（例如，基于运动预测的物体跟踪、人脸识别）。正如前面提到的，对于需要在每个视频帧上运行多个
    AI 模型的应用程序，处理延迟尤为重要。
- en: '![Maximize Performance in Edge AI Applications](../Images/7f4b95b9683367052948b86bce43b23a.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![最大化边缘 AI 应用中的性能](../Images/7f4b95b9683367052948b86bce43b23a.png)'
- en: Figure 3\. The video analytics pipeline consists of data capture, data transfer
    and data processing.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3. 视频分析管道包括数据捕获、数据传输和数据处理。
- en: The data processing latency can be optimized using an AI accelerator with an
    architecture designed to minimize data movement across the chip and between compute
    and various levels of the memory hierarchy. Also, to improve the latency and system-level
    efficiency, the architecture must support zero (or near zero) switching time between
    models, to better support the multi-model applications we discussed earlier. Another
    factor for both improved performance and latency relates to algorithmic flexibility.
    In other words, some architectures are designed for optimal behavior only on specific
    AI models, but with the rapidly changing AI environment, new models for higher
    performance and better accuracy are appearing in what seems like every other day.
    Therefore, select an edge AI processor with no practical restrictions on model
    topology, operators, and size.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理延迟可以通过使用设计为最小化芯片上数据移动和计算与各种内存层次之间数据移动的 AI 加速器来优化。此外，为了提高延迟和系统级效率，架构必须支持模型之间的零（或接近零）切换时间，以更好地支持我们之前讨论的多模型应用。另一个提升性能和延迟的因素与算法灵活性有关。换句话说，一些架构仅针对特定
    AI 模型优化行为，但随着 AI 环境的迅速变化，每隔一天似乎就有新的高性能和更高精度的模型出现。因此，选择一种对模型拓扑、操作符和大小没有实际限制的边缘
    AI 处理器。
- en: There are many factors to be considered in maximizing performance in an edge
    AI appliance including performance and latency requirements and system overhead. 
    A successful strategy should consider an external AI accelerator to overcome the
    memory and performance limitations in the SoC’s AI engine.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在边缘 AI 设备中，最大化性能需要考虑许多因素，包括性能和延迟要求以及系统开销。成功的策略应该考虑使用外部 AI 加速器，以克服 SoC 的 AI 引擎中的内存和性能限制。
- en: '**[C.H. Chee](https://www.linkedin.com/in/c-h-chee/)** is an accomplished product
    marketing and management executive, Chee has extensive experience in promoting
    products and solutions in the semiconductor industry, focusing on vision-based
    AI, connectivity and video interfaces for multiple markets including enterprise
    and consumer. As an entrepreneur, Chee co-founded two video semiconductor start-ups
    that were acquired by a public semiconductor company. Chee led product marketing
    teams and enjoys working with a small team that focuses on achieving great results.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**[C.H. Chee](https://www.linkedin.com/in/c-h-chee/)** 是一位成功的产品营销和管理高管，Chee
    在推广半导体行业的产品和解决方案方面具有广泛经验，专注于基于视觉的 AI、连接性和视频接口，涵盖企业和消费市场。作为一名企业家，Chee 联合创办了两家视频半导体初创公司，这些公司后来被一家上市半导体公司收购。Chee
    领导过产品营销团队，喜欢与专注于取得出色成果的小团队合作。'
- en: More On This Topic
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关信息
- en: '[Maximize Your Productivity as a Data Scientist by Organizing](https://www.kdnuggets.com/2022/03/maximize-productivity-data-scientist-organizing.html)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过组织工作最大化您的数据科学家生产力](https://www.kdnuggets.com/2022/03/maximize-productivity-data-scientist-organizing.html)'
- en: '[Maximize Your Value With The 3rd Best Online Master’s In Data…](https://www.kdnuggets.com/2023/05/bay-path-maximize-value-online-masters-data-science.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过第三名最佳在线数据硕士课程最大化您的价值…](https://www.kdnuggets.com/2023/05/bay-path-maximize-value-online-masters-data-science.html)'
- en: '[Machine Learning on the Edge](https://www.kdnuggets.com/2022/10/machine-learning-edge.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[边缘上的机器学习](https://www.kdnuggets.com/2022/10/machine-learning-edge.html)'
- en: '[Windows on Snapdragon Brings Hybrid AI to Apps at the Edge](https://www.kdnuggets.com/qualcomm-windows-on-snapdragon-brings-hybrid-ai-to-apps-at-the-edge)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Windows on Snapdragon 将混合 AI 引入边缘应用](https://www.kdnuggets.com/qualcomm-windows-on-snapdragon-brings-hybrid-ai-to-apps-at-the-edge)'
- en: '[Introducing TPU v4: Googles Cutting Edge Supercomputer for Large…](https://www.kdnuggets.com/2023/04/introducing-tpu-v4-googles-cutting-edge-supercomputer-large-language-models.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍TPU v4：谷歌前沿超级计算机用于大型语言模型](https://www.kdnuggets.com/2023/04/introducing-tpu-v4-googles-cutting-edge-supercomputer-large-language-models.html)'
- en: '[The Promise of Edge AI and Approaches for Effective Adoption](https://www.kdnuggets.com/the-promise-of-edge-ai-and-approaches-for-effective-adoption)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[边缘AI的承诺及其有效采纳的方法](https://www.kdnuggets.com/the-promise-of-edge-ai-and-approaches-for-effective-adoption)'
