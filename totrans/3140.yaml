- en: Introduction to k-Nearest Neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/03/introduction-k-nearest-neighbors.html](https://www.kdnuggets.com/2018/03/introduction-k-nearest-neighbors.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: '**By [Devin Soni](https://www.linkedin.com/in/devinsoni/), Computer Science
    Student**'
  prefs: []
  type: TYPE_NORMAL
- en: '![KNN](../Images/f5fc732a89c1efe810602d5129c0b000.png)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The** k-Nearest-Neighbors (kNN)** method of classification is one of the simplest
    methods in machine learning, and is a great way to introduce yourself to machine
    learning and classification in general.At its most basic level, it is essentially
    classification by finding the most similar data points in the training data, and
    making an educated guess based on their classifications. Although very simple
    to understand and implement, this method has seen wide application in many domains,
    such as in **recommendation systems**,** semantic searching**, and** anomaly detection**.
  prefs: []
  type: TYPE_NORMAL
- en: As we would need to in any machine learning problem, we must first find a way
    to represent data points as **feature vectors**. A feature vector is our mathematical
    representation of data, and since the desired characteristics of our data may
    not be inherently numerical, preprocessing and feature-engineering may be required
    in order to create these vectors. Given data with **N** unique features, the feature
    vector would be a vector of length** N**, where entry **I** of the vector represents
    that data point’s value for feature** I**. Each feature vector can thus be thought
    of as a point in **R^N**.
  prefs: []
  type: TYPE_NORMAL
- en: Now, unlike most other methods of classification, kNN falls under **lazy learning**,
    which means that there is **no explicit training phase before classification**.
    Instead, any attempts to generalize or abstract the data is made upon classification.
    While this does mean that we can immediately begin classifying once we have our
    data, there are some inherent problems with this type of algorithm. We must be
    able to keep the entire training set in memory unless we apply some type of reduction
    to the data-set, and performing classifications can be computationally expensive
    as the algorithm parse through all data points for each classification. **For
    these reasons, kNN tends to work best on smaller data-sets that do not have many
    features.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have formed our training data-set, which is represented as an **M** x** N**
    matrix where **M** is the number of data points and **N** is the number of features,
    we can now begin classifying. The gist of the kNN method is, for each classification
    query, to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: There are two important decisions that must be made before making classifications.
    One is the value of **k** that will be used; this can either be decided arbitrarily,
    or you can try **cross-validation** to find an optimal value. The next, and the
    most complex, is the** distance metric** that will be used.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different ways to compute distance, as it is a fairly ambiguous
    notion, and the proper metric to use is always going to be determined by the data-set
    and the classification task. Two popular ones, however, are **Euclidean distance **and** Cosine
    similarity**.
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean distance is probably the one that you are most familiar with; it is
    essentially the magnitude of the vector obtained by subtracting the training data
    point from the point to be classified.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d05ff37f97ea122929ddf4ed4524f14c.png)'
  prefs: []
  type: TYPE_IMG
- en: General formula for Euclidean distance
  prefs: []
  type: TYPE_NORMAL
- en: Another common metric is Cosine similarity. Rather than calculating a magnitude,
    Cosine similarity instead uses the difference in direction between two vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ebfcde9367ef46848dd87807d63e0a99.png)'
  prefs: []
  type: TYPE_IMG
- en: General formula for Cosine similarity
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a metric can often be tricky, and it may be best to just use cross-validation
    to decide, unless you have some prior insight that clearly leads to using one
    over the other. For example, for something like word vectors, you may want to
    use Cosine similarity because the direction of a word is more meaningful than
    the sizes of the component values. Generally, both of these methods will run in
    roughly the same time, and will suffer from highly-dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: After doing all of the above and deciding on a metric, the result of the kNN
    algorithm is a decision boundary that partitions **R^N** into sections. Each section
    (colored distinctly below) represents a class in the classification problem. The
    boundaries need not be formed with actual training examples — they are instead
    calculated using the distance metric and the available training points. By taking **R^N** in
    (small) chunks, we can calculate the most likely class for a hypothetical data-point
    in that region, and we thus color that chunk as being in the region for that class.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb0c8d05603c0f84b9aaa1034acdbd3b.png)'
  prefs: []
  type: TYPE_IMG
- en: This information is all that is needed to begin implementing the algorithm,
    and doing so should be relatively simple. There are, of course, many ways to improve
    upon this base algorithm. Common modifications include weighting, and specific
    preprocessing to reduce computation and reduce noise, such as various algorithms
    for feature extraction and dimension reduction. Additionally, the kNN method has
    also been used, although less-commonly, for regression tasks, and operates in
    a manner very similar to that of the classifier through averaging.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Devin Soni](https://www.linkedin.com/in/devinsoni/)** is a computer
    science student interested in machine learning and data science. He will be a
    software engineering intern at Airbnb in 2018\. He can be reached [via LinkedIn](https://www.linkedin.com/in/devinsoni/).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/introduction-to-k-nearest-neighbors-3b534bb11d26).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Introduction to Markov Chains](/2018/03/introduction-markov-chains.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Tour of The Top 10 Algorithms for Machine Learning Newbies](/2018/02/tour-top-10-algorithms-machine-learning-newbies.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[K-Nearest Neighbors – the Laziest Machine Learning Technique](/2017/09/rapidminer-k-nearest-neighbors-laziest-machine-learning-technique.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[From Theory to Practice: Building a k-Nearest Neighbors Classifier](https://www.kdnuggets.com/2023/06/theory-practice-building-knearest-neighbors-classifier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Nearest Neighbors for Classification](https://www.kdnuggets.com/2022/04/nearest-neighbors-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[K-nearest Neighbors in Scikit-learn](https://www.kdnuggets.com/2022/07/knearest-neighbors-scikitlearn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to Binary Classification with PyCaret](https://www.kdnuggets.com/2021/12/introduction-binary-classification-pycaret.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to Clustering in Python with PyCaret](https://www.kdnuggets.com/2021/12/introduction-clustering-python-pycaret.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Essential Math for Data Science: Visual Introduction to Singular…](https://www.kdnuggets.com/2022/06/essential-math-data-science-visual-introduction-singular-value-decomposition.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
