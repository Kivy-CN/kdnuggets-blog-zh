["```py\ntext = “Artificial Intelligence (AI) is likely to be either the best or the worst thing to happen to humanity.”\n```", "```py\nimport langdetect\nfrom langdetect import detect_langs\nprint(detect_langs(text))\n```", "```py\nlen(text)\n```", "```py\nlen(set(text))\n```", "```py\nprint(sorted(set(text)))\n```", "```py\nimport nltk\nfrom nltk.tokenize import word_tokenize\ntokenized_word = word_tokenize(text)\nprint(tokenized_word)\n```", "```py\ntype(tokenized_word)\n```", "```py\ntokenized_word[2:9]\n```", "```py\nlen(tokenized_word)\n```", "```py\nlen(set(tokenized_word))\n```", "```py\nlen(set(tokenized_word)) / len(tokenized_word)\n```", "```py\ntk_low = [w.lower() for w in tokenized_word]\nprint(tk_low)\n```", "```py\nnltk.download(“punkt”)\ntk_low_np = remove_punct(tk_low)\nprint(tk_low_np)\n```", "```py\nfrom nltk.probability import FreqDist\nfdist = FreqDist(tk_low_np)\nfdist.plot(title = ‘Word frequency distribution’, cumulative = True)\n```", "```py\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words(“english”))\nprint(stop_words)\n```", "```py\nprint(‘the’ in stop_words)\n```", "```py\nfiltered_text = []\nfor w in tk_low_np:\n   if w not in stop_words:\n      filtered_text.append(w)\nprint(filtered_text)\n```", "```py\nimport re\n[w for w in filtered_text if re.search(‘st$’, w)]\n```", "```py\nlen(re.findall(r’[aeiou]’, filtered_text[0]))\n```", "```py\nx = re.sub('ce', 't', filtered_text[1])\nprint(x)\n```"]