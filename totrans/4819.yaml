- en: Deep Quantile Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度分位回归
- en: 原文：[https://www.kdnuggets.com/2018/07/deep-quantile-regression.html](https://www.kdnuggets.com/2018/07/deep-quantile-regression.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/07/deep-quantile-regression.html](https://www.kdnuggets.com/2018/07/deep-quantile-regression.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: '**By [Sachin Abeywardana](https://www.linkedin.com/in/sachinabeywardana/),
    Founder of [DeepSchool.io](http://DeepSchool.io)**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Sachin Abeywardana](https://www.linkedin.com/in/sachinabeywardana/)，[DeepSchool.io](http://DeepSchool.io)
    的创始人**'
- en: One area that Deep Learning has not explored extensively is the uncertainty
    in estimates. Most Deep Learning frameworks currently focus on giving a best estimate
    as defined by a loss function. Occasionally something beyond a point estimate
    is required to make a decision. This is where a distribution would be useful.
    Bayesian statistics lends itself to this problem really well since a distribution
    over the dataset is inferred. However, Bayesian methods so far have been rather
    slow and would be expensive to apply to large datasets.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习目前还没有广泛探讨估计中的不确定性。大多数深度学习框架目前专注于提供由损失函数定义的最佳估计。有时，除了点估计之外，还需要一些其他信息来做出决策。这时，分布将非常有用。贝叶斯统计非常适合这个问题，因为它推断了数据集上的分布。然而，迄今为止，贝叶斯方法的计算速度较慢，而且在大数据集上应用时会非常昂贵。
- en: As far as decision making goes, most people actually require quantiles as opposed
    to true uncertainty in an estimate. For instance when measuring a child’s weight
    for a given age, the weight of an individual will vary. What would be interesting
    is (for arguments sake) the 10th and 90th percentile. Note that the uncertainty
    is different to quantiles in that I could request for a confidence interval on
    the 90th quantile. This article will purely focus on inferring quantiles.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 就决策而言，大多数人实际上需要的是分位数，而不是估计中的真正不确定性。例如，在测量一个特定年龄的孩子的体重时，个体的体重会有所不同。比较有趣的是（为了论证）第
    10 和第 90 百分位数。请注意，不确定性不同于分位数，因为我可以要求第 90 分位数的置信区间。本文将纯粹关注推断分位数。
- en: Quantile Regression Loss function
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分位回归损失函数
- en: In regression the most commonly used loss function is the mean squared error
    function. If we were to take the negative of this loss and exponentiate it, the
    result would correspond to the gaussian distribution. The mode of this distribution
    (the peak) corresponds to the mean parameter. Hence, when we predict using a neural
    net that minimised this loss we are predicting the mean value of the output which
    may have been noisy in the training set.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归中，最常用的损失函数是均方误差函数。如果我们取这个损失的负值并进行指数运算，结果将对应于高斯分布。这个分布的模式（峰值）对应于均值参数。因此，当我们使用一个最小化该损失的神经网络进行预测时，我们是在预测训练集中可能存在噪声的输出均值。
- en: 'The loss in Quantile Regression for an individual data point is defined as:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 分位回归中单个数据点的损失定义为：
- en: '![](../Images/40bfe34e63f6ca33ef68a47761ff042a.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40bfe34e63f6ca33ef68a47761ff042a.png)'
- en: Loss of individual data point
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 单个数据点的损失
- en: where alpha is the required quantile (a value between 0 and 1) and
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 alpha 是所需的分位数（一个介于 0 和 1 之间的值），
- en: '![](../Images/e2b5a8b41ef98af888d44ee4d2dd50bc.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2b5a8b41ef98af888d44ee4d2dd50bc.png)'
- en: 'where f(x) is the predicted (quantile) model and y is the observed value for
    the corresponding input x. The average loss over the entire dataset is shown below:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 f(x) 是预测的（分位数）模型，y 是对应输入 x 的观测值。整个数据集的平均损失如下所示：
- en: '![](../Images/f1f0c04d829a5a7470942307398b8560.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1f0c04d829a5a7470942307398b8560.png)'
- en: Loss funtion
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数
- en: If we were to take the negative of the individual loss and exponentiate it,
    we get the distribution know as the Asymmetric Laplace distribution, shown below. **The
    reason that this loss function works is that if we were to find the area under
    the graph to the left of zero it would be alpha, the required quantile.**
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们取单个损失的负值并进行指数运算，我们会得到下图所示的非对称拉普拉斯分布。**这个损失函数有效的原因是，如果我们找到图形左侧零点下的面积，它将是
    alpha，即所需的分位数。**
- en: '![](../Images/d4b53cd78dff1dae2ba8d6546aa00ee3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4b53cd78dff1dae2ba8d6546aa00ee3.png)'
- en: Probability distribution function (pdf) of an Asymmetric Laplace distribution.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 非对称拉普拉斯分布的概率密度函数（pdf）。
- en: The case when alpha=0.5 is most likely more familiar since it corresponds to
    the Mean Absolute Error (MAE). This loss function consistently estimates the median
    (50th percentile), instead of the mean.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当 alpha=0.5 时，这种情况可能更为熟悉，因为它对应于平均绝对误差（MAE）。这个损失函数始终估计中位数（第50百分位数），而不是均值。
- en: Modelling in Keras
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Keras 中建模
- en: The forward model is no different to what you would have had when doing MSE
    regression. All that changes is the loss function. The following few lines defines
    the loss function defined in the section above.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 前向模型与进行 MSE 回归时没有不同。唯一改变的是损失函数。以下几行定义了上述部分中定义的损失函数。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When it comes to compiling the neural network, just simply do:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在编译神经网络时，只需简单地执行：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For a full example see [this Jupyter notebook](https://github.com/sachinruk/KerasQuantileModel/blob/master/Keras%20Quantile%20Model.ipynb) where
    I look at a motor cycle crash dataset over time. The results are reproduced below
    where I show the 10th 50th and 90th quantiles.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解完整示例，请参见 [这个 Jupyter 笔记本](https://github.com/sachinruk/KerasQuantileModel/blob/master/Keras%20Quantile%20Model.ipynb)，我在其中查看了摩托车事故数据集随时间的变化。结果如下面所示，我展示了第10、第50和第90百分位数。
- en: '![](../Images/da4fd8f5e51325a415feff9c2ca70f14.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da4fd8f5e51325a415feff9c2ca70f14.png)'
- en: Acceleration over time of crashed motor cycle.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 崩溃摩托车的加速度随时间的变化。
- en: '**Final Notes**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**最终说明**'
- en: '**Note that for each quantile I had to rerun the training.** This is due to
    the fact that for each quantile the loss function is different, as the quantile
    in itself is a parameter for the loss function.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**注意，我必须对每个分位数重新运行训练。** 这是因为每个分位数的损失函数是不同的，因为分位数本身是损失函数的一个参数。'
- en: Due to the fact that each model is a simple rerun, there is a risk of quantile
    cross over. i.e. the 49th quantile may go above the 50th quantile at some stage.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于每个模型都是简单的重新运行，存在分位数交叉的风险。即，第49百分位数在某些阶段可能超过第50百分位数。
- en: Note that the quantile 0.5 is the same as median, which you can attain by minimising
    Mean Absolute Error, which you can attain in Keras regardless with `loss='mae'`.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，分位数 0.5 与中位数相同，你可以通过最小化平均绝对误差（Mean Absolute Error）来获得中位数，而在 Keras 中，你可以使用
    `loss='mae'` 来实现。
- en: Uncertainty and quantiles are **not** the same thing. But most of the time you
    care about quantiles and not uncertainty.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不确定性和分位数 **不是** 同一回事。但大多数时候，你关心的是分位数，而不是不确定性。
- en: If you really do want uncertainty with deep nets checkout [http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html](http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html)
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你真的想了解深度网络的不确定性，可以查看 [http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html](http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html)
- en: '**Edit 1**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**编辑 1**'
- en: As pointed out by [Anders Christiansen](https://medium.com/@andersasac?source=post_info_responses---------0----------------) (in
    the responses) we may be able to get multiple quantiles in one go by having multiple
    objectives. Keras however combines all loss functions by a `loss_weights` argument
    as shown here: [https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models](https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models).
    Would be easier to implement this in tensorflow. If anyone beats me to it would
    be happy to change my notebook/ post to reflect this. As a rough guide if we wanted
    the quantiles 0.1, 0.5, 0.9, the last layer in Keras would have `Dense(3)` instead,
    with each node connected to a loss function.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 [Anders Christiansen](https://medium.com/@andersasac?source=post_info_responses---------0----------------)（在回应中）指出的那样，我们可以通过设置多个目标在一次运行中获得多个分位数。然而，Keras
    通过 `loss_weights` 参数将所有损失函数结合在一起，如此处所示：[https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models](https://keras.io/getting-started/functional-api-guide/#multi-input-and-multi-output-models)。在
    TensorFlow 中实现这一点会更容易。如果有人比我更早完成这个任务，我愿意更新我的笔记本/帖子以反映这一点。作为一个粗略的指南，如果我们需要分位数 0.1、0.5
    和 0.9，Keras 中的最后一层应为 `Dense(3)`，每个节点连接到一个损失函数。
- en: '**Edit 2**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**编辑 2**'
- en: 'Thanks to [Jacob Zweig](https://medium.com/@jacob.zweig?source=post_info_responses---------1----------------)
    for implementing the simultaneous multiple Quantiles in TensorFlow: [https://github.com/strongio/quantile-regression-tensorflow/blob/master/Quantile%20Loss.ipynb](https://github.com/strongio/quantile-regression-tensorflow/blob/master/Quantile%20Loss.ipynb)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢 [Jacob Zweig](https://medium.com/@jacob.zweig?source=post_info_responses---------1----------------)
    实现了 TensorFlow 中的同时多分位数： [https://github.com/strongio/quantile-regression-tensorflow/blob/master/Quantile%20Loss.ipynb](https://github.com/strongio/quantile-regression-tensorflow/blob/master/Quantile%20Loss.ipynb)
- en: If you are enjoying my tutorials/ blog posts, **consider supporting me** on
    [https://www.patreon.com/deepschoolio](https://www.patreon.com/deepschoolio) or
    by subscribing to my YouTube channel [https://www.youtube.com/user/sachinabey](https://www.youtube.com/user/sachinabey)
    (or both!).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢我的教程/博客文章，**考虑支持我** [https://www.patreon.com/deepschoolio](https://www.patreon.com/deepschoolio)
    或订阅我的 YouTube 频道 [https://www.youtube.com/user/sachinabey](https://www.youtube.com/user/sachinabey)（或者两者都订阅！）。
- en: '**Bio: [Sachin Abeywardana](https://www.linkedin.com/in/sachinabeywardana/)**
    is a PhD in Machine Learning and Founder of [DeepSchool.io](http://DeepSchool.io).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**简历：[Sachin Abeywardana](https://www.linkedin.com/in/sachinabeywardana/)**
    是机器学习博士及 [DeepSchool.io](http://DeepSchool.io) 创始人。'
- en: '[Original](https://towardsdatascience.com/deep-quantile-regression-c85481548b5a).
    Reposted with permission.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://towardsdatascience.com/deep-quantile-regression-c85481548b5a)。经许可转载。'
- en: '**Related:**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[DeepSchool.io: Deep Learning Learning](/2017/12/deepschool-io-deep-learning-learning.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DeepSchool.io：深度学习学习](/2017/12/deepschool-io-deep-learning-learning.html)'
- en: '[Docker for Data Science](/2018/01/docker-data-science.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学的 Docker](/2018/01/docker-data-science.html)'
- en: '[Using Genetic Algorithm for Optimizing Recurrent Neural Networks](/2018/01/genetic-algorithm-optimizing-recurrent-neural-network.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用遗传算法优化递归神经网络](/2018/01/genetic-algorithm-optimizing-recurrent-neural-network.html)'
- en: '* * *'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的 IT'
- en: '* * *'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Comparing Linear and Logistic Regression](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[比较线性回归和逻辑回归](https://www.kdnuggets.com/2022/11/comparing-linear-logistic-regression.html)'
- en: '[3 Reasons Why You Should Use Linear Regression Models Instead of…](https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用线性回归模型而不是…的 3 个理由](https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html)'
- en: '[Linear vs Logistic Regression: A Succinct Explanation](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[线性回归与逻辑回归：简明解释](https://www.kdnuggets.com/2022/03/linear-logistic-regression-succinct-explanation.html)'
- en: '[KDnuggets News 22:n12, March 23: Best Data Science Books for…](https://www.kdnuggets.com/2022/n12.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻 22:n12, 3 月 23 日：最佳数据科学书籍…](https://www.kdnuggets.com/2022/n12.html)'
- en: '[An Overview of Logistic Regression](https://www.kdnuggets.com/2022/02/overview-logistic-regression.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[逻辑回归概述](https://www.kdnuggets.com/2022/02/overview-logistic-regression.html)'
- en: '[Logistic Regression for Classification](https://www.kdnuggets.com/2022/04/logistic-regression-classification.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[逻辑回归用于分类](https://www.kdnuggets.com/2022/04/logistic-regression-classification.html)'
