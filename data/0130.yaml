- en: 'Automating the Chain of Thought: How AI Can Prompt Itself to Reason'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化思维链：AI 如何自我提示进行推理
- en: 原文：[https://www.kdnuggets.com/2023/07/automating-chain-of-thought-ai-prompt-itself-reason.html](https://www.kdnuggets.com/2023/07/automating-chain-of-thought-ai-prompt-itself-reason.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/07/automating-chain-of-thought-ai-prompt-itself-reason.html](https://www.kdnuggets.com/2023/07/automating-chain-of-thought-ai-prompt-itself-reason.html)
- en: '![Automating the Chain of Thought: How AI Can Prompt Itself to Reason](../Images/7aba366ad9c38f56a4801543271f5602.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![自动化思维链：AI 如何自我提示进行推理](../Images/7aba366ad9c38f56a4801543271f5602.png)'
- en: Image created by author with Midjourney
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用 Midjourney 创建
- en: Key Points
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重点
- en: Chain-of-thought (CoT) prompting improves LM reasoning by providing step-by-step
    examples
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思维链（CoT）提示通过提供逐步示例来提升 LLM 的推理能力
- en: Manual creation of CoT demonstrations requires non-trivial human effort
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoT 演示的人工创建需要相当的人工努力
- en: This paper explores automating CoT demonstration generation using the LM itself
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本文探讨了使用 LLM 自身自动生成 CoT 演示的方法
- en: The proposed Auto-CoT method clusters questions then samples diverse ones for
    self-prompting
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出的 Auto-CoT 方法先对问题进行聚类，然后从中抽取多样化的问题进行自我提示
- en: Experiments show Auto-CoT matches manually created CoT, without human involvement
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验表明，Auto-CoT 的表现与人工创建的 CoT 相匹配，无需人工参与
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The paper "[**Automatic Chain of Thought Prompting in Large Language Models**](https://arxiv.org/abs/2210.03493)"
    explores automated ways to create effective "chain of thought" (CoT) prompts for
    large language models (LLMs) like GPT-4\. CoT prompting involves showing the LLM
    examples that demonstrate step-by-step reasoning chains mapping from a question
    to a final answer. This improves performance on complex reasoning tasks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 论文《[**大型语言模型中的自动化思维链提示**](https://arxiv.org/abs/2210.03493)》探讨了为大型语言模型（LLMs）如
    GPT-4 创建有效的“思维链”（CoT）提示的自动化方法。CoT 提示涉及向 LLM 展示示例，这些示例展示了从问题到最终答案的逐步推理链。这可以提升在复杂推理任务中的表现。
- en: Discussion
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论
- en: The best CoT prompting results, however, currently require humans to manually
    create demonstrations, with hand-crafted questions and detailed reasoning steps
    tailored to each task. The authors propose eliminating this manual effort by having
    the LLM automatically generate its own CoT demonstrations for prompting. Their
    key method, called Auto-CoT, works by first clustering the questions of a given
    task based on their semantic similarity. Auto-CoT then samples a diverse set of
    questions covering different clusters. For each sampled question, Auto-CoT uses
    the LLM itself in zero-shot mode to produce a reasoning chain from the question
    to an answer. It applies simple heuristics to select chains based on length and
    simplicity.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前最好的 CoT 提示结果仍然需要人工手动创建演示，包括为每个任务量身定制的精心设计的问题和详细的推理步骤。作者提出通过让 LLM 自动生成自己的
    CoT 演示来消除这种人工努力。他们的关键方法称为 Auto-CoT，其工作原理是首先根据语义相似性对给定任务的问题进行聚类。Auto-CoT 然后从不同的聚类中抽取一组多样化的问题。对于每个抽样的问题，Auto-CoT
    使用 LLM 自身以零样本模式生成从问题到答案的推理链。它应用简单的启发式方法，根据长度和简单性选择推理链。
- en: The authors perform experiments evaluating Auto-CoT on 10 reasoning datasets
    spanning arithmetic, common sense, and symbolic logic problems. The results show
    that Auto-CoT matches or exceeds the performance of CoT prompting based on manually
    created demonstrations, without requiring any human effort to design demonstrations.
    A key insight is that using diversity-based sampling over similarity-based retrieval
    to select the prompting questions mitigates the impact of imperfect demonstrations
    generated by the LLM's zero-shot reasoning. Auto-CoT also substantially outperforms
    baselines like retrieving similar questions or random sampling for the demonstrations.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在涵盖算术、常识和符号逻辑问题的 10 个推理数据集上评估了 Auto-CoT。结果表明，Auto-CoT 的表现与基于人工创建演示的 CoT 提示相匹配或超出，不需要任何人工设计演示。一个关键见解是，使用基于多样性的抽样而不是基于相似性的检索来选择提示问题，可以减轻由
    LLM 的零样本推理生成的不完善演示的影响。Auto-CoT 在性能上也显著超越了类似问题检索或随机抽样等基线方法。
- en: Overall, the work provides strong evidence that LLMs can prompt themselves to
    demonstrate complex multi-step reasoning. Auto-CoT essentially composes one LLM
    that generates a diverse set of CoT examples, with another LLM that uses those
    examples for inference. The authors suggest this self-prompting approach could
    significantly extend prompting techniques and make LLMs much better few-shot learners
    on complex reasoning tasks. Limitations include potential computational costs
    and issues scaling to more unconstrained problems. But the ability to automate
    prompting reduces human effort and customization needs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这项工作提供了有力的证据，表明LLM可以自我提示以展示复杂的多步骤推理。Auto-CoT本质上是一个LLM生成多样的CoT示例，另一个LLM使用这些示例进行推理。作者认为，这种自我提示的方法可能显著扩展提示技术，并使LLM在复杂推理任务中的少样本学习能力大大提高。局限性包括潜在的计算成本和在更多无约束问题上的扩展问题。但自动化提示的能力减少了人力和定制需求。
- en: Research Q&A
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 研究问答
- en: '*How does Auto-CoT compare to other methods that automate prompt creation,
    like retrieval-augmented prompting?*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*Auto-CoT与其他自动化提示创建的方法（如检索增强提示）相比如何？*'
- en: Retrieval-augmented prompting retrieves related data examples to use for prompting,
    rather than having the LLM generate demonstrations. A key difference is that Auto-CoT
    doesn't require a dataset of labeled examples and instead relies on the LLM's
    own zero-shot reasoning. Retrieval may be more sample-efficient but requires data
    collection. Auto-CoT is fully automated but can suffer from imperfect demonstrations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强提示通过检索相关数据示例来进行提示，而不是让LLM生成演示。一个关键的区别是，Auto-CoT不需要标记示例的数据集，而是依赖LLM自身的零-shot推理。检索可能更具样本效率，但需要数据收集。Auto-CoT完全自动化，但可能会受到演示不完美的影响。
- en: '*Could Auto-CoT be applied to natural language generation tasks beyond logical
    reasoning?*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*Auto-CoT能否应用于逻辑推理之外的自然语言生成任务？*'
- en: The clustering and self-prompting approach seems promising for less structured
    textual tasks where coherence is important. For example, Auto-CoT could provide
    writing planning examples for creative writing, or dialog illustrations for conversational
    bots. The key challenges would be defining appropriate clustering methods and
    training the LLM's zero-shot generation for high-quality demonstrations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类和自我提示的方法在那些结构不太规范但连贯性重要的文本任务中显得很有前景。例如，Auto-CoT可以为创造性写作提供写作规划示例，或为对话机器人提供对话示例。主要挑战在于定义合适的聚类方法以及训练LLM的零-shot生成以提供高质量的演示。
- en: '*What is innovative about this research?*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*这项研究的创新之处是什么？*'
- en: The key innovation is using the LLM itself to generate demonstrations for prompting,
    instead of relying on manual creation. This allows prompting to become more automated
    and task-adaptive. The clustering to select diverse questions for self-prompting
    is also innovative.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的创新在于使用LLM本身生成演示以进行提示，而不是依赖手动创建。这使得提示变得更加自动化和任务自适应。选择多样化问题以进行自我提示的聚类方法也是一种创新。
- en: '*What are the broader implications of this research?*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*这项研究的广泛意义是什么？*'
- en: This research could significantly reduce the human effort and expertise needed
    to design effective prompts. It may allow LLMs to learn new tasks more quickly
    and from less data, enhancing their few-shot learning capabilities. The self-prompting
    approach could be applied to extend prompting techniques like in-context learning.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究可能显著减少设计有效提示所需的人力和专业知识。它可能使LLM能够更快地学习新任务，并从更少的数据中获得知识，增强其少样本学习能力。自我提示的方法可以应用于扩展类似上下文学习的提示技术。
- en: '*What are some potential issues or oversights with this research as presented,
    if any?*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*这项研究中是否存在潜在问题或遗漏？*'
- en: A potential issue is that Auto-CoT relies on clustering questions based on similarity
    features from Sentence-BERT. Performance could suffer on tasks where semantic
    similarity doesn't align well with reasoning similarity. The approach also likely
    incurs higher compute costs than standard prompting.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个潜在问题是Auto-CoT依赖于基于Sentence-BERT的相似性特征进行问题聚类。在语义相似性与推理相似性不匹配的任务上，性能可能会受到影响。这种方法可能还会比标准提示方法产生更高的计算成本。
- en: '*What are the logical next research steps from this research?*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*这项研究的逻辑下一步是什么？*'
- en: Important next steps include exploring how Auto-CoT scales to more complex and
    open-ended reasoning tasks, integrating it with retrieval of external knowledge
    sources, and studying if the approach can be learned more sample-efficiently through
    meta-learning rather than relying solely on a pre-trained LLM. Analyzing the interplay
    between cluster count, sample size, and performance is also an open question.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的下一步包括探索 Auto-CoT 如何扩展到更复杂和开放的推理任务，将其与外部知识源的检索集成，并研究该方法是否可以通过元学习以更高效的样本学习，而不是仅依赖于预训练的大型语言模型。分析集群数量、样本大小和性能之间的相互关系也是一个待解问题。
- en: Takeaways
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收获
- en: Auto-CoT reduces the need for hand-crafted demonstrations to prompt LMs
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Auto-CoT 减少了对手工制作示例来提示语言模型的需求。
- en: Self-prompting with Auto-CoT composes one LM generating diverse examples, and
    another inferring
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Auto-CoT 的自我提示组合一个生成多样示例的语言模型，另一个进行推理。
- en: Diversity in sampling questions is key to overcoming imperfect zero-shot reasoning
    chains
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本问题的多样性是克服不完美零样本推理链的关键。
- en: The approach could extend prompting techniques and make LMs better few-shot
    learners
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法可以扩展提示技术，并使语言模型成为更好的少量样本学习者。
- en: Auto-CoT demonstrates the promise of automating prompting to reduce human effort
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Auto-CoT 展示了自动化提示以减少人工努力的潜力。
- en: Next steps include scaling Auto-CoT to more complex reasoning tasks and larger
    LMs
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一步包括将 Auto-CoT 扩展到更复杂的推理任务和更大的语言模型。
- en: '**[Matthew Mayo](https://www.linkedin.com/in/mattmayo13/)** ([**@mattmayo13**](https://twitter.com/mattmayo13))
    is a Data Scientist and the Editor-in-Chief of KDnuggets, the seminal online Data
    Science and Machine Learning resource. His interests lie in natural language processing,
    algorithm design and optimization, unsupervised learning, neural networks, and
    automated approaches to machine learning. Matthew holds a Master''s degree in
    computer science and a graduate diploma in data mining. He can be reached at editor1
    at kdnuggets[dot]com.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Matthew Mayo](https://www.linkedin.com/in/mattmayo13/)** ([**@mattmayo13**](https://twitter.com/mattmayo13))
    是数据科学家和 KDnuggets 的主编，这是一个开创性的在线数据科学和机器学习资源。他的兴趣包括自然语言处理、算法设计与优化、无监督学习、神经网络以及机器学习的自动化方法。Matthew
    拥有计算机科学硕士学位和数据挖掘研究生文凭。可以通过 editor1 at kdnuggets[dot]com 联系他。'
- en: More On This Topic
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关阅读
- en: '[Exploring Tree of Thought Prompting: How AI Can Learn to Reason…](https://www.kdnuggets.com/2023/07/exploring-tree-of-thought-prompting-ai-learn-reason-through-search.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[探索思维树提示：AI 如何通过搜索学习推理……](https://www.kdnuggets.com/2023/07/exploring-tree-of-thought-prompting-ai-learn-reason-through-search.html)'
- en: '[Unraveling the Power of Chain-of-Thought Prompting in Large Language Models](https://www.kdnuggets.com/2023/07/power-chain-thought-prompting-large-language-models.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[揭示链式思维提示在大型语言模型中的力量](https://www.kdnuggets.com/2023/07/power-chain-thought-prompting-large-language-models.html)'
- en: '[Parallel Processing in Prompt Engineering: The Skeleton-of-Thought…](https://www.kdnuggets.com/parallel-processing-in-prompt-engineering-the-skeleton-of-thought-technique)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[提示工程中的并行处理：思维骨架法……](https://www.kdnuggets.com/parallel-processing-in-prompt-engineering-the-skeleton-of-thought-technique)'
- en: '[Unlocking Reliable Generations through Chain-of-Verification: A…](https://www.kdnuggets.com/unlocking-reliable-generations-through-chain-of-verification)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[通过验证链解锁可靠生成：一……](https://www.kdnuggets.com/unlocking-reliable-generations-through-chain-of-verification)'
- en: '[Thought Propagation: An Analogical Approach to Complex Reasoning…](https://www.kdnuggets.com/thought-propagation-an-analogical-approach-to-complex-reasoning-with-large-language-models)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[思维传播：一种类比方法解决复杂推理……](https://www.kdnuggets.com/thought-propagation-an-analogical-approach-to-complex-reasoning-with-large-language-models)'
- en: '[6 Data Science Technologies You Need to Build Your Supply Chain Pipeline](https://www.kdnuggets.com/2022/01/6-data-science-technologies-need-build-supply-chain-pipeline.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[构建供应链管道所需的6种数据科学技术](https://www.kdnuggets.com/2022/01/6-data-science-technologies-need-build-supply-chain-pipeline.html)'
