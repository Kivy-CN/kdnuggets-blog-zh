["```py\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nimport numpy as np\n\nmodel = VGG19(weights='imagenet', include_top=False)\n\nimg_path = 'elephant.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\nfeatures = model.predict(x)\n```", "```py\n!wget --no-check-certificate \\\n    http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz \\\n    -O food.tar.gz\n!tar xzvf food.tar.gz\n```", "```py\nplt.imshow(Image.open(\"food-101/images/beignets/2802124.jpg\"))\nplt.axis('off')\nplt.show()\n```", "```py\nbase_dir = 'food-101/images'\ntrain_datagen = ImageDataGenerator(rescale=1./255, \n                                   shear_range=0.2,\n                                   zoom_range=0.2, \n                                   horizontal_flip=True,\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1,\n                                   validation_split=0.2\n                                   )\nvalidation_gen = ImageDataGenerator(rescale=1./255,validation_split=0.2)\n```", "```py\nimage_size = (200, 200)\ntraining_set = train_datagen.flow_from_directory(base_dir,\n                                                 seed=101,                                                 \n                                                 target_size=image_size,\n                                                 batch_size=32,\n                                                 subset = \"training\",\n                                                 class_mode='categorical')\n```", "```py\nvalidation_set = validation_gen.flow_from_directory(base_dir, \n                                               target_size=image_size,\n                                               batch_size=32, \n                                               subset = \"validation\",\n                                               class_mode='categorical')\n```", "```py\nmodel = Sequential([\n\n    Conv2D(filters=32,kernel_size=(3,3),  input_shape = (200, 200, 3),activation='relu'),\n    MaxPooling2D(pool_size=(2,2)),\n\n    Conv2D(filters=32,kernel_size=(3,3), activation='relu'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.25),\n\n    Conv2D(filters=64,kernel_size=(3,3), activation='relu'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.25),\n\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.25),\n    Dense(101, activation='softmax')\n])\n```", "```py\nmodel.compile(optimizer='adam',\n              loss=keras.losses.CategoricalCrossentropy(),\n              metrics=[keras.metrics.CategoricalAccuracy()])\n```", "```py\ncallback = EarlyStopping(monitor='loss', patience=3)\nhistory = model.fit(training_set,validation_data=validation_set, epochs=100,callbacks=[callback])\n```", "```py\n#pip install layer-sdk -qqq\nimport layer\nfrom layer.decorators import model, fabric,pip_requirements\n# Authenticate a Layer account \n# The trained model will be saved there. \nlayer.login()\n# Initialize a project, the trained model will be save under this project. \nlayer.init(\"image-classification\")\n@pip_requirements(packages=[\"wget\",\"tensorflow\",\"keras\"])\n@fabric(\"f-gpu-small\")\n@model(name=\"food-vision\")\ndef train():\n    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n    import tensorflow as tf\n    from tensorflow import keras\n    from tensorflow.keras import Sequential\n    from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,Dropout\n    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n    from tensorflow.keras.callbacks import EarlyStopping\n    import os\n    import matplotlib.pyplot as plt \n    from PIL import Image\n    import numpy as np\n    import pandas as pd\n    import tarfile\n    import wget\n    wget.download(\"http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\")\n    food_tar = tarfile.open('food-101.tar.gz')\n    food_tar.extractall('.') \n    food_tar.close()\n    plt.imshow(Image.open(\"food-101/images/beignets/2802124.jpg\"))\n    plt.axis('off')\n    layer.log({\"Sample image\":plt.gcf()})\n    base_dir = 'food-101/images'\n    class_names = os.listdir(base_dir)\n    train_datagen = ImageDataGenerator(rescale=1./255, \n                                   shear_range=0.2,\n                                   zoom_range=0.2, \n                                   horizontal_flip=True,\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1,\n                                   validation_split=0.2\n                                   )\n    validation_gen = ImageDataGenerator(rescale=1./255,validation_split=0.2)\n    image_size = (200, 200)\n    training_set = train_datagen.flow_from_directory(base_dir,\n                                                 seed=101,                                                 \n                                                 target_size=image_size,\n                                                 batch_size=32,\n                                                 subset = \"training\",\n                                                 class_mode='categorical')\n    validation_set = validation_gen.flow_from_directory(base_dir, \n                                               target_size=image_size,\n                                               batch_size=32, \n                                               subset = \"validation\",\n                                               class_mode='categorical')\n    model = Sequential([\n\n    Conv2D(filters=32,kernel_size=(3,3),  input_shape = (200, 200, 3),activation='relu'),\n    MaxPooling2D(pool_size=(2,2)),\n\n    Conv2D(filters=32,kernel_size=(3,3), activation='relu'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.25),\n\n    Conv2D(filters=64,kernel_size=(3,3), activation='relu'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.25),\n\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.25),\n    Dense(101, activation='softmax')])\n    model.compile(optimizer='adam',\n              loss=keras.losses.CategoricalCrossentropy(),\n              metrics=[keras.metrics.CategoricalAccuracy()])\n    callback = EarlyStopping(monitor='loss', patience=3)\n    epochs=20\n    history = model.fit(training_set,validation_data=validation_set, epochs=epochs,callbacks=[callback])\n    metrics_df = pd.DataFrame(history.history)\n    layer.log({\"Metrics\":metrics_df})\n    loss, accuracy = model.evaluate(validation_set)\n    layer.log({\"Accuracy on test dataset\":accuracy})\n    metrics_df[[\"loss\",\"val_loss\"]].plot()\n    layer.log({\"Loss plot\":plt.gcf()})\n    metrics_df[[\"categorical_accuracy\",\"val_categorical_accuracy\"]].plot()\n    layer.log({\"Accuracy plot\":plt.gcf()})\n    return model\n```", "```py\nlayer.run([train])\n```", "```py\nfrom keras.preprocessing import image\nimport numpy as np\nimage_model = layer.get_model('layer/image-classification/models/food-vision').get_train()\n!wget --no-check-certificate \\\nhttps://upload.wikimedia.org/wikipedia/commons/b/b1/Buttermilk_Beignets_%284515741642%29.jpg \\\n    -O /tmp/Buttermilk_Beignets_.jpg\ntest_image = image.load_img('/tmp/Buttermilk_Beignets_.jpg', target_size=(200, 200))\ntest_image = image.img_to_array(test_image)\n\ntest_image = test_image / 255.0\ntest_image = np.expand_dims(test_image, axis=0)\n\nprediction = image_model.predict(test_image)\n\nprediction[0][0]\n```", "```py\nclass_names = os.listdir(base_dir)\nscores = tf.nn.softmax(prediction[0])\nscores = scores.numpy()\nf\"{class_names[np.argmax(scores)]} with a { (100 * np.max(scores)).round(2) } percent confidence.\"\n```"]