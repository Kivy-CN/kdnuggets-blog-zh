- en: 'Machine Learning Is Not Like Your Brain Part 3: Fundamental Architecture'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习与大脑的不同 第三部分：基本架构
- en: 原文：[https://www.kdnuggets.com/2022/06/machine-learning-like-brain-part-3-fundamental-architecture.html](https://www.kdnuggets.com/2022/06/machine-learning-like-brain-part-3-fundamental-architecture.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2022/06/machine-learning-like-brain-part-3-fundamental-architecture.html](https://www.kdnuggets.com/2022/06/machine-learning-like-brain-part-3-fundamental-architecture.html)
- en: '![Machine Learning Is Not Like Your Brain Part 3: Fundamental Architecture](../Images/9fe3c28be5e8e7d37ee7dc2fc4681152.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习与大脑的不同 第三部分：基本架构](../Images/9fe3c28be5e8e7d37ee7dc2fc4681152.png)'
- en: Photo by [Alex wong](https://unsplash.com/@killerfvith?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/fundamental-architecture?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Alex wong](https://unsplash.com/@killerfvith?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/s/photos/fundamental-architecture?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Today’s artificial intelligence (AI) can do some extraordinary things. Its functionality,
    though, has very little to do with the way in which a human brain works to achieve
    the same tasks. For AI to overcome its inherent limitations and advance to artificial
    general intelligence, we must recognize the differences between the brain and
    its artificial counterparts.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的人工智能（AI）能够做一些非凡的事情。然而，它的功能与人类大脑完成相同任务的方式几乎没有关系。为了使AI克服其固有的局限性并发展到人工通用智能，我们必须认识到大脑与其人工对应物之间的差异。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业道路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你所在组织的IT需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: With that in mind, this nine-part series examines the capabilities and limitations
    of biological neurons and how these relate to machine learning (ML). In the first
    two parts of this series, we examined how a neuron’s slowness makes an ML approach
    to learning implausible in neurons, and how the fundamental algorithm of the perceptron
    differs from a biological neuron model involving spikes. Part Three examines the
    fundamental architecture underlying ML and the brain.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，本系列的九部分将探讨生物神经元的能力和局限性，以及这些与机器学习（ML）的关系。在本系列的前两部分中，我们考察了神经元的缓慢如何使ML学习方法在神经元中显得不切实际，以及感知器的基本算法如何与涉及尖峰的生物神经元模型不同。第三部分探讨了ML和大脑的基本架构。
- en: We’ve all seen diagrams of the orderly layers of ML neural networks in which
    the neurons in each layer are fully connected to those in the next. In contrast,
    the brain appears to contain lots of apparently disordered connections. Because
    it is so difficult to trace out individual connections within the brain, we don’t
    know what the brain’s layering structure actually is. It is obvious, though, that
    it is not the orderly layer-by-layer structure of ML.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都见过ML神经网络中有序层次的示意图，其中每一层的神经元都与下一层的神经元完全连接。相反，大脑似乎包含了许多看似无序的连接。由于很难追踪大脑中的单个连接，我们不知道大脑的分层结构到底是什么。尽管如此，显然它不是ML的有序分层结构。
- en: This fundamental difference drastically impacts the algorithms which can be
    used in ML. While we don’t think much about how solidly perceptrons and ML algorithms
    rely on the orderly layered structure, a problem arises if we allow perceptrons
    to have synapses from any arbitrary neuron in the network.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基本差异严重影响了在机器学习（ML）中可以使用的算法。虽然我们不太关注感知器和ML算法如何依赖有序的分层结构，但如果我们允许感知器与网络中的任何任意神经元建立突触，就会出现问题。
- en: Synapse Counts
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 突触数量
- en: In a neural network where each layer is fully connected to the next, the number
    of synapses increases with the square of the number of neurons in each layer (assuming
    all layers are the same size for easy calculation). If you allow any neuron to
    connect to any other in the network, the synapse count goes up with the square
    of the total number of neurons in the network. A 10 layer network with 1,000 neurons
    per layer would have a total of 9 million synapses (the output layer doesn’t need
    synapses). If any neuron may connect to any other, there is the potential for
    100 million synapses, with a likely 11-fold performance hit.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一层都与下一层完全连接的神经网络中，突触的数量随着每一层神经元数量的平方增加（假设所有层大小相同以便于计算）。如果允许网络中的任何神经元与其他任何神经元连接，突触的数量会随着网络中神经元总数的平方增加。一个每层有1000个神经元的10层网络总共有900万个突触（输出层不需要突触）。如果任何神经元可以连接到任何其他神经元，理论上可能有1亿个突触，这可能会导致11倍的性能损失。
- en: In the brain, most of these synapses don’t exist. The neocortex has 16 billion
    (16x109) neurons with the potential for any neuron to connect to any other or
    256x1018 possible synapses—256 exabytes with 1 byte per synapse, a ridiculously
    large number. The number of actual synapses, however, is estimated at 104 per
    neuron totaling only 16x1013 which is only a *staggeringly* large number (160
    terabytes).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在大脑中，大多数这样的突触并不存在。新皮层具有160亿（16x10^9）个神经元，每个神经元都有可能与其他任何神经元连接，即256x10^18个可能的突触——如果每个突触占用1字节，则总共有256艾字节，这个数字极其庞大。然而，实际的突触数量估计为每个神经元104个，总共仅为16x10^13，这只是一个*惊人的*大数字（160太字节）。
- en: A huge proportion of these synapses must have near-zero weights. These “synapses-in-waiting”
    are there to store a memory by changing weight quickly should the need arise.
    Regardless, we end up with a “sparse array,” with mostly zero entries. While it’s
    straightforward to come up with a structure whereby only non-zero synapses are
    represented, the blistering performance improvement of today’s GPUs is likely
    lost if we do that.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这些突触中的很大一部分必须具有接近零的权重。这些“待用突触”用来存储记忆，通过快速改变权重来应对需要。然而，我们最终会得到一个“稀疏数组”，其中大多数条目为零。虽然很容易设计一个只表示非零突触的结构，但如果这样做，今天的GPU惊人的性能提升可能会被浪费。
- en: Perceptron Algorithms
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知器算法
- en: The fundamental algorithm of the perceptron sums the input values from the previous
    layer. If you allow connections from subsequent layers, you need to differentiate
    between neuron content which has already been calculated from previous layers
    and those which have yet to be calculated. If the algorithm doesn’t include this
    extension, then the perceptron values will be dependent on the order of processing
    of the neurons. In a multi-thread implementation, this makes the perceptron values
    indeterminant.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器的基本算法对来自上一层的输入值进行求和。如果允许来自后续层的连接，则需要区分已经从前一层计算过的神经元内容和那些尚未计算的内容。如果算法不包括这个扩展，则感知器的值将依赖于神经元的处理顺序。在多线程实现中，这会使得感知器的值变得不可确定。
- en: 'To correct for this problem, the perceptron algorithm needs two phases with
    two internal values: the PreviousValue and the CurrentValue. In the first phase,
    the algorithm calculates its CurrentValue based on the PreviousValues of the input
    neurons. In the second phase, it transfers its current value to the previous value.
    In this way, all of the PreviousValues used in the calculation are stable for
    the duration of the calculation in the first phase.  This algorithm change clears
    up the problem, again with a performance penalty.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，感知器算法需要两个阶段和两个内部值：PreviousValue 和 CurrentValue。在第一个阶段，算法基于输入神经元的 PreviousValues
    计算其 CurrentValue。在第二个阶段，它将当前值转移到前一个值。通过这种方式，所有在计算中使用的 PreviousValues 在第一个阶段的计算期间都是稳定的。这个算法的变化解决了这个问题，但代价是性能下降。
- en: There is an implementation of this two-phase algorithm in the open-source Brain
    Simulator II in the file NeuronBase.cpp ([https://github.com/FutureAIGuru/BrainSimII](https://github.com/FutureAIGuru/BrainSimII)).
    To get that performance back, we could build custom hardware or capitalize on
    the strengths of spiking models where no processing is needed unless the neuron
    fires. On a 16-core desktop computer, the above algorithm has been clocked at
    2.5 billion synapses per second.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个两阶段算法的实现可以在开源的Brain Simulator II中找到，文件为NeuronBase.cpp（[https://github.com/FutureAIGuru/BrainSimII](https://github.com/FutureAIGuru/BrainSimII)）。为了恢复这种性能，我们可以构建定制硬件或利用脉冲模型的优势，其中只有在神经元发射时才需要处理。在一台16核桌面计算机上，上述算法的处理速度达到了每秒25亿个突触。
- en: Data Structure
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据结构
- en: Because each of the 16 billion neurons of the human neocortex cannot possibly
    be fully connected to all the other neurons, we can shift to a data structure
    which is more like the biological structure. In this way, each neuron only represents
    the useful synapses by maintaining a list or array of just those synapses. This
    is much more efficient in terms of memory and potentially faster because there
    is no need to process all of those zero-entries. This really hurts the GPU processing
    advantage, however, because today’s GPUs are not good at processing the small
    arrays, loops, and if-clauses needed to support this structure.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人类新皮层的160亿个神经元不可能完全连接到所有其他神经元，我们可以转向一种更接近生物结构的数据结构。通过这种方式，每个神经元只表示有用的突触，通过维护一个仅包含这些突触的列表或数组。这在内存使用上更高效，且可能更快，因为无需处理所有这些零条目。然而，这确实削弱了GPU处理的优势，因为今天的GPU在处理支持这种结构的小数组、循环和条件语句方面表现不佳。
- en: Backpropagation
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: The real problem of adapting a more random neuron/synapse structure to ML is
    that the backpropagation algorithm doesn’t adapt to it very well. Backpropagation’s
    gradient descent relies on a stable error surface. As soon as we allow for looping
    connections, that surface is no longer constant over time.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 将更随机的神经元/突触结构适应到机器学习中的实际问题在于反向传播算法对这种结构的适应性较差。反向传播的梯度下降依赖于稳定的误差面。一旦我们允许循环连接，这个面就不再是时间上的常量。
- en: '*In Part Four of this series, the neuron’s limited ability to represent the
    precise values on which ML depends will be addressed.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*在本系列的第四部分，将讨论神经元在表示机器学习所依赖的精确值方面的局限性。*'
- en: '**[Charles Simon](https://futureai.guru/Founder.aspx)** is a nationally recognized
    entrepreneur and software developer, and the CEO of FutureAI. Simon is the author
    of Will the Computers Revolt?: Preparing for the Future of Artificial Intelligence,
    and the developer of Brain Simulator II, an AGI research software platform. For
    more information, [visit here](https://futureai.guru/Founder.aspx).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**[查尔斯·西蒙](https://futureai.guru/Founder.aspx)** 是一位全国知名的企业家和软件开发者，也是FutureAI的首席执行官。西蒙是《计算机会反叛吗？：为人工智能的未来做准备》的作者，并且是AGI研究软件平台Brain
    Simulator II的开发者。欲了解更多信息，[请访问这里](https://futureai.guru/Founder.aspx)。'
- en: More On This Topic
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Machine Learning Is Not Like Your Brain Part One: Neurons Are Slow,…](https://www.kdnuggets.com/2022/04/machine-learning-like-brain-part-one-neurons-slow-slow-slow.html)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习不像你的大脑 第一部分：神经元的慢……](https://www.kdnuggets.com/2022/04/machine-learning-like-brain-part-one-neurons-slow-slow-slow.html)'
- en: '[Machine Learning Is Not Like Your Brain Part Two: Perceptrons vs Neurons](https://www.kdnuggets.com/2022/05/machine-learning-like-brain-part-two-perceptrons-neurons.html)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习不像你的大脑 第二部分：感知机与神经元](https://www.kdnuggets.com/2022/05/machine-learning-like-brain-part-two-perceptrons-neurons.html)'
- en: '[Machine Learning Is Not Like Your Brain Part 4: The Neuron’s…](https://www.kdnuggets.com/2022/06/machine-learning-like-brain-part-4-neuron-limited-ability-represent-precise-values.html)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习不像你的大脑 第四部分：神经元的……](https://www.kdnuggets.com/2022/06/machine-learning-like-brain-part-4-neuron-limited-ability-represent-precise-values.html)'
- en: '[Machine Learning Is Not Like Your Brain Part 5: Biological Neurons…](https://www.kdnuggets.com/2022/07/machine-learning-like-brain-part-5-biological-neurons-cant-summation-inputs.html)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习不像你的大脑 第五部分：生物神经元……](https://www.kdnuggets.com/2022/07/machine-learning-like-brain-part-5-biological-neurons-cant-summation-inputs.html)'
- en: '[Machine Learning Is Not Like Your Brain Part 6: The Importance of…](https://www.kdnuggets.com/2022/08/machine-learning-like-brain-part-6-importance-precise-synapse-weights-ability-set-quickly.html)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习不像你的大脑 第六部分：……的重要性](https://www.kdnuggets.com/2022/08/machine-learning-like-brain-part-6-importance-precise-synapse-weights-ability-set-quickly.html)'
- en: '[Machine Learning is Not Like Your Brain Part Seven: What Neurons…](https://www.kdnuggets.com/2022/08/machine-learning-like-brain-part-seven-neurons-good.html)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习不像你的大脑 第七部分：神经元的作用…](https://www.kdnuggets.com/2022/08/machine-learning-like-brain-part-seven-neurons-good.html)'
