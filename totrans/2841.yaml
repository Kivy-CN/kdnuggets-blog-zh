- en: 5 Ways to Apply Ethics to AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/12/5-ways-apply-ethics-ai.html](https://www.kdnuggets.com/2019/12/5-ways-apply-ethics-ai.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Marek Rogala](https://twitter.com/marekrog), CTO at Appsilon**'
  prefs: []
  type: TYPE_NORMAL
- en: '*In a [previous post](https://appsilon.com/dusting-under-the-bed-machine-learners-responsibility-for-the-future-of-our-society/),
    I expressed my happiness that I got to present at *[*ML in PL*](https://conference.mlinpl.org/)* in
    Warsaw. I had the opportunity to take a step back and reflect a bit on the ethics
    of what we do as practitioners of data science and builders of machine learning
    models. It’s an important topic and doesn’t receive the attention that it should.  *'
  prefs: []
  type: TYPE_NORMAL
- en: '*The algorithms we build affect lives. *'
  prefs: []
  type: TYPE_NORMAL
- en: '*I have researched this topic quite a lot, and during that time I have found
    a number of stories that made a huge impression on me. Here are six more lessons
    based on real life examples that I think we should all remember as people working
    in machine learning, whether you’re a researcher, engineer, or a decision-maker. *'
  prefs: []
  type: TYPE_NORMAL
- en: It’s time to show your Cards
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/68b436bd5645c3e0ec7d568a1326d583.png)'
  prefs: []
  type: TYPE_IMG
- en: It’s time for a more positive example, a practice we can follow in our daily
    work. OpenAI has finally released the full [GPT-2 model](https://openai.com/blog/better-language-models/) for
    text generation. OpenAI noticed that the model is so powerful that it could be
    used in very bad ways (from testing it personally, I can confirm that it is often
    super realistic). So in February they released a limited version, and started
    a process. They invited researchers to experiment with the model, they asked people
    to build detection systems to see the accuracy of the method to detect if something
    was created by a bot or not. They are also hiring social scientists, because as
    engineers we should know our limits and we don’t have to understand all implications
    of models we release. But we can collaborate with those who do.
  prefs: []
  type: TYPE_NORMAL
- en: One of the tools that they used is something that we can all use in our daily
    work — **Model Cards**. This was [suggested by several people at Google](https://ai.google/research/pubs/pub48120/).
    A Model Card shows in a standardized way the intended use and the mis-use cases.
    It shows how the data was collected, so that researchers can experiment and notice
    some mistakes in the process. The Card can contain caveats and recommendations.
    Whether you’re releasing to the public or just internally, I think it’s useful
    to complete an “M-card.” I think OpenAI did this right. So that brings us to Lesson
    6.
  prefs: []
  type: TYPE_NORMAL
- en: '**Lesson 6: Evaluate risks. Communicate intended usage.**'
  prefs:
  - PREF_BQ
  - PREF_H3
  type: TYPE_NORMAL
- en: Onward. I saw this on Twitter last week. Some researchers are showing off a
    model that will use faces to pay for entrance to the London Underground.
  prefs: []
  type: TYPE_NORMAL
- en: Facial recognition could make your commute MUCH easier [pic.twitter.com/B3SISYq0Zb](https://t.co/B3SISYq0Zb)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Mashable (@mashable) [November 9, 2019](https://twitter.com/mashable/status/1193088083492630528?ref_src=twsrc%5Etfw)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I was shocked that they didn’t mention any risks whatsoever, such as, for example,
    the potential for law enforcement abuse, privacy issues, surveillance, migrant
    rights, biases, and abuse by authoritarian states. There are huge implications.
    So, Lesson 7: it’s easy to get press for a cool model, but we shouldn’t be like
    those researchers from Bristol. We should make sure that if a video is featured
    like this, the risks are called out.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lesson 7: It’s easy to get media coverage. Make sure risks are communicated. **'
  prefs:
  - PREF_BQ
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/a4335775911823e04d9df26e65e79f4a.png)Here’s another positive
    example that I’d like to show you — it’s a [talk by Evan Estola](https://www.youtube.com/watch?v=MqoRzNhrTnQ) who
    is the lead machine learning engineer at [Meetup](https://www.meetup.com/). He
    gave a useful talk called “When Recommendation Systems Go Bad” about some of the
    decisions that they have made.  He reminds us of Goodhart’s law:'
  prefs: []
  type: TYPE_NORMAL
- en: “When a measure becomes a target, it ceases to be a good measure.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “We have an ethical obligation not to teach our machines to be prejudiced,”
    he adds. For example, in the US, there are more men than women in tech roles.
    So should the Meetup recommendation model discourage women from attending tech
    meetups because they are mostly attended by men? Of course not. But if it is not
    intentionally designed otherwise, a model can easily infer from the data that
    women aren’t interested in tech events and then turn around and essentially maintain
    gender stereotypes. So Lesson 8…
  prefs: []
  type: TYPE_NORMAL
- en: '**Lesson 8: Remember that a metric is always a proxy for what we care about.**'
  prefs:
  - PREF_BQ
  - PREF_H3
  type: TYPE_NORMAL
- en: And what about the issue of government regulation? The following is the most
    shocking example to me. Maybe some of you are aware that there was a genocide
    in Myanmar last year. Thousands of the Rohingya people died at the hands of the
    military, police, and other members from the majority group. Facebook finally
    admitted this year that they didn’t do enough — the platform became a way for
    people to spread violence and violent content. So basically people from the majority
    group spread hate about the ethnic minority Rohingya. They practice different
    religions so that only helped increase the violence.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3dc9bea082dc25213b68d96faf12025b.png)'
  prefs: []
  type: TYPE_IMG
- en: One of the worst things about the situation was that Facebook executives were
    warned as early as 2013\. After five years, there was a huge outburst of violence.
    In 2015, after the first warning, Facebook had only four Burmese-speaking contractors
    reviewing the content — for sure not enough. They just didn’t care enough.
  prefs: []
  type: TYPE_NORMAL
- en: '[Rachel Thomas](https://www.youtube.com/watch?v=WC1kPtG8Iz8) compared two reactions
    from Facebook. One is for Myanmar, where Facebook boasted that they added “dozens”
    of content reviewers for Burmese. During the same year, they hired 1,500 content
    reviewers in Germany. Why is that? Because Germany threatened Facebook (and others)
    with a $50M fine if they didn’t comply with the Hate Speech law. This is an example
    of how regulations can help, because it makes managers who are mostly focused
    on profit to treat risks seriously.'
  prefs: []
  type: TYPE_NORMAL
- en: Here is a personal example about regulation. I have two small children, so I
    have become an expert about car seats. In the past, it was claimed by many that
    cars can’t be regulated. Drivers were blamed for safety issues. Fast forward a
    bit, and it is calculated that children are five times safer in rear-facing car
    seats as opposed to front-facing. Regulations differ in various countries. In
    Sweden, they have regulations that essentially favor the use of rear-facing car
    seats. Consequently, from 1992 to 2013, only 15 children have died in auto accidents.
    By contrast, in Poland, which does not have such a regulation, 70 to 150 children
    die **each year** in auto accidents.
  prefs: []
  type: TYPE_NORMAL
- en: Regulation will come to AI eventually. The question is whether it will be wise
    or stupid. Technical people are often opposed to regulation because it’s often
    poorly designed and enacted. But I think it’s because we need to make it wise.
    We will eventually have regulation around AI, but it’s not determined what quality
    it’ll be and when this will happen.
  prefs: []
  type: TYPE_NORMAL
- en: '**Lesson 9: Regulation is our ally, not our enemy. Advocate for wise regulation.**'
  prefs:
  - PREF_BQ
  - PREF_H3
  type: TYPE_NORMAL
- en: Final example. At [Appsilon](https://appsilon.com/) we devote quite a bit of
    our time to “AI for Good” initiatives. So we work with NGOs to put AI models to
    work to study climate change, to help protect wildlife and so on, and this is
    great, I’m happy to see other companies doing that. But we should be aware of
    a phenomenon called **technologism**.
  prefs: []
  type: TYPE_NORMAL
- en: There’s a book by Kentaro Toyama, it’s titled “[Geek Heresy](https://geekheresy.org/).”
    Mr. Toyama is a Microsoft engineer who was sent to India to help social change
    and improve people’s lives through technology. He found that people are making
    lots of mistakes by applying the Western perspective to try to fix everything
    through technology. He shows many examples of how high hopes for fixing problems
    with technology have failed.
  prefs: []
  type: TYPE_NORMAL
- en: We should work with domain experts a lot and solve the simple problems **first** with
    the right depth, so that we build a common understanding between domain experts
    and engineers. Engineers need to learn the roots of the problems and the domain
    experts need to learn what is possible with the technology. Only then can really
    useful ideas emerge.
  prefs: []
  type: TYPE_NORMAL
- en: '**Lesson 10: In AI 4 Good, work closely with domain experts and beware technologism. **'
  prefs:
  - PREF_BQ
  - PREF_H3
  type: TYPE_NORMAL
- en: '![](../Images/1988efbe7edd6da14c91ceed457a6a45.png)'
  prefs: []
  type: TYPE_IMG
- en: The algorithms we build affect lives. Via the internet and social media they
    can literally shape how you think. They affect healthcare, jobs, court cases.
    Given that less than half a percent of the population knows how to code, think
    what a tiny fraction of that number actually understands AI. So we have the awesome
    and exciting responsibility to shape the future of our society so that it is bright.
  prefs: []
  type: TYPE_NORMAL
- en: '![You know about problems other people don''t. You are responsible for the
    shape of our society.](../Images/9b17bae0c05cc89deb22acdd4956cd5c.png)'
  prefs: []
  type: TYPE_IMG
- en: You know about problems other people don’t. You are responsible for the shape
    of our society.
  prefs: []
  type: TYPE_NORMAL
- en: Do you have your own “lessons”? Please add them in the comments below.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading! Follow me on Twitter [@marekog](https://twitter.com/marekrog).
  prefs: []
  type: TYPE_NORMAL
- en: '**Follow Appsilon Data Science on Social Media**'
  prefs: []
  type: TYPE_NORMAL
- en: Follow[ @Appsilon](https://twitter.com/appsilon) on Twitter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Follow us on[ LinkedIn](https://www.linkedin.com/company/appsilon)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sign up for our[ newsletter](https://appsilon.com/blog/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try out our R Shiny[ open source](https://appsilon.com/opensource/) packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bio: [Marek Rogala](https://twitter.com/marekrog)** is the CTO at Appsilon.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://appsilon.com/5-ways-to-apply-ethics-to-ai/?nabe=4634331497365504:0).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Dusting Under the Bed: Machine Learners’ Responsibility for the Future of
    Our Society](/2019/12/machine-learners-responsibility-future-society.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Statistical Traps Data Scientists Should Avoid](/2019/10/statistical-traps-data-scientists-avoid.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Designing Ethical Algorithms](/2019/03/designing-ethical-algorithms.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Ways to Apply AI to Small Data Sets](https://www.kdnuggets.com/2022/02/5-ways-apply-ai-small-data-sets.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Ethics of AI: Navigating the Future of Intelligent Machines](https://www.kdnuggets.com/2023/04/ethics-ai-navigating-future-intelligent-machines.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using the apply() Method with Pandas Dataframes](https://www.kdnuggets.com/2022/07/apply-method-pandas-dataframes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MLOps: The Best Practices and How To Apply Them](https://www.kdnuggets.com/2022/04/mlops-best-practices-apply.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What is Chebychev''s Theorem and How Does it Apply to Data Science?](https://www.kdnuggets.com/2022/11/chebychev-theorem-apply-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, November 30: What is Chebychev''s Theorem and How…](https://www.kdnuggets.com/2022/n46.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
