- en: A Beginner’s Guide to Data Engineering – Part II
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初学者的数据工程指南 – 第二部分
- en: 原文：[https://www.kdnuggets.com/2018/03/beginners-guide-data-engineering-part-2.html](https://www.kdnuggets.com/2018/03/beginners-guide-data-engineering-part-2.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/03/beginners-guide-data-engineering-part-2.html](https://www.kdnuggets.com/2018/03/beginners-guide-data-engineering-part-2.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](/2018/03/beginners-guide-data-engineering-part-2.html?page=2#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](/2018/03/beginners-guide-data-engineering-part-2.html?page=2#comments)'
- en: '**By [Robert Chang](https://www.linkedin.com/in/robert-chang-877b1720/), Airbnb**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者 [Robert Chang](https://www.linkedin.com/in/robert-chang-877b1720/)，Airbnb**。'
- en: '![Header image](../Images/8eaf2321867f07225ee55fa3de019591.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![标题图像](../Images/8eaf2321867f07225ee55fa3de019591.png)'
- en: '[Image Credit](https://www.archdaily.com/295502/hangar-16-inaqui-carnicero-architecture/50aa9e31b3fc4b0b54000045-hangar-16-inaqui-carnicero-architecture-image):
    A transformed modern warehouse at Hangar 16, Madrid (Cortesía de Iñaqui Carnicero
    Arquitectura)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[图片来源](https://www.archdaily.com/295502/hangar-16-inaqui-carnicero-architecture/50aa9e31b3fc4b0b54000045-hangar-16-inaqui-carnicero-architecture-image):
    马德里 Hangar 16 的改造现代仓库（由 Iñaqui Carnicero Arquitectura 提供）'
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Recapitulation
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: In [**A Beginner’s Guide to Data Engineering** — **Part I**](https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-i-4227c5c457d7)**, **I
    explained that an organization’s analytics capability is built layers upon layers.
    From collecting raw data and building data warehouses to applying Machine Learning,
    we saw why data engineering plays a critical role in all of these areas.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [**初学者的数据工程指南** — **第一部分**](https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-i-4227c5c457d7)**中，**我解释了组织的分析能力是如何层层建立的。从收集原始数据和构建数据仓库到应用机器学习，我们看到了数据工程在所有这些领域中扮演的重要角色。
- en: One of any data engineer’s most highly sought-after skills is the ability to
    design, build, and maintain data warehouses. I defined what data warehousing is
    and discussed its three common building blocks — **E**xtract, **T**ransform, and **L**oad,
    where the name ETL comes from.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师最受欢迎的技能之一是设计、构建和维护数据仓库的能力。我定义了数据仓库的概念，并讨论了其三个常见构建块 — **提取**（**E**xtract）、**转换**（**T**ransform）和**加载**（**L**oad），也就是
    ETL 的由来。
- en: For those who are new to ETL processes, I introduced a few popular open source
    frameworks built by companies like LinkedIn, Pinterest, Spotify, and highlight
    Airbnb’s own open-sourced tool Airflow. Finally, I argued that data scientist
    can learn data engineering much more effectively with the SQL-based ETL paradigm.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些初涉 ETL 过程的人，我介绍了一些由 LinkedIn、Pinterest、Spotify 等公司构建的流行开源框架，并强调了 Airbnb
    自有的开源工具 Airflow。最后，我认为数据科学家可以通过基于 SQL 的 ETL 模式更有效地学习数据工程。
- en: Part II Overview
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第二部分 概述
- en: The discussion in part I was somewhat high level. In Part II (this post), I
    will share more technical details on how to build good data pipelines and highlight
    ETL best practices. Primarily, I will use Python, Airflow, and SQL for our discussion.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分的讨论有些高层次。在第二部分（本帖）中，我将分享更多关于如何构建良好数据管道的技术细节，并强调 ETL 最佳实践。主要地，我将使用 Python、Airflow
    和 SQL 进行讨论。
- en: First, I will introduce the concept of **Data Modeling**, a design process where
    one carefully defines table schemas and data relations to capture business metrics
    and dimensions. We will learn **Data Partitioning**, a practice that enables more
    efficient querying and data backfilling. After this section, readers will understand
    the basics of data warehouse and pipeline design.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我将介绍**数据建模**的概念，这是一个设计过程，通过精确定义表模式和数据关系来捕捉业务指标和维度。我们将学习**数据分区**，这是一种可以更高效查询和填充数据的实践。完成这一部分后，读者将理解数据仓库和管道设计的基础知识。
- en: In later sections, I will dissect the anatomyof an **Airflow job. **Readers
    will learn how to use sensors, operators, and transfers to operationalize the
    concepts of extraction, transformation, and loading. We will highlight **ETL best
    practices**, drawing from real life examples such as Airbnb, Stitch Fix, Zymergen,
    and more.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续部分，我将剖析**Airflow 作业**的结构。读者将学习如何使用传感器、操作符和传输来将提取、转换和加载的概念付诸实践。我们将重点介绍**ETL
    最佳实践**，并从实际生活中的例子，如 Airbnb、Stitch Fix、Zymergen 等，进行分析。
- en: By the end of this post, readers will appreciate the versatility of Airflow
    and the concept of [*configuration as code*](https://airflow.apache.org/#principles).
    We will see, in fact, that Airflow has many of these best practices already built
    in.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 到了本文末尾，读者将欣赏到 Airflow 的多功能性以及[*配置即代码*](https://airflow.apache.org/#principles)的概念。事实上，我们将看到，Airflow
    已经内置了许多这些最佳实践。
- en: Data Modeling
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据建模
- en: '![](../Images/c04d876021524a21b2bb3e3cb0414615.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c04d876021524a21b2bb3e3cb0414615.png)'
- en: '[Image credit](https://digital-photography-school.com/lake-tekapo-stars/):
    Star Schema, when used correctly, can be as beautiful as the actual sky'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[图片来源](https://digital-photography-school.com/lake-tekapo-stars/): 星型模式，在正确使用时，可以像实际的*天空*一样美丽'
- en: When a user interacts with a product like Medium, her information, such as her
    avatar, saved posts, and number of views are all captured by the system. In order
    to serve them accurately and on time to users, it is critical to optimize the
    production databases for *online transaction processing* (OLTP for short).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户与像 Medium 这样的产品互动时，她的信息，例如头像、保存的帖子和浏览次数，都会被系统捕获。为了准确及时地为用户提供服务，优化生产数据库以进行*在线事务处理*（简称
    OLTP）至关重要。
- en: When it comes to building an *online analytical processing* system (OLAP for
    short), the objective is rather different. The designer need to focus on insight
    generation, meaning analytical reasoning can be translated into queries easily
    and statistics can be computed efficiently. This analytics-first approach often
    involves a design process called **data modeling.**
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到构建*在线分析处理*系统（简称 OLAP）时，目标则有所不同。设计师需要专注于洞察生成，这意味着分析推理可以轻松转化为查询，统计数据可以高效计算。这种以分析为主的方式通常涉及称为**数据建模**的设计过程。
- en: '**Data Modeling, Normalization, and Star Schema**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据建模、规范化和星型模式**'
- en: To give an example of the design decisions involved, we often need to decide
    the extent to which tables should be [**normalized**](https://en.wikipedia.org/wiki/Database_normalization).
    Generally speaking, normalized tables have simpler schemas, more standardized
    data, and carry less redundancy. However, a proliferation of smaller tables also
    means that tracking data relations requires more diligence, querying patterns
    become more complex (more `JOINs`), and there are more ETL pipelines to maintain.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以设计决策为例，我们通常需要决定表的**规范化**程度。一般来说，规范化的表具有更简单的模式、更标准化的数据，并且冗余更少。然而，较多的小表也意味着跟踪数据关系需要更多的细心，查询模式变得更加复杂（更多`JOINs`），而且需要维护更多的ETL管道。
- en: On the other hand, it is often much easier to query from a denormalized table
    (aka a wide table), because all of the metrics and dimensions are already pre-joined.
    Given their larger sizes, however, data processing for wide tables is slower and
    involves more upstream dependencies. This makes maintenance of ETL pipelines more
    difficult because the unit of work is not as modular.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，从非规范化表（即宽表）中查询数据通常要容易得多，因为所有的指标和维度已经预先连接。然而，由于其较大的尺寸，宽表的数据处理速度较慢，并且涉及更多的上游依赖。这使得
    ETL 管道的维护变得更加困难，因为工作单元不够模块化。
- en: Among the many design patterns that try to balance this trade-off, one of the
    most commonly-used patterns, and the one we use at [Airbnb](https://ieondemand.com/presentations/building-airbnb-s-data-culture-insights-from-5-years-of-hypergrowth?_ga=2.230925083.5245429.1516779379-1586560381.1516779379),
    is called [**star schema**](https://en.wikipedia.org/wiki/Star_schema). The name
    arose because tables organized in star schema can be visualized with a star-like
    pattern. This design focuses on building normalized tables, specifically fact
    and dimension tables. When needed, denormalized tables can be built from these
    smaller normalized tables. This design strives for a balance between ETL maintainability
    and ease of analytics.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多尝试平衡这一权衡的设计模式中，最常用的模式之一，也是我们在 [Airbnb](https://ieondemand.com/presentations/building-airbnb-s-data-culture-insights-from-5-years-of-hypergrowth?_ga=2.230925083.5245429.1516779379-1586560381.1516779379)
    使用的模式，被称为 [**星型模式**](https://en.wikipedia.org/wiki/Star_schema)。这个名字来源于星型模式下的表可以被可视化成星形图案。这种设计专注于构建规范化表，特别是事实表和维度表。当需要时，可以从这些较小的规范化表中构建非规范化表。这种设计努力在ETL可维护性和分析便捷性之间取得平衡。
- en: '![](../Images/6c703ed92b4505c125c49a049f0965c1.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c703ed92b4505c125c49a049f0965c1.png)'
- en: The star schema organized table in a star-like pattern, with a fact table at
    the center, surrounded by dim tables
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 星型模式将表组织成星形图案，以事实表为中心，周围环绕着维度表。
- en: '****Fact & Dimension Tables****'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '****事实表和维度表****'
- en: 'To understand how to build denormalized tables from fact tables and dimension
    tables, we need to discuss their respective roles in more detail:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解如何从事实表和维度表构建非规范化表，我们需要更详细地讨论它们各自的角色：
- en: '**Fact tables **typically contain point-in-time transactional data. Each row
    in the table can be extremely simple and is often represented as a unit of transaction.
    Because of their simplicity, they are often the source of truth tables from which
    business metrics are derived. For example, at Airbnb, we have various fact tables
    that track transaction-like events such as bookings, reservations, alterations,
    cancellations, and more.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事实表** 通常包含时间点的事务数据。表中的每一行可能非常简单，通常表示为一个事务单位。由于其简单性，它们通常是从中派生业务指标的真实来源。例如，在
    Airbnb，我们有各种事实表，用于跟踪类似事务的事件，如预订、预留、变更、取消等。'
- en: '**Dimension tables **typically contain slowly changing attributes of specific
    entities, and attributes sometimes can be organized in a hierarchical structure.
    These attributes are often called “dimensions”, and can be joined with the fact
    tables, as long as there is a foreign key available in the fact table. At Airbnb,
    we built various dimension tables such as users, listings, and markets that help
    us to slice and dice our data.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**维度表** 通常包含特定实体的缓慢变化的属性，这些属性有时可以组织成层次结构。这些属性通常被称为“维度”，并且可以与事实表连接，只要事实表中有外键。在
    Airbnb，我们建立了各种维度表，如用户、房源和市场，帮助我们切分和分析数据。'
- en: Below is a simple example of how fact tables and dimension tables (both are
    normalized tables) can be joined together to answer basic analytics question such
    as how many bookings occurred in the past week in each market. Shrewd users can
    also imagine that if additional metrics `m_a, m_b, m_c` and dimensions `dim_x,
    dim_y, dim_z` are projected in the final `SELECT` clause, a denormalized table
    can be easily built from these normalized tables.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简单的示例，展示了事实表和维度表（两者都是规范化表）如何结合在一起，以回答基本的分析问题，例如过去一周每个市场发生了多少预订。聪明的用户还可以想象，如果在最终的`SELECT`子句中投影了额外的度量`m_a,
    m_b, m_c`和维度`dim_x, dim_y, dim_z`，则可以从这些规范化表中轻松构建一个非规范化表。
- en: Normalized tables can be used to answer ad-hoc questions or to build denormalized
    tables
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 规范化表可用于回答临时问题或构建非规范化表。
- en: Data Partitioning & Backfilling Historical Data
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据分区与填补历史数据
- en: '![](../Images/d99d4c9905f720e1427ce2fa60f2b199.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d99d4c9905f720e1427ce2fa60f2b199.png)'
- en: In an era where data storage cost is low and computation is cheap, companies
    now can afford to store all of their historical data in their warehouses rather
    than throwing it away. The advantage of such an approach is that companies can
    re-process historical data in response to new changes as they see fit.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据存储成本低廉且计算便宜的时代，公司现在可以负担得起将所有历史数据存储在数据仓库中，而不是将其丢弃。这种方法的优点是公司可以根据需要重新处理历史数据，以应对新的变化。
- en: '**Data Partitioning by Datestamp**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**按日期戳进行数据分区**'
- en: With so much data readily available, running queries and performing analytics
    can become inefficient over time. In addition to following SQL best practices
    such as “filter early and often”, “project only the fields that are needed”, one
    of the most effective techniques to improve query performance is to partition
    data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据量的增加，运行查询和进行分析可能会随着时间的推移变得低效。除了遵循SQL最佳实践，如“提前且频繁过滤”和“仅投影所需字段”之外，提高查询性能的最有效技术之一就是数据分区。
- en: The basic idea behind [**data partitioning**](https://en.wikipedia.org/wiki/Partition_%28database%29) is
    rather simple — instead of storing all the data in one chunk, we break it up into
    independent, self-contained chunks. Data from the same chunk will be assigned
    with the same partition key, which means that any subset of the data can be looked
    up extremely quickly. This technique can greatly improve query performance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[**数据分区**](https://en.wikipedia.org/wiki/Partition_%28database%29)的基本思想相当简单——我们将数据拆分成独立的、自包含的块，而不是将所有数据存储在一个大块中。来自同一块的数据将被分配相同的分区键，这意味着任何数据子集都可以被极其快速地查找。这项技术可以显著提高查询性能。'
- en: In particular, one common partition key to use is **datestamp **(`ds` for short),
    and for good reason. First, in data storage system like [S3](https://aws.amazon.com/s3/),
    raw data is often organized by datestamp and stored in time-labeled directories.
    Furthermore, the unit of work for a batch ETL job is typically one day, which
    means new date partitions are created for each daily run. Finally, many analytical
    questions involve counting events that occurred in a specified time range, so
    querying by datestamp is a very common pattern. It is no wonder that datestamp
    is a popular choice for data partitioning!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，使用的一个常见分区键是**日期戳**（简称`ds`），这是有充分理由的。首先，在像[S3](https://aws.amazon.com/s3/)这样的数据存储系统中，原始数据通常按日期戳组织并存储在时间标记的目录中。此外，批量ETL作业的工作单位通常为一天，这意味着每次每日运行都会创建新的日期分区。最后，许多分析性问题涉及统计在指定时间范围内发生的事件，因此按日期戳查询是一种非常常见的模式。难怪日期戳是数据分区的热门选择！
- en: A table that is partitioned by ds
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一个由`ds`分区的表
- en: '**Backfilling Historical Data**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**填补历史数据**'
- en: Another important advantage of using datestamp as the partition key is the ease
    of data backfilling. When a ETL pipeline is built, it computes metrics and dimensions
    forward, not backward. Often, we might desire to revisit the historical trends
    and movements. In such cases, we would need to compute metric and dimensions in
    the past — We called this process **data backfilling**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用日期戳作为分区键的另一个重要优势是数据填补的便利性。当ETL管道被构建时，它会向前计算指标和维度，而不是向后计算。通常，我们可能希望重新访问历史趋势和动态。在这种情况下，我们需要计算过去的指标和维度——我们称这个过程为**数据填补**。
- en: 'Backfilling is so common that Hive built in the functionality of [**dynamic
    partitions**](https://cwiki.apache.org/confluence/display/Hive/DynamicPartitions)**, **a
    construct that perform the same SQL operations over many partitions and perform
    multiple insertions at once. To illustrate how useful dynamic partitions can be,
    consider a task where we need to backfill the number of bookings in each market
    for a dashboard, starting from `earliest_ds` to `latest_ds` . We might do something
    like this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 填补历史数据如此常见，以至于Hive内置了[**动态分区**](https://cwiki.apache.org/confluence/display/Hive/DynamicPartitions)功能，这是一种对多个分区执行相同SQL操作并同时执行多个插入的结构。为了说明动态分区的实用性，考虑一个任务，我们需要填补每个市场的预订数量，以供仪表板使用，从`earliest_ds`到`latest_ds`。我们可能会这样做：
- en: 'The operation above is rather tedious, since we are running the same query
    many times but on different partitions. If the time range is large, this work
    can become quickly repetitive. When dynamic partitions are used, however, we can
    greatly simplify this work into just one query:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 上述操作相当繁琐，因为我们在不同的分区上多次运行相同的查询。如果时间范围很大，这项工作可能会变得非常重复。然而，当使用动态分区时，我们可以将这项工作大大简化为仅一个查询：
- en: Notice the extra `ds` in the `SELECT` and `GROUP BY` clause, the expanded range
    in the `WHERE` clause, and how we changed the syntax from `PARTITION (ds= '{{ds}}')` to `PARTITION
    (ds)` . The beauty of dynamic partitions is that we wrap all the same work that
    is needed with a `GROUP BY ds` and insert the results into the relevant ds partitions
    all at once. This query pattern is very powerful and is used by many of Airbnb’s
    data pipelines. In a later section, I will demonstrate how one can write an Airflow
    job that incorporates backfilling logic using [Jinja](http://jinja.pocoo.org/) control
    flow.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意`SELECT`和`GROUP BY`子句中的额外`ds`，`WHERE`子句中扩展的范围，以及我们如何将语法从`PARTITION (ds= '{{ds}}')`更改为`PARTITION
    (ds)`。动态分区的美在于，我们将所需的所有相同工作封装在一个`GROUP BY ds`中，并一次性将结果插入到相关的ds分区中。这种查询模式非常强大，许多Airbnb的数据管道都在使用。在后面的部分，我将演示如何编写一个包含回填逻辑的Airflow作业，使用[Jinja](http://jinja.pocoo.org/)控制流。
- en: More On This Topic
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[A Beginner’s Guide to Data Engineering](https://www.kdnuggets.com/2023/07/beginner-guide-data-engineering.html)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学者数据工程指南](https://www.kdnuggets.com/2023/07/beginner-guide-data-engineering.html)'
- en: '[A Beginner''s Guide to Anomaly Detection Techniques in Data Science](https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学者数据科学中异常检测技术指南](https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html)'
- en: '[Introduction to Data Science: A Beginner''s Guide](https://www.kdnuggets.com/2023/07/introduction-data-science-beginner-guide.html)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学入门：初学者指南](https://www.kdnuggets.com/2023/07/introduction-data-science-beginner-guide.html)'
- en: '[Beginner’s Guide to Data Cleaning with Pyjanitor](https://www.kdnuggets.com/beginners-guide-to-data-cleaning-with-pyjanitor)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学者数据清洗指南：Pyjanitor](https://www.kdnuggets.com/beginners-guide-to-data-cleaning-with-pyjanitor)'
- en: '[A Beginner''s Guide to End to End Machine Learning](https://www.kdnuggets.com/2021/12/beginner-guide-end-end-machine-learning.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初学者端到端机器学习指南](https://www.kdnuggets.com/2021/12/beginner-guide-end-end-machine-learning.html)'
- en: '[Essential Machine Learning Algorithms: A Beginner''s Guide](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基础机器学习算法：初学者指南](https://www.kdnuggets.com/2021/05/essential-machine-learning-algorithms-beginners.html)'
