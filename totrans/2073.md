# 37 个原因解释你的神经网络为何不起作用

> 原文：[https://www.kdnuggets.com/2017/08/37-reasons-neural-network-not-working.html/2](https://www.kdnuggets.com/2017/08/37-reasons-neural-network-not-working.html/2)

### III. 实现问题

![](../Images/d1d7015310dac70d5bd4d0899f6d5a97.png)

参考：https://xkcd.com/1838/

#### 16\. 尝试解决问题的简化版本

这将有助于找到问题所在。例如，如果目标输出是对象类别和坐标，尝试将预测限制为仅对象类别。

#### 17\. 寻找“随机会”损失的正确性

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业轨道。

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析技能

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织进行 IT 支持

* * *

再次参考优秀的[CS231n](http://cs231n.github.io/neural-networks-3/#sanitycheck)：*用小参数初始化，不使用正则化。例如，如果我们有 10 个类别，随机情况下我们将有 10% 的时间得到正确的类别，而 Softmax 损失是正确类别的负对数概率，因此： -ln(0.1) = 2.302。*

之后，尝试增加正则化强度，这应该会增加损失。

#### 18\. 检查你的损失函数

如果你实现了自己的损失函数，检查它是否有错误并添加单元测试。通常，我的损失函数会略微不准确，从而在微妙的方式上影响网络的性能。

#### 19\. 验证损失输入

如果你使用了框架提供的损失函数，确保你传递给它的是它所期望的。例如，在 PyTorch 中，我会混淆 NLLLoss 和 CrossEntropyLoss，因为前者需要 softmax 输入，而后者则不需要。

#### 20\. 调整损失权重

如果你的损失函数由几个较小的损失函数组成，确保它们相对彼此的大小是正确的。这可能需要测试不同的损失权重组合。

#### 21\. 监控其他指标

有时候损失函数并不是网络训练是否正常的最佳预测指标。如果可以，使用其他指标如准确率。

#### 22\. 测试任何自定义层

你是否自己实现了网络中的任何层？检查并仔细核对，确保它们按预期工作。

#### 23\. 检查“冻结”层或变量

检查是否无意中禁用了应可学习的某些层/变量的梯度更新。

#### 24\. 增加网络规模

也许你的网络的表达能力不足以捕捉目标函数。尝试增加更多的层或在全连接层中增加更多的隐藏单元。

#### 25\. 检查隐藏维度错误

如果你的输入看起来像(k, H, W) = (64, 64, 64)，很容易忽略与维度错误相关的错误。使用奇怪的输入维度（例如，每个维度使用不同的质数）并检查它们在网络中的传播。

#### 26\. 探索梯度检查

如果你手动实现了梯度下降，梯度检查可以确保你的反向传播正常工作。更多信息：[1](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/) [2](http://cs231n.github.io/neural-networks-3/#gradcheck) [3](https://www.coursera.org/learn/machine-learning/lecture/Y3s6r/gradient-checking)。

### IV. 训练问题

![](../Images/38da157d4f92ff6ffd306f58f3beba9e.png)

版权归属：http://carlvondrick.com/ihog/

#### 27\. 解决非常小的数据集

**对数据的一个小子集进行过拟合，并确保它有效。**例如，仅用1或2个示例进行训练，看看你的网络是否能学会区分这些示例。然后再增加每类的样本数量。

#### 28\. 检查权重初始化

如果不确定，使用[Xavier](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)或[He](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf)初始化。此外，你的初始化可能导致你陷入不良的局部最小值，因此尝试不同的初始化，看看是否有帮助。

#### 29\. 更改你的超参数

也许你使用了一组特别差的超参数。如果可能，尝试进行[网格搜索](http://scikit-learn.org/stable/modules/grid_search.html)。

#### 30\. 减少正则化

过多的正则化可能导致网络严重欠拟合。减少正则化，例如dropout、batch norm、权重/偏置L2正则化等。在优秀的“[Practical Deep Learning for coders](http://course.fast.ai/)”课程中，[Jeremy Howard](https://twitter.com/jeremyphoward)建议先解决欠拟合。这意味着你要充分过拟合训练数据，然后再处理过拟合。

#### 31\. 给它时间

也许你的网络需要更多时间进行训练，才开始做出有意义的预测。如果损失持续下降，可以让它继续训练。

#### 32\. 切换从训练模式到测试模式

一些框架中的层，如Batch Norm、Dropout以及其他层在训练和测试期间表现不同。切换到适当的模式可能有助于网络更好地预测。

#### 33\. 可视化训练

+   监控每层的激活、权重和更新。确保它们的幅度匹配。例如，参数（权重和偏置）更新的幅度[应为 1-e3](https://cs231n.github.io/neural-networks-3/#summary)。

+   考虑使用可视化库如[Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)和[Crayon](https://github.com/torrvision/crayon)。在紧急情况下，你也可以打印权重/偏置/激活值。

+   注意层激活值是否远大于0。尝试使用Batch Norm或ELUs。

+   [Deeplearning4j](https://deeplearning4j.org/visualization#usingui) 指出权重和偏差的直方图应注意什么：

> “对于权重，这些直方图在一段时间后应具有**大致的高斯（正态）**分布。对于偏差，这些直方图通常从0开始，通常会最终**大致呈高斯分布**（LSTM是一个例外）。注意参数是否发散到正负无穷大。注意偏差是否变得非常大。如果类别分布非常不平衡，这有时会发生在分类的输出层。”

+   检查层更新，它们应具有高斯分布。

#### 34\. 尝试不同的优化器

选择的优化器不应该阻止你的网络训练，除非你选择了特别差的超参数。然而，适当的优化器可以帮助你在最短的时间内获得最多的训练。描述你正在使用的算法的论文应该指定优化器。如果没有，我倾向于使用Adam或普通的带动量的SGD。

查看这个[精彩的帖子](http://ruder.io/optimizing-gradient-descent/) 来了解更多关于梯度下降优化器的内容。

#### 35\. 梯度爆炸/消失

+   检查层更新，因为非常大的值可能表示梯度爆炸。梯度裁剪可能会有所帮助。

+   检查层激活情况。[Deeplearning4j](https://deeplearning4j.org/visualization#usingui) 提供了一个很好的指导：*“激活的标准偏差应在0.5到2.0之间。显著超出这个范围可能表示激活消失或爆炸。”*

#### 36\. 增加/减少学习率

低学习率会导致模型收敛非常缓慢。

高学习率在开始时会快速减少损失，但可能很难找到一个好的解决方案。

通过将当前学习率乘以0.1或10来调整它。

#### 37\. 克服NaNs

获取NaN（非数字）在训练RNN时是一个更大的问题（从我听说的）。一些修复方法：

+   降低学习率，特别是当你在前100次迭代中遇到NaNs时。

+   NaNs可能源于除以零或自然对数为零或负数。

+   Russell Stewart 对于[如何处理NaNs](http://russellsstewart.com/notes/0.html) 提供了很好的建议。

+   尝试逐层评估你的网络，看看NaNs出现的位置。

**简介：[Slav Ivanov](https://twitter.com/slavivanov)** 是保加利亚索非亚的企业家和机器学习从业者。他在https://blog.slavv.com 上写关于机器学习的博客。之前构建了http://postplanner.com。

[原文](https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607)。经许可转载。

相关：

+   [深度学习和神经网络基础知识：初学者的基本概念](/2017/08/deep-learning-neural-networks-primer-basic-concepts-beginners.html)

+   [更快更精准地训练你的深度学习模型：快照集成——以1的成本实现M个模型](/2017/08/train-deep-learning-faster-snapshot-ensembling.html)

+   [机器学习中的优化：稳健还是全局最小值？](/2017/06/robust-global-minimum.html)

### 更多相关话题

+   [你应该使用线性回归模型而不是……的3个理由](https://www.kdnuggets.com/2021/08/3-reasons-linear-regression-instead-neural-networks.html)

+   [停止学习数据科学以寻找目标并找到目标……](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)

+   [成为优秀数据科学家所需的5项关键技能](https://www.kdnuggets.com/2021/12/5-key-skills-needed-become-great-data-scientist.html)

+   [每个初学者数据科学家应该掌握的6种预测模型](https://www.kdnuggets.com/2021/12/6-predictive-models-every-beginner-data-scientist-master.html)

+   [一个90亿美元的AI失败案例，详尽分析](https://www.kdnuggets.com/2021/12/9b-ai-failure-examined.html)

+   [建立一个稳固的数据团队](https://www.kdnuggets.com/2021/12/build-solid-data-team.html)
