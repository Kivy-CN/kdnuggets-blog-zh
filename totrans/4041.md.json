["```py\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import LinearSVC\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\n\ndef create_dataset(\n    n_samples=1000, weights=(0.01, 0.01, 0.98), n_classes=3, class_sep=0.8, n_clusters=1\n):\n    return make_classification(\n        n_samples=n_samples,\n        n_features=2,\n        n_informative=2,\n        n_redundant=0,\n        n_repeated=0,\n        n_classes=n_classes,\n        n_clusters_per_class=n_clusters,\n        weights=list(weights),\n        class_sep=class_sep,\n        random_state=0,\n    )\n\ndef plot_decision_function(X, y, clf, ax):\n    plot_step = 0.02\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(\n        np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)\n    )\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    ax.contourf(xx, yy, Z, alpha=0.4)\n    ax.scatter(X[:, 0], X[:, 1], alpha=0.8, c=y, edgecolor=\"k\")\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n\nax_arr = (ax1, ax2, ax3, ax4)\nweights_arr = (\n    (0.01, 0.01, 0.98),\n    (0.01, 0.05, 0.94),\n    (0.2, 0.1, 0.7),\n    (0.33, 0.33, 0.33),\n)\nfor ax, weights in zip(ax_arr, weights_arr):\n    X, y = create_dataset(n_samples=1000, weights=weights)\n    clf = LinearSVC().fit(X, y)\n    plot_decision_function(X, y, clf, ax)\n    ax.set_title(\"Linear SVC with y={}\".format(Counter(y))) \n```", "```py\npip install -U imbalanced-learn\n```", "```py\n# Import necessary libraries and modules\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.under_sampling import NearMiss\n\n# Generate the dataset with different class weights\nfeatures, labels = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    weights=[0.95, 0.05],\n    flip_y=0,\n    random_state=0,\n)\n\n# Print the distribution of classes\ndist_classes = Counter(labels)\nprint(\"Before Undersampling:\")\nprint(dist_classes)\n\n# Generate a scatter plot of instances, labeled by class\nfor class_label, _ in dist_classes.items():\n    instances = np.where(labels == class_label)[0]\n    plt.scatter(features[instances, 0], features[instances, 1], label=str(class_label))\nplt.legend()\nplt.show()\n\n# Set up the undersampling method\nundersampler = NearMiss(version=1, n_neighbors=3)\n\n# Apply the transformation to the dataset\nfeatures, labels = undersampler.fit_resample(features, labels)\n\n# Print the new distribution of classes\ndist_classes = Counter(labels)\nprint(\"After Undersampling:\")\nprint(dist_classes)\n\n# Generate a scatter plot of instances, labeled by class\nfor class_label, _ in dist_classes.items():\n    instances = np.where(labels == class_label)[0]\n    plt.scatter(features[instances, 0], features[instances, 1], label=str(class_label))\nplt.legend()\nplt.show()\n```", "```py\nfrom imblearn.under_sampling import CondensedNearestNeighbour\n\ncnn = CondensedNearestNeighbour(random_state=42)\nX_res, y_res = cnn.fit_resample(X, y)\n```", "```py\nfrom imblearn.under_sampling import TomekLinks\n\ntl = TomekLinks()\nX_res, y_res = tl.fit_resample(X, y)\n\nprint('Original dataset shape:', Counter(y))\nprint('Resample dataset shape:', Counter(y_res))\n```"]