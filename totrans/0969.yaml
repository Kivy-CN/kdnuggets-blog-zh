- en: Dimensionality Reduction Techniques in Data Science
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/09/dimensionality-reduction-techniques-data-science.html](https://www.kdnuggets.com/2022/09/dimensionality-reduction-techniques-data-science.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Dimensionality Reduction Techniques in Data Science](../Images/800a46a763162a8519b021b1f506541a.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Image by Editor
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing data with a list of variables in machine learning requires a lot of
    resources and computations, not to mention the manual labor that goes with it.
    This is precisely where the **dimensionality reduction techniques** come into
    the picture. The dimensionality reduction technique is a process that transforms
    a high-dimensional dataset into a lower-dimensional dataset without losing the
    valuable properties of the original data. These **dimensionality reduction techniques**
    are basically a part of the data pre-processing step, performed before training
    the model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: What is Dimensionality Reduction in Data Science?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you are training a model that could predict the next day's weather based
    on the various climatic conditions of the present day. The present-day conditions
    could be based on sunlight, humidity, cold, temperature, and many millions of
    such environmental features, which are too complex to analyze. Hence, we can lessen
    the number of features by observing which of them are strongly correlated with
    each other and clubbing them into one.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '![Dimensionality Reduction Techniques in Data Science](../Images/8b21115b2119e4c5418d96147df87680.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
- en: image by Auhtor
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can club humidity and rainfall into a single dependent feature since
    we know they are strongly correlated. That's it! This is how the dimensionality
    reduction technique is used to compress complex data into a simpler form without
    losing the essence of the data. Moreover, data science and AI experts are now
    also using **data science solutions** to leverage business ROI. Data visualization,
    data mining, predictive analytics, and other **data analytics services** by Datatobiz
    are changing the business game.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Why is Dimensionality Reduction Necessary?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: machine learning and Deep Learning techniques are performed by inputting a vast
    amount of data to learn about fluctuations, trends, and patterns. Unfortunately,
    such huge data consists of many features, which often leads to a curse of dimensionality.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, sparsity is a common occurrence in large datasets. Sparsity refers
    to having negligible or no value features, and if it is inputted in a training
    model, it performs poorly on testing. In addition, such redundant features cause
    problems in clustering the similar features of the data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, to counter the curse of dimensionality, dimensionality reduction techniques
    come to the rescue. The answers to the question of why dimensionality reduction
    is useful are:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: The model performs more accurately since redundant data will be removed, which
    will lead to less room for assumption.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Less usage of computational resources, which will save time and financial budget
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A few machine learning/Deep Learning techniques do not work on high-dimensional
    data, a problem that will be solved once the dimension reduces.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clean and non-sparse data will give rise to more statistically significant results
    because clustering of such data is easier and more accurate.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let us understand **which algorithms are used for dimensionality reduction
    of data** with examples.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: What are Dimensionality Reduction Techniques
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dimensionality reduction techniques are broadly divided into two categories,
    namely,
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Linear Methods
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Non-Linear Methods
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. Linear Methods
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCA
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Principal Component Analysis (PCA) is one of the used DR techniques in data
    science. Consider a set of '*p*' variables that are correlated with each other.
    This technique reduces this set of '*p*' variables into a smaller number of uncorrelated
    variables, usually denoted by '*k*', where (*k<p*). These '*k*' variables are
    called principal components, and their variation is similar to the original dataset.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: PCA is used to figure out the correlation among features, which it combines
    together. As a result, the resultant dataset has lesser features that are linearly
    correlated with each other. This way, the model performs the reduction of correlated
    features while simultaneously calculating maximum variance in the original dataset.
    After finding the directions of this variance, it directs them into a smaller
    dimensional space which gives rise to new components called principal components.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: These components are pretty sufficient in representing the original features.
    Therefore, it reduces the reconstruction error while finding out the optimum components.
    This way, data is reduced, making the machine learning algorithms perform better
    and faster.  PrepAI is one of the perfect examples of AI that has made use of
    the PCA technique in the backend to generate questions from a given raw text intelligently.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Factor Analysis
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This technique is an extension of Principal Component Analysis (PCA). The main
    focus of this technique is not just to reduce the dataset. It focuses more on
    finding out latent variables, which are results of other variables from the dataset.
    They are not measured directly in a single variable.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Latent variables are also called factors. Hence, the process of building a model
    which measures these latent variables is known as factor analysis. It not only
    helps in reducing the variables but also helps in distinguishing response clusters.
    For example, you have to build a model which will predict customer satisfaction.
    You will prepare a questionnaire that has questions like,
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在变量也称为因子。因此，建立一个测量这些潜在变量的模型的过程称为因子分析。它不仅有助于减少变量，还帮助区分响应簇。例如，你需要建立一个预测客户满意度的模型。你将准备一个问卷，其中包含类似的问题，
- en: '"Are you happy with our product?"'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: “你对我们的产品满意吗？”
- en: '"Would you share your experience with your acquaintances?"'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: “你愿意与熟人分享你的经验吗？”
- en: If you want to create a variable to rate customer satisfaction, you will either
    average the responses or create a factor-dependent variable. This can be performed
    using PCA and keeping the first factor as a principal component.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想创建一个变量来评估客户满意度，你可以选择对反馈进行平均或创建一个因子依赖的变量。这可以通过PCA完成，并将第一个因子作为主成分。
- en: Linear Discriminant Analysis
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性判别分析
- en: It is a dimensionality reduction technique that is used mainly for supervised
    classification problems. Logistic Regression fails in multi-classification. Hence,
    LDA comes into the picture to counter that shortcoming. It efficiently discriminates
    between training variables in their respective classes. Moreover, it is different
    from PCA as it calculates a linear combination between the input features to optimize
    the process of distinguishing different classes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种主要用于监督分类问题的降维技术。逻辑回归在多分类问题中效果不佳。因此，LDA应运而生，以弥补这一不足。它有效地区分了各个类别的训练变量。此外，它与PCA不同，因为它通过计算输入特征之间的线性组合来优化区分不同类别的过程。
- en: 'Here is an example to help you understand LDA:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子帮助你理解LDA：
- en: 'Consider a set of balls belonging to two classes: Red Balls and Blue Balls.
    Imagine they are plotted on a 2D plane randomly, such that they cannot be separated
    into two distinct classes using a straight line. In such cases, LDA is used, which
    can convert a 2D graph into a 1D graph, thereby maximizing the distinction between
    the classes of balls. The balls are projected to a new axis which separates them
    into their classes in the best possible way. The new axis is formed using two
    steps:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一组属于两类的球：红球和蓝球。假设它们随机地绘制在二维平面上，以至于不能用直线将它们分成两个不同的类别。在这种情况下，使用LDA可以将二维图转换为一维图，从而最大限度地区分球的类别。球被投影到一个新的轴上，这个轴以最佳方式将它们分隔到各自的类别中。新的轴是通过两个步骤形成的：
- en: By maximizing the distances between the means of two classes
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过最大化两个类别均值之间的距离
- en: By minimizing the variation within each individual class
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过最小化每个类别内部的变异
- en: SVD
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SVD
- en: Consider data with '*m*' columns. Truncated Singular Value Decomposition method
    (TSVD) is a projection method where these '*m*' columns (features) are projected
    into a subspace with '*m*' or lesser columns without losing the characteristics
    of the data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具有'*m*'列的数据。截断奇异值分解方法（TSVD）是一种投影方法，其中这些'*m*'列（特征）被投影到一个具有'*m*'列或更少列的子空间中，而不会丢失数据的特征。
- en: An example where TSVD can be used is a dataset containing reviews about e-commerce
    products. The review column is mostly left blank, which gives rise to null values
    in the data, and TSVD tackles it efficiently. This method can be implemented easily
    using the TruncatedSVD() function.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一个TSVD可以使用的例子是包含电子商务产品评论的数据集。评论列通常是空白的，这会导致数据中出现空值，而TSVD可以有效处理这种情况。这个方法可以通过TruncatedSVD()函数轻松实现。
- en: While the PCA uses dense data, SVD uses sparse data. Moreover, the covariance
    matrix is used for PCA factorization, whereas TSVD is done on a data matrix.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: PCA使用的是密集数据，而SVD使用的是稀疏数据。此外，PCA因式分解使用协方差矩阵，而TSVD是在数据矩阵上进行的。
- en: 2\. Non-Linear Methods
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 非线性方法
- en: Kernel PCA
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核PCA
- en: PCA is quite efficient for datasets that are linearly separable. However, if
    we apply it to datasets that are non-linear, the reduced dimension of the dataset
    may not be accurate. Hence, this is where Kernel PCA becomes efficient.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: PCA对于线性可分的数据集非常有效。然而，如果我们将它应用于非线性数据集，数据集的降维可能不准确。因此，这时核PCA变得高效。
- en: The dataset undergoes a kernel function and is temporarily projected into a
    higher dimensional feature space. Here, the classes are transformed and can be
    separated linearly and distinguished with the help of a straight line. Further,
    a general PCA is applied, and the data is projected back onto a reduced dimensional
    space. Conducting this linear dimensionality reduction method in that space will
    be as good as conducting non-linear dimensionality reduction in the actual space.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集经过一个核函数并暂时投影到更高维的特征空间。在这里，类别被转换，可以用直线分开并加以区分。然后，应用一般的 PCA，将数据投影回降维空间。在该空间中进行线性降维方法将与在实际空间中进行非线性降维一样有效。
- en: 'Kernel PCA operates on 3 important hyperparameters: the number of components
    we wish to retain, the type of kernel we want to use, and the kernel coefficient.
    There are different types of the kernel, namely, ''linear'', ''poly'', ''rbf'',
    ''sigmoid'', ''cosine''. Radial Basis Function kernel (RBF) is widely used among
    them.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 核主成分分析（Kernel PCA）操作有 3 个重要的超参数：我们希望保留的组件数量、我们要使用的核类型和核系数。有不同类型的核，即“线性”（'linear'）、“多项式”（'poly'）、“径向基函数”（'rbf'）、“
    sigmoid”和“余弦”（'cosine'）。其中，径向基函数核（RBF）被广泛使用。
- en: T-Distributed Stochastic Neighbor Embedding
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: T-分布随机邻域嵌入（T-Distributed Stochastic Neighbor Embedding）
- en: It is a non-linear dimensionality reduction method primarily applied for data
    visualization, image processing, and NLP. T-SNE has a flexible parameter, namely,
    'perplexity'. It showcases how to maintain attendance between global and local
    aspects of the dataset. It gives an estimate of the number of close neighbors
    of each data point. Also, it transforms the similarities between different data
    points into joint probabilities, and the Kullback-Leibler divergence is minimized
    between the joint probabilities of low-dimensional embedding and high-dimensional
    datasets. Moreover, T-SNE also comes up with a cost function that is not convex
    in nature, and with different initializations, one can get different results.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种非线性降维方法，主要用于数据可视化、图像处理和自然语言处理（NLP）。T-SNE 具有一个灵活的参数，即“困惑度”（'perplexity'）。它展示了如何在数据集的全局和局部方面之间保持平衡。它提供了每个数据点的近邻数量的估计。此外，它将不同数据点之间的相似性转换为联合概率，并在低维嵌入和高维数据集的联合概率之间最小化
    Kullback-Leibler 散度。此外，T-SNE 还提供了一个非凸的成本函数，使用不同的初始化方法可能会得到不同的结果。
- en: T-SNE preserves only minimum pairwise distances or local similarities, while
    PCA preserves maximum pairwise distances to maximize variance. Also, PCA or TSVD
    is highly recommended to reduce the dimensions of the features in the dataset
    to exceed more than 50 because T-SNE fails in this case.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: T-SNE 仅保留最小的成对距离或局部相似性，而 PCA 保留最大对距离以最大化方差。此外，推荐使用 PCA 或 TSVD 将数据集中特征的维度减少到超过
    50，因为在这种情况下 T-SNE 会失效。
- en: Multi-Dimensional Scaling
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多维缩放（Multi-Dimensional Scaling）
- en: Scaling refers to making the data simpler by reducing it to a lower dimension.
    It is a non-linear dimensionality reduction technique that showcases the distances
    or dissimilarities between the sets of features in a visualized manner. Features
    with shorter distances are considered similar, whereas those with larger distances
    are dissimilar.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放指的是通过将数据减少到较低的维度来简化数据。这是一种非线性降维技术，通过可视化的方式展示特征集之间的距离或差异。特征之间距离较短被认为是相似的，而距离较大的则被认为是不相似的。
- en: MDS reduces the data dimension and interprets the dissimilarities in the data.
    Also, data doesn't lose its essence after scaling down; two data points will always
    be at the same distance irrespective of their dimension. This technique can only
    be applied to matrices having relational data, such as correlations, distances,
    etc. Let's understand this with the help of an example.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: MDS 减少数据维度并解释数据中的差异。此外，数据在缩小后不会失去本质；两个数据点的距离总是相同的，无论它们的维度如何。此技术仅适用于具有关系数据的矩阵，例如相关性、距离等。让我们通过一个示例来理解这一点。
- en: Consider you have to make a map, where you are provided with a list of city
    locations. The map should also showcase the distances between the two cities.
    The only possible method to do this would be to measure the distances between
    the cities with the help of a meter tape. But what if you are only provided with
    the distances between the cities and their similarities instead of the city locations?
    You could still draw a map using logical assumptions and a wide knowledge of geometry.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Here, you are basically applying MDS to create a map. MDS observes the differences
    in the dataset and creates a map that calculates the original distances and tells
    you where they are located.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Isometric Mapping (Isomap)
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is a non-linear dimensionality reduction technique that is basically an extension
    of MDS or Kernel PCA. It reduces dimensionality by connecting every feature on
    the basis of their curved or geodesic distances between their nearest neighbors.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Isomap is initiated by building a neighborhood network. Then, it uses graph
    distance to estimate the geodesic distance between every pair of points. Finally,
    the dataset is embedded in a lower dimension by decomposing the eigenvalues of
    the geodesic matrix. It can be specified how many neighbors to consider for each
    data point using the *n_neighbours* hyperparameter of the Isomap() class. This
    class implements the Isomap algorithm.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: What is the need for Dimensionality Reduction in Data Mining?
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data mining is the process of observing hidden patterns, relations, and anomalies
    within vast datasets in order to estimate outcomes. Vast datasets have many variables
    increasing at an exponential rate. Therefore, finding and analyzing patterns in
    them during the data mining process takes lots of resources and computational
    time. Hence, the dimensionality reduction technique can be applied while data
    mining to limit those data features by clubbing them and still be sufficient enough
    to represent the original dataset.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and Disadvantages of Dimensionality Reduction
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Advantages
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Storage space and the processing time are less
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-collinearity of the dependent variables is removed
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduced chances of overfitting the model
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Visualization becomes easier
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disadvantages
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some amount of data is lost.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA cannot be applied where data cannot be defined through mean and covariance.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not every variable needs to be linearly correlated, which PCA tends to find.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Labeled data is required for LDA to function, which is not available in a few
    cases.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A vast amount of data is generated every second. So, analyzing them with optimal
    use of resources and with accuracy is equally important. Dimensionality Reduction
    techniques help in data pre-processing in a precise and efficient manner—no wonder
    why it is considered a boon for data scientists.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '**[Kavika Roy](https://www.datatobiz.com/)** is Head of Information Management
    at [DataToBiz](https://www.datatobiz.com/). She is responsible for the identification,
    acquisition, distribution, and organization of technical oversight. Her strong
    attention to detail lets her deliver precise information regarding functional
    aspects to the right audience.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Kavika Roy](https://www.datatobiz.com/)** 是 [DataToBiz](https://www.datatobiz.com/)
    的信息管理部门负责人。她负责技术监督的识别、获取、分配和组织。她对细节的高度关注使她能够向正确的受众提供有关功能方面的精确信息。'
- en: More On This Topic
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[KDnuggets News, August 31: The Complete Data Science Study Roadmap…](https://www.kdnuggets.com/2022/n35.html)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，8月31日：完整的数据科学学习路线图…](https://www.kdnuggets.com/2022/n35.html)'
- en: '[Top AI and Data Science Tools and Techniques for 2022 and Beyond](https://www.kdnuggets.com/2022/03/nvidia-0317-top-ai-data-science-tools-techniques-2022-beyond.html)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2022年及以后顶级人工智能和数据科学工具与技术](https://www.kdnuggets.com/2022/03/nvidia-0317-top-ai-data-science-tools-techniques-2022-beyond.html)'
- en: '[A Beginner''s Guide to Anomaly Detection Techniques in Data Science](https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中的异常检测技术初学者指南](https://www.kdnuggets.com/2023/05/beginner-guide-anomaly-detection-techniques-data-science.html)'
- en: '[The Role of Resampling Techniques in Data Science](https://www.kdnuggets.com/2023/02/role-resampling-techniques-data-science.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中重采样技术的作用](https://www.kdnuggets.com/2023/02/role-resampling-techniques-data-science.html)'
- en: '[Exploratory Data Analysis Techniques for Unstructured Data](https://www.kdnuggets.com/2023/05/exploratory-data-analysis-techniques-unstructured-data.html)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用于非结构化数据的探索性数据分析技术](https://www.kdnuggets.com/2023/05/exploratory-data-analysis-techniques-unstructured-data.html)'
- en: '[7 Techniques to Handle Imbalanced Data](https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[处理不平衡数据的7种技术](https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html)'
