- en: 'Getting Started with PyTorch Part 1: Understanding How Automatic Differentiation
    Works'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 入门第 1 部分：理解自动微分如何工作
- en: 原文：[https://www.kdnuggets.com/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html](https://www.kdnuggets.com/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html](https://www.kdnuggets.com/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html?page=2#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html?page=2#comments)'
- en: '**By [Ayoosh Kathuria](https://www.linkedin.com/in/ayoosh-kathuria-44a319132/),
    Research Intern**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Ayoosh Kathuria](https://www.linkedin.com/in/ayoosh-kathuria-44a319132/)，研究实习生**'
- en: When I started to code neural networks, I ended up using what everyone else
    around me was using. TensorFlow.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当我开始编写神经网络代码时，我最终使用了周围人都在使用的 TensorFlow。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: But recently, PyTorch has emerged as a major contender in the race to be the
    king of deep learning frameworks. What makes it really luring is it’s dynamic
    computation graph paradigm. Don’t worry if the last line doesn’t make sense to
    you now. By the end of this post, it will. But take my word that it makes debugging
    neural networks way easier.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但最近，PyTorch 已经成为深度学习框架的主要竞争者。它真正吸引人的地方在于其动态计算图范式。如果最后一句话现在对你没有意义，也不用担心。到这篇文章结束时，你会明白的。但请相信我，它使得调试神经网络变得容易得多。
- en: I've been using PyTorch a few months now and I've never felt better. I have
    more energy. My skin is clearer. My eye sight has improved.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我已经使用 PyTorch 几个月了，感觉从未如此好。我更有精力了。我的皮肤更清晰了。我的视力也有所改善。
- en: ''
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Andrej Karpathy (@karpathy) [May 26, 2017](https://twitter.com/karpathy/status/868178954032513024?ref_src=twsrc%5Etfw)
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: — Andrej Karpathy (@karpathy) [2017年5月26日](https://twitter.com/karpathy/status/868178954032513024?ref_src=twsrc%5Etfw)
- en: If you’re wondering why your energy has been low lately, switch to PyTorch!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道为什么你最近精力低下，试试 PyTorch 吧！
- en: Prerequisites
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前提条件
- en: 'Before we begin, I must point out that you should have at least the basic idea
    about:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我必须指出，您至少应对以下内容有基本了解：
- en: Concepts related to training of neural networks, particularly backpropagation
    and gradient descent.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与神经网络训练相关的概念，特别是反向传播和梯度下降。
- en: Applying the chain rule to compute derivatives.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用链式法则计算导数。
- en: How classes work in Python. (Or a general idea about Object Oriented Programming)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 中的类如何工作。（或面向对象编程的一般概念）
- en: In case, you’re missing any of the above, I’ve provided links at the end of
    the article to guide you.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您缺少上述任何内容，我已在文章末尾提供了链接以供参考。
- en: So, it’s time to get started with PyTorch. This is the first in a series of
    tutorials on PyTorch.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，是时候开始学习 PyTorch 了。这是 PyTorch 系列教程中的第一篇。
- en: This is the part 1 where I’ll describe the basic building blocks, and *Autograd*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第一部分，我将描述基本构建块和*Autograd*。
- en: '**NOTE: **An important thing to notice is that the tutorial is made for PyTorch
    0.3 and lower versions. The latest version on offer is 0.4\. I’ve decided to stick
    with 0.3 because as of now, 0.3 is the version that is shipped in Conda and pip
    channels. Also, most of PyTorch code that is used in open source hasn’t been updated
    to incorporate some of the changes proposed in 0.4\. I, however, will point out
    at certain places where things differ in 0.3 and 0.4.'
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 一个重要的事情是，这个教程是针对 PyTorch 0.3 及更低版本制作的。现在提供的最新版本是 0.4。我决定坚持使用 0.3，因为到目前为止，0.3
    是 Conda 和 pip 渠道中提供的版本。此外，开源社区中使用的大多数 PyTorch 代码尚未更新以纳入 0.4 中提出的一些更改。不过，我会在某些地方指出
    0.3 和 0.4 之间的不同。'
- en: 'Building Block #1 : Tensors'
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '构建模块 #1：张量'
- en: If you’ve ever done machine learning in python, you’ve probably come across
    NumPy. The reason why we use Numpy is because it’s much faster than Python lists
    at doing matrix ops. Why? Because it does most of the heavy lifting in C.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经在 Python 中做过机器学习，你可能遇到过 NumPy。我们使用 NumPy 的原因是它在执行矩阵操作时比 Python 列表快得多。为什么？因为它在
    C 中完成了大部分繁重的工作。
- en: But, in case of training deep neural networks, NumPy arrays simply don’t cut
    it. I’m too lazy to do the actual calculations here (google for “FLOPS in one
    iteration of ResNet to get an idea), but code utilising NumPy arrays alone would
    take months to train some of the state of the art networks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在训练深度神经网络时，NumPy 数组根本不够用。我懒得在这里做实际计算（可以在谷歌中搜索“ResNet 一次迭代的 FLOPS 以获取大致了解”），但仅使用
    NumPy 数组的代码可能需要几个月的时间来训练一些最先进的网络。
- en: This is where **Tensors** come into play. PyTorch provides us with a data structure
    called a *Tensor*, which is very similar to NumPy’s *ndarray.* But unlike the
    latter, **tensors can tap into the resources of a GPU to significantly speed up
    matrix operations.**
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**这就是** **张量** **发挥作用的地方。PyTorch 提供了一种称为** *张量* **的数据结构，这与 NumPy 的** *ndarray*
    **非常相似。但与后者不同，** **张量可以利用 GPU 的资源显著加快矩阵操作的速度。**'
- en: Here is how you make a Tensor.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你如何创建一个张量。
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Building Block #2 : Computation Graph'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '构建模块 #2：计算图'
- en: Now, we are at the business side of things. When a neural network is trained,
    we need to compute gradients of the loss function, with respect to every weight
    and bias, and then update these weights using gradient descent.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们进入了实际操作的部分。当神经网络被训练时，我们需要计算损失函数的梯度，针对每个权重和偏置，然后使用梯度下降法更新这些权重。
- en: With neural networks hitting billions of weights, doing the above step efficiently
    can make or break the feasibility of training.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 随着神经网络权重达到数十亿，如何高效地执行上述步骤可能会影响训练的可行性。
- en: '**Building Block #2.1: Computation Graphs**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**构建模块 #2.1：计算图**'
- en: Computation graphs lie at the heart of the way modern deep learning networks
    work, and PyTorch is no exception. Let us first get the hang of what they are.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图是现代深度学习网络工作原理的核心，PyTorch 也不例外。让我们首先了解它们是什么。
- en: 'Suppose, your model is described like this:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，你的模型描述如下：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If I were to actually draw the computation graph, it would probably look like
    this.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我实际绘制计算图，它可能会看起来像这样。
- en: '![](../Images/b1185cdfd81dacc7da52b88949b4cd17.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1185cdfd81dacc7da52b88949b4cd17.png)'
- en: Computation Graph for our Model
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的计算图
- en: '**NOW**, you must note, that the above figure is not entirely an accurate representation
    of how the graph is represented under the hood by PyTorch. However, for now, it’s
    enough to drive our point home.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**现在**，你必须注意，上面的图并不完全准确地表示 PyTorch 内部如何表示图形。然而，目前这足以说明我们的观点。'
- en: Why should we create such a graph when we can sequentially execute the operations
    required to compute the output?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要创建这样的图形，而不是顺序执行计算输出所需的操作呢？
- en: Imagine, what were to happen, if you didn’t merely have to calculate the output
    but also train the network. You’ll have to compute the gradients for all the weights
    labelled by purple nodes. That would require you to figure your way around chain
    rule, and then update the weights.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果你不仅需要计算输出，还需要训练网络会发生什么。你将不得不为所有由紫色节点标记的权重计算梯度。这将要求你弄清楚链式法则，然后更新这些权重。
- en: '**The computation graph is simply a data structure that allows you to efficiently
    apply the chain rule to compute gradients for all of your parameters.**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算图只是一个数据结构，允许你高效地应用链式法则来计算所有参数的梯度。**'
- en: '![](../Images/142a367a1968d0a892540f69c74dedc3.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/142a367a1968d0a892540f69c74dedc3.png)'
- en: Applying the chain rule using computation graphs
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用计算图应用链式法则
- en: Here are a couple of things to notice. First, that the directions of the arrows
    are now reversed in the graph. That’s because we are backpropagating, and arrows
    marks the flow of gradients backwards.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几件事需要注意。首先，箭头的方向现在在图中被反转了。这是因为我们在进行反向传播，箭头标记了梯度反向流动的方向。
- en: Second, for the sake of these example, you can think of the gradients I have
    written as *edge weights*. Notice, these gradients don’t require chain rule to
    be computed.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，为了这些示例，你可以将我写的梯度视为 *边权重*。注意，这些梯度不需要链式法则来计算。
- en: Now, in order to compute the *gradient of any node, say, L, with respect of
    any other node, say c ( dL / dc) *all we have to do is.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了计算 *任何节点的梯度，例如 L，相对于任何其他节点，例如 c (dL / dc)*，我们只需做以下操作。
- en: Trace the path *from L to c*. This would be* L → d → c.*
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跟踪路径 *从 L 到 c*。这将是 *L → d → c*。
- en: 'Multiply all the *edge weights* as you traverse along this path. The quantity
    you end up with is: ( *dL / dd ) * *( *dd / dc ) = ( dL / dc)*'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在沿着这条路径遍历时，乘以所有的 *边权重*。你最终得到的量是：( *dL / dd* ) * ( *dd / dc* ) = (dL / dc)
- en: If there are multiple paths, add their results. For example in case of *dL/da, *we
    have two paths.* L → d → c → a and L → d → b→ a. *We add their contributions to
    get the gradient of* L w.r.t. a.*
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有多条路径，请将它们的结果相加。例如，在 *dL/da* 的情况下，我们有两条路径：*L → d → c → a* 和 *L → d → b → a*。我们将它们的贡献相加，以获得
    *L 对 a 的梯度*。
- en: '*[*( *dL / dd ) * *( *dd / dc ) * *( *dc / da )] *+ *[*( *dL / dd ) * *( *dd
    / db ) * *( *db / da )]*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*[*( *dL / dd* ) * ( *dd / dc* ) * ( *dc / da* )] * + *[*( *dL / dd* ) * (
    *dd / db* ) * ( *db / da* )]*'
- en: In principle, one could start at *L*, and start traversing the graph backwards,
    calculating gradients for every node that comes along the way.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，可以从 *L* 开始，向后遍历图，计算每个经过节点的梯度。
- en: More On This Topic
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Getting Started with PyTorch Lightning](https://www.kdnuggets.com/2022/12/getting-started-pytorch-lightning.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开始使用 PyTorch Lightning](https://www.kdnuggets.com/2022/12/getting-started-pytorch-lightning.html)'
- en: '[Getting Started with PyTorch in 5 Steps](https://www.kdnuggets.com/5-steps-getting-started-pytorch)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[用 5 个步骤开始使用 PyTorch](https://www.kdnuggets.com/5-steps-getting-started-pytorch)'
- en: '[How to Ace Data Science Assessment Test by Using Automatic EDA Tools](https://www.kdnuggets.com/2022/04/ace-data-science-assessment-test-automatic-eda-tools.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何通过使用自动 EDA 工具来应对数据科学评估测试](https://www.kdnuggets.com/2022/04/ace-data-science-assessment-test-automatic-eda-tools.html)'
- en: '[How I Did Automatic Image Labeling Using Grounding DINO](https://www.kdnuggets.com/2023/05/automatic-image-labeling-grounding-dino.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我如何使用 Grounding DINO 进行自动图像标记](https://www.kdnuggets.com/2023/05/automatic-image-labeling-grounding-dino.html)'
- en: '[How ChatGPT Works: The Model Behind The Bot](https://www.kdnuggets.com/2023/04/chatgpt-works-model-behind-bot.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT 如何运作：机器人背后的模型](https://www.kdnuggets.com/2023/04/chatgpt-works-model-behind-bot.html)'
- en: '[The Burtch Works 2023 Data Science & AI Professionals Salary Report…](https://www.kdnuggets.com/2023/08/burtch-works-2023-data-science-ai-professionals-salary-report.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Burtch Works 2023 数据科学与 AI 专业人士薪资报告…](https://www.kdnuggets.com/2023/08/burtch-works-2023-data-science-ai-professionals-salary-report.html)'
