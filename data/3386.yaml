- en: A Quick Introduction to Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络快速介绍
- en: 原文：[https://www.kdnuggets.com/2016/11/quick-introduction-neural-networks.html](https://www.kdnuggets.com/2016/11/quick-introduction-neural-networks.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2016/11/quick-introduction-neural-networks.html](https://www.kdnuggets.com/2016/11/quick-introduction-neural-networks.html)
- en: '**By Ujjwal Karn.**'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**由Ujjwal Karn编写。**'
- en: An Artificial Neural Network (ANN) is a computational model that is inspired
    by the way biological neural networks in the human brain process information.
    Artificial Neural Networks have generated a lot of excitement in Machine Learning
    research and industry, thanks to many breakthrough results in speech recognition,
    computer vision and text processing. In this blog post we will try to develop
    an understanding of a particular type of Artificial Neural Network called the
    Multi Layer Perceptron.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANN）是一个计算模型，灵感来自于人脑中生物神经网络处理信息的方式。由于在语音识别、计算机视觉和文本处理方面取得了许多突破性成果，人工神经网络在机器学习研究和行业中引起了极大的关注。在这篇博客文章中，我们将试图深入了解一种特定类型的人工神经网络，即多层感知器。
- en: A Single Neuron
  id: totrans-4
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单一神经元
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业轨道。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织进行IT工作'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'The basic unit of computation in a neural network is the **neuron**, often
    called a **node** or **unit**. It receives input from some other nodes, or from
    an external source and computes an output. Each input has an associated **weight**
    (w), which is assigned on the basis of its relative importance to other inputs.
    The node applies a function ***f*** (defined below) to the weighted sum of its
    inputs as shown in Figure 1 below:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的基本计算单元是**神经元**，通常称为**节点**或**单元**。它从其他节点或外部源接收输入，并计算输出。每个输入都有一个相关的**权重**（w），根据其对其他输入的相对重要性分配。该节点将一个函数***f***（如下所定义）应用于其输入的加权和，如下图1所示：
- en: '![Screen Shot 2016-08-09 at 3.42.21 AM.png](../Images/087f833f5d2b288a97430ef2a13b9e1c.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![Screen Shot 2016-08-09 at 3.42.21 AM.png](../Images/087f833f5d2b288a97430ef2a13b9e1c.png)'
- en: 'Figure 1: a single neuron'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1：单一神经元
- en: The above network takes numerical inputs **X1** and **X2** and has weights **w1**
    and **w2** associated with those inputs. Additionally, there is another input
    **1** with weight **b** (called the **Bias**) associated with it. We will learn
    more details about role of the bias later.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 上述网络接受数值输入**X1**和**X2**，并具有与这些输入相关的权重**w1**和**w2**。此外，还有另一个输入**1**，与之关联的权重为**b**（称为**偏置**）。我们稍后会了解偏置的作用。
- en: The output **Y** from the neuron is computed as shown in the Figure 1\. The
    function ***f*** is non-linear and is called the **Activation Function**. The
    purpose of the activation function is to introduce non-linearity into the output
    of a neuron. This is important because most real world data is non linear and
    we want neurons to *learn* thesenon linear representations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元的输出**Y**如图1所示计算。函数***f***是非线性的，被称为**激活函数**。激活函数的目的是将非线性引入神经元的输出中。这一点很重要，因为大多数现实世界的数据是非线性的，我们希望神经元能够*学习*这些非线性表示。
- en: 'Every activation function (or *non-linearity*) takes a single number and performs
    a certain fixed mathematical operation on it [2]. There are several activation
    functions you may encounter in practice:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每个激活函数（或*非线性*）接收一个数字，并对其执行某种固定的数学操作[2]。在实践中你可能会遇到几种激活函数：
- en: '**Sigmoid:** takes a real-valued input and squashes it to range between 0 and
    1'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sigmoid：** 将实值输入压缩到0和1之间的范围'
- en: σ(x) = 1 / (1 + exp(−x))
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: σ(x) = 1 / (1 + exp(−x))
- en: '**tanh:** takes a real-valued input and squashes it to the range [-1, 1]'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tanh：** 将实值输入压缩到范围[-1, 1]'
- en: tanh(x) = 2σ(2x) − 1
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: tanh(x) = 2σ(2x) − 1
- en: '**ReLU**: ReLU stands for Rectified Linear Unit. It takes a real-valued input
    and thresholds it at zero (replaces negative values with zero)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReLU**：ReLU 代表修正线性单元。它接受一个实值输入并将其阈值设置为零（将负值替换为零）。'
- en: f(x) = max(0, x)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) = max(0, x)
- en: The below figures [2] show each of the above activation functions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图 [2] 显示了上述每种激活函数。
- en: '![Screen Shot 2016-08-08 at 11.53.41 AM](../Images/e2a53a976104ff7403495f9f57f16eef.png)Figure
    2: different activation functions'
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '![屏幕截图 2016-08-08 11.53.41](../Images/e2a53a976104ff7403495f9f57f16eef.png)图
    2：不同的激活函数'
- en: '**Importance of Bias:** The main function of Bias is to provide every node
    with a trainable constant value (in addition to the normal inputs that the node
    receives). See [this link](http://stackoverflow.com/q/2480650/3297280) to learn
    more about the role of bias in a neuron.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏置的重要性**：偏置的主要功能是为每个节点提供一个可训练的常数值（除了节点接收的正常输入之外）。有关偏置在神经元中作用的更多信息，请参见[这个链接](http://stackoverflow.com/q/2480650/3297280)。'
- en: Feedforward Neural Network
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 前馈神经网络
- en: The feedforward neural network was the first and simplest type of artificial
    neural network devised [3]. It contains multiple neurons (nodes) arranged in **layers**.
    Nodes from adjacent layers have **connections** or **edges** between them. All
    these connections have **weights** associated with them.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络是最早且最简单的人工神经网络类型[3]。它包含多个排列在**层**中的神经元（节点）。相邻层之间的节点具有**连接**或**边**。所有这些连接都有与之相关的**权重**。
- en: An example of a feedforward neural network is shown in Figure 3.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3 显示了一个前馈神经网络的示例。
- en: '![Screen Shot 2016-08-09 at 4.19.50 AM.png](../Images/0cf754ed44cd5d076cb69756caa4820d.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![屏幕截图 2016-08-09 04.19.50](../Images/0cf754ed44cd5d076cb69756caa4820d.png)'
- en: 'Figure 3: an example of feedforward neural network'
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3：前馈神经网络的示例
- en: More On This Topic
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Quick Data Science Tips and Tricks to Learn SAS](https://www.kdnuggets.com/2022/05/sas-quick-data-science-tips-tricks-learn.html)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习 SAS 的快速数据科学技巧和窍门](https://www.kdnuggets.com/2022/05/sas-quick-data-science-tips-tricks-learn.html)'
- en: '[7 Pandas Plotting Functions for Quick Data Visualization](https://www.kdnuggets.com/7-pandas-plotting-functions-for-quick-data-visualization)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7 个 Pandas 绘图函数，快速数据可视化](https://www.kdnuggets.com/7-pandas-plotting-functions-for-quick-data-visualization)'
- en: '[A Quick Overview of Voronoi Diagrams](https://www.kdnuggets.com/2022/11/quick-overview-voronoi-diagrams.html)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Voronoi 图的快速概述](https://www.kdnuggets.com/2022/11/quick-overview-voronoi-diagrams.html)'
- en: '[A Quick Guide to Find the Right Minds for Annotation](https://www.kdnuggets.com/2022/04/quick-guide-find-right-minds-annotation.html)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[快速指南：找到合适的标注人员](https://www.kdnuggets.com/2022/04/quick-guide-find-right-minds-annotation.html)'
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 PyTorch 的可解释神经网络](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
- en: '[Deep Neural Networks Don''t Lead Us Towards AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度神经网络不会引导我们走向 AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
