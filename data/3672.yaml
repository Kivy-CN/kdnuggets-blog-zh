- en: What are Large Language Models and How Do They Work?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型是什么，它们是如何工作的？
- en: 原文：[https://www.kdnuggets.com/2023/05/large-language-models-work.html](https://www.kdnuggets.com/2023/05/large-language-models-work.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/05/large-language-models-work.html](https://www.kdnuggets.com/2023/05/large-language-models-work.html)
- en: '![What are Large Language Models and How Do They Work?](../Images/dc765d725c1cf93ea1d7055551f5f815.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![大型语言模型是什么，它们是如何工作的？](../Images/dc765d725c1cf93ea1d7055551f5f815.png)'
- en: Image by Editor
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑提供的图像
- en: What are Large Language Models?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型是什么？
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业之路。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您的组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Large language models](https://saturncloud.io/glossary/large-language-models/) are
    a type of [artificial intelligence](https://saturncloud.io/glossary/artificial-intelligence/) (AI)
    model designed to understand, generate, and manipulate natural language. These
    models are trained on vast amounts of text data to learn the patterns, grammar,
    and semantics of human language. They leverage deep learning techniques, such
    as neural networks, to process and analyze the textual information.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[大型语言模型](https://saturncloud.io/glossary/large-language-models/)是一种[人工智能](https://saturncloud.io/glossary/artificial-intelligence/)（AI）模型，旨在理解、生成和处理自然语言。这些模型在大量文本数据上进行训练，以学习人类语言的模式、语法和语义。它们利用深度学习技术，如神经网络，来处理和分析文本信息。'
- en: The primary purpose of large language models is to perform various natural language
    processing ([NLP](https://saturncloud.io/glossary/natural-language-processing-nlp/))
    tasks, such as text classification, sentiment analysis, machine translation, summarization,
    question-answering, and content generation. Some well-known large language models
    include OpenAI’s GPT (Generative Pre-trained Transformer) series, with GPT-4 being
    one of the most famous, Google’s [BERT](https://saturncloud.io/glossary/bert/) (Bidirectional
    Encoder Representations from Transformers), and Transformer architectures in general.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型的主要目的是执行各种自然语言处理（[NLP](https://saturncloud.io/glossary/natural-language-processing-nlp/)）任务，如文本分类、情感分析、机器翻译、摘要生成、问答和内容生成。一些知名的大型语言模型包括
    OpenAI 的 GPT（生成预训练变换器）系列，其中 GPT-4 是最著名的之一，Google 的 [BERT](https://saturncloud.io/glossary/bert/)（双向编码器表示变换器），以及
    Transformer 架构的一般应用。
- en: How Large Language Models Work
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型的工作原理
- en: Large language models work by using deep learning techniques to analyze and
    learn from vast amounts of text data, enabling them to understand, generate, and
    manipulate human language for various natural language processing tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型通过使用深度学习技术分析和学习大量文本数据，使其能够理解、生成和处理人类语言，以执行各种自然语言处理任务。
- en: A. Pre-training, Fine-Tuning and Prompt-Based Learning
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A. 预训练、微调和基于提示的学习
- en: 'Pre-training on massive text corpora: Large language models (LLMs) are pre-trained
    on enormous text datasets, which often encompass a significant portion of the
    internet. By learning from diverse sources, LLMs capture the structure, patterns,
    and relationships within language, enabling them to understand context and generate
    coherent text. This pre-training phase helps LLMs build a robust knowledge base
    that serves as a foundation for various natural language processing tasks.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模文本语料库上进行预训练：大型语言模型（LLMs）在庞大的文本数据集上进行预训练，这些数据集通常涵盖了互联网的显著部分。通过从各种来源学习，LLMs
    捕捉语言中的结构、模式和关系，使其能够理解上下文并生成连贯的文本。这一预训练阶段帮助 LLMs 建立一个强大的知识基础，作为各种自然语言处理任务的基础。
- en: 'Fine-tuning on task-specific labeled data: After pre-training, LLMs are fine-tuned
    using smaller, labeled datasets specific to particular tasks and domain, such
    as sentiment analysis, machine translation, or question answering. This fine-tuning
    process allows the models to adapt their general language understanding to the
    nuances of the target tasks, resulting in improved performance and accuracy.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在任务特定的标记数据上进行微调：在预训练之后，LLM 会使用针对特定任务和领域（如情感分析、机器翻译或问答）的较小标记数据集进行微调。这一微调过程使模型能够将其一般语言理解适应目标任务的细微差别，从而提高性能和准确性。
- en: Prompt based-learning differs from traditional LLM training approaches, such
    as those used for GPT-3 and BERT, which demand pre-training on unlabeled data
    and subsequent task-specific fine-tuning with labeled data. Prompt-based learning
    models, on the other hand, can adjust autonomously for various tasks by integrating
    domain knowledge through the use of prompts.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于提示的学习与传统的 LLM 训练方法有所不同，例如 GPT-3 和 BERT 的训练方法，这些方法需要在未标记的数据上进行预训练，然后用标记数据进行任务特定的微调。而基于提示的学习模型则可以通过使用提示整合领域知识，自主调整以适应各种任务。
- en: The success of the output generated by a prompt-based model is heavily reliant
    on the prompt’s quality. An expertly formulated prompt can steer the model towards
    generating precise and pertinent outputs. Conversely, an inadequately designed
    prompt may yield illogical or unrelated outputs. The craft of devising efficient
    prompts is referred to as prompt engineering.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 基于提示的模型生成的输出的成功严重依赖于提示的质量。精心设计的提示可以引导模型生成精确且相关的输出。相反，设计不当的提示可能会产生不合逻辑或无关的输出。设计有效提示的技巧被称为提示工程。
- en: B. Transformer architecture
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B. Transformer 架构
- en: 'Self-attention mechanism: The transformer architecture, which underpins many
    LLMs, introduces a self-attention mechanism that revolutionized the way language
    models process and generate text. Self-attention enables the models to weigh the
    importance of different words in a given context, allowing them to selectively
    focus on relevant information when generating text or making predictions. This
    mechanism is computationally efficient and provides a flexible way to model complex
    language patterns and long-range dependencies.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制：支撑许多大型语言模型（LLM）的 Transformer 架构引入了一种自注意力机制，这一机制彻底改变了语言模型处理和生成文本的方式。自注意力机制使模型能够在给定的上下文中权衡不同单词的重要性，从而在生成文本或进行预测时选择性地关注相关信息。该机制计算效率高，并提供了一种灵活的方式来建模复杂的语言模式和长距离依赖。
- en: 'Positional encoding and embeddings: In the transformer architecture, input
    text is first converted into embeddings, which are continuous vector representations
    that capture the semantic meaning of words. Positional encoding is then added
    to these embeddings to provide information about the relative positions of words
    in a sentence. This combination of embeddings and positional encoding allows the
    transformer to process and generate text in a context-aware manner, enabling it
    to understand and produce coherent language.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码和嵌入：在 Transformer 架构中，输入文本首先被转换为嵌入，这些嵌入是连续的向量表示，用于捕捉单词的语义含义。然后，将位置编码添加到这些嵌入中，以提供有关句子中单词相对位置的信息。这种嵌入和位置编码的结合使
    Transformer 能够以上下文感知的方式处理和生成文本，从而理解和生成连贯的语言。
- en: C. Tokenization methods and techniques
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C. 分词方法和技术
- en: 'Tokenization is the process of converting raw text into a sequence of smaller
    units, called tokens, which can be words, subwords, or characters. Tokenization
    is an essential step in the pipeline of LLMs, as it allows the models to process
    and analyze text in a structured format. There are several tokenization methods
    and techniques used in LLMs:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是将原始文本转换为一系列较小单位（称为标记）的过程，这些单位可以是单词、子词或字符。分词是 LLM 管道中的一个重要步骤，因为它使模型能够以结构化格式处理和分析文本。LLM
    中使用了几种分词方法和技术：
- en: 'Word-based tokenization: This method splits text into individual words, treating
    each word as a separate token. While simple and intuitive, word-based tokenization
    can struggle with out-of-vocabulary words and may not efficiently handle languages
    with complex morphology.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 基于单词的分词：这种方法将文本拆分成单独的单词，将每个单词视为一个独立的标记。虽然简单直观，但基于单词的分词在处理词汇表外的词时可能会遇到困难，并且可能无法有效处理具有复杂形态的语言。
- en: 'Subword-based tokenization: Subword-based methods, such as Byte Pair Encoding
    (BPE) and WordPiece, split text into smaller units that can be combined to form
    whole words. This approach enables LLMs to handle out-of-vocabulary words and
    better capture the structure of different languages. BPE, for instance, merges
    the most frequently occurring character pairs to create subword units, while WordPiece
    employs a data-driven approach to segment words into subword tokens.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 基于子词的分词：子词方法，如字节对编码（BPE）和WordPiece，将文本拆分为可以组合成完整单词的较小单元。这种方法使LLMs能够处理词汇表外的单词，并更好地捕捉不同语言的结构。例如，BPE通过合并最常出现的字符对来创建子词单元，而WordPiece则采用数据驱动的方法将单词分割成子词标记。
- en: 'Character-based tokenization: This method treats individual characters as tokens.
    Although it can handle any input text, character-based tokenization often requires
    larger models and more computational resources, as it needs to process longer
    sequences of tokens.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 基于字符的分词：这种方法将单个字符视为标记。虽然它可以处理任何输入文本，但基于字符的分词通常需要更大的模型和更多的计算资源，因为它需要处理更长的标记序列。
- en: Applications of Large Language Models
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型的应用
- en: A. Text generation and completion
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A. 文本生成与完成
- en: LLMs can generate coherent and fluent text that closely mimics human language,
    making them ideal for applications like creative writing, chatbots, and virtual
    assistants. They can also complete sentences or paragraphs based on a given prompt,
    demonstrating impressive language understanding and context-awareness.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs可以生成连贯流畅的文本，紧密模拟人类语言，使其成为创意写作、聊天机器人和虚拟助手等应用的理想选择。它们还可以根据给定的提示完成句子或段落，展示了出色的语言理解和上下文意识。
- en: B. Sentiment analysis
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B. 情感分析
- en: LLMs have shown exceptional performance in [sentiment analysis](https://saturncloud.io/glossary/sentiment-analysis/) tasks,
    where they classify text according to its sentiment, such as positive, negative,
    or neutral. This ability is widely used in areas such as customer feedback analysis,
    social media monitoring, and market research.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在[sentiment analysis](https://saturncloud.io/glossary/sentiment-analysis/)任务中表现出色，在这些任务中，它们根据情感将文本分类为正面、负面或中性。这种能力广泛应用于客户反馈分析、社交媒体监测和市场研究等领域。
- en: C. Machine translation
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C. 机器翻译
- en: LLMs can also be used to perform machine translation, allowing users to translate
    text between different languages. LLMs like Google Translate and DeepL have demonstrated
    impressive accuracy and fluency, making them invaluable tools for communication
    across language barriers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs还可以用于机器翻译，允许用户在不同语言之间翻译文本。像Google Translate和DeepL这样的LLMs已经展示了令人印象深刻的准确性和流畅性，使其成为跨语言障碍沟通的宝贵工具。
- en: D. Question answering
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: D. 问答系统
- en: LLMs can answer questions by processing natural language input and providing
    relevant answers based on their knowledge base. This capability has been used
    in various applications, from customer support to education and research assistance.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs可以通过处理自然语言输入并根据其知识库提供相关答案来回答问题。这一能力已被应用于各种场景，从客户支持到教育和研究辅助。
- en: E. Text summarization
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: E. 文本摘要
- en: LLMs can generate concise summaries of long documents or articles, making it
    easier for users to grasp the main points quickly. Text summarization has numerous
    applications, including news aggregation, content curation, and research assistance.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs可以生成长文档或文章的简明摘要，使用户能够更快地掌握主要观点。文本摘要在新闻聚合、内容策划和研究辅助等方面具有广泛的应用。
- en: Conclusion
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Large language models represent a significant advancement in natural language
    processing and have transformed the way we interact with language-based technology.
    Their ability to pre-train on massive amounts of data and fine-tune on task-specific
    datasets has resulted in improved accuracy and performance on a range of language
    tasks. From text generation and completion to sentiment analysis, machine translation,
    question answering, and text summarization, LLMs have demonstrated remarkable
    capabilities and have been applied in numerous domains.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型代表了自然语言处理领域的重大进展，改变了我们与语言技术互动的方式。它们在大量数据上进行预训练，并在特定任务数据集上进行微调，从而提高了在各种语言任务上的准确性和性能。从文本生成与完成到情感分析、机器翻译、问答系统和文本摘要，LLMs展示了非凡的能力，并已在众多领域得到应用。
- en: However, these models are not without challenges and limitations. Computational
    resources, bias and fairness, model interpretability, and controlling generated
    content are some of the areas that require further research and attention. Nevertheless,
    the potential impact of LLMs on NLP research and applications is immense, and
    their continued development will likely shape the future of AI and language-based
    technology.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些模型也面临挑战和限制。计算资源、偏见与公平性、模型可解释性以及控制生成内容是需要进一步研究和关注的领域。尽管如此，大语言模型对NLP研究和应用的潜在影响巨大，它们的持续发展可能会塑造AI和基于语言的技术的未来。
- en: If you want to build your own large language models, sign up at [Saturn Cloud](http://www.saturncloud.io/)
    to get started with free cloud computing and resources.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想构建自己的大语言模型，可以在 [Saturn Cloud](http://www.saturncloud.io/) 注册，开始使用免费的云计算和资源。
- en: '**[Saturn Cloud](https://www.linkedin.com/company/saturn-cloud/)** is a data
    science and machine learning platform flexible enough for any team supporting
    Python, R, and more. Scale, collaborate, and utilize built-in management capabilities
    to aid you when you run your code. Spin up a notebook with 4TB of RAM, add a GPU,
    connect to a distributed cluster of workers, and more. Saturn also automates DevOps
    and ML infrastructure engineering, so your team can focus on analytics.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Saturn Cloud](https://www.linkedin.com/company/saturn-cloud/)** 是一个数据科学和机器学习平台，灵活支持Python、R等多种语言。可以扩展、协作并利用内置管理功能来帮助你运行代码。可以创建一个具有4TB
    RAM的笔记本，添加GPU，连接到分布式工作节点等。Saturn还自动化了DevOps和ML基础设施工程，使你的团队可以专注于分析。'
- en: '[Original](https://saturncloud.io/blog/what-are-large-language-models-and-how-do-they-work/).
    Reposted with permission.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[原始文章](https://saturncloud.io/blog/what-are-large-language-models-and-how-do-they-work/)。经许可转载。'
- en: More On This Topic
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关主题
- en: '[What Are Foundation Models and How Do They Work?](https://www.kdnuggets.com/2023/05/foundation-models-work.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是基础模型，它们是如何工作的？](https://www.kdnuggets.com/2023/05/foundation-models-work.html)'
- en: '[Strategies for Optimizing Performance and Costs When Using Large…](https://www.kdnuggets.com/strategies-for-optimizing-performance-and-costs-when-using-large-language-models-in-the-cloud)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用大型…优化性能和成本的策略](https://www.kdnuggets.com/strategies-for-optimizing-performance-and-costs-when-using-large-language-models-in-the-cloud)'
- en: '[Top Open Source Large Language Models](https://www.kdnuggets.com/2022/09/john-snow-top-open-source-large-language-models.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[顶级开源大语言模型](https://www.kdnuggets.com/2022/09/john-snow-top-open-source-large-language-models.html)'
- en: '[Learn About Large Language Models](https://www.kdnuggets.com/2023/03/learn-large-language-models.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[了解大语言模型](https://www.kdnuggets.com/2023/03/learn-large-language-models.html)'
- en: '[Introducing Healthcare-Specific Large Language Models from John Snow Labs](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[介绍John Snow Labs的医疗保健专用大语言模型](https://www.kdnuggets.com/2023/04/john-snow-introducing-healthcare-specific-large-language-models-john-snow-labs.html)'
- en: '[AI: Large Language & Visual Models](https://www.kdnuggets.com/2023/06/ai-large-language-visual-models.html)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AI: 大语言模型与视觉模型](https://www.kdnuggets.com/2023/06/ai-large-language-visual-models.html)'
