- en: 'Ensemble Methods Explained in Plain English: Bagging'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2021/05/ensemble-methods-explained-plain-english-bagging.html](https://www.kdnuggets.com/2021/05/ensemble-methods-explained-plain-english-bagging.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '**By [Claudia Ng](https://www.linkedin.com/in/claudian37/), Senior Data Scientist**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will go over a popular homogenous model ensemble method —
    bagging. Homogenous ensembles combine a large number of base estimators or weak
    learners of the same algorithm.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'The principle behind homogenous ensembles is the idea of “wisdom of the crowd”
    — the collective predictions of many diverse models is better than any set of
    predictions made by a single model. There are three requirements to achieve this:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: The models must be independent;
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each model performs slightly better than random guessing;
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All individual models have similar performance on their own.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When these three requirements are satisfied, adding more models should improve
    the performance of your ensemble.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble methods help to reduce variance and combat overfitting to your train
    dataset, thus allowing your model to better learn generalized patterns rather
    than overfitting to the noise in your train dataset.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How Bagging Works
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In bagging, a large number of independent weak models are combined to learn
    the same task with the same goal. The term “bagging” comes from `**b**ootstrap
    + **agg**regat**ing**`, whereby each weak learner is trained on a random subsample
    of data sampled with replacement (bootstrapping), and then the models’ predictions
    are aggregated.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping guarantees independence and diversity, because each subsample
    of data is sampled separately with replacement and we are left with different
    subsets to train our base estimators.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: The base estimators are weak learners that perform only slightly better than
    random guessing. An example of such a model is a shallow decision tree limited
    to a maximum depth of three. The predictions from these models are then combined
    through averaging.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Bagging can be applied to both classification and regression problems. For regression
    problems, the final predictions will be an average (soft voting) of the predictions
    from base estimators. For classification problems, the final predictions will
    be the majority vote (hard voting).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a9eebf7d3097b0745810830dfd1012f.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Diagram of Bagging Algorithms
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Bagging Algorithms with Scikit-Learn
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can build your own bagging algorithm using `[BaggingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html)` or `[BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)` in
    the Python package Scikit-Learn.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: To begin, instantiate your base estimator and enter this as your base estimator
    in `BaggingRegressor` or `BaggingClassifier`. Below are an example of a bagging
    regressor with a linear regression as the base estimator, and an example of a
    bagging classifier with a decision tree classifier as the base estimator. The
    default number of estimators is 10.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，实例化你的基础估算器，并将其作为`BaggingRegressor`或`BaggingClassifier`中的基础估算器。下面是一个以线性回归作为基础估算器的袋装回归器示例，以及一个以决策树分类器作为基础估算器的袋装分类器示例。默认的估算器数量为10。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Random Forest
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林
- en: Random forest® is a popular example of a bagging algorithm. It uses averaging
    to ensemble a number of individual decision trees trained on a subset of the train
    dataset.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林®是袋装算法的一个流行示例。它使用平均值将多个在训练数据集子集上训练的单独决策树进行集成。
- en: 'Using [scikit-learn’s random forest algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) in
    Python, you can specify tree-specific parameters. The following are some important
    hyperparameters to tune so that it is optimized for your dataset:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[scikit-learn的随机森林算法](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)在Python中，你可以指定树特定的参数。以下是一些需要调整的重要超参数，以使其针对你的数据集进行优化：
- en: '`n_estimators`: number of trees to train to be aggregated. Usually between
    100 to 500 trees is enough and generally, more trees will improve your model (with
    diminishing returns) but it will also be more computationally expensive;'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`：训练的树木数量以进行聚合。通常100到500棵树就足够了，通常更多的树会改进你的模型（但收益递减），但计算成本也会更高；'
- en: '`max_depth` : the maximum depth of a tree. A deeper tree will help to reduce
    the bias but at the expense of increasing variance. The aggregation of multiple
    trees in the random forest algorithm can help to combat this, but you should still
    be careful;'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：树的最大深度。较深的树有助于减少偏差，但会增加方差。随机森林算法中多个树的聚合可以帮助解决这一问题，但仍需小心；'
- en: '`max_features`: the maximum number of features to consider at each split. A
    good starting point is usually the square root of the number of features.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`：每次分裂时考虑的最大特征数。一个好的起点通常是特征数量的平方根。'
- en: Another important concept in the random forest algorithm is the out-of-bag (OOB)
    score. When performing bootstrapping, there will be instances left out of the
    subsamples used to train the estimators. These out-of-sample instances can be
    used to evaluate the model to obtain an out-of-bag (OOB) score, in essence like
    a pseudo-validation set for the random forest model. To obtain the OOB score,
    set `oob_score=True` when initializing your random forest object.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法中的另一个重要概念是袋外（OOB）评分。在执行自助法时，会有实例未被纳入用于训练估算器的子样本。这些样本外的实例可以用来评估模型，得到一个袋外（OOB）评分，本质上类似于随机森林模型的伪验证集。要获得OOB评分，在初始化随机森林对象时将`oob_score=True`。
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that scikit-learn’s random forest algorithms will return an error if there
    are null values in your features, so remember to fill null values with pandas’ `fillna` before
    calling fit on your data, otherwise it will throw an error.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果特征中存在空值，scikit-learn的随机森林算法会返回错误，因此请记得在调用fit之前用pandas的`fillna`填充空值，否则会抛出错误。
- en: Pros and Cons of Bagging
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 袋装的优缺点
- en: '**Reduces variance**: Given that the sampling is done truly randomly with bootstrapping,
    bagging usually helps to reduce variance and combat overfitting.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少方差**：由于采样是通过自助法真正随机进行的，袋装通常有助于减少方差并对抗过拟合。'
- en: '**Easy to parallelize**: Estimators are independent, so models can be built
    in parallel with bagging.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易于并行化**：估算器是独立的，因此可以在袋装过程中并行构建模型。'
- en: '**Greater stability and robustness**: The high number of estimators aggregated
    together help to provide more stability and robustness to the predictions.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更高的稳定性和鲁棒性**：大量聚合的估算器有助于提供更高的稳定性和鲁棒性。'
- en: '**Difficult to interpret**: The final predictions of a bagging algorithm are
    based on the mean predictions from the base estimators. While this improves accuracy,
    it becomes more difficult to interpret the model.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**难以解释**：袋装算法的最终预测基于基础估算器的平均预测。虽然这提高了准确性，但模型变得更难以解释。'
- en: Summary
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: Bagging is based on the idea of collective learning, where many independent
    weak learners are trained on bootstrapped subsamples of data and then aggregated
    via averaging. It can be applied to both classification and regression problems.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging 基于集体学习的思想，其中许多独立的弱学习器在引导的子样本数据上进行训练，然后通过平均进行聚合。它可以应用于分类和回归问题。
- en: 'The random forest algorithm is a popular example of a bagging algorithm. When
    tuning hyperparameters in the random forest algorithm for your dataset, the important
    three areas to pay attention to are: i) number of trees (`n_estimators`), ii)
    prune the trees (start with `max_depth` but also explore samples required in a
    node and/or for splitting), iii) the maximum number of features to consider at
    each split (`max_features`).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法是一个流行的 bagging 算法示例。在为你的数据集调整随机森林算法的超参数时，需要关注三个重要领域：i) 树的数量 (`n_estimators`)，ii)
    修剪树（从 `max_depth` 开始，但也探索节点中所需的样本和/或分裂），iii) 每次分裂时考虑的最大特征数量 (`max_features`)。
- en: Bagging is a very powerful concept and example of model ensemble methods. Hope
    this inspires you to try out a bagging algorithm next time you approach a predictive
    modeling problem!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging 是一个非常强大的模型集成方法示例。希望这能激励你在下次处理预测建模问题时尝试使用 bagging 算法！
- en: '**Bio: [Claudia Ng](https://www.linkedin.com/in/claudian37/)** is a Senior
    Data Scientist.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Claudia Ng](https://www.linkedin.com/in/claudian37/)** 是高级数据科学家。'
- en: '[Original](https://pub.towardsai.net/ensemble-methods-explained-in-plain-english-bagging-47bef8ac7690).
    Reposted with permission.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://pub.towardsai.net/ensemble-methods-explained-in-plain-english-bagging-47bef8ac7690)。经授权转载。'
- en: '**Related:**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[XGBoost: What it is, and when to use it](/2020/12/xgboost-what-when.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost：它是什么，何时使用它](/2020/12/xgboost-what-when.html)'
- en: '[A Comprehensive Guide to Ensemble Learning – Exactly What You Need to Know](/2021/05/comprehensive-guide-ensemble-learning.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[全面的集成学习指南——你需要知道的所有内容](/2021/05/comprehensive-guide-ensemble-learning.html)'
- en: '[Microsoft Explores Three Key Mysteries of Ensemble Learning](/2021/02/microsoft-explores-three-key-mysteries-ensemble-learning.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[微软探索集成学习的三个关键谜团](/2021/02/microsoft-explores-three-key-mysteries-ensemble-learning.html)'
- en: '* * *'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的 IT 工作'
- en: '* * *'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[10 Basic Statistical Concepts in Plain English](https://www.kdnuggets.com/10-basic-statistical-concepts-in-plain-english)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10 个基本统计概念（通俗易懂）](https://www.kdnuggets.com/10-basic-statistical-concepts-in-plain-english)'
- en: '[When Would Ensemble Techniques be a Good Choice?](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[何时集成技术是一个好选择？](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
- en: '[Ensemble Learning with Examples](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[带示例的集成学习](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习技术：Python 中随机森林的逐步讲解](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
- en: '[Centroid Initialization Methods for k-means Clustering](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[k-means 聚类的质心初始化方法](https://www.kdnuggets.com/2020/06/centroid-initialization-k-means-clustering.html)'
- en: '[Alternative Feature Selection Methods in Machine Learning](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习中的替代特征选择方法](https://www.kdnuggets.com/2021/12/alternative-feature-selection-methods-machine-learning.html)'
