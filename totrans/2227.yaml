- en: Unveiling Unsupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/unveiling-unsupervised-learning](https://www.kdnuggets.com/unveiling-unsupervised-learning)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Unveiling Unsupervised Learning](../Images/ef0e31549bbc4dde78a53ff85e3a7fb5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: What Is Unsupervised Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, **unsupervised learning** is a paradigm that involves training
    an algorithm on an *unlabeled* dataset. So there’s no supervision or labeled outputs.
  prefs: []
  type: TYPE_NORMAL
- en: In unsupervised learning, the goal is to discover patterns, structures, or relationships
    within the data itself, rather than predicting or classifying based on labeled
    examples. It involves exploring the inherent structure of the data to gain insights
    and make sense of complex information.
  prefs: []
  type: TYPE_NORMAL
- en: This guide will introduce you to unsupervised learning. We’ll start by going
    over the differences between supervised and unsupervised learning—to lay the ground
    for the remainder of the discussion. We’ll then cover the key unsupervised learning
    techniques and the popular algorithms within them.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised vs. Unsupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Supervised and unsupervised machine learning are two different approaches used
    in the field of artificial intelligence and data analysis. Here''s a brief summary
    of their key differences:'
  prefs: []
  type: TYPE_NORMAL
- en: Training Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In supervised learning, the algorithm is trained on a **labeled dataset**, where
    input data is paired with corresponding desired output (labels or target values).
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning, on the other hand, involves working with an **unlabeled
    dataset**, where there are no predefined output labels.
  prefs: []
  type: TYPE_NORMAL
- en: Objective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of supervised learning algorithms is to **learn a relationship**—**a
    mapping**—from the input to the output space. Once the mapping is learned, we
    can use the model to predict the output values or class label for unseen data
    points.
  prefs: []
  type: TYPE_NORMAL
- en: In unsupervised learning, the goal is to **find patterns**, structures, or relationships
    within the data, often for clustering data points into groups, exploratory analysis
    or feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Common Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Classification** (assigning a class label—one of the many predefined categories—to
    a previously unseen data point) and **regression** (predicting continuous values)
    are common tasks in supervised learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clustering** (grouping similar data points) and **dimensionality reduction**
    (reducing the number of features while preserving important information) are common
    tasks in unsupervised learning. We’ll discuss these in greater detail shortly.'
  prefs: []
  type: TYPE_NORMAL
- en: When To Use
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Supervised learning is widely used when the desired output is known and well-defined,
    such as spam email detection, image classification, and medical diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning is used when there is limited or no prior knowledge about
    the data and the objective is to uncover hidden patterns or gain insights from
    the data itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a summary of the differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unveiling Unsupervised Learning](../Images/ff5a48170413c9ac9e58712a4791acd8.png)'
  prefs: []
  type: TYPE_IMG
- en: Supervised vs. Unsupervised Learning | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Summing up**: Supervised learning focuses on learning from labeled data to
    make predictions or classifications, while unsupervised learning seeks to discover
    patterns and relationships within unlabeled data. Both approaches have their own
    applications—based on the nature of the data and the problem at hand.'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Learning Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed, in unsupervised learning, we have the input data and are tasked
    with finding meaningful patterns or representations within that data. Unsupervised
    learning algorithms do so by identifying similarities, differences, and relationships
    among the data points without being provided with predefined categories or labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this discussion, we’ll go over the two main unsupervised learning techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality Reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What Is Clustering?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clustering involves grouping similar data points together into clusters based
    on some *similarity measure*. The algorithm aims to find natural groups or categories
    within the data where data points in the same cluster are *more similar* to each
    other than to those in other clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the dataset grouped into different clusters we can essentially
    label them. And if needed, we can perform supervised learning on the clustered
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: What Is Dimensionality Reduction?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dimensionality reduction refers to techniques that reduce the number of features—dimensions—in
    the data while preserving important information. High-dimensional data can be
    complex and difficult to work with, so dimensionality reduction helps in simplifying
    the data for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Both clustering and dimensionality reduction are powerful techniques in unsupervised
    learning, providing valuable insights and simplifying complex data for further
    analysis or modeling.
  prefs: []
  type: TYPE_NORMAL
- en: In the remainder of the article, let's review important clustering and dimensionality
    reduction algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering Algorithms: An Overview'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed, clustering is a fundamental technique in unsupervised learning
    that involves grouping similar data points together into clusters, where data
    points within the same cluster are more similar to each other than to those in
    other clusters. Clustering helps identify natural divisions within the data, which
    can provide insights into patterns and relationships.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various algorithms used for clustering, each with its own approach
    and characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: K-Means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: K-Means clustering is a simple, robust, and commonly used algorithm. It partitions
    the data into a predefined number of clusters (K) by iteratively updating cluster
    centroids based on the mean of data points within each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: It iteratively refines cluster assignments until convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how the K-Means clustering algorithm works:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize K cluster centroids.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign each data point—based on the chosen distance metric—to the nearest cluster
    centroid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update centroids by computing the mean of data points in each cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2 and 3 until convergence or a defined number of iterations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hierarchical Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hierarchical clustering creates a tree-like structure—a **dendrogram**—of data
    points, capturing similarities at multiple levels of granularity. Agglomerative
    clustering is the most commonly used hierarchical clustering algorithm. It starts
    with individual data points as separate clusters and gradually merges them based
    on a linkage criterion, such as distance or similarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how the agglomerative clustering algorithm works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with `n` clusters: each data point as its own cluster.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merge closest data points/clusters into a larger cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat 2\. until a single cluster remains or a defined number of clusters is
    reached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The result can be interpreted with the help of a dendrogram.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Density-Based Spatial Clustering of Applications with Noise (DBSCAN)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DBSCAN identifies clusters based on the density of data points in a neighborhood.
    It can find arbitrarily shaped clusters and can also identify noise points and
    detect outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm involves the following (simplified to include the key steps):'
  prefs: []
  type: TYPE_NORMAL
- en: Select a data point and find its neighbors within a specified radius.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the point has sufficient neighbors, expand the cluster by including the neighbors
    of its neighbors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat for all points, forming clusters connected by density.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dimensionality Reduction Algorithms: An Overview'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dimensionality reduction is the process of reducing the number of features (dimensions)
    in a dataset while retaining essential information. High-dimensional data can
    be complex, computationally expensive, and is prone to overfitting. Dimensionality
    reduction algorithms help simplify data representation and visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis (PCA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Principal Component Analysis](/2023/05/principal-component-analysis-pca-scikitlearn.html)—or
    PCA—transforms data into a new coordinate system to maximize variance along the
    principal components. It reduces data dimensions while preserving as much variance
    as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how you can perform PCA for dimensionality reduction:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the covariance matrix of the input data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform eigenvalue decomposition on the covariance matrix. Compute the eigenvectors
    and eigenvalues of the covariance matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sort eigenvectors by eigenvalues in descending order.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Project data onto the eigenvectors to create a lower-dimensional representation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: t-Distributed Stochastic Neighbor Embedding (t-SNE)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first time I used t-SNE was to visualize word embeddings. t-SNE is used
    for visualization by reducing high-dimensional data to a lower-dimensional representation
    while maintaining local pairwise similarities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how t-SNE works:'
  prefs: []
  type: TYPE_NORMAL
- en: Construct probability distributions to measure pairwise similarities between
    data points in high-dimensional and low-dimensional spaces.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Minimize the divergence between these distributions using gradient descent.
    Iteratively move data points in the lower-dimensional space, adjusting their positions
    to minimize the cost function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In addition, there are deep learning architectures such as autoencoders that
    can be used for dimensionality reduction. Autoencoders are neural networks designed
    to encode and then decode data, effectively learning a compressed representation
    of the input data.
  prefs: []
  type: TYPE_NORMAL
- en: Some Applications of Unsupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s explore some applications of unsupervised learning. Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Customer Segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In marketing, businesses use unsupervised learning to segment their customer
    base into groups with similar behaviors and preferences. This helps tailor marketing
    strategies, campaigns, and product offerings. For example, retailers categorize
    customers into groups such as "budget shoppers," "luxury buyers," and "occasional
    purchasers."
  prefs: []
  type: TYPE_NORMAL
- en: Document Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can run a clustering algorithm on a corpus of documents. This helps group
    similar documents together, aiding in document organization, search, and retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly Detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unsupervised learning can be used to identify rare and unusual patterns—anomalies—in
    data. Anomaly detection has applications in fraud detection and network security
    to detect unusual—anomalous—behavior. Detecting fraudulent credit card transactions
    by identifying unusual spending patterns is a practical example.
  prefs: []
  type: TYPE_NORMAL
- en: Image Compression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clustering can be used for image compression to transform images from high-dimensional
    color space to a much lower dimensional color space. This reduces image storage
    and transmission size by representing similar pixel regions with a single centroid.
  prefs: []
  type: TYPE_NORMAL
- en: Social Network Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can analyze social network data—based on user interactions—to uncover communities,
    influencers, and patterns of interaction.
  prefs: []
  type: TYPE_NORMAL
- en: Topic Modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In natural language processing, the task of topic modeling is used to extract
    topics from a collection of text documents. This helps categorize and understand
    the main themes—topics—within a large text corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Say, we have a corpus of news articles and we don’t have the documents and their
    corresponding categories beforehand. So we can perform topic modeling on the collection
    of news articles to identify topics such as politics, technology, and entertainment.
  prefs: []
  type: TYPE_NORMAL
- en: Genomic Data Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unsupervised learning also has applications in biomedical and genomic data analysis.
    Examples include clustering genes based on their expression patterns to discover
    potential associations with specific diseases.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hope this article helped you understand the basics of unsupervised learning.
    The next time you work with a real-world dataset, try to figure out the learning
    problem at hand. And try to assess if it can be modeled as a supervised or an
    unsupervised learning problem.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re working with a dataset with high-dimensional features, try to apply
    dimensionality reduction before building the machine learning model. Keep learning!
  prefs: []
  type: TYPE_NORMAL
- en: '**[Bala Priya C](https://www.linkedin.com/in/bala-priya/)** is a developer
    and technical writer from India. She likes working at the intersection of math,
    programming, data science, and content creation. Her areas of interest and expertise
    include DevOps, data science, and natural language processing. She enjoys reading,
    writing, coding, and coffee! Currently, she''s working on learning and sharing
    her knowledge with the developer community by authoring tutorials, how-to guides,
    opinion pieces, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Unsupervised Disentangled Representation Learning in Class…](https://www.kdnuggets.com/2023/01/unsupervised-disentangled-representation-learning-class-imbalanced-dataset-elastic-infogan.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Exploring Unsupervised Learning Metrics](https://www.kdnuggets.com/2023/04/exploring-unsupervised-learning-metrics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Clustering with scikit-learn: A Tutorial on Unsupervised Learning](https://www.kdnuggets.com/2023/05/clustering-scikitlearn-tutorial-unsupervised-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hands-On with Unsupervised Learning: K-Means Clustering](https://www.kdnuggets.com/handson-with-unsupervised-learning-kmeans-clustering)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unveiling the Potential of CTGAN: Harnessing Generative AI for…](https://www.kdnuggets.com/2023/04/unveiling-potential-ctgan-harnessing-generative-ai-synthetic-data.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unveiling Midjourney 5.2: A Leap Forward in AI Image Generation](https://www.kdnuggets.com/2023/06/unveiling-midjourney-52-leap-forward.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
