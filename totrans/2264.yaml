- en: 'Vanishing Gradient Problem: Causes, Consequences, and Solutions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/02/vanishing-gradient-problem.html](https://www.kdnuggets.com/2022/02/vanishing-gradient-problem.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![Vanishing Gradient Problem: Causes, Consequences, and Solutions](../Images/0a76a6fdac48e94ed85b1ee128af5a45.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Editor
  prefs: []
  type: TYPE_NORMAL
- en: The sigmoid function is one of the most popular activations functions used for
    developing deep neural networks. The use of sigmoid function restricted the training
    of deep neural networks because it caused the vanishing gradient problem. This
    caused the neural network to learn at a slower pace or in some cases no learning
    at all. This blog post aims to describe the vanishing gradient problem and explain
    how use of the sigmoid function resulted in it.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sigmoid functions are used frequently in neural networks to activate neurons.
    It is a logarithmic function with a characteristic S shape. The output value of
    the function is between 0 and 1\. The sigmoid function is used for activating
    the output layers in binary classification problems. It is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Vanishing Gradient Problem, Explained](../Images/d8d2fa8be9674346f77dba068732404e.png)'
  prefs: []
  type: TYPE_IMG
- en: On the graph below you can see a comparison between the sigmoid function itself
    and its derivative. First derivatives of sigmoid functions are bell curves with
    values ranging from 0 to 0.25.
  prefs: []
  type: TYPE_NORMAL
- en: '![Vanishing Gradient Problem, Explained](../Images/c5aeb35ef714f979fd9fdbfb64261d9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Our knowledge of how neural networks perform forward and backpropagation is
    essential to understanding the vanishing gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![Vanishing Gradient Problem, Explained](../Images/862e4f3d9e4cf38da6981eeb8a049e45.png)'
  prefs: []
  type: TYPE_IMG
- en: Forward Propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basic structure of a neural network is an input layer, one or more hidden
    layers, and a single output layer. The weights of the network are randomly initialized
    during forward propagation. The input features are multiplied by the corresponding
    weights at each node of the hidden layer, and a bias is added to the net sum at
    each node. This value is then transformed into the output of the node using an
    activation function. To generate the output of the neural network, the hidden
    layer output is multiplied by the weights plus bias values, and the total is transformed
    using another activation function. This will be the predicted value of the neural
    network for a given input value.
  prefs: []
  type: TYPE_NORMAL
- en: '![Vanishing Gradient Problem, Explained](../Images/51d627e5d0acde8bababdde965dc010c.png)'
  prefs: []
  type: TYPE_IMG
- en: Back Propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the network generates an output, the loss function(C) indicates how well
    it predicted the output. The network performs back propagation to minimize the
    loss. A back propagation method minimizes the loss function by adjusting the weights
    and biases of the neural network. In this method, the gradient of the loss function
    is calculated with respect to each weight in the network.
  prefs: []
  type: TYPE_NORMAL
- en: In back propagation, the new weight(w[new]) of a node is calculated using the
    old weight(w[old]) and product of the learning rate(ƞ) and gradient of the loss
    function ![Vanishing Gradient Problem, Explained](../Images/26acc3e4c6a799aac5744ac88dcdb036.png).
  prefs: []
  type: TYPE_NORMAL
- en: '![Vanishing Gradient Problem, Explained](../Images/be7ea7065f53e1ede2b5b33d9f3e3cf4.png)'
  prefs: []
  type: TYPE_IMG
- en: With the chain rule of partial derivatives, we can represent gradient of the
    loss function as a product of gradients of all the activation functions of the
    nodes with respect to their weights.Therefore, the updated weights of nodes in
    the network depend on the gradients of the activation functions of each node.
  prefs: []
  type: TYPE_NORMAL
- en: For the nodes with sigmoid activation functions, we know that the partial derivative
    of the sigmoid function reaches a maximum value of 0.25\. When there are more
    layers in the network, the value of the product of derivative decreases until
    at some point the partial derivative of the loss function approaches a value close
    to zero, and the partial derivative vanishes. We call this the vanishing gradient
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: With shallow networks, sigmoid function can be used as the small value of gradient
    does not become an issue. When it comes to deep networks, the vanishing gradient
    could have a significant impact on performance. The weights of the network remain
    unchanged as the derivative vanishes. During back propagation, a neural network
    learns by updating its weights and biases to reduce the loss function. In a network
    with vanishing gradient, the weights cannot be updated, so the network cannot
    learn. The performance of the network will decrease as a result.
  prefs: []
  type: TYPE_NORMAL
- en: Method to overcome the problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The vanishing gradient problem is caused by the derivative of the activation
    function used to create the neural network. The simplest solution to the problem
    is to replace the activation function of the network. Instead of sigmoid, use
    an activation function such as ReLU.
  prefs: []
  type: TYPE_NORMAL
- en: Rectified Linear Units (ReLU) are activation functions that generate a positive
    linear output when they are applied to positive input values. If the input is
    negative, the function will return zero.
  prefs: []
  type: TYPE_NORMAL
- en: '![Vanishing Gradient Problem, Explained](../Images/3955a28d80709d4d297fc54a5546ba89.png)'
  prefs: []
  type: TYPE_IMG
- en: The derivative of a ReLU function is defined as 1 for inputs that are greater
    than zero and 0 for inputs that are negative. The graph shared below indicates
    the derivative of a ReLU function
  prefs: []
  type: TYPE_NORMAL
- en: '![Vanishing Gradient Problem, Explained](../Images/47848f7a08be7ca8124605fd7199f9c0.png)'
  prefs: []
  type: TYPE_IMG
- en: If the ReLU function is used for activation in a neural network in place of
    a sigmoid function, the value of the partial derivative of the loss function will
    be having values of 0 or 1 which prevents the gradient from vanishing. The use
    of ReLU function thus prevents the gradient from vanishing. The problem with the
    use of ReLU is when the gradient has a value of 0\. In such cases, the node is
    considered as a dead node since the old and new values of the weights remain the
    same. This situation can be avoided by the use of a leaky ReLU function which
    prevents the gradient from falling to the zero value.
  prefs: []
  type: TYPE_NORMAL
- en: Another technique to avoid the vanishing gradient problem is weight initialization.
    This is the process of assigning initial values to the weights in the neural network
    so that during back propagation, the weights never vanish.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, the vanishing gradient problem arises from the nature of the
    partial derivative of the activation function used to create the neural network.
    The problem can bevworse in deep neural networks using Sigmoid activation function.
    It can be significantly reduced by using activation functions like ReLU and leaky
    ReLU.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Tina Jacob](https://www.linkedin.com/in/tina-jacob-77068253/)** is passionate
    about data science and believes life is about learning and growing no matter what
    it brings.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[5 Concepts You Should Know About Gradient Descent and Cost Function](https://www.kdnuggets.com/2020/05/5-concepts-gradient-descent-cost-function.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90% of Today''s Code is Written to Prevent Failure, and That''s a Problem](https://www.kdnuggets.com/2022/07/90-today-code-written-prevent-failure-problem.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Back To Basics, Part Dos: Gradient Descent](https://www.kdnuggets.com/2023/03/back-basics-part-dos-gradient-descent.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gradient Descent: The Mountain Trekker''s Guide to Optimization with…](https://www.kdnuggets.com/gradient-descent-the-mountain-trekker-guide-to-optimization-with-mathematics)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Range of NLP Applications in the Real World: A Different…](https://www.kdnuggets.com/2022/03/different-solution-problem-range-nlp-applications-real-world.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Genetic Programming in Python: The Knapsack Problem](https://www.kdnuggets.com/2023/01/knapsack-problem-genetic-programming-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
