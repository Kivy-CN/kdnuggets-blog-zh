- en: 'Naive Bayes: A Baseline Model for Machine Learning Classification Performance'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/04/naive-bayes-baseline-model-machine-learning-classification-performance.html/2](https://www.kdnuggets.com/2019/04/naive-bayes-baseline-model-machine-learning-classification-performance.html/2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Multinomial Naive Bayes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, the categorical variables will need to be encoded.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then we can create our training and test sets
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, we can go on to fit our model and make predictions
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: It seems that the predictions have all returned 'yes'. This will have implications
    when evaluating the model as you will see.
  prefs: []
  type: TYPE_NORMAL
- en: Lets make a confusion matrix with pandas as I personally do not like the confusion
    matrix in Scikitlearn.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The Multinomaial model gives us an accuracy of 60%
  prefs: []
  type: TYPE_NORMAL
- en: '**The RECALL (TRUE POSITIVE RATE) for the model is 100% due to there being
    no false negatives as there were no ''0'' classes predicted. Recall is calculated
    by [True positive/(True Positive+False Negative)].** Unfortunately, this is not
    acceptable because it unfathomable to have a 100% recall in a real world situation.
    This is merely the mathematics at play that require human interpretation to assess
    its suitability.'
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian Naive Bayes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the Gaussian Naive Bayes prefers continuous data, we are going to use the
    [Pima Indians Diabetes datset](https://www.kaggle.com/uciml/pima-indians-diabetes-database)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As we can see all the features are continuous.
  prefs: []
  type: TYPE_NORMAL
- en: Now lets test to see whether the features follow a Gaussian Distribution (Normal
    Distribution) as it is a required assumption of the Gaussian Naive Bayes model
    (although it can still be used if the data is not normally distributed)
  prefs: []
  type: TYPE_NORMAL
- en: The loop will tell us whether the data is normally distributed using the famous
    Shapiro-Wilkes test.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**None of the features appear to be normally distributed.**'
  prefs: []
  type: TYPE_NORMAL
- en: Lets take it one step further and visualize their distributions
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![pima-diabetes-histogram](../Images/1f931e10b3854819507e2bba3e123b3b.png)'
  prefs: []
  type: TYPE_IMG
- en: Histogram of Pima Diabetes Features
  prefs: []
  type: TYPE_NORMAL
- en: Upon visual inspection BMI and Blood Pressure seem to follow a normal distribution
    but the outliers on either side and the hypothesis test will have us think otherwise.
    Although the assumption
  prefs: []
  type: TYPE_NORMAL
- en: does not hold, we can still move forward to fit the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This time we can compute a Recall (True Positive Rate) as now both classes have
    been predicted.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: I use `average='binary'` because our target variable is a binary target (0 and
    1).
  prefs: []
  type: TYPE_NORMAL
- en: The model gives us a True Positive Rate (Recall) of 62%.
  prefs: []
  type: TYPE_NORMAL
- en: 'I had trouble obtaining the Accuracy for the model so we can just compute it
    manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The Gaussian model gives us 74% accuracy
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of Naive Bayes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Can handle missing values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Missing values are ignored while preparing the model and ignored when a probability
    is calculated for a class value.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Can handle small sample sizes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Naive Bayes does not require a large amount of training data. It merely needs
    enough data to understand the probabilistic relationship between each attribute
    in isolation with the target variable. **If only little training data is available,
    Naive Bayes would usually perform better than other models**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs well despite violation of independence assumption
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Even though independence rarely holds for real world data, the model will still
    perform as usual.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Easily interpretable and has fast prediction time in comparison.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Naive Bayes is not a black-box algorithm and the end result can be easily interpreted
    to an audience.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Can handle both numeric and categorical data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Naive Bayes is a classifier and will therefore perform better with categorical
    data. Although numeric data will also suffice, it assumes all numeric data are
    normally distributed which is unlikely in real world data.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Disadvantages of Naive Bayes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Naive Assumption**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Naive Bayes assumes that all features are independent of each other. In real
    life it is almost impossible to obtain a set of predictors that are completely
    independent of each other.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cannot incorporate interactions between the features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model's performance will be highly sensitive to skewed data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the training set is not representative of the class distributions of the
    overall population, the prior estimates will be incorrect.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zero Frequency problem**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Categorical variables that have a category in the test data but was not in the
    training data will be assigned a probability of **zero (0)** and will be unable
    to make a prediction.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*As a solution, a **smoothing technique** must be applied to the category.
    One of the simplest and most famous techniques is the [Laplace Smoothing Technique](https://stats.stackexchange.com/questions/108797/in-naive-bayes-why-bother-with-laplacian-smoothing-when-we-have-unknown-words-i).
    Python''s Sklearn implements laplace smoothing by default.*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlated features in the dataset must be removed or else are voted twice in
    the model and will **over-inflate the importance of that feature**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why use Naive Bayes as a baseline Classifier for performance?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'My thoughts as to why Naive Bayes should be the first model to create and compare
    is that:'
  prefs: []
  type: TYPE_NORMAL
- en: '**It heavily relies on the prior target class probability for predictions.
    Inaccurate or unrealistic priors can lead to misleading results. Because Naive
    Bayes is a probability based machine learning technique, the probability of the
    target will greatly affect the final prediction.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since you do not have to remove missing values, you will not have to risk losing
    any of your original data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The independence assumption is practically never satisfied and therefore the
    results are not very trustworthy since its most basic assumption is flawed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interactions between features are not accounted for in the model. However features
    in the real world almost always have interactions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no error or variance to minimize but only to seek the higher probability
    of a class given the predictors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the above can be used as valid points that other classifiers should be
    built to outperform the Naive Bayes model. While Naive Bayes is great for **spam
    filtering** and **Recommendation Systems**, it is probably not ideal in most other
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Overall Naive Bayes is fast, powerful and interpretable. However, the overreliance
    on the prior probability of the target variable can create very misleading and
    inaccurate results. Classifiers such as Decision Trees, Logistic Regression, Random
    Forests and Ensemble methods should be able to outperform Naive bayes to be an
    actually useful. This is is no way removes Naive Bayes as a powerful classifier.
    The independence assumption, inability to handle interactions, and gaussian distribution
    assumption make it a very difficult algorithm to trust with prediction on its
    own as these models will have to be continuously upated.
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Naive Bayes from Scratch using Python only – No Fancy Frameworks](/2018/10/naive-bayes-from-scratch-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Machine Learning Finds “Fake News” with 88% Accuracy](/2017/04/machine-learning-fake-news-accuracy.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unfolding Naive Bayes From Scratch](/2018/09/unfolding-naive-bayes.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Gaussian Naive Bayes, Explained](https://www.kdnuggets.com/2023/03/gaussian-naive-bayes-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Naïve Bayes Algorithm: Everything You Need to Know](https://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, April 13: Python Libraries Data Scientists Should…](https://www.kdnuggets.com/2022/n15.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[More Performance Evaluation Metrics for Classification Problems You…](https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A Guide to Train an Image Classification Model Using Tensorflow](https://www.kdnuggets.com/2022/12/guide-train-image-classification-model-tensorflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Classification Metrics: Your Guide to Assessing Model…](https://www.kdnuggets.com/understanding-classification-metrics-your-guide-to-assessing-model-accuracy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
