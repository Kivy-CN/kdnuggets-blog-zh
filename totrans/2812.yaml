- en: 'Decision Tree Intuition: From Concept to Application'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2020/02/decision-tree-intuition.html](https://www.kdnuggets.com/2020/02/decision-tree-intuition.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[comments](#comments)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2fbcc0528a1f4b47e01961814dfb1c2e.png)'
  prefs: []
  type: TYPE_IMG
- en: A decision tree is one of the popular and powerful machine learning algorithms
    that I have learned. It is a non-parametric supervised learning method that can
    be used for both classification and regression tasks. The goal is to create a
    model that predicts the value of a target variable by learning simple decision
    rules inferred from the data features. For a classification model, the target
    values are discrete in nature, whereas, for a regression model, the target values
    are represented by continuous values. Unlike the black box type of algorithms
    such as Neural Network,** Decision Trees** are comparably easier to understand
    because it shares internal decision-making logic (you will find details in the
    following session).
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Despite the fact that many data scientists believe it’s an old method and they
    may have some doubts of its accuracy due to an overfitting problem, the more recent
    tree-based models, for example, Random forest (bagging method), gradient boosting
    (boosting method) and XGBoost (boosting method) are built on the top of decision
    tree algorithm. Therefore, the concepts and algorithms behind **Decision Trees **are
    strongly worth understanding!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/43782b4091a23968fac7a66a5190701f.png)'
  prefs: []
  type: TYPE_IMG
- en: There are 4 popular types of decision tree algorithms: **ID3**, **CART (Classification
    and Regression Trees)**, **Chi-Square**, and **Reduction in Variance.**
  prefs: []
  type: TYPE_NORMAL
- en: '*In this blog, I will only focus on the classification trees and the explanations
    of ID3 and CART.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d9817fea6ecd8c71e50a13cd69f391a.png)'
  prefs: []
  type: TYPE_IMG
- en: Imagine you play tennis every Sunday and you invite your best friend, Clare
    to come with you every time. Clare sometimes comes to join but sometimes not.
    For her, it depends on a number of factors, for example, weather, temperature,
    humidity, and wind. I would like to use the dataset below to predict whether or
    not Clare will join me to play tennis. An intuitive way to do this is through
    a Decision Tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/87d51f688db1932e95e90fcebcc9b8ba.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](../Images/891149077bc9ab94fff3f171781152ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this **Decision Tree** diagram, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Root Node:**The first split which decides the entire population or sample
    data should further get divided into two or more homogeneous sets. In our case,
    the Outlook node.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Splitting:**It is a process of dividing a node into two or more sub-nodes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Decision Node:**This node decides whether/when a sub-node splits into further
    sub-nodes or not. Here we have, Outlook node, Humidity node, and Windy node.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Leaf:**Terminal Node that predicts the outcome (categorical or continuous
    value). The coloured nodes, i.e., Yes and No nodes, are the leaves.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Question: Base on which attribute (feature) to split? What is the best split?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Answer: Use the attribute with the highest **I*****nformation Gain **or** Gini
    Gain**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ID3 (Iterative Dichotomiser)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ID3 decision tree algorithm uses Information Gain to decide the splitting points.
    In order to measure how much information we gain, we can use ***entropy ***to
    calculate the homogeneity of a sample.
  prefs: []
  type: TYPE_NORMAL
- en: '*Question: What is “Entropy”? and What is its function?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Answer: It is a measure of the amount of uncertainty in a data set. *Entropy
    controls how a Decision Tree decides to splitthe data. It actually affects how
    a **Decision Tree** draws its boundaries.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**The equation of Entropy:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b834c414864cbedc2eb53f2e081cac1.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The logarithm of the **probability** distribution is useful as a measure of **entropy**.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba5a1a41aff720c6a6ad6af99478fa30.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Entropy vs. Probability.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition: Entropy in Decision Tree stands for homogeneity.'
  prefs: []
  type: TYPE_NORMAL
- en: If the sample is completely homogeneous, the entropy is 0 (prob= 0 or 1), and
    if the sample is evenly distributed across classes, it has an entropy of 1 (prob
    =0.5).
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to make splits that minimize entropy. We use *information gain* to
    determine the best split.
  prefs: []
  type: TYPE_NORMAL
- en: Let me show you how to calculate the information gain step by step in the case
    of playing tennis. Here I will only show you how to calculate the Information
    Gain and Entropy of Outlook.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Calculate the Entropy of one attribute — Prediction: Clare Will
    Play Tennis/ Clare Will Not Play Tennis'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this illustration, I will use this contingency table to calculate the entropy
    of our target variable: Played? (Yes/No). There are 14 observations (10 “Yes”
    and 4 “No”). The probability (p) of ‘Yes’ is 0.71428(10/14), and the probability
    of ‘No’ is 0.28571 (4/14). You can then calculate the entropy of our target variable
    using the equation above.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/efc3f408ef6a106271d0c31beb37241b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 2**: Calculate the Entropy for each feature using the contingency table'
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate, I use Outlook as an example to explain how to calculate its Entropy.
    There are a total of 14 observations. Summing across the rows we can see there
    are 5 of them belong to Sunny, 4 belong to Overcast, and 5 belong to Rainy. Therefore,
    we can find the probability of Sunny, Overcast, and Rainy and then calculate their
    entropy one by one using the above equation. The calculation steps are shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2fb40a421db809a2042308f87f5da082.png)'
  prefs: []
  type: TYPE_IMG
- en: '*An example of calculating the entropy of feature 2 (Outlook).*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition: **Information Gain** is the decrease or increase in Entropy value
    when the node is split.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The equation of Information Gain:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e36bf0e62f86308085e0a38a54ee98eb.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Information Gain from* X *on* Y*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c5045d95b030d831e2ba8e733b7f66d3.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The information gain of outlook is 0.147.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b10c92744ac10621b92dbadd39d50e5c.png)'
  prefs: []
  type: TYPE_IMG
- en: '*sklearn.tree.**DecisionTreeClassifier: **“entropy” means for the information
    gain.*'
  prefs: []
  type: TYPE_NORMAL
- en: In order to visualise how to construct a decision tree using **information gain**,
    I have simply applied sklearn.tree.**DecisionTreeClassifier **to generate the
    diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 3***: Choose attribute with the **largest Information Gain** as the
    Root Node'
  prefs: []
  type: TYPE_NORMAL
- en: The information gain of ‘Humidity’ is the highest at 0.918\. Humidity is the
    root node.
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 4****: A *branch with an entropy of 0 is a leaf node, while a branch
    with entropy more than 0 needs further splitting.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Step 5***: Nodes are grown recursively in the ID3 algorithm until all data
    is classified.'
  prefs: []
  type: TYPE_NORMAL
- en: You might hear of the **C4.5 algorithm**, an improvement of ID3 uses the **Gain
    Ratio **as an extension to information gain. The advantage of using Gain Ratio
    is to handle the issue of bias by normalizing the information gain using Split
    Info. I won’t go into details of C4.5 here. For more information, please check
    out [here](https://www.datacamp.com/community/tutorials/decision-tree-classification-python) (DataCamp).
  prefs: []
  type: TYPE_NORMAL
- en: CART (Classification and Regression Tree)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another decision tree algorithm CART uses the *Gini method *to create split
    points, including the *Gini Index (Gini Impurity) and Gini Gain.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition of Gini Index: The probability of assigning a wrong label to a sample
    by picking the label randomly and is also used to measure feature importance in
    a tree.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00887d6088d38cb1d3729ed790915cdc.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The equation of Gini Index.*'
  prefs: []
  type: TYPE_NORMAL
- en: Let me show you how to calculate Gini Index and Gini Gain :)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb84c8dacf63d3025eb36e55330c1ee9.png)'
  prefs: []
  type: TYPE_IMG
- en: After calculating Gini Gain for every attribute, sklearn.tree.**DecisionTreeClassifier **will
    choose the attribute with the **largest Gini Gain** as the Root Node. *A *branch
    with Gini of 0 is a leaf node, while a branch with Gini more than 0 needs further
    splitting. Nodes are grown recursively until all data is classified (see the detail
    below).
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned, CART can also handle the regression problem using a different
    splitting criterion: Mean Squared Error (MSE) to determine the splitting points.
    The output variable of a Regression Tree is numerical, and the input variables
    allow a mixture of continuous and categorical variables. You can check out more
    information about the regression trees through [DataCamp](https://www.datacamp.com/courses/machine-learning-with-tree-based-models-in-python).'
  prefs: []
  type: TYPE_NORMAL
- en: Great! You now should understand how to calculate Entropy, Information Gain,
    Gini Index, and Gini Gain!
  prefs: []
  type: TYPE_NORMAL
- en: '*Question: so…which should I use? Gini Index or Entropy?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Answer: Generally, the result should be the same… I personally prefer Gini
    Index because it doesn’t involve a more computationally intensive *log* to calculate.
    But why not try both.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let me summarize in a table format!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3315c10ab0a6e178568a71af31dcbe1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Building a Decision Tree using Scikit Learn
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Scikit Learn](https://en.wikipedia.org/wiki/Scikit-learn) is a free software
    machine learning library for the Python programming language.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: importing data'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2**: converting categorical variables into dummies/indicator variables'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/947b8248bd1ba382a2f2555152a100b4.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The categorical variables of ‘Temperature’, ‘Outlook’ and ‘Windy’ are all
    converted into dummies.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3**: separating the training set and test set'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4**: importing Decision Tree Classifier via sklean'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5**: visualising the decision tree diagram'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/cc72ce5e277e6deceff8a808e12a74b9.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The tree depth: 3.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**For the coding and dataset, please check out [here](https://github.com/clareyan/Decision-Tree-Intuition-From-Concept-to-Application).**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4018a9a410d788a450707a01e439ad58.png)'
  prefs: []
  type: TYPE_IMG
- en: '*If the condition of ‘Humidity’ is lower or equal to 73.5, it is pretty sure
    that Clare will play tennis!*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c9976be945714d5da3a03469c4f3534.png)'
  prefs: []
  type: TYPE_IMG
- en: In order to improve the model performance (Hyperparameters Optimization), you
    should adjust the hyperparameters. For more details, please check out [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).
  prefs: []
  type: TYPE_NORMAL
- en: The major disadvantage of Decision Trees is overfitting, especially when a tree
    is particularly deep. Fortunately, the more recent tree-based models, including
    random forest and XGBoost, are built on the top of the decision tree algorithm,
    and they generally perform better with a strong modeling technique and much more
    dynamic than a single decision tree. Therefore, understanding the concepts and
    algorithms behind **Decision Trees **thoroughlyis super helpful in constructing
    a good foundation of learning data science and machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary: Now you should know'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How to construct a Decision Tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to calculate ‘Entropy’ and ‘Information Gain’
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to calculate the ‘Gini Index’ and ‘Gini Gain’
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the best split?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to plot a Decision Tree Diagram in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Original](https://towardsdatascience.com/decision-tree-intuition-from-concept-to-application-530744294bb6).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Decision Tree Algorithm, Explained](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Decision Trees for Classification in Python](https://www.kdnuggets.com/2019/08/understanding-decision-trees-classification-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Decision Trees — An Intuitive Introduction](https://www.kdnuggets.com/2019/02/decision-trees-introduction.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Simplifying Decision Tree Interpretability with Python & Scikit-learn](https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Decision Tree Algorithm, Explained](https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding by Implementing: Decision Tree](https://www.kdnuggets.com/2023/02/understanding-implementing-decision-tree.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Telling a Great Data Story: A Visualization Decision Tree](https://www.kdnuggets.com/2021/02/telling-great-data-story-visualization-decision-tree.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Random Forest vs Decision Tree: Key Differences](https://www.kdnuggets.com/2022/02/random-forest-decision-tree-key-differences.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets™ News 22:n09, Mar 2: Telling a Great Data Story: A…](https://www.kdnuggets.com/2022/n09.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
