- en: How Activation Functions Work in Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2022/06/activation-functions-work-deep-learning.html](https://www.kdnuggets.com/2022/06/activation-functions-work-deep-learning.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let''s start with a definition of **activation function**:'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_BQ
  - PREF_H2
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '* * *'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**“In artificial neural networks, each neuron forms a weighted sum of its inputs
    and passes the resulting scalar value through a function referred to as an activation
    function.”**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —[Definition from Wikipedia](https://en.wikipedia.org/wiki/Activation_function)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sounds a little complicated? Don’t worry! After reading this article, you will
    have a better understanding of activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/8d4304aeeb1d39b7ac754f055981f1e6.png)'
  prefs: []
  type: TYPE_IMG
- en: In humans, our brain receives input from the outside world, performs processing
    on the neuron receiving input and activates the neuron tail to generate required
    decisions. Similarly, in neural networks, we provide input as images, sounds,
    numbers, etc., and processing is performed on the artificial neuron, with an algorithm
    activating the correct final neuron layer to generate results.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need activation functions?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An activation function determines if a neuron should be **activated or not activated**.
    This implies that it will use some simple mathematical operations to determine
    if the neuron’s input to the network is relevant or not relevant in the prediction
    process.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to introduce **non-linearity** to an artificial neural network and
    generate output from a collection of input values fed to a layer is the purpose
    of the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Activation functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Activation functions can be divided into three types:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear Activation Function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Binary Step Function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Non-linear Activation Functions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Linear Activation Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The linear activation function, often called the **identity activation function**,
    is proportional to the input. The range of the linear activation function will
    be (-∞ to ∞). The linear activation function simply adds up the weighted total
    of the inputs and returns the result.
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/15a61eb4276c282836fc2e706c9442b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear Activation Function — Graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, it can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/151bf2885a57ef72a7ac4c54ba18e9c8.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear Activation Function — Equation
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros and Cons**'
  prefs: []
  type: TYPE_NORMAL
- en: It is not a binary activation because the linear activation function only delivers
    a range of activations. We can surely connect a few neurons together, and if there
    are multiple activations, we can calculate the max (or soft max) based on that.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The derivative of this activation function is a constant. That is to say, the
    gradient is unrelated to the x (input).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binary Step Activation Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **threshold value** determines whether a neuron should be activated or not
    activated in a binary step activation function.
  prefs: []
  type: TYPE_NORMAL
- en: The activation function compares the input value to a threshold value. If the
    input value is greater than the threshold value, the neuron is activated. It’s
    disabled if the input value is less than the threshold value, which means its
    output isn’t sent on to the next or hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/10e4ae23fa8708e44ca18e6a0e10b212.png)'
  prefs: []
  type: TYPE_IMG
- en: Binary Step Function — Graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the binary activation function can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/47bd118991d87fcd6c5dfe70168733df.png)'
  prefs: []
  type: TYPE_IMG
- en: Binary Step Activation Function — Equation
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros and Cons**'
  prefs: []
  type: TYPE_NORMAL
- en: It cannot provide multi-value outputs — for example, it cannot be used for multi-class
    classification problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The step function’s gradient is zero, which makes the back propagation procedure
    difficult.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-linear Activation Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The non-linear activation functions are the most-used activation functions.
    They make it uncomplicated for an artificial neural network model to adapt to
    a variety of data and to differentiate between the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Non-linear activation functions allow the stacking of multiple layers of neurons,
    as the output would now be a non-linear combination of input passed through multiple
    layers. Any output can be represented as a functional computation output in a
    neural network.
  prefs: []
  type: TYPE_NORMAL
- en: These activation functions are mainly divided basis on their range and curves.
    The remainder of this article will outline the major non-linear activiation functions
    used in neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Sigmoid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sigmoid accepts a number as input and returns a number between 0 and 1\. It’s
    simple to use and has all the desirable qualities of activation functions: nonlinearity,
    continuous differentiation, monotonicity, and a set output range.'
  prefs: []
  type: TYPE_NORMAL
- en: This is mainly used in **binary classification problems.** This sigmoid function
    gives the probability of an existence of a particular class.
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/1dd0239de89470cbb6c12ddd05c294c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Sigmoid Activation Function — Graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, it can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/a354aae8799bdd4770a600624273cb60.png)'
  prefs: []
  type: TYPE_IMG
- en: Sigmoid Activation Function — Equation
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros and Cons**'
  prefs: []
  type: TYPE_NORMAL
- en: It is non-linear in nature. Combinations of this function are also non-linear,
    and it will give an analogue activation, unlike binary step activation function.
    It has a smooth gradient too, and It’s good for a classifier type problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output of the activation function is always going to be in the range (0,1)
    compared to (-∞, ∞) of linear activation function. As a result, we’ve defined
    a range for our activations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoid function gives rise to a problem of **“Vanishing gradients”** and Sigmoids
    saturate and kill gradients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its output **isn’t zero centred**, and it makes the gradient updates go too
    far in different directions. The output value is between zero and one, so it makes
    optimization harder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The network either refuses to learn more or is extremely slow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.TanH (Hyperbolic Tangent)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TanH compress a real-valued number to the range **[-1, 1]**. It’s non-linear,
    But it’s different from Sigmoid,and its output is **zero-centered**. The main
    advantage of this is that the negative inputs will be mapped strongly to the negative
    and zero inputs will be mapped to almost zero in the graph of TanH.
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/95bba17f69f7a53fd4b21393d23585e5.png)'
  prefs: []
  type: TYPE_IMG
- en: TanH Activation Function — Graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, TanH function can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/fb603f8d328bf76b6ef20935478456c5.png)'
  prefs: []
  type: TYPE_IMG
- en: TanH Activation Function — Equation
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros and Cons**'
  prefs: []
  type: TYPE_NORMAL
- en: TanH also has the vanishing gradient problem, but the gradient is stronger for
    TanH than sigmoid (derivatives are steeper).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TanH is zero-centered, and gradients do not have to move in a specific direction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3.ReLU (Rectified Linear Unit)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ReLU stands for Rectified Linear Unit and is one of the most commonly used activation
    function in the applications. It’s solved the problem of vanishing gradient because
    the maximum value of the gradient of ReLU function is one. It also solved the
    problem of saturating neuron, since the slope is never zero for ReLU function.
    The range of ReLU is between **0 and infinity.**
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/0369c70833265a0315e80122dfe52e5f.png)'
  prefs: []
  type: TYPE_IMG
- en: ReLU Activation Function — Graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, it can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/0e329f7594f219053f638c84a2d05baa.png)'
  prefs: []
  type: TYPE_IMG
- en: ReLU Activation Function — Equation
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros and Cons**'
  prefs: []
  type: TYPE_NORMAL
- en: Since only a certain number of neurons are activated, the ReLU function is far
    more computationally efficient when compared to the sigmoid and TanH functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLU accelerates the convergence of gradient descent towards the global minimum
    of the loss function due to its linear, non-saturating property.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of its limitations is that **it should only be used within hidden layers**
    of an artificial neural network model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some gradients can be fragile during training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other words, For activations in the region (x<0) of ReLu, the gradient will
    be 0 because of which the weights will not get adjusted during descent. That means,
    those neurons, which go into that state will stop responding to variations in
    input (simply because the gradient is 0, nothing changes.) This is called the
    **dying ReLu problem**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.Leaky ReLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Leaky ReLU is an upgraded version of the ReLU activation function to solve the
    dying ReLU problem, as it has a small positive slope in the negative area. But,
    the consistency of the benefit across tasks is presently ambiguous.
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/d9ad66067d83564315ce55cf1946de49.png)'
  prefs: []
  type: TYPE_IMG
- en: Leaky ReLU Activation Function — Graph
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, it can be represented as,
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/ad8ffedd0c6939c1bc789804bbfa5910.png)'
  prefs: []
  type: TYPE_IMG
- en: Leaky ReLU Activation Function — Equation
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros and Cons**'
  prefs: []
  type: TYPE_NORMAL
- en: The advantages of Leaky ReLU are the same as that of ReLU, in addition to the
    fact that it does enable back propagation, even for negative input values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making minor modification of negative input values, the gradient of the left
    side of the graph comes out to be a real (non-zero) value. As a result, there
    would be no more dead neurons in that area.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The predictions may not be steady for negative input values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.ELU (Exponential Linear Units)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ELU is also one of the variations of ReLU which also solves the dead ReLU problem.
    ELU, just like leaky ReLU also considers negative values by introducing a new
    alpha parameter and multiplying it will another equation.
  prefs: []
  type: TYPE_NORMAL
- en: ELU is slightly more computationally expensive than leaky ReLU, and it’s very
    similar to ReLU except negative inputs. They are both in identity function shape
    for positive inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/b2f9279c8d4987ceab71751a00d71528.png)'
  prefs: []
  type: TYPE_IMG
- en: ELU Activation Function-Graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, it can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/a71669c3b75ad8457e02a84b81f41dfc.png)'
  prefs: []
  type: TYPE_IMG
- en: ELU Activation Function — Equation
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros and Cons**'
  prefs: []
  type: TYPE_NORMAL
- en: ELU is a strong alternative to ReLU. Different from the ReLU, ELU can produce
    negative outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exponential operations are there in ELU, So it increases the computational time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No learning about the ‘a’ value takes place, and exploding gradient problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6\. Softmax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A combination of many sigmoids is referred to as the Softmax function. It determines
    relative probability. Similar to the sigmoid activation function, the Softmax
    function returns the probability of each class/labels. **In multi-class classification,
    softmax activation function is most commonly** **used for the last layer of the
    neural network.**
  prefs: []
  type: TYPE_NORMAL
- en: The softmax function gives the probability of the current class with respect
    to others. This means that it also considers the possibility of other classes
    too.
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/ca087644f0ff080de42f7998ff705f24.png)'
  prefs: []
  type: TYPE_IMG
- en: Softmax Activation Function — Graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, it can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/778fe4f04a08fdf86f75f65b951e180e.png)'
  prefs: []
  type: TYPE_IMG
- en: Softmax Activation Function — Equation
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros and Cons**'
  prefs: []
  type: TYPE_NORMAL
- en: It mimics the one encoded label better than the absolute values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We would lose information if we used absolute (modulus) values, but the exponential
    takes care of this on its own.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The softmax function **should be used for multi-label classification** and regression
    task as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7\. Swish
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Swish allows for the propagation of a few numbers of negative weights, whereas
    ReLU sets all non-positive weights to zero. This is a crucial property that determines
    the success of non-monotonic smooth activation functions, such as Swish’s, in
    progressively deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: It’s a self-gated activation function created by Google researchers.
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/10c2ae768b3b654d151f1fdc1293d310.png)'
  prefs: []
  type: TYPE_IMG
- en: Swish Activation Function — Graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, it can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How Activation Functions Work in Deep Learning](../Images/4cf850d754d377b9c455429f5aef4209.png)'
  prefs: []
  type: TYPE_IMG
- en: Swish Activation Function — Equation
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros and Cons**'
  prefs: []
  type: TYPE_NORMAL
- en: Swish is a smooth activation function that means that it does not suddenly change
    direction like ReLU does near x equal to zero. Rather, it smoothly bends from
    0 towards values < 0 and then upwards again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-positive values were zeroed out in ReLU activation function. Negative numbers,
    on the other hand, may be valuable for detecting patterns in the data. Because
    of the sparsity, large negative numbers are wiped out, resulting in a win-win
    situation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The swish activation function being non-monotonous enhances the term of input
    data and weight to be learnt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slightly more computationally expensive and More problems with the algorithm
    will probably arise given time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While choosing the proper activation function, the following problems and issues
    must be considered:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vanishing gradient** is a common problem encountered during neural network
    training. Like a sigmoid activation function, some activation functions have a
    small output range (0 to 1). So a huge change in the input of the sigmoid activation
    function will create a small modification in the output. Therefore, the derivative
    also becomes small. These activation functions are only used for shallow networks
    with only a few layers. When these activation functions are applied to a multi-layer
    network, the gradient may become too small for expected training.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploding gradients** are situations in which massive incorrect gradients
    build during training, resulting in huge updates to neural network model weights.
    When there are exploding gradients, an unstable network might form, and training
    cannot be completed. Due to exploding gradients, the weights’ values can potentially
    grow to the point where they overflow, resulting in loss in **NaN** values.'
  prefs: []
  type: TYPE_NORMAL
- en: Final Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All hidden layers generally use the same activation functions. **ReLU** activation
    function **should only** be used in the hidden layer for better results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoid and TanH activation functions **should not be utilized** in hidden layers
    due to the vanishing gradient, since they make the model more susceptible to problems
    during training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swish function is used in artificial neural networks having a depth **more than
    40 layers.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression problems should use linear activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binary classification problems should use the sigmoid activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiclass classification problems shold use the softmax activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural network architecture and their usable activation functions,
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolutional Neural Network (CNN): ReLU activation function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recurrent Neural Network (RNN): TanH or sigmoid activation functions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find countless additional articles evaluating activation function comparisons.
    I suggest you get your hands dirty and practise well.
  prefs: []
  type: TYPE_NORMAL
- en: If you have any questions, feel free to ask me on [LinkedIn](https://www.linkedin.com/in/parthibanmarimuthu/)
    or [Twitter](https://twitter.com/parthibharathiv).
  prefs: []
  type: TYPE_NORMAL
- en: '**[Parthiban Marimuthu](https://www.linkedin.com/in/parthibanmarimuthu/)**
    ([@parthibharathiv](https://twitter.com/parthibharathiv)) lives in Chennai, India,
    and works at Spritle Software. Parthiban studies and transforms data science prototypes,
    designs machine learning systems, and researches and implements appropriate ML
    algorithms and tools.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Unveiling Neural Magic: A Dive into Activation Functions](https://www.kdnuggets.com/unveiling-neural-magic-a-dive-into-activation-functions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets™ News 22:n03, Jan 19: A Deep Look Into 13 Data…](https://www.kdnuggets.com/2022/n03.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[KDnuggets News, August 3: 10 Most Used Tableau Functions • Is…](https://www.kdnuggets.com/2022/n31.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How Does Logistic Regression Work?](https://www.kdnuggets.com/2022/07/logistic-regression-work.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Learn How Different Data Visualizations Work](https://www.kdnuggets.com/2022/09/datacamp-learn-different-data-visualizations-work.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[What are Large Language Models and How Do They Work?](https://www.kdnuggets.com/2023/05/large-language-models-work.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
