- en: 'Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NExT-GPT 介绍：任何到任何的多模态大语言模型
- en: 原文：[https://www.kdnuggets.com/introduction-to-nextgpt-anytoany-multimodal-large-language-model](https://www.kdnuggets.com/introduction-to-nextgpt-anytoany-multimodal-large-language-model)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/introduction-to-nextgpt-anytoany-multimodal-large-language-model](https://www.kdnuggets.com/introduction-to-nextgpt-anytoany-multimodal-large-language-model)
- en: '![Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](../Images/759e062253b4a9e3c294ac44bcd62830.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![NExT-GPT 介绍：任何到任何的多模态大语言模型](../Images/759e062253b4a9e3c294ac44bcd62830.png)'
- en: Image by Editor
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由编辑提供
- en: In recent years, Generative AI research has evolved in a way that has changed
    how we work. From developing content, planning our work, and finding answers to
    creating artwork, it’s all possible now with Generative AI. However, each model
    usually works for certain use cases, e.g., GPT for text-to-text, Stable Diffusion
    for text-to-image, and many others.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，生成式AI研究的发展改变了我们的工作方式。从内容开发、工作规划、寻找答案，到创建艺术作品，现在都可以通过生成式AI实现。然而，每个模型通常适用于某些特定的用例，例如，GPT
    用于文本到文本，Stable Diffusion 用于文本到图像，以及许多其他模型。
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的IT需求'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The model capable of performing multiple tasks is called the multimodal model.
    Much state-of-the-art research is moving in the multimodal direction as it’s proven
    useful in many conditions. This is why one of the exciting research regarding
    multimodal people need to know is the [NExT-GPT](https://arxiv.org/pdf/2309.05519.pdf).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 能够执行多种任务的模型称为多模态模型。许多前沿研究正在向多模态方向发展，因为它在许多条件下已被证明是有用的。这也是为什么多模态研究中令人兴奋的一个方向是
    [NExT-GPT](https://arxiv.org/pdf/2309.05519.pdf)。
- en: NExT-GPT is a multimodal model that could transform anything into anything.
    So, how does it work? Let’s explore it further.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: NExT-GPT 是一个可以将任何东西转换为任何东西的多模态模型。那么，它是如何工作的呢？让我们进一步探索。
- en: NExT-GPT Introduction
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NExT-GPT 介绍
- en: 'NExT-GPT is an any-to-any multimodal LLM that can handle four different kinds
    of input and output: text, images, videos, and audio. The research was initiated
    by the research group called [NExT++ of the National University of Singapore](https://www.nextcenter.org/).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: NExT-GPT 是一个任何到任何的多模态LLM，能够处理四种不同类型的输入和输出：文本、图像、视频和音频。该研究由 [新加坡国立大学的NExT++研究小组](https://www.nextcenter.org/)
    发起。
- en: The overall representation of the NExT-GPT model is shown in the image below.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: NExT-GPT 模型的整体表示如下面的图像所示。
- en: '![Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](../Images/5f3b1d0fe21edbaefa094c994bb7be0e.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![NExT-GPT 介绍：任何到任何的多模态大语言模型](../Images/5f3b1d0fe21edbaefa094c994bb7be0e.png)'
- en: NExT-GPT LLM Model ([Wu *et al*. (2023)](https://arxiv.org/pdf/2309.05519.pdf))
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: NExT-GPT LLM 模型 ([Wu *et al*. (2023)](https://arxiv.org/pdf/2309.05519.pdf))
- en: 'NExT-GPT model consists of three parts of works:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: NExT-GPT 模型由三个部分组成：
- en: Establish encoders for input from various modalities and represent them into
    a language-like input that LLM could accept,
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为来自各种模态的输入建立编码器，并将其表示为LLM可以接受的类语言输入，
- en: Utilizing the open-source LLM as the core to process the input for both semantic
    understanding and reasoning with additional unique modality signal,
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用开源LLM作为核心，处理输入以进行语义理解和推理，并添加独特的模态信号，
- en: Provide multimodal signal into different encoders and generate the result to
    the appropriate modalities.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将多模态信号提供给不同的编码器，并将结果生成到适当的模态。
- en: An example of the NExT-GPT inferences process can be seen in the image below.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: NExT-GPT 推理过程的一个示例可以在下面的图像中看到。
- en: '![Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](../Images/36bfc2d94328d91f1f314f6955d82673.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![NExT-GPT 介绍：任何到任何的多模态大语言模型](../Images/36bfc2d94328d91f1f314f6955d82673.png)'
- en: NExT-GPT inference Process ([Wu *et al*. (2023)](https://arxiv.org/pdf/2309.05519.pdf))
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: NExT-GPT推理过程 ([Wu *et al*. (2023)](https://arxiv.org/pdf/2309.05519.pdf))
- en: We can see in the image above that depending on the tasks that we want, the
    encoder and decoder would switch to the appropriate modalities. This process can
    only happen because NExT-GPT utilizes a concept called modality-switching instruction
    tuning so the model can conform with the user's intention.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的图像中我们可以看到，根据我们需要的任务，编码器和解码器会切换到适当的模态。这个过程之所以能发生，是因为NExT-GPT利用了一个叫做模态切换指令调优的概念，使得模型能够符合用户的意图。
- en: The researchers have tried to experiment with various combinations of modalities.
    Overall, the NExT-GPT performance can be summarized in the graph below.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员尝试了多种模态组合的实验。总体而言，NExT-GPT的性能可以在下图中总结。
- en: '![Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](../Images/ddbf67c258e36190111962e38b0edf0c.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![NExT-GPT简介：任意到任意的多模态大型语言模型](../Images/ddbf67c258e36190111962e38b0edf0c.png)'
- en: NExT-GPT Overall Performance Result ([Wu *et al*. (2023)](https://arxiv.org/pdf/2309.05519.pdf))
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: NExT-GPT总体性能结果 ([Wu *et al*. (2023)](https://arxiv.org/pdf/2309.05519.pdf))
- en: NExT-GPT's best performance is the Text and Audio input to produce Images, followed
    by the Text, Audio, and Image input to produce Image results. The least performing
    action is the Text and Video input to produce Video output.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: NExT-GPT的最佳表现是文本和音频输入生成图像，其次是文本、音频和图像输入生成图像结果。表现最差的是文本和视频输入生成视频输出。
- en: An example of the NExT-GPT capability is shown in the image below.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了NExT-GPT能力的一个示例。
- en: '![Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](../Images/635190c69b67ec01e1b628ac9c1d7d4b.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![NExT-GPT推理过程](../Images/635190c69b67ec01e1b628ac9c1d7d4b.png)'
- en: 'Text-to-Text+Image+Audio from NExT-GPT (Source: [NExT-GPT web](https://next-gpt.github.io/))'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: NExT-GPT的文本到文本+图像+音频（来源：[NExT-GPT官网](https://next-gpt.github.io/)）
- en: The result above shows that interacting with the NExT-GPT can produce Audio,
    Text, and Images appropriate to the user's intention. It’s shown that NExT-GPT
    can act quite well and is pretty reliable.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果显示，与NExT-GPT互动可以生成符合用户意图的音频、文本和图像。数据显示NExT-GPT的表现相当出色，且非常可靠。
- en: Another example of NExT-GPT is shown in the image below.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了NExT-GPT的另一个示例。
- en: '![Introduction to NExT-GPT: Any-to-Any Multimodal Large Language Model](../Images/4b238e3be3fc5a3039d9c824af04e7a2.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![NExT-GPT简介：任意到任意的多模态大型语言模型](../Images/4b238e3be3fc5a3039d9c824af04e7a2.png)'
- en: 'Text+Imaget-to-Text+Audio from NExT-GPT (Source: [NExT-GPT web](https://next-gpt.github.io/))'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 文本+图像到文本+音频的NExT-GPT（来源：[NExT-GPT官网](https://next-gpt.github.io/)）
- en: The image above shows that NExT-GPT can handle two kinds of modalities to produce
    Text and Audio output. It’s shown how the model is versatile enough.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了NExT-GPT可以处理两种模态以生成文本和音频输出。这显示了模型的多功能性。
- en: If you want to try the model, you can set up the model and environment from
    their [GitHub page](https://github.com/NExT-GPT/NExT-GPT). Additionally, you can
    try out the demo on the following [page](https://4c5b3bd137f4eec297.gradio.live/).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想尝试这个模型，你可以从他们的[GitHub页面](https://github.com/NExT-GPT/NExT-GPT)设置模型和环境。此外，你可以在以下[页面](https://4c5b3bd137f4eec297.gradio.live/)试用演示。
- en: Conclusion
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: NExT-GPT is a multimodal model that accepts input data and produces output in
    text, image, audio, and video. This model works by utilizing a specific encoder
    for the modalities and switching to appropriate modalities according to the user's
    intention. The performance experiment result shows a good result and promising
    work that can be used in many applications.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: NExT-GPT是一个多模态模型，接受文本、图像、音频和视频的数据输入，并生成相应的输出。该模型通过利用特定的编码器处理模态，并根据用户的意图切换到适当的模态。性能实验结果显示良好，且具有很大的应用潜力。
- en: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**** is a data science
    assistant manager and data writer. While working full-time at Allianz Indonesia,
    he loves to share Python and data tips via social media and writing media. Cornellius
    writes on a variety of AI and machine learning topics.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**[](https://www.linkedin.com/in/cornellius-yudha-wijaya/)**[Cornellius Yudha
    Wijaya](https://www.linkedin.com/in/cornellius-yudha-wijaya/)** 是一位数据科学助理经理和数据撰稿人。在全职工作于Allianz
    Indonesia期间，他喜欢通过社交媒体和写作媒体分享Python和数据技巧。Cornellius在各种AI和机器学习话题上撰写文章。'
- en: More On This Topic
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关内容
- en: '[Multimodal Grounded Learning with Vision and Language](https://www.kdnuggets.com/2022/11/multimodal-grounded-learning-vision-language.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[带有视觉和语言的多模态基础学习](https://www.kdnuggets.com/2022/11/multimodal-grounded-learning-vision-language.html)'
- en: '[Multimodal Models Explained](https://www.kdnuggets.com/2023/03/multimodal-models-explained.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多模态模型解析](https://www.kdnuggets.com/2023/03/multimodal-models-explained.html)'
- en: '[The Ultimate Open-Source Large Language Model Ecosystem](https://www.kdnuggets.com/2023/05/ultimate-opensource-large-language-model-ecosystem.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[终极开源大型语言模型生态系统](https://www.kdnuggets.com/2023/05/ultimate-opensource-large-language-model-ecosystem.html)'
- en: '[Exploring the Zephyr 7B: A Comprehensive Guide to the Latest Large…](https://www.kdnuggets.com/exploring-the-zephyr-7b-a-comprehensive-guide-to-the-latest-large-language-model)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[探索Zephyr 7B：最新大型语言模型的全面指南](https://www.kdnuggets.com/exploring-the-zephyr-7b-a-comprehensive-guide-to-the-latest-large-language-model)'
- en: '[Free Mastery Course: Become a Large Language Model Expert](https://www.kdnuggets.com/ree-mastery-course-become-a-large-language-model-expert)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[免费精通课程：成为大型语言模型专家](https://www.kdnuggets.com/ree-mastery-course-become-a-large-language-model-expert)'
- en: '[7 Steps to Mastering Large Language Model Fine-tuning](https://www.kdnuggets.com/7-steps-to-mastering-large-language-model-fine-tuning)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[掌握大型语言模型微调的7个步骤](https://www.kdnuggets.com/7-steps-to-mastering-large-language-model-fine-tuning)'
