["```py\nversion: '2'\nservices:\n    glue-service:\n        image: amazon/aws-glue-libs:glue_libs_1.0.0_image_01\n        container_name: \"glue_ontainer_demo\"\n        build:\n            context: .\n            dockerfile: Dockerfile\n        ports:\n            - \"8000:8000\"\n        volumes:\n            - .:/opt\n        links:\n            - localstack-s3\n        environment:\n          S3_ENDPOINT: http://localstack:4566\n    localstack-s3:\n      image: localstack/localstack\n      container_name: \"localstack_container_demo\"\n      volumes:\n        - ./stubs/s3:/tmp/localstack\n      environment:\n        - SERVICES=s3\n        - DEFAULT_REGION=us-east-1\n        - HOSTNAME=localstack\n        - DATA_DIR=/tmp/localstack/data\n        - HOSTNAME_EXTERNAL=localstack\n      ports:\n        - \"4566:4566\"\n```", "```py\nFROM python:3.6.10\n\nWORKDIR /opt\n\n# By copying over requirements first, we make sure that Docker will cache\n# our installed requirements rather than reinstall them on every build\nCOPY requirements.txt /opt/requirements.txt\nRUN pip install -r requirements.txt\n\n# Now copy in our code, and run it\nCOPY . /opt\n```", "```py\nmoto[all]==2.0.5\n```", "```py\nimport boto3\nimport os\nfrom pyspark.sql import SparkSession\n```", "```py\ndef add_to_bucket(bucket_name: str, file_name: str):\n    try:\n        # host.docker.internal\n        s3 = boto3.client('s3',\n                          endpoint_url=\"http://host.docker.internal:4566\",\n                          use_ssl=False,\n                          aws_access_key_id='mock',\n                          aws_secret_access_key='mock',\n                          region_name='us-east-1')\n        s3.create_bucket(Bucket=bucket_name)\n\n        file_key = f'{os.getcwd()}/{file_name}'\n        with open(file_key, 'rb') as f:\n            s3.put_object(Body=f, Bucket=bucket_name, Key=file_name)\n        print(file_name)\n\n        return s3\n    except Exception as e:\n        print(e)\n        return None\n```", "```py\ndef create_testing_pyspark_session():\n    print('creating pyspark session')\n    sparksession = (SparkSession.builder\n                    .master('local[2]')\n                    .appName('pyspark-demo')\n                    .enableHiveSupport()\n                    .getOrCreate())\n\n    hadoop_conf = sparksession.sparkContext._jsc.hadoopConfiguration()\n    hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n    hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")\n    hadoop_conf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n    hadoop_conf.set(\"com.amazonaws.services.s3a.enableV4\", \"true\")\n    hadoop_conf.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n    hadoop_conf.set(\"fs.s3a.access.key\", \"mock\")\n    hadoop_conf.set(\"fs.s3a.secret.key\", \"mock\")\n    hadoop_conf.set(\"fs.s3a.session.token\", \"mock\")\n    hadoop_conf.set(\"fs.s3a.endpoint\", \"http://host.docker.internal:4566\")\n    return sparksession\n```", "```py\ntest_bucket = 'dummypysparkbucket'\n# Write to S3 bucket\nadd_to_bucket(bucket_name=test_bucket, file_name='dummy.csv')\nspark_session = create_testing_pyspark_session()\nfile_path = f's3a://{test_bucket}/dummy.csv'\n\n# Read from s3 bucket\ndata_df = spark_session.read.option('delimiter', ',').option('header', 'true').option('inferSchema',\n                                                                                      'False').format('csv').load(\n    file_path)\nprint(data_df.show())\n```", "```py\n# Write to S3 as parquet\nwrite_path = f's3a://{test_bucket}/testparquet/'\ndata_df.write.parquet(write_path, mode='overwrite')\n```"]