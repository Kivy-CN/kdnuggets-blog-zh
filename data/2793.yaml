- en: Making sense of ensemble learning techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解集成学习技术
- en: 原文：[https://www.kdnuggets.com/2020/03/making-sense-ensemble-learning-techniques.html](https://www.kdnuggets.com/2020/03/making-sense-ensemble-learning-techniques.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/03/making-sense-ensemble-learning-techniques.html](https://www.kdnuggets.com/2020/03/making-sense-ensemble-learning-techniques.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Ido Zehori](https://www.linkedin.com/in/ido-zehori/), Data Science Team
    Leader at [Bigabid](https://www.bigabid.com/)**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者：[Ido Zehori](https://www.linkedin.com/in/ido-zehori/)，[Bigabid](https://www.bigabid.com/)的数据科学团队负责人**'
- en: '![Figure](../Images/733e44c46833a8497ef2f1341a1d187c.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/733e44c46833a8497ef2f1341a1d187c.png)'
- en: 'Image source: [Packt](https://hub.packtpub.com/what-is-ensemble-learning/)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Packt](https://hub.packtpub.com/what-is-ensemble-learning/)
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织IT需求'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: For many companies/data scientists that specialize or work with machine learning
    (ML), ensemble learning methods have become the weapons of choice. As ensemble
    learning methods combine multiple base models, together they have a greater ability
    to produce a much more accurate ML model. For example, at Bigabid we’ve been ensemble
    learning to successfully solve a variety of problems ranging from optimizing LTV
    ([Customer Lifetime Value](https://www.qualtrics.com/experience-management/customer/customer-lifetime-value/))
    to [fraud detection](https://searchsecurity.techtarget.com/definition/fraud-detection).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多专注于机器学习（ML）的公司/数据科学家来说，集成学习方法已经成为首选工具。由于集成学习方法结合了多个基础模型，因此它们能够生成更为准确的ML模型。例如，在Bigabid，我们通过集成学习成功解决了从优化LTV（[客户终身价值](https://www.qualtrics.com/experience-management/customer/customer-lifetime-value/)）到[欺诈检测](https://searchsecurity.techtarget.com/definition/fraud-detection)的各种问题。
- en: 'It is hard not overstate the importance of ensemble learning to the overall
    ML process, including the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)
    and the three main ensemble techniques: bagging, boosting and stacking. These
    powerful techniques should be a part of any data scientist’s tool kit, as they
    are concepts that are encountered everywhere. Plus, understanding their underlying
    mechanism is at the heart of the field of machine learning.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 难以过分强调集成学习在整体ML过程中的重要性，包括[偏差-方差权衡](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)以及三种主要的集成技术：装袋、提升和堆叠。这些强大的技术应成为任何数据科学家工具包的一部分，因为它们是普遍存在的概念。此外，理解它们的基本机制是机器学习领域的核心。
- en: 'Ensemble Learning Methods: An Overview'
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成学习方法概述
- en: Ensemble learning is an ML paradigm where numerous base models (which are often
    referred to as “weak learners”) are combined and trained to solve the same problem.
    This method is based on the theory that by correctly combining several base models
    together, these weak learners can be used as building blocks for designing more
    complex ML models. Together these ensemble models (aptly called “strong learners”)
    achieve better, more accurate results.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习是一种ML范式，其中多个基础模型（通常被称为“弱学习者”）被组合和训练以解决同一问题。这种方法基于这样的理论：通过正确地组合多个基础模型，这些弱学习者可以作为设计更复杂ML模型的构建块。这些集成模型（恰如其分地称为“强学习者”）能够实现更好、更准确的结果。
- en: In other words, a weak learner is merely a base model that alone performs rather
    poorly. In fact, its accuracy level is barely above chance, meaning that it predicts
    the outcome only slightly better than a random guess would. These weak learners
    will often be computationally simple as well. Typically, the reason base models
    don’t perform very well by themselves is because they either have a high bias
    or too much variance, which makes them weak.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，弱学习者只是一个单独表现较差的基础模型。实际上，它的准确性水平仅略高于偶然情况，这意味着它的预测结果仅比随机猜测稍好。这些弱学习者通常也会很简单。通常，基础模型之所以自身表现不佳，是因为它们要么有高偏差，要么方差过大，使它们变得薄弱。
- en: This is where ensemble learning comes in. This method attempts to try to reduce
    the general error by combining several weak learners together. Think of ensemble
    models as the data science version of the expression “two heads are better than
    one.” If one model works well, then a number of models working together can do
    even better.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是集成学习的作用。该方法试图通过将多个弱学习者组合在一起，来减少总体误差。可以将集成模型视为“两个脑袋总比一个好”的数据科学版本。如果一个模型表现良好，那么多个模型共同工作可以做得更好。
- en: A Word about the Bias-Variance Tradeoff
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于偏差-方差权衡的一点说明
- en: 'It’s important to understand the concept of a weak learner and why it earned
    this name in the first place, as the reason comes down to either bias or variance.
    More specifically, the prediction error of an ML model, namely the difference
    between the trained model and the ground truth, can be broken down into the sum
    of the following: the bias and the variance. For instance:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 理解弱学习者的概念以及为什么它会得到这个名字非常重要，因为其原因归结为偏差或方差。更具体地说，机器学习模型的预测错误，即训练模型与真实值之间的差异，可以分解为以下两部分：偏差和方差。例如：
- en: '**Error due to Bias:** This is the difference between a model’s expected prediction
    and the precise value that we are aiming to predict.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差导致的错误：** 这是模型的预期预测与我们希望预测的精确值之间的差异。'
- en: '**Error due to Variance:** This is the variance of a model prediction for a
    specified data point.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差导致的错误：** 这是模型对特定数据点预测的方差。'
- en: If a model is too simplistic and doesn’t have many parameters, then it may have
    high bias and low variance. In contrast, if a model has many parameters, then
    it may have high variance and low bias. As such, it’s necessary to find the right
    balance without underfitting or overfitting the data, as this tradeoff in complexity
    is the reason why there exists a tradeoff between variance and bias. Simply put,
    an algorithm can’t simultaneously be more complex and less complex at the same
    time.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个模型过于简单且没有很多参数，那么它可能有高偏差和低方差。相比之下，如果一个模型有很多参数，那么它可能有高方差和低偏差。因此，有必要找到适当的平衡，避免对数据的欠拟合或过拟合，因为这种复杂性权衡是存在偏差和方差之间权衡的原因。简单来说，一个算法不能同时变得更复杂和更简单。
- en: Ensemble Learning Techniques - Combining Weak Learners
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成学习技术 - 结合弱学习者
- en: 'There are three main ensemble techniques: bagging, boosting and stacking. There
    are defined as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习有三种主要技术：自助法、提升法和堆叠法。它们的定义如下：
- en: '**Bagging:**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**自助法（Bagging）：**'
- en: Bagging attempts to incorporate similar learners on small-sample populations
    and calculates the average of all the predictions. Generally, bagging allows you
    to use different learners in different populations. By doing so, this method helps
    to reduce the variance error.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 自助法试图在小样本群体上结合相似的学习者，并计算所有预测的平均值。通常，自助法允许你在不同的样本群体中使用不同的学习者。通过这样做，这种方法有助于减少方差错误。
- en: '**Boosting**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**提升法（Boosting）**'
- en: Boosting is an iterative method that fine-tunes the weight of an observation
    according to the most recent classification. If an observation was incorrectly
    classified, this method will increase the weight of that observation in the next
    round (in which the next model will be trained) and will be less prone to misclassification.
    Similarly, if an observation was classified correctly, then it will reduce its
    weight for the next classifier. The weight represents how important the correct
    classification of the specific data point should be, as this enables the sequential
    classifiers to focus on examples previously misclassified. Generally, boosting
    reduces the bias error and forms strong predictive models, but at times they may
    overfit on the training data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 提升是一种迭代方法，它根据最新的分类结果微调观察的权重。如果一个观察被错误分类，该方法将在下一轮（即训练下一个模型的轮次）增加该观察的权重，并减少误分类的倾向。类似地，如果一个观察被正确分类，那么它将在下一个分类器中减少其权重。权重表示特定数据点正确分类的重要性，这使得顺序分类器可以集中关注之前被误分类的例子。通常，提升方法能减少偏差错误，形成强大的预测模型，但有时可能会在训练数据上过拟合。
- en: '**Stacking**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**堆叠**'
- en: Stacking is a clever way of combining the information provided by different
    models. With this method, a learner of any sort can be used to combine different
    learners’ outputs. The result can be a decrease in bias or variance determined
    by which combined models are used.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠是一种巧妙的方法，通过组合不同模型提供的信息。使用这种方法，可以将任何类型的学习者用于结合不同学习者的输出。结果可能是通过使用哪些组合模型来减少偏差或方差。
- en: '**The Promise of Ensemble Learning**'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**集成学习的承诺**'
- en: Ensemble learning is about combining multiple base models to achieve a more
    effective and accurate ensemble model that features more powerful properties and
    thus, performs better. Ensemble learning methods have successfully set record
    performances on challenging datasets and are constantly a part of the winning
    submissions of [Kaggle competitions.](https://www.kaggle.com/competitions)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习涉及将多个基础模型结合，以实现更有效、更准确的集成模型，这种模型具有更强大的特性，因此表现更佳。集成学习方法在挑战性数据集上成功创下了记录，并且经常成为
    [Kaggle 竞赛](https://www.kaggle.com/competitions)获胜提交的一部分。
- en: It’s worth noting that even with the three main ensemble techniques – bagging,
    boosting and stacking – variants are still possible and can be better designed
    to more effectively adapt to specific problems, such as classifications, regression,
    time-series analyses, etc. This first requires an understanding of the problem
    at hand and to be creative in approaching problem solving!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，即使是三种主要的集成技术——装袋、提升和堆叠——也可以有变体，这些变体可以更好地设计以更有效地适应特定问题，如分类、回归、时间序列分析等。这首先需要了解当前的问题，并在解决问题时保持创造性！
- en: Using ensemble learning methods is a great, promising way to start approaching
    any problem.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用集成学习方法是一种很好的、有前景的解决问题的方法。
- en: '**Bio: [Ido Zehori](https://www.linkedin.com/in/ido-zehori/)** is the Data
    Science Team Leader at [Bigabid](https://www.bigabid.com/), a data science company
    that has developed a second-generation DSP optimized for in-app advertising user
    acquisition & re-engagement, providing both the scale of a tier-1 DSP and the
    precision of a cutting edge DMP.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Ido Zehori](https://www.linkedin.com/in/ido-zehori/)** 是 [Bigabid](https://www.bigabid.com/)
    的数据科学团队负责人，该公司开发了针对应用内广告用户获取和重新参与优化的第二代 DSP，提供了一级 DSP 的规模和尖端 DMP 的精准度。'
- en: '**Related:**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Ensemble Methods for Machine Learning: AdaBoost](/2019/09/ensemble-methods-machine-learning-adaboost.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习方法：AdaBoost](/2019/09/ensemble-methods-machine-learning-adaboost.html)'
- en: '[Random Forest® — A Powerful Ensemble Learning Algorithm](/2020/01/random-forest-powerful-ensemble-learning-algorithm.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机森林® — 强大的集成学习算法](/2020/01/random-forest-powerful-ensemble-learning-algorithm.html)'
- en: '[Explaining Black Box Models: Ensemble and Deep Learning Using LIME and SHAP](/2020/01/explaining-black-box-models-ensemble-deep-learning-lime-shap.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[解释黑箱模型：使用 LIME 和 SHAP 的集成学习和深度学习](/2020/01/explaining-black-box-models-ensemble-deep-learning-lime-shap.html)'
- en: More On This Topic
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习技术：Python 中随机森林的详细介绍](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
- en: '[When Would Ensemble Techniques be a Good Choice?](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[何时使用集成技术是一个好选择？](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
- en: '[Ensemble Learning with Examples](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[带有示例的集成学习](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
- en: '[The Significance of Data Quality in Making a Successful Machine…](https://www.kdnuggets.com/2022/03/significance-data-quality-making-successful-machine-learning-model.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据质量在成功机器学习模型中的重要性](https://www.kdnuggets.com/2022/03/significance-data-quality-making-successful-machine-learning-model.html)'
- en: '[Ploomber vs Kubeflow: Making MLOps Easier](https://www.kdnuggets.com/2022/02/ploomber-kubeflow-mlops-easier.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ploomber与Kubeflow：让MLOps变得更简单](https://www.kdnuggets.com/2022/02/ploomber-kubeflow-mlops-easier.html)'
- en: '[Making Intelligent Document Processing Smarter: Part 1](https://www.kdnuggets.com/2023/02/making-intelligent-document-processing-smarter-part-1.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[让智能文档处理更智能：第1部分](https://www.kdnuggets.com/2023/02/making-intelligent-document-processing-smarter-part-1.html)'
