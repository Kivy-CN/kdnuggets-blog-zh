- en: What Is Dimension Reduction In Data Science?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学中的降维是什么？
- en: 原文：[https://www.kdnuggets.com/2019/01/dimension-reduction-data-science.html](https://www.kdnuggets.com/2019/01/dimension-reduction-data-science.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2019/01/dimension-reduction-data-science.html](https://www.kdnuggets.com/2019/01/dimension-reduction-data-science.html)
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](#comments)'
- en: We have access to a large amounts of data now. The large amount of data can
    lead us to situations where by we take every possible data that is available to
    us and feed it into a forecasting model to predict our target variable. This article
    aims to explain the common issues associated with introduction of large set of
    features and provides solutions which we can utilise to resolve those problems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以访问大量数据。大量数据可能会导致我们将所有可用数据输入到预测模型中以预测目标变量。本文旨在解释引入大量特征时常见的问题，并提供我们可以利用的解决方案来解决这些问题。
- en: '*It is crucial for every data scientist and machine learning expert to understand
    what dimension reduction techniques are and when to use them.*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*每个数据科学家和机器学习专家都必须理解降维技术是什么以及何时使用它们。*'
- en: '![](../Images/006d7d77736e6ecab546ceb5e7acf823.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/006d7d77736e6ecab546ceb5e7acf823.png)'
- en: '**Photo by [Sergi Kabrera](https://unsplash.com/@skabrera?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**照片由 [Sergi Kabrera](https://unsplash.com/@skabrera?utm_source=medium&utm_medium=referral)
    提供，发布在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)**'
- en: '**Let’s Understand The Issues Better**'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**让我们更好地理解这些问题**'
- en: Occasionally we gather data for our data science project and end up gathering
    a large set of features. Some of these features (known as variables) are not as
    important as others. Sometimes the features themselves are correlated with each
    other. And occasionally we end up over-fitting the problem by introducing too
    many features. The large number of features make the data set sparse.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们为数据科学项目收集数据，结果收集了一大堆特征。其中一些特征（称为变量）并不像其他特征那么重要。有时这些特征之间是相关的。偶尔我们会因为引入过多的特征而导致过拟合问题。大量特征使数据集变得稀疏。
- en: Furthermore, it takes a much larger space to store a data set with a large number
    of features. Moreover, it can get very difficult to analyse and visualize a data
    set with a large number of dimensions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，存储具有大量特征的数据集需要更大的空间。而且，分析和可视化具有大量维度的数据集会变得非常困难。
- en: '*Dimension reduction can reduce the time that is required to train our machine
    learning model and it can also benefit in eliminating over-fitting.*'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*降维可以减少训练机器学习模型所需的时间，同时也有助于消除过拟合。*'
- en: This article outlines the techniques which we can follow to compress our data
    set onto a new feature subspace of lower dimensionality. I will also be providing
    details of important dimension reduction techniques.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文概述了我们可以遵循的技术，以将数据集压缩到一个新的低维特征子空间中。我还将提供有关重要降维技术的详细信息。
- en: '*Please read FinTechExplained d*[*isclaimer*](https://medium.com/p/87dba77241c7?source=your_stories_page---------------------------)*.*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*请阅读 FinTechExplained 的*[免责声明](https://medium.com/p/87dba77241c7?source=your_stories_page---------------------------)*。*'
- en: '**How Do I Define Dimension Reduction?**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**我如何定义降维？**'
- en: Imagine you want to e-mail a large set of files to your friend. Uploading and
    sending the files might take a longer time. You can speed up the process of uploading
    of the files by zipping the files and e-mail the zipped file instead. Zipping
    the file compresses large quantity of data into smaller equivalent sets.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你想将一大堆文件通过电子邮件发送给朋友。上传和发送这些文件可能会花费更长时间。你可以通过将文件压缩后再发送电子邮件来加快文件上传的过程。压缩文件将大量数据压缩成较小的等效集合。
- en: '*Dimension reduction is the same principal as zipping the data.*'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*降维与压缩数据的原理相同。*'
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Dimension reduction compresses large set of features onto a new feature subspace
    of lower dimensional without losing the important information.*'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*降维是将大量特征压缩到一个较低维度的新特征子空间中，而不丢失重要信息。*'
- en: Although the slight difference is that dimension reduction techniques will lose
    some of the information when the dimensions are reduced.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有一个小的区别，即降维技术在降低维度时会丢失一些信息。
- en: It is harder to visualise a large set of dimensions. Dimension reduction techniques
    can be employed to make a 20+ dimension feature space into 2 or 3 dimension subspace.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉化大量维度是比较困难的。维度减少技术可以将20多个维度的特征空间转变为2或3维子空间。
- en: '**What Are Different Dimension Reduction Techniques?**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**不同的维度减少技术有哪些？**'
- en: 'Before we take a deep dive into the key techniques, let’s quickly understand
    the two main areas of machine learning:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解关键技术之前，让我们快速了解机器学习的两个主要领域：
- en: Supervised — when the results of the training set are known
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有监督的——当训练集的结果已知时
- en: Unsupervised — when the final outcome is not known
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无监督的——当最终结果未知时
- en: 'If you want to get a better understanding of machine learning then have a look
    at my article:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想更好地理解机器学习，请查看我的文章：
- en: '[Machine Learning in 8 Minutes](https://medium.com/fintechexplained/introduction-to-machine-learning-4b2d7c57613b)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[8分钟了解机器学习](https://medium.com/fintechexplained/introduction-to-machine-learning-4b2d7c57613b)'
- en: 'There are a large number of techniques to reduce the dimensions such as forward/backward
    feature selection or combining the dimensions together by calculating weighted
    average of the correlated features. However in this article I will explore two
    of the main techniques of dimension reduction:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多减少维度的技术，如前向/后向特征选择或通过计算相关特征的加权平均来合并维度。然而，在这篇文章中，我将探讨两种主要的维度减少技术：
- en: 'Linear Discriminant Analysis (LDA):'
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 线性判别分析 (LDA)：
- en: LDA is used for compressing supervised data
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: LDA用于压缩监督数据。
- en: When we have a large set of features (classes), and our data is normally distributed
    and the features are not correlated with each other then we can use LDA to reduce
    the number of dimensions. LDA is a generalised version of Fisher’s linear discriminant.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有大量特征（类别），且数据呈正态分布且特征之间没有相关性时，我们可以使用LDA来减少维度。LDA是Fisher线性判别的广义版本。
- en: '*Calculate z-score to normalise the features that are highly skewed.*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*计算z-score以规范化高度偏斜的特征。*'
- en: 'If you want to understand how to enrich features and calculate z-score then
    have a look at this article:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解如何丰富特征和计算z-score，请查看这篇文章：
- en: '[Processing Data To Improve Machine Learning Models Accuracy](https://medium.com/fintechexplained/processing-data-to-improve-machine-learning-models-accuracy-de17c655dc8e)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[处理数据以提高机器学习模型的准确性](https://medium.com/fintechexplained/processing-data-to-improve-machine-learning-models-accuracy-de17c655dc8e)'
- en: 'Sci-kit learn offers easy to use LDA tools:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Sci-kit learn提供了易于使用的LDA工具：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code will result in producing three LDA components for the entire data
    set.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将生成整个数据集的三个LDA组件。
- en: '![](../Images/a9b51b9491e5ee73e742e88d09cb8150.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9b51b9491e5ee73e742e88d09cb8150.png)'
- en: '**Photo by [NASA](https://unsplash.com/@nasa?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**照片由 [NASA](https://unsplash.com/@nasa?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)**'
- en: '**Principal component analysis (PCA):**'
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**主成分分析 (PCA)：**'
- en: They are mainly used for compressing unsupervised data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 它们主要用于压缩无监督数据。
- en: PCA is a very useful technique that can help de-noise and detect patterns in
    data. PCA is used in reducing dimensions in images, textual contents and in speech
    recognition systems.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种非常有用的技术，可以帮助去噪声和检测数据中的模式。PCA用于减少图像、文本内容和语音识别系统中的维度。
- en: 'Sci-kit learn library offers a powerful PCA component classifier. This code
    snippet illustrates how to create PCA components:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Sci-kit learn库提供了强大的PCA组件分类器。以下代码片段演示了如何创建PCA组件：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**It is wise to understand how PCA works.**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**了解PCA的工作原理是明智的。**'
- en: Understanding PCA
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理解PCA
- en: 'This section of the article provides an overview of the process:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的这一部分提供了过程的概述：
- en: PCA technique analyses the entire data set and then finds the points with maximum
    variance.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA技术分析整个数据集，然后找到具有最大方差的点。
- en: It creates new variables such that there is a linear relationship between the
    new and original variables such that the variance is maximised.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它创建了新的变量，使得新变量与原始变量之间存在线性关系，从而最大化方差。
- en: Covariance matrix is then created for the features to understand their multi-collinearity.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后为特征创建协方差矩阵以理解其多重共线性。
- en: Once the variance-covariance matrix is computed, PCA then uses the gathered
    information to reduce the dimensions. It computes orthogonal axes from the original
    feature axes. These are the axes of directions with maximum variance.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦计算了方差-协方差矩阵，PCA将使用收集到的信息来减少维度。它从原始特征轴计算正交轴。这些是具有最大方差的方向轴。
- en: Firstly the eigenvectors of the variance-covariance matrix are calculated. The
    vector represents the directions of maximum variance which are known as the principal
    components. The eigenvalues are then created that define magnitude of the principal
    components.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先计算方差-协方差矩阵的特征向量。该向量表示最大方差的方向，这些方向称为主成分。然后生成定义主成分大小的特征值。
- en: '*The eigenvalues are the PCA components.*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*特征值是PCA组件。*'
- en: Therefore, for N dimensions, there will be a NxN variance-covariance matrix
    and as a result, we will have a eigen vector of N values and N eigen values matrix.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于N维度，将有一个NxN方差-协方差矩阵，因此，我们将有一个N值和N特征值矩阵的特征向量。
- en: 'We can use following python modules to create the components:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下Python模块来创建组件：
- en: Use linalg.eig to create eigen vectors
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用linalg.eig创建特征向量
- en: Use numpy.cov to compute variance-covariance matrix
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用numpy.cov计算方差-协方差矩阵
- en: We need to take the eigen vectors that represent the our data set best. These
    are the vectors which we have highest eigenvalues.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要选择最能代表数据集的特征向量。这些是具有最高特征值的向量。
- en: '*Take eigen vectors that capture about 70% of the variance.*'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*选择捕捉大约70%方差的特征向量。*'
- en: '*Remember eigenvectors with largest eigenvalues are the ones with highest variance
    and they are closest to the original data set. Also larger the number of eigenvectors,
    slower the computation performance.*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*记住，具有最大特征值的特征向量是方差最高的，并且最接近原始数据集。同时，特征向量数量越多，计算性能越慢。*'
- en: '*I normally take 2–3 top eigen vectors to represent the data set.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*我通常选择2-3个顶级特征向量来代表数据集。*'
- en: 'If we want to keep sci-kit learn to give us all of the PCA components so that
    we can assess the variance then initialise PCA with None components:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望scikit-learn提供所有的PCA组件以便评估方差，则将PCA初始化为None组件：
- en: '![](../Images/7cf98875a6bedcf8791460f87c4e2e29.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7cf98875a6bedcf8791460f87c4e2e29.png)'
- en: '**Photo by [Chang Qing](https://unsplash.com/@lee0201?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**照片由 [Chang Qing](https://unsplash.com/@lee0201?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)**'
- en: '*It is important to normalise/standardise the data before performing PCA because
    PCA is sensitive to the scale of the data in the features.*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*在执行PCA之前，规范化/标准化数据是重要的，因为PCA对特征数据的尺度敏感。*'
- en: 'Kernel principal component analysis (KDA):'
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 核主成分分析 (KDA)：
- en: '*They are used for Nonlinear dimensionality reduction*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*它们用于非线性降维。*'
- en: When we have non-linear features then we can project them onto a larger feature
    set to remove their correlations and to make them linear.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们拥有非线性特征时，可以将它们投影到更大的特征集上，以去除它们的相关性并使其线性。
- en: Essentially, non-linear data is mapped and transformed onto a higher-dimensional
    space. Then PCA is used to reduce the dimensions. However, one downside of this
    approach is that it is computationally very expensive.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，非线性数据被映射和转换到更高维空间。然后使用PCA来减少维度。然而，这种方法的一个缺点是计算开销非常大。
- en: Just like in PCA, we first compute variance-covariance matrix and then eigen
    vectors and eigen values are prepared with the highest variance to compute principal
    components.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在PCA中，我们首先计算方差-协方差矩阵，然后准备具有最高方差的特征向量和特征值来计算主成分。
- en: We then compute kernel matrix. This requires us to construct a similarity matrix.
    The matrix is then decomposed via creating eigen values and eigen vectors.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算核矩阵。这要求我们构建相似性矩阵。矩阵随后通过创建特征值和特征向量进行分解。
- en: 'Sci-Kit learn offers Kernal PCA modules. To use Kernal PCA, we can use following
    snippet of code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Sci-Kit Learn提供了核PCA模块。要使用核PCA，我们可以使用以下代码片段：
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Gamma is a tuning parameter of the RBF kernel.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Gamma是RBF核的调节参数。
- en: '![](../Images/cb6d57451b3efb60222c26792b1d2e47.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb6d57451b3efb60222c26792b1d2e47.png)'
- en: '**Photo by [Billy Huynh](https://unsplash.com/@billy_huy?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**照片由 [Billy Huynh](https://unsplash.com/@billy_huy?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)**'
- en: Benefits Of Dimension Reduction
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 降维的好处
- en: This section briefly outlines the core benefits of reducing dimensions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本节简要概述了降维的核心好处。
- en: We have access to a large set of data now. When we are building forecasting
    models that are trained on images, sound and/or textual contents then the input
    feature sets can end up having a large set of features. It increases space, further
    adds over-fitting and slows down the time to train the models. Occasionally features
    are introduced that end up adding more noise than expected.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以访问大量的数据。当我们构建训练于图像、声音和/或文本内容上的预测模型时，输入特征集可能会有大量的特征。这增加了空间需求，进一步导致过拟合，并减慢了模型训练的时间。有时，特征的引入会带来比预期更多的噪声。
- en: One of the key methodologies to improve efficiency in computational intensive
    tasks is to reduce the dimensions after ensuring most of the key information is
    maintained. It also eliminates features with strong correlation between them and
    reduces over-fitting.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 提高计算密集型任务效率的关键方法之一是减少维度，同时确保大部分关键信息得以保留。这也消除了具有强相关性的特征，并减少了过拟合。
- en: Summary
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总结
- en: This article provided an overview of the techniques which we can follow to compress
    our data set onto a new feature subspace of lower dimensionality. It also provided
    details of important dimension reduction techniques.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本文概述了我们可以遵循的技术，以将数据集压缩到新的低维特征子空间中。它还提供了重要降维技术的详细信息。
- en: Lastly the benefits of dimension reductions were summarised.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，总结了降维的好处。
- en: Please let me know if there are any questions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有任何问题，请告诉我。
- en: '[Original](https://medium.com/fintechexplained/what-is-dimension-reduction-in-data-science-2aa5547f4d29).
    Reposted with permission.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://medium.com/fintechexplained/what-is-dimension-reduction-in-data-science-2aa5547f4d29)。经授权转载。'
- en: '**Resources:**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：**'
- en: '[On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning
    education](https://www.kdnuggets.com/education/online.html)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在线和基于网页的：分析、数据挖掘、数据科学、机器学习教育](https://www.kdnuggets.com/education/online.html)'
- en: '[Software for Analytics, Data Science, Data Mining, and Machine Learning](https://www.kdnuggets.com/software/index.html)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[分析、数据科学、数据挖掘和机器学习的软件](https://www.kdnuggets.com/software/index.html)'
- en: '**Related:**'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[The Data Science Gold Rush: Top Jobs in Data Science and How to Secure Them](https://www.kdnuggets.com/2019/01/top-jobs-data-science.html)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学淘金热：数据科学中的热门职位及如何获取](https://www.kdnuggets.com/2019/01/top-jobs-data-science.html)'
- en: '[Dimensionality Reduction : Does PCA really improve classification outcome?](https://www.kdnuggets.com/2018/07/dimensionality-reduction-pca-improve-classification-results.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[降维 : PCA 是否真的能改善分类结果？](https://www.kdnuggets.com/2018/07/dimensionality-reduction-pca-improve-classification-results.html)'
- en: '[Must-Know: What is the curse of dimensionality?](https://www.kdnuggets.com/2017/04/must-know-curse-dimensionality.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[必知：什么是维度灾难？](https://www.kdnuggets.com/2017/04/must-know-curse-dimensionality.html)'
- en: More On This Topic
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Dimensionality Reduction Techniques in Data Science](https://www.kdnuggets.com/2022/09/dimensionality-reduction-techniques-data-science.html)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学中的降维技术](https://www.kdnuggets.com/2022/09/dimensionality-reduction-techniques-data-science.html)'
- en: '[Stop Learning Data Science to Find Purpose and Find Purpose to…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[停止学习数据科学以寻找目标，并寻找目标以…](https://www.kdnuggets.com/2021/12/stop-learning-data-science-find-purpose.html)'
- en: '[Data Science Minimum: 10 Essential Skills You Need to Know to Start…](https://www.kdnuggets.com/2020/10/data-science-minimum-10-essential-skills.html)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学最低要求：你需要了解的 10 项基本技能](https://www.kdnuggets.com/2020/10/data-science-minimum-10-essential-skills.html)'
- en: '[KDnuggets™ News 22:n06, Feb 9: Data Science Programming…](https://www.kdnuggets.com/2022/n06.html)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets™ 新闻 22:n06, 2 月 9 日：数据科学编程…](https://www.kdnuggets.com/2022/n06.html)'
- en: '[Data Science Definition Humor: A Collection of Quirky Quotes…](https://www.kdnuggets.com/2022/02/data-science-definition-humor.html)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据科学定义幽默：一系列古怪的名言…](https://www.kdnuggets.com/2022/02/data-science-definition-humor.html)'
- en: '[5 Data Science Projects to Learn 5 Critical Data Science Skills](https://www.kdnuggets.com/2022/03/5-data-science-projects-learn-5-critical-data-science-skills.html)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5 个数据科学项目以学习 5 个关键数据科学技能](https://www.kdnuggets.com/2022/03/5-data-science-projects-learn-5-critical-data-science-skills.html)'
