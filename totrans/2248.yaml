- en: Unlock the Secrets to Choosing the Perfect Machine Learning Algorithm!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2023/07/ml-algorithm-choose.html](https://www.kdnuggets.com/2023/07/ml-algorithm-choose.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: One of the key decisions you need to make when solving a data science problem
    is which [machine learning](https://medium.com/p/313730eb5aa2) algorithm to use.
  prefs: []
  type: TYPE_NORMAL
- en: There are hundreds of machine learning algorithms to choose from, each with
    its own advantages and disadvantages. Some algorithms may work better than others
    on specific types of problems or on specific data sets.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The [“No Free Lunch” (NFL) theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem) states
    that there is no one algorithm that works best for every problem, or in other
    words, all algorithms have the same performance when their performance is averaged
    over all the possible problems.
  prefs: []
  type: TYPE_NORMAL
- en: '![Which ML Algorithm to Choose?](../Images/db7baeaa9d1b6dda200b7c6976d98f00.png)'
  prefs: []
  type: TYPE_IMG
- en: Different machine learning models
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will discuss the main points you should consider when choosing
    a model for your problem and how to compare different machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Key Algorithm Aspects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following list contains 10 questions you may ask yourself when considering
    a specific machine-learning algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Which type of problems can the algorithm solve? Can the algorithm solve only
    regression or classification problems, or can it solve both? Can it handle multi-class/multi-label
    problems or only binary classification problems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the algorithm have any assumptions about the data set? For example, some
    algorithms assume that the data is linearly separable (e.g., perceptron or linear
    SVM), while others assume that the data is normally distributed (e.g., Gaussian
    Mixture Models).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are there any guarantees about the performance of the algorithm? For example,
    if the algorithm tries to solve an optimization problem (as in logistic regression
    or neural networks), is it guaranteed to find the global optimum or only a local
    optimum solution?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How much data is needed to train the model effectively? Some algorithms, like
    deep neural networks, are more data-savvy than others.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the algorithm tend to overfit? If so, does the algorithm provide ways to
    deal with overfitting?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the runtime and memory requirements of the algorithm, both during training
    and prediction time?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which data preprocessing steps are required to prepare the data for the algorithm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How many hyperparameters does the algorithm have? Algorithms that have a lot
    of hyperparameters take more time to train and tune.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can the results of the algorithm be easily interpreted? In many problem domains
    (such as medical diagnosis), we would like to be able to explain the model’s predictions
    in human terms. Some models can be easily visualized (such as decision trees),
    while others behave more like a black box (e.g., neural networks).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the algorithm support online (incremental) learning, i.e., can we train
    it on additional samples without rebuilding the model from scratch?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Algorithm Comparison Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For example, let’s take two of the most popular algorithms: [decision trees](https://medium.com/@roiyeho/decision-trees-part-1-da4e613d2369) and [neural
    networks](https://medium.com/@roiyeho/perceptrons-the-first-neural-network-model-8b3ee4513757),
    and compare them according to the above criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decision trees can handle both classification and regression problems. They
    can also easily handle multi-class and multi-label problems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decision tree algorithms do not have any specific assumptions about the data
    set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A decision tree is built using a greedy algorithm, which is not guaranteed to
    find the optimal tree (i.e., the tree that minimizes the number of tests required
    to classify all the training samples correctly). However, a decision tree can
    achieve 100% accuracy on the training set if we keep extending its nodes until
    all the samples in the leaf nodes belong to the same class. Such trees are usually
    not good predictors, as they overfit the noise in the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decision trees can work well even on small or medium-sized data sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decision trees can easily overfit. However, we can reduce overfitting by using
    tree pruning. We can also use [ensemble methods](https://medium.com/@roiyeho/introduction-to-ensemble-methods-226a5a421687) such
    as random forests that combine the output of multiple decision trees. These methods
    suffer less from overfitting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The time to build a decision tree is *O*(*n*²*p*), where n is the number of
    training samples, and *p* is the number of features. The prediction time in decision
    trees depends on the height of the tree, which is usually logarithmic in *n*,
    since most decision trees are fairly balanced.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decision trees do not require any data preprocessing. They can seamlessly handle
    different types of features, including numerical and categorical features. They
    also do not require normalization of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decision trees have several key hyperparameters that need to be tuned, especially
    if you are using pruning, such as the maximum depth of the tree and which impurity
    measure to use to decide how to split the nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decision trees are simple to understand and interpret, and we can easily visualize
    them (unless the tree is very large).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decision trees cannot be easily modified to take into account new training samples
    since small changes in the data set can cause large changes in the topology of
    the tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural networks are one of the most general and flexible machine learning models
    that exist. They can solve almost any type of problem, including classification,
    regression, time series analysis, automatic content generation, etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Neural networks do not have assumptions about the data set, but the data needs
    to be normalized.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Neural networks are trained using gradient descent. Thus, they can only find
    a local optimum solution. However, there are various techniques that can be used
    to avoid getting stuck in local minima, such as momentum and adaptive learning
    rates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deep neural nets require a lot of data to train in the order of millions of
    sample points. In general, the larger the network is (the more layers and neurons
    it has), more we need data to train it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Networks that are too large might memorize all the training samples and not
    generalize well. For many problems, you can start from a small network (e.g.,
    with only one or two hidden layers) and gradually increase its size until you
    start overfitting the training set. You can also add [regularization](https://medium.com/@roiyeho/regularization-19b1879415a1) in
    order to deal with overfitting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The training time of a neural network depends on many factors (the size of the
    network, the number of gradient descent iterations needed to train it, etc.).
    However, prediction time is very fast since we only need to do one forward pass
    over the network to get the label.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Neural networks require all the features to be numerical and normalized.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Neural networks have a lot of hyperparameters that need to be tuned, such as
    the number of layers, the number of neurons in each layer, which activation function
    to use, the learning rate, etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The predictions of neural networks are hard to interpret as they are based on
    the computation of a large number of neurons, each of which has only a small contribution
    to the final prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Neural networks can easily adapt to include additional training samples, as
    they use an incremental learning algorithm (stochastic gradient descent).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Time Complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following table compares the training and prediction times of some popular
    algorithms (*n* is the number of training samples and *p* is the number of features).
  prefs: []
  type: TYPE_NORMAL
- en: '![Which ML Algorithm to Choose?](../Images/c71fb69ae4ca01cad051997816b89691.png)'
  prefs: []
  type: TYPE_IMG
- en: Most Successful Algorithms in Kaggle Competitions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to a survey that was done in 2016, the most frequently used algorithms
    by Kaggle competition winners were gradient boosting algorithms (XGBoost) and
    neural networks (see [this article](https://www.kaggle.com/code/msjgriffiths/r-what-algorithms-are-most-successful-on-kaggle/report?scriptVersionId=0)).
  prefs: []
  type: TYPE_NORMAL
- en: Amongst the 29 Kaggle competition winners in 2015, 8 of them used XGBoost, 9
    used deep neural nets, and 11 used an ensemble of both.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost was mainly used in problems that dealt with structured data (e.g., relational
    tables), whereas neural networks were more successful in handling unstructured
    problems (e.g., problems that deal with image, voice, or text).
  prefs: []
  type: TYPE_NORMAL
- en: It would be interesting to check if this is still the situation today or whether
    the trends have changed (is anyone up for the challenge?)
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: '**[Dr. Roi Yehoshua](https://www.linkedin.com/in/roi-yehoshua/)** is a teaching
    professor at Northeastern University in Boston, teaching classes that make up
    the Master''s program in Data Science. His research in multi-robot systems and
    reinforcement learning has been published in the top leading journals and conferences
    in AI. He is also a top writer on the Medium social platform, where he frequently
    publishes articles on Data Science and Machine Learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://medium.com/towards-artificial-intelligence/which-ml-algorithm-to-choose-f9caf674219e).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Unlock the Secrets of LLMs in 60-Minute with Andrej Karpathy](https://www.kdnuggets.com/unlock-the-secrets-of-llms-in-a-60-minute-with-andrej-karpathy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Choosing the Right Clustering Algorithm for Your Dataset](https://www.kdnuggets.com/2019/10/right-clustering-algorithm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unlock the Power of AI - A Special Release by KDnuggets and Machine…](https://www.kdnuggets.com/2023/07/mlm-unlock-power-ai-special-release-kdnuggets-machine-learning-mastery.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unlock Your Potential with This FREE DevOps Crash Course](https://www.kdnuggets.com/2023/03/corise-unlock-potential-with-this-free-devops-crash-course.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ChatGPT-Powered Data Exploration: Unlock Hidden Insights in Your Dataset](https://www.kdnuggets.com/2023/07/chatgptpowered-data-exploration-unlock-hidden-insights-dataset.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Unlock your next move: Save up to 67% on in-demand data upskilling](https://www.kdnuggets.com/2023/03/datacamp-unlock-next-move-save-67-indemand-data-upskilling.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
