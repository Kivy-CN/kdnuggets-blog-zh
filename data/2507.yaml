- en: Getting Started with PyTorch Lightning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 PyTorch Lightning
- en: 原文：[https://www.kdnuggets.com/2021/10/getting-started-pytorch-lightning.html](https://www.kdnuggets.com/2021/10/getting-started-pytorch-lightning.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2021/10/getting-started-pytorch-lightning.html](https://www.kdnuggets.com/2021/10/getting-started-pytorch-lightning.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '![Getting Started with PyTorch Lightning](../Images/d90c82e8ba6cac010727d13ce27b16a8.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![开始使用 PyTorch Lightning](../Images/d90c82e8ba6cac010727d13ce27b16a8.png)'
- en: '**Getting Started with PyTorch Lightning: a High-Level Library for High Performance
    Research**'
  id: totrans-4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**开始使用 PyTorch Lightning：一个高性能研究的高级库**'
- en: Libraries like TensorFlow and PyTorch take care of most of the intricacies of
    building deep learning models that train and infer fast. Predictably, this leaves
    machine learning engineers spending most of their time on the next level up in
    abstraction, running hyperparameter search, validating performance, and versioning
    models and experiments to keep track of everything.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 像 TensorFlow 和 PyTorch 这样的库处理了大部分构建深度学习模型的复杂细节，这些模型训练和推理速度快。可以预见，这使得机器学习工程师将大部分时间花在更高抽象层级的工作上，如运行超参数搜索、验证性能，以及对模型和实验进行版本控制以跟踪一切。
- en: There’s a lot more to deep learning than just gluing some layers together.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习不仅仅是将一些层粘在一起。
- en: If PyTorch and TensorFlow (and now JAX) are the deep learning cake, higher-level
    libraries are the icing. For years now TensorFlow has had its “icing on the cake”
    in the high-level Keras API, which became an official part of TensorFlow itself
    with the release of TF 2.0 in 2019\. Similarly, PyTorch users have benefited from
    the high-level fastai library, which is exceptionally well-suited for efficiency
    and transfer learning. This makes fastai a favorite of successful data scientists
    on the Kaggle contest platform. More recently, another streamlined wrapper for
    PyTorch has been quickly gaining steam in the aptly named [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 PyTorch 和 TensorFlow（现在还有 JAX）是深度学习的“蛋糕”，那么高级库就是“糖霜”。多年来，TensorFlow 一直拥有其“糖霜”——高级
    Keras API，这在 2019 年 TF 2.0 发布时成为 TensorFlow 本身的一部分。类似地，PyTorch 用户也受益于高级的 fastai
    库，这对于高效性和迁移学习非常合适，使 fastai 成为 Kaggle 比赛平台上成功数据科学家的最爱。最近，另一个针对 PyTorch 的简化封装在名为
    [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) 的库中迅速获得了关注。
- en: PyTorch Lighting has actually been around, at least in some capacity, since
    2019\. It started as a sort of side project undertaken by William Falcon during
    his PhD research at New York University. By the time 2020 rolled around (and we
    mean the 2020 that started in March) PyTorch Lightning was no longer just a personal
    project as Falcon[ announced venture funding](https://medium.com/pytorch/pytorch-lightning-0-7-1-release-and-venture-funding-dd12b2e75fb3).
    Around the same time the open source (under the Apache 2.0 License) repository
    moved from Falcon’s personal GitHub profile to its own dedicated profile. As of
    this writing PyTorch Lightning has grown to over 15,000 stars and nearly 2,000
    forks, becoming nearly as popular as[ fastai](https://github.com/fastai/fastai) (which
    has over 21,000 stars) and handily more popular than the in-house high-level library
    from PyTorch,[ Ignite](https://github.com/pytorch/ignite), which has about 4,000
    stars!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Lightning 实际上自 2019 年以来就存在了，至少在某种程度上。它起初是威廉·法尔孔（William Falcon）在纽约大学博士研究期间的一个副项目。到
    2020 年（我们指的是从三月开始的 2020 年）时，PyTorch Lightning 不再只是一个个人项目，因为法尔孔[宣布了风险投资](https://medium.com/pytorch/pytorch-lightning-0-7-1-release-and-venture-funding-dd12b2e75fb3)。与此同时，开源（根据
    Apache 2.0 许可）代码库也从法尔孔的个人 GitHub 账户迁移到了自己的专用账户。截至目前，PyTorch Lightning 已经成长为超过
    15,000 个星标和近 2,000 个分支，几乎与[fastai](https://github.com/fastai/fastai)（拥有超过 21,000
    个星标）一样受欢迎，明显比 PyTorch 的内部高级库[Ignite](https://github.com/pytorch/ignite)（拥有约 4,000
    个星标）更受欢迎！
- en: Where fastai was designed to facilitate the inaugural fastai course,[ Practical
    Deep Learning for Coders](https://course.fast.ai/), PyTorch Lightning is intended
    to streamline production research. Fastai has a focus on transfer learning and
    efficiency and its ease of use has made it a popular high-level library on the
    Kaggle data science competition platform, with over[ 4,500 notebooks](https://www.kaggle.com/search?q=fastai) referencing
    the library. Compare that to just[ over 100](https://www.kaggle.com/search?q=ignite) notebook
    results referring to PyTorch Ignite, and[ about 500](https://www.kaggle.com/search?q=PyTorch+Lightning) for
    PyTorch Lightning. PyTorch Lightning is a relatively newer library, but it also
    targets a different demographic. PyTorch Lightning streamlines the engineering
    aspects of developing a new model, such as logging, validation and hooks, and
    it’s targeted toward machine learning researchers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: fastai 旨在方便首个 fastai 课程，[Practical Deep Learning for Coders](https://course.fast.ai/)，而
    PyTorch Lightning 则旨在简化生产研究。Fastai 专注于迁移学习和效率，其易用性使其成为 Kaggle 数据科学竞赛平台上流行的高级库，已有超过[4,500
    个笔记本](https://www.kaggle.com/search?q=fastai)引用该库。相比之下，PyTorch Ignite 只有[100 多个](https://www.kaggle.com/search?q=ignite)笔记本结果，而
    PyTorch Lightning 约有[500 个](https://www.kaggle.com/search?q=PyTorch+Lightning)。PyTorch
    Lightning 是一个相对较新的库，但它也面向不同的受众。PyTorch Lightning 简化了开发新模型的工程方面，例如日志记录、验证和钩子，它的目标是机器学习研究人员。
- en: Research is all about answering falsifying questions, and in this tutorial we’ll
    take a look at what PyTorch Lightning can do for us to make that process easier.
    We’ll set up a simple mock research question of whether there is any advantage
    to using a “fancy” activation function (such as the so-called[ swish function](https://en.wikipedia.org/wiki/Swish_function))
    versus a more standard rectified linear unit ([ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)).
    We’ll use the vanishingly small (in terms of both number of samples and image
    size) digits dataset from SciKit-Learn to set up our experiment. Starting with
    digits should make this an accessible project for someone running the code on
    an efficient laptop, but readers are encouraged to swap in a more realistic images
    dataset like CIFAR10 for extra credit.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 研究就是回答和证伪问题，在本教程中，我们将探讨 PyTorch Lightning 能为我们做些什么以简化这一过程。我们将设置一个简单的模拟研究问题：使用一种“华丽”的激活函数（例如所谓的
    [swish function](https://en.wikipedia.org/wiki/Swish_function)）是否比使用更标准的修正线性单元（[ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))）具有优势。我们将使用
    SciKit-Learn 的极小（无论是样本数量还是图像大小）数字数据集来设置我们的实验。从数字开始应该使这个项目对在高效笔记本上运行代码的人来说较为可及，但鼓励读者使用像
    CIFAR10 这样的更实际的图像数据集以获得额外积分。
- en: As a library designed for production research, PyTorch Lightning streamlines
    hardware support and distributed training as well, and we’ll show how easy it
    is to move training to a GPU toward the end.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个旨在生产研究的库，PyTorch Lightning 也简化了硬件支持和分布式训练，我们将在最后展示将训练转移到 GPU 的简便性。
- en: '****Getting Started: Installing PyTorch Lightning****'
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '****开始使用：安装 PyTorch Lightning****'
- en: 'Like many Python projects these days, PyTorch Lightning installs easily using
    pip, and we recommend using your favorite virtual environment manager to manage
    installs and dependencies without cluttering up your base Python installation.
    We’ll provide three examples, the first of which is using `virtualenv` and pip,
    and we are assuming you are using a Unix-style command line on Linux or Mac, or
    that you are savvy enough to adapt the examples for Windows using something like
    Git Bash or Anaconda Prompt. After navigating to the project folder for this tutorial:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 像许多现代 Python 项目一样，PyTorch Lightning 可以通过 pip 轻松安装，我们建议使用你喜欢的虚拟环境管理工具来管理安装和依赖项，而不会使你的基础
    Python 安装变得杂乱。我们将提供三个示例，第一个是使用 `virtualenv` 和 pip，我们假设你正在使用 Linux 或 Mac 上的 Unix
    风格命令行，或者你足够熟练，可以使用类似 Git Bash 或 Anaconda Prompt 的工具来适应 Windows。导航到本教程的项目文件夹后：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can also use[ Anaconda](https://www.anaconda.com/) to manage your virtual
    environment:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用 [Anaconda](https://www.anaconda.com/) 来管理你的虚拟环境：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Or even combine the two, creating a new anaconda environment and then using
    pipt o install packages. For more general usage there are some[ caveats](https://www.anaconda.com/blog/using-pip-in-a-conda-environment) to
    using pip and Anaconda together, but for purposes of this tutorial it should be
    fine:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 或者甚至将两者结合起来，创建一个新的 Anaconda 环境，然后使用 pip 安装包。对于更一般的使用，有一些[警告](https://www.anaconda.com/blog/using-pip-in-a-conda-environment)涉及到
    pip 和 Anaconda 的结合使用，但对于本教程的目的应该没问题：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Using PyTorch Lightning
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 PyTorch Lightning
- en: The design strategy employed by PyTorch Lightning revolves around the LightningModule
    class. This class, itself inheriting from the `pytorch.nn.Module` class, provides
    a convenient entry point and attempts to organize as much of the training and
    validation process as possible all in one place.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Lightning 所采用的设计策略围绕 LightningModule 类展开。该类本身继承自 `pytorch.nn.Module`
    类，提供了一个方便的入口，并尝试将尽可能多的训练和验证过程组织在一个地方。
- en: A key feature of this strategy is that the contents of a typical training and
    validation loop is instead defined in the model itself, accessible via a `fit` API
    very similar to keras, fastai, or even SciKit-Learn. Unlike those other examples
    where `fit` is accessed through the model itself, in PyTorch Lightning `fit` is
    accessed via a Trainer object. But that’s getting ahead of ourselves, first let’s
    set the stage for our experiment by importing everything we’ll need.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 该策略的一个关键特性是，典型的训练和验证循环的内容被定义在模型本身中，可以通过类似于 keras、fastai 或 SciKit-Learn 的 `fit`
    API 访问。与其他示例中通过模型本身访问 `fit` 不同，在 PyTorch Lightning 中，`fit` 是通过 Trainer 对象访问的。但这还要再往后说，首先让我们通过导入我们需要的所有东西来为实验做准备。
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then we can go ahead and define our model:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以继续定义我们的模型：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Notably, training functionality is devolved to the module itself in the `training_step` function.
    Most ML practitioners having some practice with PyTorch will already be quite
    familiar with the practice of overloading the `forward` function, and LightningModule
    objects have many more methods to overload for fine-grained control of the relatively
    painless logging and evaluation features that are built-in.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，训练功能被委托给 `training_step` 函数中的模块本身。大多数有一些 PyTorch 实践经验的机器学习从业者已经对重载 `forward`
    函数相当熟悉，而 LightningModule 对象还有许多其他方法可以重载，以实现对内置的相对轻松的日志记录和评估功能的精细控制。
- en: The code that defines our `MyClassifier` model class might seem pretty verbose,
    but this strategy massively simplifies things when it’s time to actually start
    training, which we’ll see later. There are plenty of other callbacks and functions
    that are included in the `LightningModule` class, and all of them can be overloaded
    for more fine-tuned control. A full list of these callbacks can be found[ in the
    PyTorch Lightning documentation.](https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html#hooks)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 定义我们 `MyClassifier` 模型类的代码可能看起来比较冗长，但这种策略在实际开始训练时大大简化了操作，我们稍后会看到。`LightningModule`
    类中包含了许多其他的回调和函数，所有这些都可以被重载以实现更精细的控制。完整的回调列表可以在[PyTorch Lightning 文档中找到](https://pytorch-lightning.readthedocs.io/en/latest/extensions/callbacks.html#hooks)。
- en: For this tutorial, we’ll also define a `torch.utils.data.Dataset` object to
    wrap the digits dataset from SciKit-Learn. This should make it easy to rapidly
    get everything working before switching to a larger and more informative dataset
    like MNIST or CIFAR10.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们还将定义一个`torch.utils.data.Dataset`对象，以封装来自 SciKit-Learn 的数字数据集。这应该能让我们在切换到更大且信息量更丰富的数据集（如
    MNIST 或 CIFAR10）之前，快速使一切正常运行。
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'With all that out of the way, actually launching a training run becomes incredibly
    simple. All we have to do is create a dataset and feed it into a `DataLoader`,
    instantiate our model, create a PyTorch Lightning `Trainer` object, and call the
    trainer’s fit method. Here’s a simplified version:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 处理完这些之后，实际启动训练过程变得极其简单。我们只需创建一个数据集并将其输入到`DataLoader`中，实例化我们的模型，创建一个 PyTorch
    Lightning `Trainer` 对象，然后调用训练器的 fit 方法。以下是一个简化版本：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: But of course we will want to continuously log validation metrics throughout
    the training process, making use of the `validation_step` and `validation_epoch_end` methods
    we overloaded in our model. Here’s the actual code I use to launch a training
    run, using the `if __name__ == "__main__":` pattern that provides a simple entry
    point for running a Python file as a module.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们还希望在整个训练过程中持续记录验证指标，利用我们在模型中重载的 `validation_step` 和 `validation_epoch_end`
    方法。以下是我用来启动训练过程的实际代码，使用了 `if __name__ == "__main__":` 模式，为以模块形式运行 Python 文件提供了简单的入口点。
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: When you run the code above, you should see a progress bar displayed in your
    terminal that looks something like the one below.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行上述代码时，你应该会在终端中看到一个进度条，类似于下面的样子。
- en: '![pl-terminal-1.png](../Images/28210cdaae631256933e335d905b6fb6.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![pl-terminal-1.png](../Images/28210cdaae631256933e335d905b6fb6.png)'
- en: After allowing training to run for a while, have a look in your working directory
    and you’ll notice a new folder called **lightning_logs**. This is where PyTorch
    Lightning records your training sessions, and you can quickly boot up a Tensorboard
    session to see how things are going. After launching tensorboard with the line
    below, use a browser to navigate to localhost:6006 (by default) to open up the
    dashboard.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在允许训练运行一段时间后，查看你的工作目录，你会注意到一个名为**lightning_logs**的新文件夹。这是 PyTorch Lightning
    记录训练会话的地方，你可以快速启动 Tensorboard 会话来查看进展。在使用下面的命令启动 tensorboard 后，使用浏览器导航到 localhost:6006（默认）以打开仪表板。
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If it took you a few starts and stops to get training to take off, you’ll notice
    a list of training runs displayed in the left sidebar with version_0, version_1,
    version_2 and so on. PyTorch Lightning automatically versions your training runs
    this way, so it should be pretty easy to compare a few different experimental
    conditions or random seeds..
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你花了一些时间才使训练顺利进行，你会注意到左侧边栏中显示了一个训练运行的列表，其中有 version_0、version_1、version_2 等等。PyTorch
    Lightning 自动对你的训练运行进行版本控制，因此比较几种不同的实验条件或随机种子应该是非常容易的。
- en: For example, if we wanted to run our little experiment comparing the efficacy
    of using Swish versus ReLU activations, we can use the code below.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想要运行一个小实验来比较使用 Swish 和 ReLU 激活函数的效果，我们可以使用下面的代码。
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: And after running our little experiment we’ll find our results nicely logged
    for our perusal in Tensorboard.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行我们的实验后，我们会发现结果被很好地记录在 Tensorboard 中，供我们查阅。
- en: '![pl-tensorboard-1.png](../Images/60cae6ec1ddb4e9612d2f76104311e20.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![pl-tensorboard-1.png](../Images/60cae6ec1ddb4e9612d2f76104311e20.png)'
- en: You’ll probably notice we have the option to run training on the much larger
    MNIST dataset. At 60,000 training samples of 28 by 28 pixel images, it’s closer
    to a useful real-world dataset than the miniaturized sklearn digits dataset, which
    provides fewer than 2,000 samples of 8 by 8 images. However, you probably won’t
    want to run 6 replicate training runs on the MNIST dataset using an underpowered
    laptop CPU, so we’ll want to move everything over to a GPU first.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到我们有选项在更大的 MNIST 数据集上进行训练。MNIST 数据集包含 60,000 个 28 x 28 像素的图像训练样本，比起提供不到
    2,000 个 8 x 8 图像样本的微型 sklearn digits 数据集，它更接近于一个有用的现实世界数据集。然而，你可能不想在性能不足的笔记本电脑
    CPU 上运行 6 次重复训练，因此我们首先要将所有内容转移到 GPU 上。
- en: If you are already used to building experiments and training pipelines in standard
    PyTorch from scratch, you probably know the frustration of a forgotten tensor
    languishing on a CPU device, and the show-stopping errors they generate. It’s
    usually an easy fix, but frustrating nonetheless.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经习惯从零开始在标准 PyTorch 中构建实验和训练管道，你可能知道被遗忘的张量滞留在 CPU 设备上的挫败感，以及它们产生的致命错误。这通常很容易解决，但依然令人沮丧。
- en: '**Using a GPU for Training**'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**使用 GPU 进行训练**'
- en: 'If you’re working with a machine with an available GPU, you can easily use
    it to train. To launch training on the GPU instead of the CPU, we’ll have to modify
    some of the code:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用一台可用 GPU 的机器，你可以轻松地使用它进行训练。为了在 GPU 上启动训练而不是 CPU，我们需要修改一些代码：
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: That’s right, by modifying a single line of code defining the trainer object
    we can run training on the GPU. No worrying about forsaken tensors and with all
    the convenience of logging and validation we built into our original model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 没错，通过修改定义训练器对象的一行代码，我们可以在 GPU 上运行训练。不用担心被遗忘的张量，并且拥有我们在原始模型中构建的日志记录和验证的所有便利。
- en: '![pl-tensorboard-2.png](../Images/040ed16ce6762f52159139f7c5ae3211.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![pl-tensorboard-2.png](../Images/040ed16ce6762f52159139f7c5ae3211.png)'
- en: '****Next Steps****'
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '****下一步****'
- en: A striking aspect of working with PyTorch Lightning is that it seems to get
    easier the further along you go. Defining our `MyClassifer` model was a little
    more complicated than a model of similar complexity sub-classed from `torch.nn.Module` up
    front, but once we had training, validation, and logging all taken care of by
    the `LightningModule` model, every subsequent step was easier than it would have
    been normally.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch Lightning 的一个显著特点是，随着你不断深入，它似乎变得越来越容易。定义我们的`MyClassifer`模型比从`torch.nn.Module`子类化的相似复杂度模型稍微复杂一些，但一旦训练、验证和日志记录都由`LightningModule`模型处理，之后的每一步都比正常情况要简单。
- en: PyTorch Lightning also makes managing hardware a breeze, and we caught a glimpse
    of just how simple this is when we switched to training MNIST on a GPU. PyTorch
    Lightning also readily facilitates training on more esoteric hardware like Google’s
    Tensor Processing Units, and on multiple GPUs, and it is being developed in parallel
    alongside [Grid](https://www.grid.ai/), a cloud platform for scaling up experiments
    using PyTorch Lightning, and [Lightning Bolts](https://www.pytorchlightning.ai/bolts) a
    modular toolbox of deep learning examples driven by the PyTorch Lightning community.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Lightning 还简化了硬件管理，我们在将 MNIST 训练切换到 GPU 时，简单性一目了然。PyTorch Lightning
    还方便地支持更为冷门的硬件，如谷歌的张量处理单元（TPU），以及多个 GPU，并且它与 [Grid](https://www.grid.ai/)、一个用于扩展使用
    PyTorch Lightning 的实验的云平台，和 [Lightning Bolts](https://www.pytorchlightning.ai/bolts)，一个由
    PyTorch Lightning 社区驱动的深度学习示例模块工具箱，正在同步开发。
- en: That covers our “Hello, World” introduction to PyTorch Lightning, but we’ve
    barely scratched the surface of what Lightning intends to deliver to your deep
    learning workflow.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这涵盖了我们对 PyTorch Lightning 的“Hello, World”介绍，但我们仅仅触及了 Lightning 打算为你的深度学习工作流提供的表面。
- en: '**In our next PyTorch Lightning tutorial, we’ll dive into two complementary
    PyTorch Lightning libraries: Lightning Flash and TorchMetrics.** TorchMetrics
    unsurprisingly provides a modular approach to define and track useful metrics
    across batches and devices, while Lightning Flash offers a suite of functionality
    facilitating more efficient transfer learning and data handling, and a recipe
    book of state-of-the-art approaches to typical deep learning problems.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**在我们的下一个 PyTorch Lightning 教程中，我们将深入探讨两个互补的 PyTorch Lightning 库：Lightning
    Flash 和 TorchMetrics。** TorchMetrics 提供了一个模块化的方法来定义和跟踪批次和设备上的有用指标，而 Lightning
    Flash 提供了一整套功能，促进了更高效的迁移学习和数据处理，并且包含了应对典型深度学习问题的最新方法的食谱。'
- en: '**Now, on to our next PyTorch Lightning tutorial:**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**接下来，我们将进入下一节 PyTorch Lightning 教程：**'
- en: '[PyTorch Lightning Tutorial #2: Using TorchMetrics and Lightning Flash](https://www.exxactcorp.com/blog/Deep-Learning/advanced-pytorch-lightning-using-torchmetrics-and-lightning-flash)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[PyTorch Lightning 教程 #2：使用 TorchMetrics 和 Lightning Flash](https://www.exxactcorp.com/blog/Deep-Learning/advanced-pytorch-lightning-using-torchmetrics-and-lightning-flash)'
- en: '**Bio: [Kevin Vu](https://www.kdnuggets.com/author/kevin-vu)** manages Exxact
    Corp blog and works with many of its talented authors who write about different
    aspects of Deep Learning.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介：[Kevin Vu](https://www.kdnuggets.com/author/kevin-vu)** 负责管理 Exxact Corp
    博客，并与许多才华横溢的作者合作，这些作者撰写关于深度学习不同方面的文章。'
- en: '[Original](https://www.exxactcorp.com/blog/Deep-Learning/getting-started-with-pytorch-lightning).
    Reposted with permission.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[原文](https://www.exxactcorp.com/blog/Deep-Learning/getting-started-with-pytorch-lightning)。转载已获许可。'
- en: '**Related:**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关：**'
- en: '[Introduction to PyTorch Lightning](/2021/10/introduction-pytorch-lightning.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch Lightning 介绍](/2021/10/introduction-pytorch-lightning.html)'
- en: '[How to deploy PyTorch Lightning models to production](/2020/11/deploy-pytorch-lightning-models-production.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何将 PyTorch Lightning 模型部署到生产环境](/2020/11/deploy-pytorch-lightning-models-production.html)'
- en: '[PyTorch Multi-GPU Metrics Library and More in New PyTorch Lightning Release](/2020/07/pytorch-multi-gpu-metrics-library-pytorch-lightning.html)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch 多 GPU 指标库及更多内容的新 PyTorch Lightning 版本](/2020/07/pytorch-multi-gpu-metrics-library-pytorch-lightning.html)'
- en: '* * *'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析能力'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的 IT'
- en: '* * *'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: More On This Topic
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Getting Started with PyTorch Lightning](https://www.kdnuggets.com/2022/12/getting-started-pytorch-lightning.html)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开始使用 PyTorch Lightning](https://www.kdnuggets.com/2022/12/getting-started-pytorch-lightning.html)'
- en: '[Introduction to Deep Learning Libraries: PyTorch and Lightning AI](https://www.kdnuggets.com/introduction-to-deep-learning-libraries-pytorch-and-lightning-ai)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深度学习库简介：PyTorch和Lightning AI](https://www.kdnuggets.com/introduction-to-deep-learning-libraries-pytorch-and-lightning-ai)'
- en: '[Getting Started with PyTorch in 5 Steps](https://www.kdnuggets.com/5-steps-getting-started-pytorch)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5步开始使用PyTorch](https://www.kdnuggets.com/5-steps-getting-started-pytorch)'
- en: '[Using Lightning AI Studio For Free](https://www.kdnuggets.com/using-lightning-ai-studio-for-free)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[免费使用Lightning AI Studio](https://www.kdnuggets.com/using-lightning-ai-studio-for-free)'
- en: '[Getting Started with SQL Cheatsheet](https://www.kdnuggets.com/2022/08/getting-started-sql-cheatsheet.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQL备忘单入门](https://www.kdnuggets.com/2022/08/getting-started-sql-cheatsheet.html)'
- en: '[Getting Started with PyCaret](https://www.kdnuggets.com/2022/11/getting-started-pycaret.html)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开始使用PyCaret](https://www.kdnuggets.com/2022/11/getting-started-pycaret.html)'
