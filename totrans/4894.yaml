- en: Is Learning Rate Useful in Artificial Neural Networks?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2018/01/learning-rate-useful-neural-network.html](https://www.kdnuggets.com/2018/01/learning-rate-useful-neural-network.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)'
  prefs: []
  type: TYPE_IMG
- en: This article will help you understand why we need the learning rate and whether
    it is useful or not for training an artificial neural network. Using a very simple
    Python code for a single layer perceptron, the learning rate value will get changed
    to catch its idea.
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  prefs: []
  type: TYPE_NORMAL
- en: An obstacle for newbies in artificial neural networks is the learning rate.
    I was asked many times about the effect of the learning rate in the training of
    the artificial neural networks (ANNs). Why we use learning rate? What is the best
    value for the learning rate? In this article, I will try to make things simpler
    by providing an example that shows how learning rate is useful in order to train
    an ANN. I will start by explaining our example with Python code before working
    with the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**'
  prefs: []
  type: TYPE_NORMAL
- en: A very very simple example is used to get us out of complexity and allow us
    to just focus on the learning rate. A single numerical input will get applied
    to a single layer perceptron. If the input is 250 or smaller, its value will get
    returned as the output of the network. If the input is larger than 250, then it
    will be clipped to just 250\. The following table shows the 6 samples used for
    training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bdcb17c10de3c2ec4e956f201ac6d85a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**ANN Architecture**'
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of the ANN used is shown in the next figure. There are just
    input and output layers. The input layer has just a single neuron for our single
    input. The output layer has just a single neuron for generating the output. The
    output layer neuron is responsible for mapping the input to the correct output.
    There is also a bias applied to the output layer neuron with weight ***b*** and
    value ***+1***. There is also a weight ***W*** for the input.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94a9409e811a7869b85ea922f5cc67ea.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Activation Function**'
  prefs: []
  type: TYPE_NORMAL
- en: The equation and the graph of the activation function used in this example are
    as shown in the next figure. When the input is below or equal to 250, the output
    will be the same as the input. Otherwise, it will be clipped to 250.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97066b4183b4e99264208931ddd63d88.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Implementation using Python**'
  prefs: []
  type: TYPE_NORMAL
- en: The Python code implementing the entire network is shown below. We will discuss
    all of it until making it easy as much as possible then focus on changing the
    learning rate to find out how it affects the network training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Lines 17 and 18 are responsible for creating two arrays (inputs and desired_output)
    holding the training input and output data presented in the previous table. Each
    input will have an output according to the activation function used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Line 16 creates an array of the network weights. There are just two weights:
    one for the input and another for the bias. They were randomly initialized to
    0.05 for the bias and 0.1 for the input.'
  prefs: []
  type: TYPE_NORMAL
- en: The activation function itself is implemented using the activation_function(inpt)
    method from line 3 to 7\. It accepts a single argument which is the input and
    returns a single value which is the expected output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because there may be an error in the prediction, we need to measure that error
    to know how far we are from the correct prediction. For that reason, there is
    a method implemented from line 9 to 10 called prediction_error(desired, expected)
    that accepts two inputs: the desired and expected outputs. That method just calculates
    the absolute difference between each desired and expected outputs. The best value
    for any error is for sure 0\. This is the optimal value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What if there was a prediction error? In this case, we must make a change to
    the network. But what exactly to change? It is the network weights. For updating
    the network weights, there is a method called update_weights(weights, predicted, idx)
    defined from line 13 to 14\. It accepts three inputs: old weights, predicted output,
    and the index of the input that has a false prediction. This method applies the
    following equation to update the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc8b0e0c701ece674dbc601279191261.png)'
  prefs: []
  type: TYPE_IMG
- en: The equation uses the weights of the current step (n) to generate the weights
    of the next step (n+1). This equation is what we will use for knowing how the
    learning rate affects the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we need to concatenate all of these together to make the network learn.
    This is done using the training_loop(inpt, weights) method defined from line 20
    to 31\. It goes into a training loop. The loop is used to map the inputs to their
    outputs with the least possible prediction error.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loop does three operations:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.      Output Prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 2.      Error Calculation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.      Updating Weights.
  prefs: []
  type: TYPE_NORMAL
- en: After getting the idea of the example and its Python code, let us start showing
    how the learning rate is useful in order to get the best results.
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning Rate**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previously discussed example, line 13 has the weights update equation
    in which the learning rate is used. At first, let us assume that we have not used
    the learning rate completely. The equation will as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let us see the effect of removing the learning rate. In the iteration of the
    training loop, the network has the following inputs (b=0.05 and W=0.1, Input =
    60, and desired output=60).
  prefs: []
  type: TYPE_NORMAL
- en: The expected output which is the result of the activation function as in line
    25 will be activation_function(0.05(+1) + 0.1(60)). The predicted output will
    be 6.05.
  prefs: []
  type: TYPE_NORMAL
- en: In line 26, the prediction error will be calculated by getting the difference
    between the desired and the predicted output. The error will be abs(60-6.05)=53.95.
  prefs: []
  type: TYPE_NORMAL
- en: Then in line 27 the weights will get updated according to the above equation.
    The new weights will be [0.05, 0.1] + (53.95)*60 = [0.05, 0.1] + 3237 = [3237.05,
    3237.1].
  prefs: []
  type: TYPE_NORMAL
- en: It seems that the new weights are too different from the previous weights. Each
    weight got increased by 3,237 which is too large. But let us continue making the
    next prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next iteration, the network will have these inputs applied: (b=3237.05
    and W=3237.1, Input = 40, and desired output=40). The expected output will be
    activation_function((3237.05 + 3237.1(40)) = 250\. The prediction error will be
    abs(40 - 250) = 210\. The error is very large. It is larger than the previous
    error. Thus we have to update the weights again. According to the above equation,
    the new weights will be [3237.05, 3237.1] + (-210)*40 = [3237.05, 3237.1] + -8400
    = [-5162.95, -5162.9].'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next table summarizes the results of the first three iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f274b272653f8d81d3df3858f7954883.png)'
  prefs: []
  type: TYPE_IMG
- en: As we go into more iterations, the results get worse. The magnitude of the weights
    is changing rapidly and sometimes with changing its signs. They are moving from
    very large positive value to very large negative value. How can we stop this large
    and abrupt changes in the weights? How to scale down the value by which the weights
    are updated?
  prefs: []
  type: TYPE_NORMAL
- en: If we looked at the value by which the weights are changing by from the previous
    table, it seems that the value is very large. This means that the network changes
    its weights with large speed. It is like someone that makes large moves within
    small times. At one time, the person is in the far east and after a very short
    time, that person will be in the far west. We just need to make it slower.
  prefs: []
  type: TYPE_NORMAL
- en: If we are able to scale down this value to get smaller then everything will
    be alright. But how?
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting back to the part of the code that generates this value, it looks that
    the update equation is what generates it. Specifically this part:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We can scale this part by multiplying it by a small value such as 0.1\. So,
    rather than generating 3237.0 as the updated value in the first iteration, it
    will be reduced to just 323.7\. We can even scale this value to a smaller value
    by decreasing the scale value to say 0.001\. Using 0.001, the value will be just
    3.327.
  prefs: []
  type: TYPE_NORMAL
- en: We can catch it now. This scaling value is the learning rate. Choosing small
    values for the learning rate makes the rate of weights update smaller and avoids
    abrupt changes. As the value gets larger as the changes are faster and as a result
    bad results.
  prefs: []
  type: TYPE_NORMAL
- en: '**But what is the best value for the learning rate?**'
  prefs: []
  type: TYPE_NORMAL
- en: There is no value we can say it is the best value for the learning rate. The
    learning rate is a hyperparameter. A hyperparameter has its value determined by
    experiments. We try different values and use the value that gives best results.
    There are some ways that just helps you select values of hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Testing Network**'
  prefs: []
  type: TYPE_NORMAL
- en: For our problem, I deduced that a value of .00001 works fine. After training
    the network with that learning rate, we can make a test. The following table shows
    the results of prediction of 4 new testing samples. It seems that results are
    now much better after using the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2face38a2a1266e29bdb60faf7af8beb.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Bio: [Ahmed Gad](https://www.linkedin.com/in/ahmedfgad/)** received his B.Sc.
    degree with excellent with honors in information technology from the Faculty of
    Computers and Information (FCI), Menoufia University, Egypt, in July 2015\. For
    being ranked first in his faculty, he was recommended to work as a teaching assistant
    in one of the Egyptian institutes in 2015 and then in 2016 to work as a teaching
    assistant and a researcher in his faculty. His current research interests include
    deep learning, machine learning, artificial intelligence, digital signal processing,
    and computer vision.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://www.linkedin.com/pulse/learning-rate-useful-artificial-neural-networks-ahmed-gad/).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[TensorFlow: Building Feed-Forward Neural Networks Step-by-Step](/2017/10/tensorflow-building-feed-forward-neural-networks-step-by-step.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An Overview of 3 Popular Courses on Deep Learning](/2017/10/3-popular-courses-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5 Free Resources for Furthering Your Understanding of Deep Learning](/2017/10/5-free-resources-furthering-understanding-deep-learning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[The Base Rate Fallacy and its Impact on Data Science](https://www.kdnuggets.com/2023/04/base-rate-fallacy-impact-data-science.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Increase Your Callback Rate With A LinkedIn Profile](https://www.kdnuggets.com/increase-your-callback-rate-with-a-linkedin-profile)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Neural Networks and Deep Learning: A Textbook (2nd Edition)](https://www.kdnuggets.com/2023/07/aggarwal-neural-networks-deep-learning-textbook-2nd-edition.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10 Simple Things to Try Before Neural Networks](https://www.kdnuggets.com/2021/12/10-simple-things-try-neural-networks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Interpretable Neural Networks with PyTorch](https://www.kdnuggets.com/2022/01/interpretable-neural-networks-pytorch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Deep Neural Networks Don''t Lead Us Towards AGI](https://www.kdnuggets.com/2021/12/deep-neural-networks-not-toward-agi.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
