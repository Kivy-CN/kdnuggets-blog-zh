- en: 8 Open-Source Alternative to ChatGPT and Bard
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8个开源替代ChatGPT和Bard
- en: 原文：[https://www.kdnuggets.com/2023/04/8-opensource-alternative-chatgpt-bard.html](https://www.kdnuggets.com/2023/04/8-opensource-alternative-chatgpt-bard.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/04/8-opensource-alternative-chatgpt-bard.html](https://www.kdnuggets.com/2023/04/8-opensource-alternative-chatgpt-bard.html)
- en: '![8 Open-Source Alternative to ChatGPT and Bard](../Images/a2e5643af552ce470bbbdfae6a562743.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![8个开源替代ChatGPT和Bard](../Images/a2e5643af552ce470bbbdfae6a562743.png)'
- en: Image by Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: 1\. LLaMA
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. LLaMA
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The [LLaMA](https://arxiv.org/abs/2302.13971) project encompasses a set of foundational
    language models that vary in size from 7 billion to 65 billion parameters. These
    models were training on millions of tokens, and it was training on publicly available
    datasets exclusively. As a result, LLaMA-13B outperforms GPT-3 (175B), and LLaMA-65B
    is performing similarly to the best models like Chinchilla-70B and PaLM-540B.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLaMA](https://arxiv.org/abs/2302.13971) 项目包括一组基础语言模型，参数规模从70亿到650亿不等。这些模型在数百万个标记上进行了训练，且仅使用公开可用的数据集。因此，LLaMA-13B
    超过了 GPT-3 (175B)，而 LLaMA-65B 的表现则与 Chinchilla-70B 和 PaLM-540B 等最佳模型相当。'
- en: '![8 Open-Source Alternative to ChatGPT and Bard](../Images/90ba0e0c93caae0a278d5a6fad77384c.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![8个开源替代ChatGPT和Bard](../Images/90ba0e0c93caae0a278d5a6fad77384c.png)'
- en: Image from [LLaMA](https://arxiv.org/abs/2302.13971)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [LLaMA](https://arxiv.org/abs/2302.13971)
- en: '**Resources:  **'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：**'
- en: 'Research Paper: [LLaMA: Open and Efficient Foundation Language Models (arxiv.org)](https://arxiv.org/abs/2302.13971)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '研究论文: [LLaMA: 开放且高效的基础语言模型 (arxiv.org)](https://arxiv.org/abs/2302.13971)'
- en: 'GitHub: [facebookresearch/llama](https://github.com/facebookresearch/llama)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GitHub: [facebookresearch/llama](https://github.com/facebookresearch/llama)'
- en: 'Demo: [Baize Lora 7B](https://huggingface.co/spaces/project-baize/baize-lora-7B)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '演示: [Baize Lora 7B](https://huggingface.co/spaces/project-baize/baize-lora-7B)'
- en: 2\. Alpaca
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. Alpaca
- en: Stanford [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) claims that
    it can compete with ChatGPT and anyone can reproduce it in less than 600$. The
    Alpaca 7B is finetuned from the LLaMA 7B model on 52K instruction-following demonstrations.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福 [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) 声称它可以与 ChatGPT
    竞争，任何人都可以用不到 600$ 复制。Alpaca 7B 是从 LLaMA 7B 模型在 52K 指令跟随演示上进行微调的。
- en: '![8 Open-Source Alternative to ChatGPT and Bard](../Images/60ffb50f39a318ece225f0d288473b80.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![8个开源替代ChatGPT和Bard](../Images/60ffb50f39a318ece225f0d288473b80.png)'
- en: Training recipe | Image from [Stanford CRFM](https://crfm.stanford.edu/2023/03/13/alpaca.html)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 训练配方 | 图片来自 [斯坦福 CRFM](https://crfm.stanford.edu/2023/03/13/alpaca.html)
- en: '**Resources:  **'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：**'
- en: 'Blog: [Stanford CRFM](https://crfm.stanford.edu/2023/03/13/alpaca.html)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '博客: [斯坦福 CRFM](https://crfm.stanford.edu/2023/03/13/alpaca.html)'
- en: 'GitHub: [tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GitHub: [tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)'
- en: 'Demo: [Alpaca-LoRA](https://huggingface.co/spaces/tloen/alpaca-lora) (The official
    demo was drop and this is a recreation of Alpaca model)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '演示: [Alpaca-LoRA](https://huggingface.co/spaces/tloen/alpaca-lora)（官方演示已下架，这是
    Alpaca 模型的重建版）'
- en: 3\. Vicuna
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. Vicuna
- en: '[Vicuna](https://vicuna.lmsys.org/) is finetuned from the LLaMA model on user-shared
    conversations collected from [ShareGPT](https://sharegpt.com/). The model Vicuna-13B
    has achieved more than 90%* quality of OpenAI ChatGPT and Google Bard. It has
    also outperformed LLaMA and Stanford Alpaca models in 90% of cases. The cost of
    training Vicuna was around 300$.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[Vicuna](https://vicuna.lmsys.org/) 是从 LLaMA 模型在 [ShareGPT](https://sharegpt.com/)
    收集的用户共享对话上进行微调的。模型 Vicuna-13B 达到了超过 90%* 的 OpenAI ChatGPT 和 Google Bard 质量。它在
    90% 的情况下也超越了 LLaMA 和斯坦福 Alpaca 模型。训练 Vicuna 的成本约为 300$。'
- en: '![8 Open-Source Alternative to ChatGPT and Bard](../Images/4e49e3c4c6f80310070ddf7de4694914.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![8 个开源替代品，代替 ChatGPT 和 Bard](../Images/4e49e3c4c6f80310070ddf7de4694914.png)'
- en: Image from [Vicuna](https://vicuna.lmsys.org/)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Vicuna](https://vicuna.lmsys.org/)
- en: '**Resources:  **'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：  **'
- en: 'Blog post: [Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT
    Quality](https://vicuna.lmsys.org/)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '博客文章: [Vicuna: 一个开源聊天机器人以 90%* ChatGPT 质量令人印象深刻](https://vicuna.lmsys.org/)'
- en: 'GitHub: [lm-sys/FastChat](https://github.com/lm-sys/FastChat#fine-tuning)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GitHub: [lm-sys/FastChat](https://github.com/lm-sys/FastChat#fine-tuning)'
- en: 'Demo: [FastChat (lmsys.org)](https://chat.lmsys.org/)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '演示: [FastChat (lmsys.org)](https://chat.lmsys.org/)'
- en: 4\. OpenChatKit
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. OpenChatKit
- en: '[OpenChatKit: Open-Source ChatGPT Alternative](/2023/03/openchatkit-opensource-chatgpt-alternative.html)
    is a complete tools kit for creating your chatbot. It provides instruction for
    training your own Instruction-tuned large language model, fine-tuning the model,
    extensible retrieval system for updating the bot response, and bot moderation
    for filtering out questions.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenChatKit: 开源 ChatGPT 替代品](/2023/03/openchatkit-opensource-chatgpt-alternative.html)
    是一个完整的工具包，用于创建你的聊天机器人。它提供了训练自己定制化大语言模型的说明、模型微调、可扩展的检索系统用于更新机器人响应，以及用于过滤问题的机器人管理。'
- en: '![8 Open-Source Alternative to ChatGPT and Bard](../Images/468f34c99053ce78c84dabf1bb795862.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![8 个开源替代品，代替 ChatGPT 和 Bard](../Images/468f34c99053ce78c84dabf1bb795862.png)'
- en: Image from [TOGETHER](https://www.together.xyz/blog/openchatkit)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [TOGETHER](https://www.together.xyz/blog/openchatkit)
- en: As we can see, the GPT-NeoXT-Chat-Base-20B model has outperformed base mode
    GPT-NoeX on question and answer, extraction, and classification tasks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，GPT-NeoXT-Chat-Base-20B 模型在问题回答、提取和分类任务上优于基础版 GPT-NeoX。
- en: '**Resources:  **'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：  **'
- en: 'Blog Post: [Announcing OpenChatKit — TOGETHER](https://www.together.xyz/blog/openchatkit)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '博客文章: [宣布 OpenChatKit — TOGETHER](https://www.together.xyz/blog/openchatkit)'
- en: 'GitHub: [togethercomputer/OpenChatKit](https://github.com/togethercomputer/OpenChatKit)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GitHub: [togethercomputer/OpenChatKit](https://github.com/togethercomputer/OpenChatKit)'
- en: 'Demo: [OpenChatKit ](https://huggingface.co/spaces/togethercomputer/OpenChatKit)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '演示: [OpenChatKit ](https://huggingface.co/spaces/togethercomputer/OpenChatKit)'
- en: 'Model card: [togethercomputer/GPT-NeoXT-Chat-Base-20B](https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '模型卡片: [togethercomputer/GPT-NeoXT-Chat-Base-20B](https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B)'
- en: 5\. GPT4ALL
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. GPT4ALL
- en: '[GPT4ALL](https://github.com/nomic-ai/gpt4all) is a community-driven project
    and was trained on a massive curated corpus of assistant interactions, including
    code, stories, depictions, and multi-turn dialogue. The team has provided datasets,
    model weights, data curation process, and training code to promote open-source.
    Furthermore, they have released quantized 4-bit versions of the model that can
    run on your laptop. You can even use a Python client to run the model inference.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPT4ALL](https://github.com/nomic-ai/gpt4all) 是一个社区驱动的项目，经过大量整理的助理交互数据进行训练，包括代码、故事、描写和多轮对话。团队提供了数据集、模型权重、数据整理过程和训练代码，以推动开源。此外，他们还发布了可以在笔记本电脑上运行的量化
    4 位版本模型。你甚至可以使用 Python 客户端运行模型推理。'
- en: '![8 Open-Source Alternative to ChatGPT and Bard](../Images/f0c2a201d62329663a470ed885d8cc8c.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![8 个开源替代品，代替 ChatGPT 和 Bard](../Images/f0c2a201d62329663a470ed885d8cc8c.png)'
- en: Gif from [GPT4ALL](https://github.com/nomic-ai/gpt4all)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 动图来自 [GPT4ALL](https://github.com/nomic-ai/gpt4all)
- en: '**Resources:  **'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：  **'
- en: 'Technical Report: [GPT4All](https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '技术报告: [GPT4All](https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf)'
- en: 'GitHub: [nomic-ai/gpt4al](https://github.com/nomic-ai/gpt4all)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GitHub: [nomic-ai/gpt4al](https://github.com/nomic-ai/gpt4all)'
- en: 'Demo: [GPT4All](https://huggingface.co/spaces/rishiraj/GPT4All) (non-official)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '演示: [GPT4All](https://huggingface.co/spaces/rishiraj/GPT4All)（非官方）'
- en: 'Model card: [nomic-ai/gpt4all-lora · Hugging Face](https://huggingface.co/nomic-ai/gpt4all-lora)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '模型卡片: [nomic-ai/gpt4all-lora · Hugging Face](https://huggingface.co/nomic-ai/gpt4all-lora)'
- en: 6\. Raven RWKV
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. Raven RWKV
- en: '[Raven RWKV 7B](https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B) is an
    open-source chatbot that is powered by the [RWKV](https://github.com/BlinkDL/RWKV-LM)
    language model that produces similar results to ChatGPT. The model uses RNNs that
    can match transformers in quality and scaling while being faster and saving VRAM.
    The Raven was fine-tuned on Stanford Alpaca, code-alpaca, and more datasets.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[Raven RWKV 7B](https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B) 是一个开源聊天机器人，采用
    [RWKV](https://github.com/BlinkDL/RWKV-LM) 语言模型，该模型的效果与 ChatGPT 类似。该模型使用的 RNN
    能在质量和规模上与 transformers 匹敌，同时更快且节省 VRAM。Raven 在 Stanford Alpaca、code-alpaca 等数据集上进行了微调。'
- en: '![8 Open-Source Alternative to ChatGPT and Bard](../Images/6a01f233c86592bbba3b085143579e8f.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![8 个开源 ChatGPT 和 Bard 替代品](../Images/6a01f233c86592bbba3b085143579e8f.png)'
- en: Image from [Raven RWKV 7B](https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源于 [Raven RWKV 7B](https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B)
- en: '**Resources:  **'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：  **'
- en: 'GitHub: [BlinkDL/ChatRWKV](https://github.com/BlinkDL/ChatRWKV)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub：[BlinkDL/ChatRWKV](https://github.com/BlinkDL/ChatRWKV)
- en: 'Demo: [Raven RWKV 7B](https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演示：[Raven RWKV 7B](https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B)
- en: 'Model card: [BlinkDL/rwkv-4-raven](https://huggingface.co/BlinkDL/rwkv-4-raven)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型卡：[BlinkDL/rwkv-4-raven](https://huggingface.co/BlinkDL/rwkv-4-raven)
- en: 7\. OPT
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7\. OPT
- en: '[OPT](https://arxiv.org/abs/2205.01068): Open Pre-trained Transformer Language
    Models is not great as ChatGPT, but it has shown remarkable capabilities for zero-
    and few-shot learning and Stereotypical Bias analysis. You can also integrate
    it with Alpa, Colossal-AI, CTranslate2, and FasterTransformer to get even better
    results.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[OPT](https://arxiv.org/abs/2205.01068)：开放预训练变换器语言模型虽然不如 ChatGPT 优秀，但在零样本和少样本学习以及刻板偏见分析方面表现出色。你还可以将其与
    Alpa、Colossal-AI、CTranslate2 和 FasterTransformer 集成，以获得更好的结果。'
- en: '**Note:** It is on the list because of its popularity, as it has 624,710 monthly
    downloads in the text generation category.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 它在列表中是因为它的受欢迎程度，因为在文本生成类别中每月下载量达到624,710次。'
- en: '![8 Open-Source Alternative to ChatGPT and Bard](../Images/b2d85448eb08edd4b5dcdefd49a1ffb1.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![8 个开源 ChatGPT 和 Bard 替代品](../Images/b2d85448eb08edd4b5dcdefd49a1ffb1.png)'
- en: Image from [(arxiv.org)](https://arxiv.org/abs/2205.01068)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源于 [(arxiv.org)](https://arxiv.org/abs/2205.01068)
- en: '**Resources:  **'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：  **'
- en: 'Research Paper: [OPT: Open Pre-trained Transformer Language Models (arxiv.org)](https://arxiv.org/abs/2205.01068)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '研究论文：[OPT: 开放预训练变换器语言模型 (arxiv.org)](https://arxiv.org/abs/2205.01068)'
- en: 'GitHub: [facebookresearch/metaseq](https://github.com/facebookresearch/metaseq)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub：[facebookresearch/metaseq](https://github.com/facebookresearch/metaseq)
- en: 'Demo: [A Watermark for LLMs](https://huggingface.co/spaces/tomg-group-umd/lm-watermarking)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演示：[LLMs 的水印](https://huggingface.co/spaces/tomg-group-umd/lm-watermarking)
- en: 'Model card: [facebook/opt-1.3b](https://huggingface.co/facebook/opt-1.3b)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型卡：[facebook/opt-1.3b](https://huggingface.co/facebook/opt-1.3b)
- en: 8\. Flan-T5-XXL
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8\. Flan-T5-XXL
- en: '[Flan-T5-XXL](https://huggingface.co/google/flan-t5-xxl?text=Q%3A+%28+False+or+not+False+or+False+%29+is%3F+A%3A+Let%27s+think+step+by+step#usage)
    fine-tuned T5 models on a collection of datasets phrased as instructions. The
    instruction fine-tuning dramatically improves performance on a variety of model
    classes such as PaLM, T5, and U-PaLM. The Flan-T5-XXL model is fine-tuned on more
    than 1000 additional tasks covering also more languages.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[Flan-T5-XXL](https://huggingface.co/google/flan-t5-xxl?text=Q%3A+%28+False+or+not+False+or+False+%29+is%3F+A%3A+Let%27s+think+step+by+step#usage)
    是在一系列以指令形式呈现的数据集上微调的 T5 模型。指令微调显著提高了多种模型类别（如 PaLM、T5 和 U-PaLM）的性能。Flan-T5-XXL
    模型在超过1000个额外任务上进行了微调，还覆盖了更多语言。'
- en: '![8 Open-Source Alternative to ChatGPT and Bard](../Images/fcc3022916797e0cf409be15b1274f4c.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![8 个开源 ChatGPT 和 Bard 替代品](../Images/fcc3022916797e0cf409be15b1274f4c.png)'
- en: Image from [Flan-T5-XXL](https://huggingface.co/google/flan-t5-xxl?text=Q%3A+%28+False+or+not+False+or+False+%29+is%3F+A%3A+Let%27s+think+step+by+step#usage)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源于 [Flan-T5-XXL](https://huggingface.co/google/flan-t5-xxl?text=Q%3A+%28+False+or+not+False+or+False+%29+is%3F+A%3A+Let%27s+think+step+by+step#usage)
- en: '**Resources:  **'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源：  **'
- en: 'Research Paper: [Scaling Instruction-Fine Tuned Language Models](https://arxiv.org/pdf/2210.11416.pdf)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究论文：[规模化指令微调语言模型](https://arxiv.org/pdf/2210.11416.pdf)
- en: 'GitHub: [google-research/t5x](https://github.com/google-research/t5x)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub：[google-research/t5x](https://github.com/google-research/t5x)
- en: 'Demo: [Chat Llm Streaming](https://huggingface.co/spaces/olivierdehaene/chat-llm-streaming)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演示：[Chat Llm 流媒体](https://huggingface.co/spaces/olivierdehaene/chat-llm-streaming)
- en: 'Model card: [google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl?text=Q%3A+%28+False+or+not+False+or+False+%29+is%3F+A%3A+Let%27s+think+step+by+step)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型卡：[google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl?text=Q%3A+%28+False+or+not+False+or+False+%29+is%3F+A%3A+Let%27s+think+step+by+step)
- en: Conclusion
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: There are many open-source options available, and I have mentioned popular ones.
    The open-source chatbots and models are getting better, and in the next few months,
    you will see a new model that can completely overtake ChatGPT in terms of performance.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多开源选项可供选择，我提到了其中一些受欢迎的选项。开源聊天机器人和模型正在不断改进，未来几个月你会看到一个新的模型，它的性能可能会完全超越 ChatGPT。
- en: In this blog, I have provided a list of models/chatbot frameworks that can help
    you train and build chatbots similar to ChatGPT and GPT-4\. Don’t forget to give
    them likes and stars.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我提供了一些模型/聊天机器人框架的列表，这些框架可以帮助你训练和构建类似于 ChatGPT 和 GPT-4 的聊天机器人。别忘了给它们点赞和评分。
- en: Do let me know if you have better suggestions in the comment section. I would
    love to add it in the future.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有更好的建议，请在评论区告诉我。我很乐意在未来添加它。
- en: '**[Abid Ali Awan](https://www.polywork.com/kingabzpro)** ([@1abidaliawan](https://twitter.com/1abidaliawan))
    is a certified data scientist professional who loves building machine learning
    models. Currently, he is focusing on content creation and writing technical blogs
    on machine learning and data science technologies. Abid holds a Master''s degree
    in Technology Management and a bachelor''s degree in Telecommunication Engineering.
    His vision is to build an AI product using a graph neural network for students
    struggling with mental illness.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**[Abid Ali Awan](https://www.polywork.com/kingabzpro)** ([@1abidaliawan](https://twitter.com/1abidaliawan))
    是一位认证的数据科学专业人士，喜欢构建机器学习模型。目前，他专注于内容创作，并撰写关于机器学习和数据科学技术的技术博客。Abid 拥有技术管理硕士学位和电信工程学士学位。他的愿景是使用图神经网络为那些饱受心理疾病困扰的学生开发一个
    AI 产品。'
- en: More On This Topic
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关话题
- en: '[Top 10 Tools for Detecting ChatGPT, GPT-4, Bard, and Claude](https://www.kdnuggets.com/2023/05/top-10-tools-detecting-chatgpt-gpt4-bard-llms.html)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[检测 ChatGPT、GPT-4、Bard 和 Claude 的十大工具](https://www.kdnuggets.com/2023/05/top-10-tools-detecting-chatgpt-gpt4-bard-llms.html)'
- en: '[ChatGPT vs Google Bard: A Comparison of the Technical Differences](https://www.kdnuggets.com/2023/03/chatgpt-google-bard-comparison-technical-differences.html)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT 与 Google Bard：技术差异比较](https://www.kdnuggets.com/2023/03/chatgpt-google-bard-comparison-technical-differences.html)'
- en: '[ChatGPT vs. BARD](https://www.kdnuggets.com/chatgpt-vs-bard)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGPT 与 BARD](https://www.kdnuggets.com/chatgpt-vs-bard)'
- en: '[OpenChatKit: Open-Source ChatGPT Alternative](https://www.kdnuggets.com/2023/03/openchatkit-opensource-chatgpt-alternative.html)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenChatKit：开源 ChatGPT 替代品](https://www.kdnuggets.com/2023/03/openchatkit-opensource-chatgpt-alternative.html)'
- en: '[ChatGLM-6B: A Lightweight, Open-Source ChatGPT Alternative](https://www.kdnuggets.com/2023/04/chatglm6b-lightweight-opensource-chatgpt-alternative.html)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ChatGLM-6B：一个轻量级的开源 ChatGPT 替代品](https://www.kdnuggets.com/2023/04/chatglm6b-lightweight-opensource-chatgpt-alternative.html)'
- en: '[Dolly 2.0: ChatGPT Open Source Alternative for Commercial Use](https://www.kdnuggets.com/2023/04/dolly-20-chatgpt-open-source-alternative-commercial.html)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Dolly 2.0：ChatGPT 的开源替代品，适用于商业用途](https://www.kdnuggets.com/2023/04/dolly-20-chatgpt-open-source-alternative-commercial.html)'
