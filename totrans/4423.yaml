- en: Working with Spark, Python or SQL on Azure Databricks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Azure Databricks 上使用 Spark、Python 或 SQL
- en: 原文：[https://www.kdnuggets.com/2020/08/spark-python-sql-azure-databricks.html](https://www.kdnuggets.com/2020/08/spark-python-sql-azure-databricks.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/08/spark-python-sql-azure-databricks.html](https://www.kdnuggets.com/2020/08/spark-python-sql-azure-databricks.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Ajay Ohri](http://linkedin.com/in/ajayohri), Data Science Manager**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**由 [Ajay Ohri](http://linkedin.com/in/ajayohri)，数据科学经理**'
- en: '**Azure Databricks** is an Apache Spark-based big data analytics service designed
    for data science and data engineering offered by Microsoft. It allows collaborative
    working as well as working in multiple languages like Python, Spark, R and SQL.
    Working on Databricks offers the advantages of cloud computing - scalable, lower
    cost, on demand data processing and data storage.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**Azure Databricks** 是微软提供的基于 Apache Spark 的大数据分析服务，旨在数据科学和数据工程领域。它支持协作工作以及
    Python、Spark、R 和 SQL 等多种语言的使用。使用 Databricks 的好处包括云计算的优势 - 可扩展、成本较低、按需数据处理和数据存储。'
- en: '* * *'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三个课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速开启网络安全职业生涯。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升您的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持您组织的 IT'
- en: '* * *'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Here we look at some ways to interchangeably work with Python, PySpark and SQL.
    We learn how to import in data from a CSV file by uploading it first and then
    choosing to  create it in a notebook. We learn how to convert an SQL table to
    a Spark Dataframe and convert a Spark Dataframe to a Python Pandas Dataframe.
    We also learn how to convert a Spark Dataframe to a Permanent or Temporary SQL
    Table.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将探讨一些在 Python、PySpark 和 SQL 之间互换使用的方法。我们学习如何从 CSV 文件中导入数据，先上传文件，然后选择在笔记本中创建它。我们学习如何将
    SQL 表转换为 Spark Dataframe，并将 Spark Dataframe 转换为 Python Pandas Dataframe。我们还学习如何将
    Spark Dataframe 转换为永久或临时的 SQL 表。
- en: Why do we need to learn how to interchange code between SQL, Spark and Python
    Panda Dataframe? SQL is great for easy writing and readable code for data manipulation,
    Spark is great for speed for big data as well as Machine Learning, while Python
    Pandas can be used for everything from data manipulation, machine learning as
    well as plotting in seaborn or matplotlib libraries.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要学习如何在 SQL、Spark 和 Python Panda Dataframe 之间互换代码？SQL 在数据处理方面书写简单且易读，Spark
    在大数据和机器学习方面速度极快，而 Python Pandas 可以用于数据处理、机器学习以及 seaborn 或 matplotlib 库中的绘图等各种任务。
- en: '![Image](../Images/10e3236941f0b760eb80a0b99e6a2abe.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../Images/10e3236941f0b760eb80a0b99e6a2abe.png)'
- en: We choose a SQL notebook for ease and then we choose appropriate cluster with
    appropriate RAM, Cores, Spark version etc. Even though it is a SQL notebook we
    can write python code by typing %python in front of code in that cell.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择 SQL 笔记本以方便操作，然后选择适当的集群，包括适当的 RAM、核心、Spark 版本等。即使这是一个 SQL 笔记本，我们也可以通过在该单元格的代码前输入
    %python 来编写 Python 代码。
- en: '![Image](../Images/f90682a67b15f98366a13a8280832397.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../Images/f90682a67b15f98366a13a8280832397.png)'
- en: Now let's begin the basics of data input, data inspection and data interchange
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始数据输入、数据检查和数据互换的基础知识
- en: '**Step 1** Reading in Uploaded Data'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 1 步** 读取上传的数据'
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Step 2** Create a temporary view or table from SPARK Dataframe'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 2 步** 从 SPARK Dataframe 创建临时视图或表'
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Step 3** Creating Permanent SQL Table from SPARK Dataframe'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 3 步** 从 SPARK Dataframe 创建永久 SQL 表'
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Step 4** Inspecting SQL Table'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 4 步** 检查 SQL 表'
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Step 5** Converting SQL Table to SPARK Dataframe'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 5 步** 将 SQL 表转换为 SPARK Dataframe'
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Step 6** Inspecting SPARK Dataframe'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 6 步** 检查 SPARK Dataframe'
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Step 7** Converting Spark Dataframe to Python Pandas Dataframe'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 7 步** 将 Spark Dataframe 转换为 Python Pandas Dataframe'
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Step 8** Inspecting Python Dataframe'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 8 步** 检查 Python Dataframe'
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**References**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考资料**'
- en: Introduction to Azure Databricks - [https://www.slideshare.net/jamserra/introduction-to-azure-databricks-83448539](https://www.slideshare.net/jamserra/introduction-to-azure-databricks-83448539)
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Azure Databricks 介绍 - [https://www.slideshare.net/jamserra/introduction-to-azure-databricks-83448539](https://www.slideshare.net/jamserra/introduction-to-azure-databricks-83448539)
- en: Dataframes and Datasets - [https://docs.databricks.com/spark/latest/dataframes-datasets/index.html](https://docs.databricks.com/spark/latest/dataframes-datasets/index.html)
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dataframes 和 Datasets - [https://docs.databricks.com/spark/latest/dataframes-datasets/index.html](https://docs.databricks.com/spark/latest/dataframes-datasets/index.html)
- en: Optimize conversion between PySpark and pandas DataFrames - [https://docs.databricks.com/spark/latest/spark-sql/spark-pandas.html](https://docs.databricks.com/spark/latest/spark-sql/spark-pandas.html)
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化 PySpark 与 pandas DataFrames 之间的转换 - [https://docs.databricks.com/spark/latest/spark-sql/spark-pandas.html](https://docs.databricks.com/spark/latest/spark-sql/spark-pandas.html)
- en: pyspark package - [https://spark.apache.org/docs/latest/api/python/pyspark.html](https://spark.apache.org/docs/latest/api/python/pyspark.html)
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: pyspark 包 - [https://spark.apache.org/docs/latest/api/python/pyspark.html](https://spark.apache.org/docs/latest/api/python/pyspark.html)
- en: '**Bio: [Ajay Ohri](http://linkedin.com/in/ajayohri)** is Data Science Manager
    (Publicis Sapient) and author of 4 books on data science including R for Cloud
    Computing and Python for R Users.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介： [Ajay Ohri](http://linkedin.com/in/ajayohri)** 是数据科学经理（Publicis Sapient）以及《R
    for Cloud Computing》和《Python for R Users》等4本数据科学书籍的作者。'
- en: '**Related**:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容**：'
- en: '[Apache Spark on Dataproc vs. Google BigQuery](/2020/07/apache-spark-dataproc-vs-google-bigquery.html)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Apache Spark 在 Dataproc 与 Google BigQuery 之间的比较](/2020/07/apache-spark-dataproc-vs-google-bigquery.html)'
- en: '[Containerization of PySpark Using Kubernetes](/2020/08/containerization-pyspark-kubernetes.html)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Kubernetes 容器化 PySpark](/2020/08/containerization-pyspark-kubernetes.html)'
- en: '[5 Apache Spark Best Practices For Data Science](/2020/08/5-spark-best-practices-data-science.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5 个 Apache Spark 数据科学最佳实践](/2020/08/5-spark-best-practices-data-science.html)'
- en: More On This Topic
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Optimizing Data Analytics: Integrating GitHub Copilot in Databricks](https://www.kdnuggets.com/optimizing-data-analytics-integrating-github-copilot-in-databricks)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[优化数据分析：在 Databricks 中集成 GitHub Copilot](https://www.kdnuggets.com/optimizing-data-analytics-integrating-github-copilot-in-databricks)'
- en: '[A Guide to Working with SQLite Databases in Python](https://www.kdnuggets.com/a-guide-to-working-with-sqlite-databases-in-python)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Python 中处理 SQLite 数据库的指南](https://www.kdnuggets.com/a-guide-to-working-with-sqlite-databases-in-python)'
- en: '[Working With Sparse Features In Machine Learning Models](https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在机器学习模型中处理稀疏特征](https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html)'
- en: '[Working with Big Data: Tools and Techniques](https://www.kdnuggets.com/working-with-big-data-tools-and-techniques)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[处理大数据：工具和技术](https://www.kdnuggets.com/working-with-big-data-tools-and-techniques)'
- en: '[Getting Deep Learning working in the wild: A Data-Centric Course](https://www.kdnuggets.com/2022/04/corise-deep-learning-wild-data-centric-course.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[让深度学习在实际应用中发挥作用：数据驱动课程](https://www.kdnuggets.com/2022/04/corise-deep-learning-wild-data-centric-course.html)'
- en: '[6 Soft Skills for Data Scientists Working Remotely](https://www.kdnuggets.com/2022/05/6-soft-skills-data-scientists-working-remotely.html)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6 种远程工作的数据科学家的软技能](https://www.kdnuggets.com/2022/05/6-soft-skills-data-scientists-working-remotely.html)'
