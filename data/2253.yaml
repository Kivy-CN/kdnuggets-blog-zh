- en: Overcoming Imbalanced Data Challenges in Real-World Scenarios
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 克服实际场景中的数据不平衡挑战
- en: 原文：[https://www.kdnuggets.com/2023/07/overcoming-imbalanced-data-challenges-realworld-scenarios.html](https://www.kdnuggets.com/2023/07/overcoming-imbalanced-data-challenges-realworld-scenarios.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2023/07/overcoming-imbalanced-data-challenges-realworld-scenarios.html](https://www.kdnuggets.com/2023/07/overcoming-imbalanced-data-challenges-realworld-scenarios.html)
- en: '[Benchmarking the performance](https://medium.com/toloka/choosing-the-best-architecture-for-your-text-classification-task-aee30ecc7870)
    of popular NLP architectures is an important step in building an understanding
    of available options when approaching the text classification task. Here we’ll
    go deeper into the meat of it and explore one of the most common challenges associated
    with the classification — data imbalance. And if you ever applied ML to a real-world
    classification dataset, you’re most likely familiar with it.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[对流行NLP架构的性能进行基准测试](https://medium.com/toloka/choosing-the-best-architecture-for-your-text-classification-task-aee30ecc7870)是建立对可用选项的理解的重要步骤，尤其是在处理文本分类任务时。在这里，我们将深入探讨与分类相关的最常见挑战之一——数据不平衡。如果你曾经将机器学习应用于实际的分类数据集，你可能对此已经很熟悉了。'
- en: Understanding Imbalance in Data Classification
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据分类中的不平衡
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三名课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速入门网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌IT支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持你的组织的IT需求'
- en: '* * *'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: In data classification, we're often concerned with the distribution of data
    points across classes. A balanced dataset has roughly the same number of points
    in all classes, making it easier to work with. However, real-world datasets are
    often imbalanced.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分类中，我们通常关心的是数据点在各个类别中的分布。一个平衡的数据集在所有类别中大致有相同数量的点，这使得处理起来更容易。然而，实际世界中的数据集往往是不平衡的。
- en: Imbalanced data can cause problems because a model might learn to label everything
    with the most frequent class, ignoring the actual input. This can happen if the
    dominant class is so prevalent that the model isn't penalized much for misclassifying
    the minority class. Additionally, underrepresented classes may not have enough
    data for the model to learn meaningful patterns.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据不平衡可能会导致问题，因为模型可能会学习将所有内容标记为最频繁的类别，从而忽略实际输入。如果主导类别过于普遍，模型在对少数类别进行误分类时不会受到太大惩罚，这种情况可能会发生。此外，代表性不足的类别可能没有足够的数据让模型学习到有意义的模式。
- en: Is imbalance something that needs to be corrected? Imbalance is a feature of
    data, and a good question to start with is whether we want to do anything about
    it at all. There are tricks to make the training process easier for a model. Optionally,
    we might manipulate the training process or the data itself to let the model know
    which classes are especially important for us but it should be justified by a
    business need or domain knowledge. Further, we’ll discuss these tricks and manipulations
    in more detail.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡是需要纠正的问题吗？不平衡是数据的一种特征，一个好的问题是我们是否真的想对其采取措施。有一些技巧可以让模型的训练过程更容易。我们可能会选择操控训练过程或数据本身，以便让模型知道哪些类别对我们特别重要，但这应该由业务需求或领域知识来证明。此外，我们将更详细地讨论这些技巧和操控。
- en: To illustrate the effect of different techniques addressing data imbalance we’ll
    use the [sms-spam dataset](https://huggingface.co/datasets/sms_spam), which contains
    747 spam and 4827 ham (legitimate) texts. Though there are only two classes, the
    task will be treated as a multiclass classification problem for better generalization.
    We’ll use a roberta-base model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明不同技术处理数据不平衡的效果，我们将使用 [sms-spam数据集](https://huggingface.co/datasets/sms_spam)，该数据集包含747条垃圾短信和4827条正常短信。尽管只有两个类别，为了更好地进行泛化，我们将把任务视为多类别分类问题。我们将使用一个roberta-base模型。
- en: Keep in mind that these techniques may produce different results with other
    data. It's essential to test them on your specific dataset.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这些技术在其他数据上可能会产生不同的结果。必须在特定的数据集上测试它们。
- en: 'When training a classification model without any adjustments, we get the following
    classification report:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有任何调整的情况下训练分类模型，我们得到以下分类报告：
- en: '![Overcoming Imbalanced Data Challenges in Real-World Scenarios](../Images/79020d6580675475b1916c21e334e3ee.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![克服现实世界场景中的数据不平衡挑战](../Images/79020d6580675475b1916c21e334e3ee.png)'
- en: “Safe” tricks
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “安全”技巧
- en: Bias Initialization
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏差初始化
- en: 'Our first technique is to let the model know about data distribution from the
    beginning. We could propagate this knowledge by initializing the bias of the final
    classification layer accordingly. This trick, shared by Andrej Karpathy in his
    [Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines:~:text=Huber%20losses%2C%20etc.-,init%20well.,-Initialize%20the%20final),
    helps the model start with an informed perspective. In our case of multiclass
    classification, we use softmax as the final activation, and we want the model''s
    output at initialization to reflect the data distribution. To achieve this, we
    solve the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个技巧是让模型从一开始就了解数据分布。我们可以通过相应地初始化最终分类层的偏差来传播这一知识。这个技巧由Andrej Karpathy在他的[神经网络训练配方](http://karpathy.github.io/2019/04/25/recipe/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines:~:text=Huber%20losses%2C%20etc.-,init%20well.,-Initialize%20the%20final)中分享，有助于模型以知情的视角开始。在我们的多类分类情况下，我们使用softmax作为最终激活函数，我们希望模型在初始化时的输出能反映数据分布。为此，我们解决以下问题：
- en: '![Overcoming Imbalanced Data Challenges in Real-World Scenarios](../Images/047441b41d2b1e96d5543b45cadaddd1.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![克服现实世界场景中的数据不平衡挑战](../Images/047441b41d2b1e96d5543b45cadaddd1.png)'
- en: As a reminder,
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，
- en: '![Overcoming Imbalanced Data Challenges in Real-World Scenarios](../Images/0394c3ffea500602567116e3d9a4de22.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![克服现实世界场景中的数据不平衡挑战](../Images/0394c3ffea500602567116e3d9a4de22.png)'
- en: Then,
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，
- en: '![Overcoming Imbalanced Data Challenges in Real-World Scenarios](../Images/4f386998077203fee0b782e6c6e87439.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![克服现实世界场景中的数据不平衡挑战](../Images/4f386998077203fee0b782e6c6e87439.png)'
- en: Here b0 and b1 are biases of negative and positive classes correspondingly,
    neg and pos are the number of elements in negative and positive classes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里b0和b1分别是负类和正类的偏差，neg和pos分别是负类和正类的元素数量。
- en: With this initialization, all metrics simply improve!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种初始化，所有指标都显著改善！
- en: '![Overcoming Imbalanced Data Challenges in Real-World Scenarios](../Images/3f32a9519358882e24e8e8e1f58228d2.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![克服现实世界场景中的数据不平衡挑战](../Images/3f32a9519358882e24e8e8e1f58228d2.png)'
- en: In Bayesian terms, this means manually setting a prior and allowing the model
    to learn the posterior during training.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯术语中，这意味着手动设置先验，并允许模型在训练过程中学习后验。
- en: Downsampling and Upweighting/Upsampling and Downweighting
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下采样和上权重/上采样和下权重
- en: These techniques also address class imbalance effectively. Both share a similar
    concept but differ in execution. Downsampling and upweighting involves reducing
    the size of the dominant class to balance the distribution, while assigning larger
    weights to examples from this class during training. The upweighting ensures that
    output probabilities still represent the observed data distribution. Conversely,
    upsampling and downweighting entails increasing the size of underrepresented classes
    and proportionally reducing their weights.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术也有效地解决了类别不平衡问题。它们有类似的概念但执行方式不同。下采样和上权重涉及减少主导类别的大小以平衡分布，同时在训练过程中给该类别的样本赋予更大的权重。上权重确保输出概率仍能代表观察到的数据分布。相反，上采样和下权重则涉及增加代表性不足类别的大小，并按比例减少它们的权重。
- en: 'Downsampling and upweighting outcome:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下采样和上权重结果：
- en: '![Overcoming Imbalanced Data Challenges in Real-World Scenarios](../Images/f89a3ccfad21b66f32e349bdd7e26b2a.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![克服现实世界场景中的数据不平衡挑战](../Images/f89a3ccfad21b66f32e349bdd7e26b2a.png)'
- en: 'Upsampling and downweighting outcome:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上采样和下权重结果：
- en: '![Overcoming Imbalanced Data Challenges in Real-World Scenarios](../Images/b9ddea52726eab417d23743750d7b76f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![克服现实世界场景中的数据不平衡挑战](../Images/b9ddea52726eab417d23743750d7b76f.png)'
- en: In both scenarios, the “spam” recall decreased, likely because the "ham" weight
    was twice that of the “spam” weight.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，“spam”的召回率下降，可能是因为“ham”的权重是“spam”权重的两倍。
- en: Focal Loss
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 焦点损失
- en: '[Focal loss](https://arxiv.org/abs/1708.02002), referred to by the authors
    as a “dynamically scaled cross entropy loss”, was introduced to address training
    on imbalanced data . It''s applicable to binary cases only, and luckily, our problem
    involves just two classes. Check out the equation below:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[焦点损失](https://arxiv.org/abs/1708.02002)，作者称之为“动态缩放交叉熵损失”，旨在解决数据不平衡的训练问题。它仅适用于二分类情况，幸运的是，我们的问题涉及的仅是两个类别。请查看下面的公式：'
- en: '![Overcoming Imbalanced Data Challenges in Real-World Scenarios](../Images/f541faebe8f65f1d0245d0de3b4d179c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![克服实际场景中的数据不平衡挑战](../Images/f541faebe8f65f1d0245d0de3b4d179c.png)'
- en: In the equation, p is the probability of a true class, ɑ is a weighting factor,
    and ???? controls how much we penalize loss depending on confidence (probability).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式中，p 是真实类别的概率，ɑ 是加权因子，???? 控制我们根据置信度（概率）惩罚损失的程度。
- en: The design ensures that examples with lower probability receive exponentially
    greater weight, pushing the model to learn about more challenging examples. The
    alpha parameter allows for different weighting between class examples.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 设计确保了概率较低的样本会获得指数级更大的权重，推动模型学习更具挑战性的示例。alpha 参数允许对类别示例进行不同的加权。
- en: By tuning the alpha and gamma combination, you can find the optimal configuration.
    To remove explicit class preference, set alpha to 0.5; however, authors have noted
    minor improvements with this balancing factor.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整 alpha 和 gamma 的组合，你可以找到最佳配置。为了消除显式的类别偏好，将 alpha 设置为 0.5；然而，作者注意到这一平衡因子带来了轻微的改进。
- en: 'Here is the best result we obtained with focal loss:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们使用焦点损失获得的最佳结果：
- en: '![Overcoming Imbalanced Data Challenges in Real-World Scenarios](../Images/4f827a4571b5948bc548fafb379766eb.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![克服实际场景中的数据不平衡挑战](../Images/4f827a4571b5948bc548fafb379766eb.png)'
- en: All metrics outperform the baseline, but it required some parameter adjustments.
    Keep in mind, it might not always work out this smoothly.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所有指标都优于基线，但这需要一些参数调整。请记住，它可能并不总是这样顺利地运行。
- en: “Not-so-safe” Tricks
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “非安全”技巧
- en: There exist well-known methods that intentionally alter the output probability
    distribution to give underrepresented classes an advantage. By using these techniques,
    we explicitly signal to the model that certain classes are crucial and shouldn’t
    be overlooked. This is often driven by a business need, like detecting financial
    fraud or offensive comments, which is more important than accidentally mislabeling
    good examples. Apply these techniques when the goal is to boost recall for specific
    classes, even if it means sacrificing other metrics.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一些已知的方法，故意改变输出概率分布，以使代表性不足的类别获得优势。通过使用这些技术，我们明确地向模型发出信号，某些类别至关重要，不应被忽视。这通常是由业务需求驱动的，例如检测金融欺诈或攻击性评论，这比错误标记良好示例更为重要。在目标是提高特定类别的召回率时，即使需要牺牲其他指标，也应应用这些技术。
- en: Weighting
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权重调整
- en: 'Weighting involves assigning unique weights to loss values for samples from
    different classes. This is an effective and adaptable method because it lets you
    indicate the significance of each class to the model. Here’s the formula for a
    multiclass weighted cross-entropy loss for a single training example:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 权重调整涉及为不同类别的样本分配独特的损失权重。这是一种有效且灵活的方法，因为它允许你向模型指示每个类别的重要性。以下是单个训练示例的多类别加权交叉熵损失公式：
- en: '![Overcoming Imbalanced Data Challenges in Real-World Scenarios](../Images/26904682178d588a75974c820dd6ea0f.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![克服实际场景中的数据不平衡挑战](../Images/26904682178d588a75974c820dd6ea0f.png)'
- en: ','
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ','
- en: where pytrue represents the probability of the true class and wytrue is the
    weight of that class.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 pytrue 代表真实类别的概率，wytrue 是该类别的权重。
- en: 'A good default method for determining weights is the inverse class frequency:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 确定权重的一个好的默认方法是逆类别频率：
- en: '![Overcoming Imbalanced Data Challenges in Real-World Scenarios](../Images/d68b23447b3c0049ce9d936597099229.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![克服实际场景中的数据不平衡挑战](../Images/d68b23447b3c0049ce9d936597099229.png)'
- en: where N is the dataset's total items, c is the class count, and ni is the ith
    class's element count
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 N 是数据集的总项数，c 是类别数，ni 是第 i 类的元素数量。
- en: 'The weights were calculated as follows: {''ham'': 0.576, ''spam'': 3.784}'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '权重计算如下：{''ham'': 0.576, ''spam'': 3.784}'
- en: 'Below are the metrics obtained using these weights:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用这些权重获得的指标：
- en: '![Overcoming Imbalanced Data Challenges in Real-World Scenarios](../Images/1624abdd000c9df45a5630adf55edfe3.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![克服实际场景中的数据不平衡挑战](../Images/1624abdd000c9df45a5630adf55edfe3.png)'
- en: The metrics surpass the baseline scenario. While this may occur, it's not always
    the case.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 指标超越了基线场景。虽然这种情况可能发生，但并不总是如此。
- en: 'However, if avoiding missed positives from a specific class is vital, consider
    increasing the class weight, which could likely boost the class recall. Let’s
    try the weights {"ham": 0.576, "spam": 10.0} to see the outcome.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '但是，如果避免从特定类别中漏掉正例至关重要，请考虑增加类别权重，这可能会提高类别的召回率。让我们尝试权重{"ham": 0.576, "spam":
    10.0}来查看结果。'
- en: 'Results are as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Overcoming Imbalanced Data Challenges in Real-World Scenarios](../Images/f709fec76da64b7700183f80cee337f2.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![克服现实场景中的数据不平衡挑战](../Images/f709fec76da64b7700183f80cee337f2.png)'
- en: As anticipated, the “spam” recall increased, though precision declined. The
    F1 score worsened compared to using inverse class frequency weights. This demonstrates
    the potential of basic loss weighting. Weighting may be beneficial even for balanced
    data to recall crucial classes.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，“垃圾邮件”的召回率增加了，但精确率下降了。与使用逆类频率权重相比，F1 分数变差了。这表明基本损失加权的潜力。即使对于平衡的数据，加权也可能有利于召回关键类别。
- en: Upsampling and downsampling.
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上采样和下采样。
- en: While similar to the methods discussed earlier, they don't include the weighting
    step. Downsampling may result in data loss, while upsampling can lead to overfitting
    the upsampled class. Though they can help, weighting is often a more efficient
    and transparent option.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然与前面讨论的方法类似，但它们不包括加权步骤。下采样可能导致数据丢失，而上采样则可能导致对上采样类别的过拟合。尽管它们可以提供帮助，但加权通常是更高效、更透明的选项。
- en: Comparing the Probabilities
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较概率
- en: 'We’ll assess the confidence of various model versions using an obvious spam-looking
    example: "*Call to claim your prize!*" See the table below for results.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个明显看起来像垃圾邮件的例子来评估各种模型版本的信心：“*请致电索取您的奖品！*”请查看下表以获取结果。
- en: '![Overcoming Imbalanced Data Challenges in Real-World Scenarios](../Images/51f6239982928f5fbaa4023bad1d6c66.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![克服现实场景中的数据不平衡挑战](../Images/51f6239982928f5fbaa4023bad1d6c66.png)'
- en: As anticipated, the weighted model shows overconfidence, while the “downsampling
    + upweighting” model is underconfident (due to upweighted “ham”) compared to the
    baseline. Notably, bias initialization increases and focal loss decreases the
    model’s confidence in the “spam” category.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，加权模型显示出过度自信，而“下采样 + 上加权”模型则表现出不足自信（由于上加权的“正常邮件”），与基线相比。值得注意的是，偏差初始化增加和焦点损失减少了模型在“垃圾邮件”类别中的信心。
- en: Summary
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In conclusion, addressing data imbalance is possible when necessary. Keep in
    mind that some techniques intentionally alter the distribution and should only
    be applied when required. Imbalance is a feature, not a bug!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，在必要时解决数据不平衡是可能的。请记住，一些技术会故意改变分布，仅在需要时应用。数据不平衡是一个特征，而不是一个错误！
- en: While we discussed probabilities, the ultimate performance metric is the one
    most important to the business. If offline tests show a model adds value, go ahead
    and test it in production.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们讨论了概率，但最终的性能指标是对业务最重要的。如果离线测试显示模型带来了价值，请继续在生产环境中测试它。
- en: In my experiments, I used the [Toloka ML](https://tolokamodels.tech) platform.
    It offers a range of ready-to-use models that can give a head start to an ML project.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的实验中，我使用了[Toloka ML](https://tolokamodels.tech)平台。它提供了一系列现成的模型，可以为机器学习项目提供良好的开端。
- en: Overall, considering data distributions for training ML models is crucial. The
    training data must represent the real-world distribution for the model to work
    effectively. If the data is inherently imbalanced, the model should account for
    it to perform well in real-life scenarios.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，考虑数据分布对于训练机器学习模型至关重要。训练数据必须代表现实世界的分布，以使模型有效。如果数据本质上是不平衡的，模型应考虑这一点以在现实场景中表现良好。
- en: '**[Sergei Petrov](https://www.linkedin.com/in/sergei-petrov-12589210b/)** is
    a Data Scientist working with a range of Deep Learning applications. His interests
    are around building efficient and scalable Machine Learning solutions beneficial
    for businesses and society.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**[谢尔盖·彼得罗夫](https://www.linkedin.com/in/sergei-petrov-12589210b/)** 是一名数据科学家，专注于各种深度学习应用。他的兴趣在于构建高效且可扩展的机器学习解决方案，这些解决方案对企业和社会都大有裨益。'
- en: More On This Topic
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Overcoming Barriers in Multi-lingual Voice Technology: Top 5…](https://www.kdnuggets.com/2023/08/overcoming-barriers-multilingual-voice-technology-top-5-challenges-innovative-solutions.html)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[克服多语言语音技术中的障碍：前 5 大挑战及创新解决方案](https://www.kdnuggets.com/2023/08/overcoming-barriers-multilingual-voice-technology-top-5-challenges-innovative-solutions.html)'
- en: '[SQL Group By and Partition By Scenarios: When and How to Combine…](https://www.kdnuggets.com/sql-group-by-and-partition-by-scenarios-when-and-how-to-combine-data-in-data-science)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[SQL 的 Group By 和 Partition By 场景：何时以及如何结合数据…](https://www.kdnuggets.com/sql-group-by-and-partition-by-scenarios-when-and-how-to-combine-data-in-data-science)'
- en: '[Exploring the Potential of Transfer Learning in Small Data Scenarios](https://www.kdnuggets.com/exploring-the-potential-of-transfer-learning-in-small-data-scenarios)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[探索小数据场景中迁移学习的潜力](https://www.kdnuggets.com/exploring-the-potential-of-transfer-learning-in-small-data-scenarios)'
- en: '[KDnuggets News, August 31: The Complete Data Science Study Roadmap…](https://www.kdnuggets.com/2022/n35.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[KDnuggets 新闻，8月31日：完整的数据科学学习路线图…](https://www.kdnuggets.com/2022/n35.html)'
- en: '[7 Techniques to Handle Imbalanced Data](https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[处理不平衡数据的7种技术](https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html)'
- en: '[Unsupervised Disentangled Representation Learning in Class…](https://www.kdnuggets.com/2023/01/unsupervised-disentangled-representation-learning-class-imbalanced-dataset-elastic-infogan.html)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在类不平衡数据集中进行无监督的解缠表示学习…](https://www.kdnuggets.com/2023/01/unsupervised-disentangled-representation-learning-class-imbalanced-dataset-elastic-infogan.html)'
