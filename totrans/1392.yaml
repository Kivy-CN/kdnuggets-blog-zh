- en: Making sense of ensemble learning techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解集成学习技术
- en: 原文：[https://www.kdnuggets.com/2020/03/making-sense-ensemble-learning-techniques.html](https://www.kdnuggets.com/2020/03/making-sense-ensemble-learning-techniques.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://www.kdnuggets.com/2020/03/making-sense-ensemble-learning-techniques.html](https://www.kdnuggets.com/2020/03/making-sense-ensemble-learning-techniques.html)
- en: '[comments](#comments)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[评论](#comments)'
- en: '**By [Ido Zehori](https://www.linkedin.com/in/ido-zehori/), Data Science Team
    Leader at [Bigabid](https://www.bigabid.com/)**'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者 [Ido Zehori](https://www.linkedin.com/in/ido-zehori/)，[Bigabid](https://www.bigabid.com/)
    数据科学团队负责人**'
- en: '![Figure](../Images/733e44c46833a8497ef2f1341a1d187c.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图示](../Images/733e44c46833a8497ef2f1341a1d187c.png)'
- en: 'Image source: [Packt](https://hub.packtpub.com/what-is-ensemble-learning/)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [Packt](https://hub.packtpub.com/what-is-ensemble-learning/)'
- en: '* * *'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Our Top 3 Course Recommendations
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的前三大课程推荐
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [谷歌网络安全证书](https://www.kdnuggets.com/google-cybersecurity)
    - 快速通道进入网络安全职业。'
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [谷歌数据分析专业证书](https://www.kdnuggets.com/google-data-analytics)
    - 提升你的数据分析技能'
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [谷歌 IT 支持专业证书](https://www.kdnuggets.com/google-itsupport)
    - 支持组织的 IT 工作'
- en: '* * *'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: For many companies/data scientists that specialize or work with machine learning
    (ML), ensemble learning methods have become the weapons of choice. As ensemble
    learning methods combine multiple base models, together they have a greater ability
    to produce a much more accurate ML model. For example, at Bigabid we’ve been ensemble
    learning to successfully solve a variety of problems ranging from optimizing LTV
    ([Customer Lifetime Value](https://www.qualtrics.com/experience-management/customer/customer-lifetime-value/))
    to [fraud detection](https://searchsecurity.techtarget.com/definition/fraud-detection).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多专注于或从事机器学习（ML）的公司/数据科学家来说，集成学习方法已经成为首选武器。由于集成学习方法结合了多个基础模型，因此它们能够更准确地生成机器学习模型。例如，在
    Bigabid，我们一直在使用集成学习成功解决各种问题，从优化 LTV（[客户终身价值](https://www.qualtrics.com/experience-management/customer/customer-lifetime-value/)）到
    [欺诈检测](https://searchsecurity.techtarget.com/definition/fraud-detection)。
- en: 'It is hard not overstate the importance of ensemble learning to the overall
    ML process, including the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)
    and the three main ensemble techniques: bagging, boosting and stacking. These
    powerful techniques should be a part of any data scientist’s tool kit, as they
    are concepts that are encountered everywhere. Plus, understanding their underlying
    mechanism is at the heart of the field of machine learning.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 难以夸大集成学习在整体机器学习过程中的重要性，包括 [偏差-方差权衡](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)
    以及三种主要的集成技术：袋装、提升和堆叠。这些强大的技术应成为任何数据科学家工具包的一部分，因为它们是无处不在的概念。此外，理解它们的基本机制是机器学习领域的核心。
- en: 'Ensemble Learning Methods: An Overview'
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成学习方法概述
- en: Ensemble learning is an ML paradigm where numerous base models (which are often
    referred to as “weak learners”) are combined and trained to solve the same problem.
    This method is based on the theory that by correctly combining several base models
    together, these weak learners can be used as building blocks for designing more
    complex ML models. Together these ensemble models (aptly called “strong learners”)
    achieve better, more accurate results.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习是一种机器学习范式，其中将多个基础模型（通常称为“弱学习器”）进行组合并训练以解决相同的问题。这种方法基于这样的理论：通过正确地组合几个基础模型，这些弱学习器可以作为设计更复杂机器学习模型的构建块。通过这种方式，这些集成模型（恰当地称为“强学习器”）能够实现更好、更准确的结果。
- en: In other words, a weak learner is merely a base model that alone performs rather
    poorly. In fact, its accuracy level is barely above chance, meaning that it predicts
    the outcome only slightly better than a random guess would. These weak learners
    will often be computationally simple as well. Typically, the reason base models
    don’t perform very well by themselves is because they either have a high bias
    or too much variance, which makes them weak.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，弱学习器仅仅是一个单独表现相当差的基础模型。事实上，它的准确度仅略高于随机猜测，这意味着它预测结果的效果仅比随机猜测好一点。这些弱学习器通常也计算简单。通常，基础模型表现不佳的原因在于它们要么有高偏差，要么有过多的方差，从而使其变得较弱。
- en: This is where ensemble learning comes in. This method attempts to try to reduce
    the general error by combining several weak learners together. Think of ensemble
    models as the data science version of the expression “two heads are better than
    one.” If one model works well, then a number of models working together can do
    even better.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是集成学习发挥作用的地方。该方法试图通过将多个弱学习器组合在一起来减少总体误差。将集成模型视为数据科学版本的“两个脑袋总比一个好”这一说法。如果一个模型效果良好，那么多个模型一起工作可以做得更好。
- en: A Word about the Bias-Variance Tradeoff
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于偏差-方差权衡的说明
- en: 'It’s important to understand the concept of a weak learner and why it earned
    this name in the first place, as the reason comes down to either bias or variance.
    More specifically, the prediction error of an ML model, namely the difference
    between the trained model and the ground truth, can be broken down into the sum
    of the following: the bias and the variance. For instance:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 理解弱学习器的概念及其名字的由来非常重要，因为原因归结于偏差或方差。更具体地说，机器学习模型的预测误差，即训练模型与真实值之间的差异，可以分解为以下几个部分的总和：偏差和方差。例如：
- en: '**Error due to Bias:** This is the difference between a model’s expected prediction
    and the precise value that we are aiming to predict.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差引起的误差：** 这是模型的期望预测与我们希望预测的精确值之间的差异。'
- en: '**Error due to Variance:** This is the variance of a model prediction for a
    specified data point.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差引起的误差：** 这是模型在指定数据点上的预测方差。'
- en: If a model is too simplistic and doesn’t have many parameters, then it may have
    high bias and low variance. In contrast, if a model has many parameters, then
    it may have high variance and low bias. As such, it’s necessary to find the right
    balance without underfitting or overfitting the data, as this tradeoff in complexity
    is the reason why there exists a tradeoff between variance and bias. Simply put,
    an algorithm can’t simultaneously be more complex and less complex at the same
    time.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个模型过于简单且参数较少，那么它可能有高偏差和低方差。相比之下，如果一个模型有许多参数，它可能有高方差和低偏差。因此，必须找到正确的平衡点，避免欠拟合或过拟合数据，因为复杂性的这种权衡正是方差与偏差之间存在权衡的原因。简单来说，一个算法不能同时变得更复杂和更简单。
- en: Ensemble Learning Techniques - Combining Weak Learners
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成学习技术 - 组合弱学习器
- en: 'There are three main ensemble techniques: bagging, boosting and stacking. There
    are defined as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种主要的集成技术：袋装法、提升法和堆叠法。它们定义如下：
- en: '**Bagging:**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**袋装法（Bagging）：**'
- en: Bagging attempts to incorporate similar learners on small-sample populations
    and calculates the average of all the predictions. Generally, bagging allows you
    to use different learners in different populations. By doing so, this method helps
    to reduce the variance error.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装法试图在小样本群体中整合相似的学习器，并计算所有预测的平均值。通常，袋装法允许在不同的群体中使用不同的学习器。通过这样做，这种方法有助于减少方差误差。
- en: '**Boosting**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**提升法（Boosting）**'
- en: Boosting is an iterative method that fine-tunes the weight of an observation
    according to the most recent classification. If an observation was incorrectly
    classified, this method will increase the weight of that observation in the next
    round (in which the next model will be trained) and will be less prone to misclassification.
    Similarly, if an observation was classified correctly, then it will reduce its
    weight for the next classifier. The weight represents how important the correct
    classification of the specific data point should be, as this enables the sequential
    classifiers to focus on examples previously misclassified. Generally, boosting
    reduces the bias error and forms strong predictive models, but at times they may
    overfit on the training data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 提升是一种迭代方法，根据最近的分类来微调观察值的权重。如果一个观察值被错误分类了，这种方法将在下一轮（即下一个模型将被训练的轮次）增加该观察值的权重，从而减少误分类的可能性。同样，如果观察值被正确分类，则会减少它在下一个分类器中的权重。权重表示特定数据点正确分类的重要程度，因为这使得顺序分类器能够关注先前被错误分类的示例。一般来说，提升减少了偏差误差并形成了强大的预测模型，但有时它们可能会在训练数据上过拟合。
- en: '**Stacking**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**堆叠**'
- en: Stacking is a clever way of combining the information provided by different
    models. With this method, a learner of any sort can be used to combine different
    learners’ outputs. The result can be a decrease in bias or variance determined
    by which combined models are used.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠是一种巧妙的将不同模型提供的信息结合起来的方法。使用这种方法，可以将任何类型的学习者用于组合不同学习者的输出。结果可能是由于使用了哪些组合模型而导致偏差或方差的减少。
- en: '**The Promise of Ensemble Learning**'
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**集成学习的前景**'
- en: Ensemble learning is about combining multiple base models to achieve a more
    effective and accurate ensemble model that features more powerful properties and
    thus, performs better. Ensemble learning methods have successfully set record
    performances on challenging datasets and are constantly a part of the winning
    submissions of [Kaggle competitions.](https://www.kaggle.com/competitions)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习是将多个基础模型结合起来，以实现一个更有效、更准确的集成模型，该模型具有更强大的属性，因此表现更好。集成学习方法在具有挑战性的数据集上成功地创下了记录，并且不断成为[Kaggle竞赛](https://www.kaggle.com/competitions)获胜提交的一部分。
- en: It’s worth noting that even with the three main ensemble techniques – bagging,
    boosting and stacking – variants are still possible and can be better designed
    to more effectively adapt to specific problems, such as classifications, regression,
    time-series analyses, etc. This first requires an understanding of the problem
    at hand and to be creative in approaching problem solving!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，即使是三种主要的集成技术——装袋、提升和堆叠——也可能存在变体，并且可以更好地设计以更有效地适应特定问题，如分类、回归、时间序列分析等。这首先需要理解手头的问题，并在解决问题时发挥创造力！
- en: Using ensemble learning methods is a great, promising way to start approaching
    any problem.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用集成学习方法是开始解决任何问题的一个很好的、有前途的方式。
- en: '**Bio: [Ido Zehori](https://www.linkedin.com/in/ido-zehori/)** is the Data
    Science Team Leader at [Bigabid](https://www.bigabid.com/), a data science company
    that has developed a second-generation DSP optimized for in-app advertising user
    acquisition & re-engagement, providing both the scale of a tier-1 DSP and the
    precision of a cutting edge DMP.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介: [Ido Zehori](https://www.linkedin.com/in/ido-zehori/)** 是[Bigabid](https://www.bigabid.com/)的数据科学团队负责人，该公司开发了一种针对应用内广告用户获取和再参与优化的第二代DSP，提供了一级DSP的规模和最前沿DMP的精确度。'
- en: '**Related:**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关内容：**'
- en: '[Ensemble Methods for Machine Learning: AdaBoost](/2019/09/ensemble-methods-machine-learning-adaboost.html)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习的集成方法：AdaBoost](/2019/09/ensemble-methods-machine-learning-adaboost.html)'
- en: '[Random Forest® — A Powerful Ensemble Learning Algorithm](/2020/01/random-forest-powerful-ensemble-learning-algorithm.html)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[随机森林® — 强大的集成学习算法](/2020/01/random-forest-powerful-ensemble-learning-algorithm.html)'
- en: '[Explaining Black Box Models: Ensemble and Deep Learning Using LIME and SHAP](/2020/01/explaining-black-box-models-ensemble-deep-learning-lime-shap.html)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[解释黑箱模型：使用LIME和SHAP进行集成和深度学习](/2020/01/explaining-black-box-models-ensemble-deep-learning-lime-shap.html)'
- en: More On This Topic
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多相关主题
- en: '[Ensemble Learning Techniques: A Walkthrough with Random Forests in Python](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[集成学习技术：Python中的随机森林详细讲解](https://www.kdnuggets.com/ensemble-learning-techniques-a-walkthrough-with-random-forests-in-python)'
- en: '[When Would Ensemble Techniques be a Good Choice?](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[何时使用集成技术是一个好选择？](https://www.kdnuggets.com/2022/07/would-ensemble-techniques-good-choice.html)'
- en: '[Ensemble Learning with Examples](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[带实例的集成学习](https://www.kdnuggets.com/2022/10/ensemble-learning-examples.html)'
- en: '[The Significance of Data Quality in Making a Successful Machine…](https://www.kdnuggets.com/2022/03/significance-data-quality-making-successful-machine-learning-model.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[成功的机器学习模型中数据质量的重要性](https://www.kdnuggets.com/2022/03/significance-data-quality-making-successful-machine-learning-model.html)'
- en: '[Ploomber vs Kubeflow: Making MLOps Easier](https://www.kdnuggets.com/2022/02/ploomber-kubeflow-mlops-easier.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ploomber 与 Kubeflow：简化 MLOps](https://www.kdnuggets.com/2022/02/ploomber-kubeflow-mlops-easier.html)'
- en: '[Making Intelligent Document Processing Smarter: Part 1](https://www.kdnuggets.com/2023/02/making-intelligent-document-processing-smarter-part-1.html)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使智能文档处理更智能：第 1 部分](https://www.kdnuggets.com/2023/02/making-intelligent-document-processing-smarter-part-1.html)'
