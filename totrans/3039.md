# 可解释人工智能（第二部分）– 模型解释策略

> 原文：[https://www.kdnuggets.com/2018/12/explainable-ai-model-interpretation-strategies.html](https://www.kdnuggets.com/2018/12/explainable-ai-model-interpretation-strategies.html)

![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [评论](/2018/12/explainable-ai-model-interpretation-strategies.html?page=2#comments)![图](../Images/259329bfc9d56d238c06bba517984824.png)

来源：Pixabay

### 介绍

本文是我系列文章中的一篇，旨在讨论***“可解释人工智能（XAI）”***。如果你还没有查看第一篇文章，我强烈建议你快速浏览一下[***“第一部分——人类可解释机器学习的重要性”***](https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476)，它涵盖了人类可解释机器学习的背景和必要性，以及模型解释的范围和标准。本文将从我们上次讨论的地方继续，并进一步扩展机器学习模型解释方法的标准，并根据范围探索解释技术。本文旨在让你对现有的传统模型解释方法、它们的局限性和挑战有一个清晰的理解。我们还将讨论经典的模型准确性与模型可解释性权衡，并最后查看主要的模型解释策略。

简要地，我们将在本文中涵盖以下方面。

+   传统模型解释技术

+   传统技术的挑战与局限性

+   准确性与可解释性权衡

+   模型解释技术

这将为我们准备好详细的模型解释实用指南，敬请关注第三部分！

### 传统模型解释技术

模型解释的核心在于找到更好地理解模型决策制定政策的方法。这是为了实现公平、问责和透明度，使人们对在实际问题中使用这些模型充满信心，这对商业和社会有很大影响。因此，有一些存在已久的技术可以用来更好地理解和解释模型。这些技术可以分为以下两个主要类别。

+   **探索性分析和可视化技术**如*聚类*和*降维*。

+   **模型性能评估指标** 如 [*精确度、召回率、准确率*](https://en.wikipedia.org/wiki/Confusion_matrix)*，*[*ROC曲线和AUC*](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)（分类模型）和[*决定系数（R平方）*](https://en.wikipedia.org/wiki/Coefficient_of_determination)*，*[*均方根误差、平均绝对误差*](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d)（回归模型）

让我们简要地详细了解这些技术。

**探索性分析和可视化**

探索性分析的想法并不是全新的。多年来，数据可视化一直是从数据中获取潜在见解的最有效工具之一。这些技术中的一些可以帮助我们识别数据中的关键特征和有意义的表示，从而指示哪些因素可能影响模型在可解释的形式下做出决策。

降维技术在这里非常有用，因为我们经常处理非常大的特征空间（维度灾难），减少特征空间有助于我们可视化并查看哪些因素可能影响模型做出特定决策。以下是一些这些技术。

+   **降维：** [主成分分析（PCA）](https://en.wikipedia.org/wiki/Principal_component_analysis)，[自组织映射（SOM）](https://en.wikipedia.org/wiki/Self-organizing_map)，[潜在语义索引](https://nlp.stanford.edu/IR-book/html/htmledition/latent-semantic-indexing-1.html)

+   [**流形学习**](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction)**:** t-分布随机邻居嵌入（[t-SNE](https://distill.pub/2016/misread-tsne/)）

+   **变分自编码器：** 一种自动生成的方法，使用[变分自编码器](https://arxiv.org/pdf/1606.05908.pdf)（VAE）

+   **聚类：** [层次聚类](https://en.wikipedia.org/wiki/Hierarchical_clustering)

现实世界问题中的一个例子可能是尝试通过检查词嵌入的语义相似性来可视化哪些文本特征可能对模型有影响，并使用t-SNE进行可视化，如下所示。

![Figure](../Images/a170df9d9993c6d08f152e05380b6078.png)

使用t-SNE可视化词嵌入（来源：[*理解特征工程（第4部分）——文本数据的深度学习方法——数据科学前沿*](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)）

你还可以使用t-SNE来可视化著名的MNIST手写数字数据集，如下图所示。

![Figure](../Images/f673989b5ae5a27375f97dae683444a4.png)

使用sklearn和t-SNE可视化MNIST数据。图片由Pramit Choudhary和Datascience.com团队提供。

另一个例子是通过利用 PCA 进行降维来可视化著名的 IRIS 数据集，如下图所示。

![图示](../Images/52b4d6050191a5f88b4b2591d6eaee84.png)

[对 IRIS 数据集的主成分分析](http://scikit-learn.org/stable/modules/decomposition.html)

除了可视化数据和特征外，一些更直观且可解释的模型，如决策树，帮助我们可视化它是如何做出某种决策的。下面的示例树帮助我们以人类可解释的方式可视化确切的规则。

![图示](../Images/74fc9f8e2e28cf9c70e62ababfa2c625.png)

可视化决策树模型的人类可解释规则（来源：[*使用 Python 的实用机器学习，Apress 2018*](https://github.com/dipanjanS/practical-machine-learning-with-python)）

然而，正如我们讨论的那样，对于其他不如树模型易于解释的模型，我们可能无法获得这些规则。此外，庞大的决策树往往变得非常难以可视化和解释。

**模型性能评估指标**

[***模型性能评估***](https://www.cs.cornell.edu/courses/cs578/2003fa/performance_measures.pdf)是数据科学生命周期中选择最佳模型的关键步骤。这使我们能够了解模型的表现，比较不同模型的性能指标，并选择最佳模型。这也使我们能够[***调整和优化超参数***](https://en.wikipedia.org/wiki/Hyperparameter_optimization)以获得在我们处理的数据上表现最佳的模型。通常，根据我们处理的问题类型，存在某些标准评估指标。

+   **监督学习——分类：** 对于分类问题，我们的主要目标是预测离散的类别响应变量。混淆矩阵在这里极其有用，我们可以从中推导出一系列有用的指标，包括准确率、精确率、召回率、F1-分数，如以下示例所示。

![图示](../Images/2c8ccd191f418a6b656e14ff7bf12402.png)

分类模型性能指标（来源：[*使用 Python 的实用机器学习，Apress 2018*](https://github.com/dipanjanS/practical-machine-learning-with-python)）

除了这些指标，我们还可以使用其他技术，如 ROC 曲线和 AUC 分数，正如下图中的葡萄酒质量预测系统所示。

![图示](../Images/9c01adeae214aa7cbb8e63cfe26d9bd0.png)

ROC 曲线和 AUC 分数（来源：[*使用 Python 的实用机器学习，Apress 2018*](https://github.com/dipanjanS/practical-machine-learning-with-python)）

ROC曲线下的面积是一种非常流行的技术，用于客观评估分类器的性能。在这里，我们通常尝试平衡真正率（TPR）和假正率（FPR）。上图告诉我们，一个`**‘高’**`级别的葡萄酒的AUC分数为`**0.9**`，这意味着模型将更高的评分分配给`**‘高’**`级别的葡萄酒（正类）的概率为90%，而不是分配给`**不是‘高’**`的类别（负类），该类别可能是`**‘中’**`或`**‘低’**`。有时，如果ROC曲线交叉，结果可能会令人误解且难以解释（来源：[*测量分类器性能：ROC曲线下面积的连贯替代方法*](https://link.springer.com/article/10.1007%2Fs10994-009-5119-5)）。

+   **监督学习 — 回归：** 对于回归问题，我们可以使用标准指标，如[决定系数 (R平方)](https://en.wikipedia.org/wiki/Coefficient_of_determination)、[均方根误差 (RMSE)](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d)和[平均绝对误差 (MAE)](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d)。

+   **无监督学习 — 聚类：** 对于基于聚类的无监督学习问题，我们可以使用诸如[轮廓系数](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)、[同质性、完整性、V度量](http://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness-and-v-measure)和[Calinski-Harabaz指数](http://scikit-learn.org/stable/modules/clustering.html#calinski-harabaz-index)等指标。

### 传统技术的局限性及更好模型解释的动机

我们在前面讨论的技术在理解我们的数据、特征以及哪些模型可能有效方面确实很有帮助。然而，它们在尝试辨别模型如何以人类可解释的方式工作的过程中存在相当大的局限性。在任何数据科学问题中，我们通常在一个静态数据集上建立模型，并获得我们的目标函数（优化后的损失函数），该函数通常在满足基于模型性能和业务要求的特定标准时部署。通常，我们利用上述的探索分析和评估指标来决定模型在数据上的总体表现。然而，在实际应用中，模型的性能往往在部署后由于数据特征的变化、增加的约束和噪声而逐渐下降和趋于平稳。这可能包括环境的变化、特征的变化以及增加的约束。因此，仅仅在相同特征集上重新训练模型是不够的，我们需要不断检查特征在决定模型预测中的重要性以及它们在新数据点上的表现如何。

例如，入侵检测系统（[IDS](https://ir.library.louisville.edu/etd/2790/)），一种网络安全应用，容易受到规避攻击，其中攻击者可能使用对抗性输入来击败安全系统（注意：对抗性输入是攻击者故意设计的例子，用于欺骗机器学习模型做出错误预测）。在这种情况下，模型的目标函数可能作为现实世界目标的一个弱替代。可能需要更好的解释来识别算法中的盲点，通过修正容易受到对抗性攻击的训练数据集来构建一个安全和可靠的模型（有关进一步阅读，请参见 Moosavi-Dezfooli 等，2016，[《DeepFool》](https://arxiv.org/pdf/1511.04599.pdf) 和 Goodfellow 等，2015，[《解释和利用对抗性示例》](https://arxiv.org/abs/1412.6572)）。

![图示](../Images/1409f75c34f0565044b40b8018016d08.png)

来源：[https://medium.com/@jrodthoughts/using-adversarial-attacks-to-make-your-deep-learning-model-look-stupid-24fb872f06fd](https://medium.com/@jrodthoughts/using-adversarial-attacks-to-make-your-deep-learning-model-look-stupid-24fb872f06fd)

此外，由于我们处理的数据特性，如在稀有类别预测问题（如欺诈或入侵检测）中，模型中常常存在偏差。指标无法帮助我们证明模型预测决策的真实情况。尽管这些传统的模型解释方式对数据科学家来说可能易于理解，但由于它们本质上是理论性的且通常是数学化的，在向（非技术性）业务利益相关者解释时存在着显著的差距，并且基于这些指标单独决定项目的成功标准是不够的。仅仅告诉业务*“我有一个准确率为90%的模型”*是不够的信息，使他们在现实世界中开始信任这个模型。我们需要对模型决策策略进行人类可解释的解释（HII），这些解释可以通过适当且直观的输入和输出来进行解释。这将使有价值的信息能够轻松地与同事（分析师、经理、数据科学家、数据工程师）共享。使用这样的解释形式，根据输入和输出进行解释，可能有助于促进更好的沟通与协作，使企业能够做出更有信心的决策（例如，[金融机构的风险评估/审计风险分析](https://www.journalofaccountancy.com/issues/2006/jul/assessingandrespondingtorisksinafinancialstatementaudit.html)）。

重申一下，我们将模型解释（新方法）定义为能够考虑***公平性***（无偏见/非歧视性）、***问责性***（可靠结果）和***透明性***（能够查询和验证预测决策）——当前涉及监督学习问题。

* * *

## 我们的前三大课程推荐

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google 网络安全证书](https://www.kdnuggets.com/google-cybersecurity) - 快速进入网络安全职业生涯

![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google 数据分析专业证书](https://www.kdnuggets.com/google-data-analytics) - 提升你的数据分析能力

![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT 支持专业证书](https://www.kdnuggets.com/google-itsupport) - 支持你的组织进行 IT 工作

* * *

### 了解更多主题

+   [你应该成为自由职业的人工智能工程师吗？](https://www.kdnuggets.com/2021/12/ucsd-become-freelance-artificial-intelligence-engineer.html)

+   [2022年人工智能项目创意](https://www.kdnuggets.com/2022/01/artificial-intelligence-project-ideas-2022.html)

+   [人工智能与元宇宙](https://www.kdnuggets.com/2022/02/artificial-intelligence-metaverse.html)

+   [人工智能系统中的不确定性量化](https://www.kdnuggets.com/2022/04/uncertainty-quantification-artificial-intelligencebased-systems.html)

+   [人工智能如何改变数据整合](https://www.kdnuggets.com/2022/04/artificial-intelligence-transform-data-integration.html)

+   [2022年最受欢迎的人工智能技能](https://www.kdnuggets.com/2022/08/indemand-artificial-intelligence-skills-learn-2022.html)
