["```py\nlibrary(dplyr)\nlibrary(patchwork)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(xgboost)\nlibrary(lightgbm)\nlibrary(keras)\nlibrary(tidyquant)\n##################### Pre-define some functions ###################################\n###################################################################################\n\nlogit2prob <- function(logit){\n  odds <- exp(logit)\n  prob <- odds / (1 + odds)\n  return(prob)\n}\n```", "```py\n###################################################################################\n###################################################################################\n\ndata(iris)\ndf <- iris %>% \n  filter(Species != \"virginica\") %>% \n  mutate(Species = +(Species == \"versicolor\"))\nstr(df)\n```", "```py\n## 'data.frame':    100 obs. of  5 variables:\n##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n##  $ Species     : int  0 0 0 0 0 0 0 0 0 0 ...\n```", "```py\n###################################################################################\n###################################################################################\n```", "```py\nplt1 <- df %>% \n  ggplot(aes(x = Sepal.Width, y = Sepal.Length, color = factor(Species))) +\n  geom_point(size = 4) +\n  theme_bw(base_size = 15) +\n  theme(legend.position = \"none\")\n\nplt2 <- df %>% \n  ggplot(aes(x = Petal.Length, y = Sepal.Length, color = factor(Species))) +\n  geom_point(size = 4) +\n  theme_bw(base_size = 15) +\n  theme(legend.position = \"none\")\n\nplt3 <- df %>% \n  ggplot(aes(x = Petal.Width, y = Sepal.Length, color = factor(Species))) +\n  geom_point(size = 4) +\n  theme_bw(base_size = 15) +\n  theme(legend.position = \"none\")\n\nplt3 <- df %>% \n  ggplot(aes(x = Sepal.Length, y = Sepal.Width, color = factor(Species))) +\n  geom_point(size = 4) +\n  theme_bw(base_size = 15) +\n  theme(legend.position = \"none\")\n\nplt4 <- df %>% \n  ggplot(aes(x = Petal.Length, y = Sepal.Width, color = factor(Species))) +\n  geom_point(size = 4) +\n  theme_bw(base_size = 15) +\n  theme(legend.position = \"none\")\n\nplt5 <- df %>% \n  ggplot(aes(x = Petal.Width, y = Sepal.Width, color = factor(Species))) +\n  geom_point(size = 4) +\n  theme_bw(base_size = 15) +\n  theme(legend.position = \"none\")\n\nplt6 <- df %>% \n  ggplot(aes(x = Petal.Width, y = Sepal.Length, color = factor(Species))) +\n  geom_point(size = 4) +\n  theme_bw(base_size = 15) +\n  theme(legend.position = \"none\")\n```", "```py\n (plt1)    /\n  (plt2 + plt3)\n```", "```py\n (plt1 + plt2) / \n  (plt5 + plt6)\n```", "```py\n###################################################################################\n###################################################################################\n\nvar_combos <- expand.grid(colnames(df[,1:4]), colnames(df[,1:4])) %>% \n  filter(!Var1 == Var2)\n\n###################################################################################\n###################################################################################\n```", "```py\nvar_combos %>% \n  head() %>% \n  kable(caption = \"Variable Combinations\", escape = F, align = \"c\", digits = 2) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), font_size = 9, fixed_thead = T, full_width = F) %>% \n  scroll_box(width = \"100%\", height = \"200px\")\n```", "```py\nboundary_lists <- map2(\n  .x = var_combos$Var1,\n  .y = var_combos$Var2,\n  ~select(df, .x, .y) %>% \n    summarise(\n      minX = min(.[[1]], na.rm = TRUE),\n      maxX = max(.[[1]], na.rm = TRUE),\n      minY = min(.[[2]], na.rm = TRUE),\n      maxY = max(.[[2]], na.rm = TRUE)\n    )\n) %>% \n  map(.,\n      ~tibble(\n        x = seq(.x$minX, .x$maxX, length.out = 200),\n        y = seq(.x$minY, .x$maxY, length.out = 200),\n      )\n  ) %>% \n  map(.,\n      ~tibble(\n        xx = rep(.x$x, each = 200),\n        yy = rep(.x$y, time = 200)\n      )\n  ) %>% \n  map2(.,\n       asplit(var_combos, 1), ~ .x %>% \n         set_names(.y))\n\n###################################################################################\n###################################################################################\n```", "```py\nboundary_lists %>% \n  map(., ~head(., 4)) %>%  \n  head(2)\n```", "```py\n## [[1]]\n## # A tibble: 4 x 2\n##   Sepal.Width Sepal.Length\n##         <dbl>        <dbl>\n## 1           2         4.3 \n## 2           2         4.31\n## 3           2         4.33\n## 4           2         4.34\n## \n## [[2]]\n## # A tibble: 4 x 2\n##   Petal.Length Sepal.Length\n##          <dbl>        <dbl>\n## 1            1         4.3 \n## 2            1         4.31\n## 3            1         4.33\n## 4            1         4.34\n```", "```py\nboundary_lists %>% \n  map(., ~head(., 4)) %>%  \n  tail(2)\n```", "```py\n## [[1]]\n## # A tibble: 4 x 2\n##   Sepal.Width Petal.Width\n##         <dbl>       <dbl>\n## 1           2       0.1  \n## 2           2       0.109\n## 3           2       0.117\n## 4           2       0.126\n## \n## [[2]]\n## # A tibble: 4 x 2\n##   Petal.Length Petal.Width\n##          <dbl>       <dbl>\n## 1            1       0.1  \n## 2            1       0.109\n## 3            1       0.117\n## 4            1       0.126\n```", "```py\n###################################################################################\n###################################################################################\n# params_lightGBM <- list(\n#   objective = \"binary\",\n#   metric = \"auc\",\n#   min_data = 1\n# )\n\n# To install Light GBM try the following in your RStudio terinal\n\n# git clone --recursive https://github.com/microsoft/LightGBM\n# cd LightGBM\n# Rscript build_r.R\n\nmodels_list <- var_combos %>%\n  mutate(modeln = str_c('mod', row_number()))  %>%\n  pmap(~ \n         {\n\n           xname = ..1\n           yname = ..2\n           modelname = ..3\n           df %>%\n             select(Species, xname, yname) %>%\n             group_by(grp = 'grp') %>%\n             nest() %>%\n             mutate(models = map(data, ~{\n\n               list(\n                 # Logistic Model\n                 Model_GLM = {\n                   glm(Species ~ ., data = .x, family = binomial(link='logit'))\n                 },\n                 # Support Vector Machine (linear)\n                 Model_SVM_Linear = {\n                   e1071::svm(Species ~ ., data = .x,  type = 'C-classification', kernel = 'linear')\n                 },\n                 # Support Vector Machine (polynomial)\n                 Model_SVM_Polynomial = {\n                   e1071::svm(Species ~ ., data = .x,  type = 'C-classification', kernel = 'polynomial')\n                 },\n                 # Support Vector Machine (sigmoid)\n                 Model_SVM_radial = {\n                   e1071::svm(Species ~ ., data = .x,  type = 'C-classification', kernel = 'sigmoid')\n                 },\n                 # Support Vector Machine (radial)\n                 Model_SVM_radial_Sigmoid = {\n                   e1071::svm(Species ~ ., data = .x,  type = 'C-classification', kernel = 'radial')\n                 },\n                 # Random Forest\n                 Model_RF = {\n                   randomForest::randomForest(formula = as.factor(Species) ~ ., data = .)\n                 },\n                 # Extreme Gradient Boosting\n                 Model_XGB = {\n                   xgboost(\n                     objective = 'binary:logistic',\n                     eval_metric = 'auc',\n                     data = as.matrix(.x[, 2:3]),\n                     label = as.matrix(.x$Species), # binary variable\n                     nrounds = 10)\n                 },\n                 # Kera Neural Network\n                 Model_Keras = {\n                   mod <- keras_model_sequential() %>% \n                     layer_dense(units = 2, activation = 'relu', input_shape = 2) %>% \n                     layer_dense(units = 2, activation = 'sigmoid')\n\n                   mod %>% compile(\n                     loss = 'binary_crossentropy',\n                     optimizer_sgd(lr = 0.01, momentum = 0.9),\n                     metrics = c('accuracy')\n                   )\n                   fit(mod, \n                       x = as.matrix(.x[, 2:3]),\n                       y = to_categorical(.x$Species, 2),\n                       epochs = 5,\n                       batch_size = 5,\n                       validation_split = 0\n                   )\n                   print(modelname)        \n                   assign(modelname, mod)                      \n\n                 },\n                 # Kera Neural Network\n                 Model_Keras_2 = {\n                   mod <- keras_model_sequential() %>% \n                     layer_dense(units = 2, activation = 'relu', input_shape = 2) %>%\n                     layer_dense(units = 2, activation = 'linear', input_shape = 2) %>%\n                     layer_dense(units = 2, activation = 'sigmoid')\n\n                   mod %>% compile(\n                     loss = 'binary_crossentropy',\n                     optimizer_sgd(lr = 0.01, momentum = 0.9),\n                     metrics = c('accuracy')\n                   )\n                   fit(mod, \n                       x = as.matrix(.x[, 2:3]),\n                       y = to_categorical(.x$Species, 2),\n                       epochs = 5,\n                       batch_size = 5,\n                       validation_split = 0\n                   )\n                   print(modelname)        \n                   assign(modelname, mod)                      \n\n                 },\n                 # Kera Neural Network                 \n                 Model_Keras_3 = {\n                   mod <- keras_model_sequential() %>% \n                     layer_dense(units = 2, activation = 'relu', input_shape = 2) %>% \n                     layer_dense(units = 2, activation = 'relu', input_shape = 2) %>%\n                     layer_dense(units = 2, activation = 'linear', input_shape = 2) %>%\n                     layer_dense(units = 2, activation = 'sigmoid')\n\n                   mod %>% compile(\n                     loss = 'binary_crossentropy',\n                     optimizer_sgd(lr = 0.01, momentum = 0.9),\n                     metrics = c('accuracy')\n                   )\n                   fit(mod, \n                       x = as.matrix(.x[, 2:3]),\n                       y = to_categorical(.x$Species, 2),\n                       epochs = 5,\n                       batch_size = 5,\n                       validation_split = 0\n                   )\n                   print(modelname)        \n                   assign(modelname, mod)                      \n\n                 },\n\n                 # LightGBM model\n                 Model_LightGBM = {\n                   lgb.train(\n                     data = lgb.Dataset(data = as.matrix(.x[, 2:3]), label = .x$Species),\n                     objective = 'binary',\n                     metric = 'auc',\n                     min_data = 1\n                     #params = params_lightGBM,\n                     #learning_rate = 0.1\n                   )\n                 }\n\n               )                      \n\n             }                               \n             ))                 \n\n         }) %>% \n  map(\n    ., ~unlist(., recursive = FALSE)\n  )\n```", "```py\nmodels_predict <- map2(models_list, boundary_lists, ~{\n  mods <- purrr::pluck(.x, \"models\")\n  dat <- .y\n  map(mods, function(x)\n    tryCatch({\n      if(attr(x, \"class\")[1] == \"glm\"){   \n        # predict the logistic model\n        tibble(\n          modelname = attr(x, \"class\")[1],\n          prediction = predict(x, newdata = dat)\n        ) %>% \n          mutate(\n            prediction = logit2prob(prediction),\n            prediction = case_when(\n              prediction > 0.5 ~ 1,\n              TRUE ~ 0\n            )\n          )\n      }    \n      else if(attr(x, \"class\")[1] == \"svm.formula\"){ \n        # predict the SVM model\n        tibble(\n          modelname = attr(x, \"class\")[1],\n          prediction = as.numeric(as.character(predict(x, newdata = dat)))\n        )\n      }\n      else if(attr(x, \"class\")[1] == \"randomForest.formula\"){  \n        # predict the RF model\n        tibble(\n          modelname = attr(x, \"class\")[1],\n          prediction = as.numeric(as.character(predict(x, newdata = dat)))\n        )\n      }    \n      else if(attr(x, \"class\")[1] == \"xgb.Booster\"){      \n        # predict the XGBoost model\n        tibble(\n          modelname = attr(x, \"class\")[1], \n          prediction = predict(x, newdata = as.matrix(dat), type = 'prob')\n        ) %>% \n          mutate(\n            prediction = case_when(\n              prediction > 0.5 ~ 1,\n              TRUE ~ 0\n            )\n          )\n      }\n      else if(attr(x, \"class\")[1] == \"keras.engine.sequential.Sequential\"){\n        # Keras Single Layer Neural Network\n        tibble(\n          modelname = attr(x, \"class\")[1],\n          prediction = predict_classes(object = x, x = as.matrix(dat))\n        )\n      }\n      else if(attr(x, \"class\")[2] == \"keras.engine.training.Model\"){\n        # NOTE:::: This is a very crude hack to have multiple keras NN models\n        # Needs fixing such that the models are named better - (not like) - (..., \"class\")[2], ..., \"class\")[3]... and so on.  \n        # Keras Single Layer Neural Network\n        tibble(\n          modelname = attr(x, \"class\")[2], # needs changing also.\n          prediction = predict_classes(object = x, x = as.matrix(dat))\n        )\n      }\n      else if(attr(x, \"class\")[1] == \"lgb.Booster\"){\n        # Light GBM model\n        tibble(\n          modelname = attr(x, \"class\")[1],\n          prediction = predict(object = x, data = as.matrix(dat), rawscore = FALSE)\n        ) %>% \n          mutate(\n            prediction = case_when(\n              prediction > 0.5 ~ 1,\n              TRUE ~ 0\n            )\n          )\n      }\n    }, error = function(e){\n      print('skipping\\n')\n    }\n    )\n  )\n}\n) %>% \n  map(.,\n      ~setNames(.,\n                map(.,\n                    ~c(.x$modelname[1]\n                    )\n                )\n      )\n  ) %>%\n  map(.,\n      ~map(.,\n           ~setNames(.,\n                     c(\n                       paste0(.x$modelname[1], \"_Model\"),\n                       paste0(.x$modelname[1], \"_Prediction\")\n                     )\n           )\n      )\n  )\n```", "```py\nplot_data <- map2(\n  .x = boundary_lists,\n  .y = map(\n    models_predict,\n    ~map(.,\n         ~tibble(.)\n    )\n  ),\n  ~bind_cols(.x, .y)\n)\n\nnames(plot_data) <- map_chr(\n  plot_data, ~c(\n    paste(\n      colnames(.)[1],\n      \"and\",\n      colnames(.)[2],\n      sep = \"_\")\n  )\n)\n```", "```py\nggplot_lists <- plot_data %>%\n  map(\n    .,\n    ~select(\n      .,\n      -contains(\"Model\")\n    ) %>%\n      pivot_longer(cols = contains(\"Prediction\"), names_to = \"Model\", values_to = \"Prediction\")\n  ) %>%\n  map(\n    .x = .,\n    ~ggplot() +\n      geom_point(aes(\n        x = !!rlang::sym(colnames(.x)[1]),\n        y = !!rlang::sym(colnames(.x)[2]),\n        color = factor(!!rlang::sym(colnames(.x)[4]))\n      ), data = .x) +\n      geom_contour(aes(\n        x = !!rlang::sym(colnames(.x)[1]),\n        y = !!rlang::sym(colnames(.x)[2]),\n        z = !!rlang::sym(colnames(.x)[4])\n      ), data = .x) +\n      geom_point(aes(\n        x = !!rlang::sym(colnames(.x)[1]),\n        y = !!rlang::sym(colnames(.x)[2]),\n        color = factor(!!rlang::sym(colnames(df)[5]))  # this is the status variable\n      ), size = 8, data = df) +\n      geom_point(aes(\n        x = !!rlang::sym(colnames(.x)[1]),\n        y = !!rlang::sym(colnames(.x)[2])\n      ), size = 8, shape = 1, data = df) +\n      facet_wrap(~Model) +\n      theme_bw(base_size = 25) +\n      theme(legend.position = \"none\")\n  )\n```", "```py\nplot_data_sampled <- plot_data %>% \n  map(\n    .,\n    ~select(\n      .,\n      -contains(\"Model\")\n    ) %>% \n      select(.,\n             c(1:2), sample(colnames(.), 2)\n             ) %>% \n      pivot_longer(\n        cols = contains(\"Prediction\"),\n        names_to = \"Model\",\n        values_to = \"Prediction\")\n    ) \n```", "```py\nplot_data_sampled %>% \n  rlist::list.sample(1) %>% \n  map(\n    .x = .,\n    ~ggplot() +\n      geom_point(aes(\n        x = !!rlang::sym(colnames(.x)[1]),\n        y = !!rlang::sym(colnames(.x)[2]),\n        color = factor(!!rlang::sym(colnames(.x)[4]))\n      ), data = .x) + \n      geom_contour(aes(\n        x = !!rlang::sym(colnames(.x)[1]),\n        y = !!rlang::sym(colnames(.x)[2]),\n        z = !!rlang::sym(colnames(.x)[4])\n      ), data = .x) +\n      geom_point(aes(\n        x = !!rlang::sym(colnames(.x)[1]),\n        y = !!rlang::sym(colnames(.x)[2]),\n        color = factor(!!rlang::sym(colnames(df)[5]))  # this is the status variable\n      ), size = 3, data = df) +\n      geom_point(aes(\n        x = !!rlang::sym(colnames(.x)[1]),\n        y = !!rlang::sym(colnames(.x)[2])\n      ), size = 3, shape = 1, data = df) +\n      facet_wrap(~Model) +\n      #coord_flip() +\n      theme_tq(base_family = \"serif\") +\n      theme(\n        #aspect.ratio = 1,\n        axis.line.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        legend.position = \"bottom\",\n        #legend.title = element_text(size = 20),\n        #legend.text = element_text(size = 10),\n        axis.title = element_text(size = 20),\n        axis.text = element_text(size = \"15\"),\n        strip.text.x = element_text(size = 15),\n        plot.title = element_text(size = 30, hjust = 0.5),\n        strip.background = element_rect(fill = 'darkred'),\n        panel.background = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        #axis.text.x = element_text(angle = 90),\n        axis.text.y = element_text(angle = 90, hjust = 0.5),\n        #axis.title.x = element_blank()\n        legend.title = element_blank(),\n        legend.text = element_text(size = 20)\n      )\n  )\n```", "```py\n## $Sepal.Width_and_Petal.Length\n```", "```py\n## Warning: Row indexes must be between 0 and the number of rows (0). Use `NA` as row index to obtain a row full of `NA` values.\n## This warning is displayed once per session.\n```", "```py\n## $Sepal.Width_and_Sepal.Length\n```", "```py\n## $Sepal.Width_and_Sepal.Length\n```", "```py\n## $Petal.Length_and_Sepal.Length\n```", "```py\n## $Petal.Width_and_Petal.Length\n```", "```py\n## $Petal.Length_and_Petal.Width\n```", "```py\n## Warning in grDevices::contourLines(x = sort(unique(data$x)), y =\n## sort(unique(data$y)), : todos los valores de z son iguales\n```", "```py\n## Warning: Not possible to generate contour data\n```", "```py\nfor(i in 1:length(plot_data)){\n  print(ggplot_lists[[i]])\n  }\n```", "```py\nsessionInfo()\n```", "```py\n## R version 3.6.1 (2019-07-05)\n## Platform: x86_64-w64-mingw32/x64 (64-bit)\n## Running under: Windows 10 x64 (build 17763)\n## \n## Matrix products: default\n## \n## locale:\n## [1] LC_COLLATE=Spanish_Spain.1252  LC_CTYPE=Spanish_Spain.1252   \n## [3] LC_MONETARY=Spanish_Spain.1252 LC_NUMERIC=C                  \n## [5] LC_TIME=Spanish_Spain.1252    \n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] tidyquant_0.5.7            quantmod_0.4-15           \n##  [3] TTR_0.23-6                 PerformanceAnalytics_1.5.3\n##  [5] xts_0.11-2                 zoo_1.8-6                 \n##  [7] lubridate_1.7.4            keras_2.2.5.0             \n##  [9] lightgbm_2.3.2             R6_2.4.1                  \n## [11] xgboost_0.90.0.1           tidyr_1.0.0               \n## [13] stringr_1.4.0              purrr_0.3.2               \n## [15] kableExtra_1.1.0.9000      knitr_1.25.4              \n## [17] ggplot2_3.2.1              patchwork_1.0.0           \n## [19] dplyr_0.8.99.9000         \n## \n## loaded via a namespace (and not attached):\n##  [1] Rcpp_1.0.3           lattice_0.20-38      class_7.3-15        \n##  [4] utf8_1.1.4           assertthat_0.2.1     zeallot_0.1.0       \n##  [7] digest_0.6.24        e1071_1.7-2          evaluate_0.14       \n## [10] httr_1.4.1           blogdown_0.15        pillar_1.4.3.9000   \n## [13] tfruns_1.4           rlang_0.4.4          lazyeval_0.2.2      \n## [16] curl_4.0             rstudioapi_0.10      data.table_1.12.8   \n## [19] whisker_0.3-2        Matrix_1.2-17        reticulate_1.14-9001\n## [22] rmarkdown_1.14       lobstr_1.1.1         labeling_0.3        \n## [25] webshot_0.5.1        readr_1.3.1          munsell_0.5.0       \n## [28] compiler_3.6.1       xfun_0.8             pkgconfig_2.0.3     \n## [31] base64enc_0.1-3      tensorflow_2.0.0     htmltools_0.3.6     \n## [34] tidyselect_1.0.0     tibble_2.99.99.9014  bookdown_0.13       \n## [37] quadprog_1.5-7       randomForest_4.6-14  fansi_0.4.1         \n## [40] viridisLite_0.3.0    crayon_1.3.4         withr_2.1.2         \n## [43] rappdirs_0.3.1       grid_3.6.1           Quandl_2.10.0       \n## [46] jsonlite_1.6.1       gtable_0.3.0         lifecycle_0.1.0     \n## [49] magrittr_1.5         scales_1.0.0         rlist_0.4.6.1       \n## [52] cli_2.0.1            stringi_1.4.3        xml2_1.2.2          \n## [55] ellipsis_0.3.0       generics_0.0.2       vctrs_0.2.99.9005   \n## [58] tools_3.6.1          glue_1.3.1           hms_0.5.1           \n## [61] yaml_2.2.0           colorspace_1.4-1     rvest_0.3.4\n```"]