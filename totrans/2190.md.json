["```py\nimport pandas as pd\n\nbank_df = pd.read_csv(\"train.csv\", index_col=\"id\")\nbank_df = bank_df.drop(['CustomerId', 'Surname'], axis=1)\nbank_df = bank_df.sample(frac=1)\nbank_df.head()\n```", "```py\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n\ncat_col = [1,2]\nnum_col = [0,3,4,5,6,7,8,9]\n\n# Filling missing categorical values\ncat_impute = SimpleImputer(strategy=\"most_frequent\")\nbank_df.iloc[:,cat_col] = cat_impute.fit_transform(bank_df.iloc[:,cat_col])\n\n# Filling missing numerical values\nnum_impute = SimpleImputer(strategy=\"median\")\nbank_df.iloc[:,num_col] = num_impute.fit_transform(bank_df.iloc[:,num_col])\n\n# Encode categorical features as an integer array.\ncat_encode = OrdinalEncoder()\nbank_df.iloc[:,cat_col] = cat_encode.fit_transform(bank_df.iloc[:,cat_col])\n\n# Scaling numerical values.\nscaler = MinMaxScaler()\nbank_df.iloc[:,num_col] = scaler.fit_transform(bank_df.iloc[:,num_col])\n\nbank_df.head()\n```", "```py\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Identify numerical and categorical columns\ncat_col = [1,2]\nnum_col = [0,3,4,5,6,7,8,9]\n\n# Transformers for numerical data\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', MinMaxScaler())\n])\n\n# Transformers for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder', OrdinalEncoder())\n])\n\n# Combine transformers into a ColumnTransformer\npreproc_pipe = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, num_col),\n        ('cat', categorical_transformer, cat_col)\n    ],\n    remainder=\"passthrough\"\n)\n\n# Apply the preprocessing pipeline\nbank_df = preproc_pipe.fit_transform(bank_df)\nbank_df[0]\n```", "```py\narray([0.712     , 0.24324324, 0.6       , 0\\.        , 0.33333333,\n       1\\.        , 1\\.        , 0.76443485, 2\\.        , 0\\.        ,\n       0\\.        ])\n```", "```py\npreproc_pipe\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nX = bank_df.drop(\"Exited\", axis=1).values\ny = bank_df.Exited.values\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=125\n)\n```", "```py\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.ensemble import RandomForestClassifier\n\nKBest = SelectKBest(chi2, k=\"all\")\nX_train = KBest.fit_transform(X_train, y_train)\nX_test = KBest.transform(X_test)\n\nmodel = RandomForestClassifier(n_estimators=100, random_state=125)\n\nmodel.fit(X_train,y_train)\n\nmodel.score(X_test, y_test)\n```", "```py\n0.8613035487063481\n```", "```py\nKBest = SelectKBest(chi2, k=\"all\")\nmodel = RandomForestClassifier(n_estimators=100, random_state=125)\n\ntrain_pipe = Pipeline(\n    steps=[\n        (\"KBest\", KBest),\n        (\"RFmodel\", model),\n    ]\n)\n\ntrain_pipe.fit(X_train,y_train)\n\ntrain_pipe.score(X_test, y_test)\n```", "```py\n0.8613035487063481\n```", "```py\ntrain_pipe\n```", "```py\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.ensemble import RandomForestClassifier\n\n#loading the data\nbank_df = pd.read_csv(\"train.csv\", index_col=\"id\")\nbank_df = bank_df.drop(['CustomerId', 'Surname'], axis=1)\nbank_df = bank_df.sample(frac=1)\n\n# Splitting data into training and testing sets\nX = bank_df.drop([\"Exited\"],axis=1)\ny = bank_df.Exited\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=125\n)\n\n# Identify numerical and categorical columns\ncat_col = [1,2]\nnum_col = [0,3,4,5,6,7,8,9]\n\n# Transformers for numerical data\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', MinMaxScaler())\n])\n\n# Transformers for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder', OrdinalEncoder())\n])\n\n# Combine pipelines using ColumnTransformer\npreproc_pipe = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, num_col),\n        ('cat', categorical_transformer, cat_col)\n    ],\n    remainder=\"passthrough\"\n)\n\n# Selecting the best features\nKBest = SelectKBest(chi2, k=\"all\")\n\n# Random Forest Classifier\nmodel = RandomForestClassifier(n_estimators=100, random_state=125)\n\n# KBest and model pipeline\ntrain_pipe = Pipeline(\n    steps=[\n        (\"KBest\", KBest),\n        (\"RFmodel\", model),\n    ]\n)\n\n# Combining the preprocessing and training pipelines\ncomplete_pipe = Pipeline(\n    steps=[\n\n        (\"preprocessor\", preproc_pipe),\n        (\"train\", train_pipe),\n    ]\n)\n\n# running the complete pipeline\ncomplete_pipe.fit(X_train,y_train)\n\n# model accuracy\ncomplete_pipe.score(X_test, y_test)\n```", "```py\n0.8592837955201874\n```", "```py\ncomplete_pipe\n```", "```py\nimport skops.io as sio\n\nsio.dump(complete_pipe, \"bank_pipeline.skops\")\n```", "```py\nnew_pipe = sio.load(\"bank_pipeline.skops\", trusted=True)\nnew_pipe\n```", "```py\nfrom sklearn.metrics import accuracy_score, f1_score\n\npredictions = new_pipe.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\nf1 = f1_score(y_test, predictions, average=\"macro\")\n\nprint(\"Accuracy:\", str(round(accuracy, 2) * 100) + \"%\", \"F1:\", round(f1, 2))\n```", "```py\nAccuracy: 86.0% F1: 0.76\n```"]