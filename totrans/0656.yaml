- en: A 2019 Guide to Human Pose Estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://www.kdnuggets.com/2019/08/2019-guide-human-pose-estimation.html](https://www.kdnuggets.com/2019/08/2019-guide-human-pose-estimation.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![c](../Images/3d9c022da2d331bb56691a9617b91b90.png) [comments](#comments)![Figure](../Images/55d0d8b6f3e8490e3924480c94501134.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [David Hofmann](https://unsplash.com/@davidhofmann) on [Unsplash](https://unsplash.com/search/photos/dance)
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Our Top 3 Course Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 1\. [Google Cybersecurity
    Certificate](https://www.kdnuggets.com/google-cybersecurity) - Get on the fast
    track to a career in cybersecurity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e225c49c3c91745821c8c0368bf04711.png) 2\. [Google Data Analytics
    Professional Certificate](https://www.kdnuggets.com/google-data-analytics) - Up
    your data analytics game'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0244c01ba9267c002ef39d4907e0b8fb.png) 3\. [Google IT Support
    Professional Certificate](https://www.kdnuggets.com/google-itsupport) - Support
    your organization in IT'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Human pose estimation refers to the process of inferring poses in an image.
    Essentially, it entails predicting the positions of a person’s joints in an image
    or video. This problem is also sometimes referred to as the localization of human
    joints. It’s also important to note that pose estimation has various sub-tasks
    such as single pose estimation, estimating poses in an image with many people,
    estimating poses in crowded places, and estimating poses in videos.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pose estimation can be performed in either 3D or 2D. Some of the applications
    of human pose estimation include:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Activity recognition](https://www.researchgate.net/publication/277411291_Real-Time_Human_Pose_Estimation_and_Gesture_Recognition_from_Depth_Images_Using_Superpixels_and_SVM_Classifier)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Animation](https://blog.deepmotion.com/2019/05/21/markerless-augmented-reality-for-ar-avatars/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Gaming](https://www.microsoft.com/en-us/research/publication/key-developments-in-human-pose-estimation-for-kinect/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Augmented reality](https://arxiv.org/abs/1806.09316)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the approaches used in the papers we’ll highlight are **bottom-up**
    and **top-down**. Essentially, in a bottom-up approach, the processing is done
    from high to low resolutions, while in top-down processing is done from low to
    high resolutions.
  prefs: []
  type: TYPE_NORMAL
- en: The top-down approach starts by identifying and localizing individual person
    instances using a bounding box [object detector](https://heartbeat.fritz.ai/a-2019-guide-to-object-detection-9509987954c3).
    This is then followed by estimating the pose of a single person. The bottom-up
    approach starts by localizing identity-free semantic entities, then grouping them
    into person instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll now look at some research that’s been conducted in an attempt to solve
    the problem of human pose estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[DeepPose: Human Pose Estimation via Deep Neural Networks](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#0057)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Efficient Object Localization Using Convolutional Networks](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#e6c7)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Human Pose Estimation with Iterative Error Feedback](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#ae35)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Stacked Hourglass Networks for Human Pose Estimation](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#b8ef)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Convolutional Pose Machines](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#480f)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[DeepCut: Joint Subset Partition and Labeling for Multi-Person Pose Estimation](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#aca3)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Simple Baselines for Human Pose Estimation and Tracking](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#9ee6)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[RMPE: Regional Multi-Person Pose Estimation](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#b177)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#7c7f)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Human Pose Estimation for Real-World Crowded Scenarios](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#9b96)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[DensePose: Dense Human Pose Estimation In The Wild](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#0c65)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up,
    Part-Based, Geometric Embedding Model](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73#36d4)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'DeepPose: Human Pose Estimation via Deep Neural Networks (CVPR, 2014)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper proposes using deep neural networks(DNNs) to tackle this ML task.
    The authors of this paper are Alexander Toshev and Christian Szegedy from Google.
    The formulation of the pose estimation itself is a DNN-based regression on the
    joints. The authors achieve state of the art results on standard benchmarks such
    as the MPII, LSP, and FLIC datasets. They also analyze the effects of jointly
    training a multi-staged architecture with repeated intermediate supervision.
  prefs: []
  type: TYPE_NORMAL
- en: '**[DeepPose: Human Pose Estimation via Deep Neural Networks](https://arxiv.org/abs/1312.4659?source=post_page-----c10b79b64b73----------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: We propose a method for human pose estimation based on Deep Neural Networks
    (DNNs). The pose estimation is formulated…
  prefs: []
  type: TYPE_NORMAL
- en: The DNN is able to capture the content of all the joints and doesn’t require
    the use of graphical models. As seen below, the network is made up of seven layers.
    A pooling layer, a convolution layer, and a fully-connected layer form part of
    these layers.
  prefs: []
  type: TYPE_NORMAL
- en: The convolution layer and fully-connected layer are the only layers that have
    learnable parameters. They both contain linear transformations followed by a rectified
    linear unit. The network takes an input image of size 220 × 220 and the learning
    rate is set to 0.0005\. The dropout regularization for the fully-connected layers
    is set to 0.6\. Some of the datasets used in this model are [Frames Labeled In
    Cinema (FLIC)](https://bensapp.github.io/flic-dataset.html) and [Leeds Sports
    Dataset](http://sam.johnson.io/research/lsp.html).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/ed678246402935f1a64e6189c27dbe0b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1312.4659)'
  prefs: []
  type: TYPE_NORMAL
- en: The figure below shows the performance of the model on the Percentage of Correct
    Parts (PCP) metric.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/09ab5fcd5228837e18a7ae3c8145dbda.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1312.4659)![Figure](../Images/d844ddd9e34ef892417fb3c069ffadd9.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[source](https://arxiv.org/abs/1312.4659)'
  prefs: []
  type: TYPE_NORMAL
- en: Efficient Object Localization Using Convolutional Networks (2015)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper proposes a ConvNet architecture that predicts the location of human
    joints in monocular RGB images. The authors of this paper are from New York University.
    The model allows for increased pooling that improves computational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Efficient Object Localization Using Convolutional Networks](https://arxiv.org/abs/1411.4280?source=post_page-----c10b79b64b73----------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: Recent state-of-the-art performance on human-body pose estimation has been achieved
    with Deep Convolutional Networks…
  prefs: []
  type: TYPE_NORMAL
- en: The network first performs body part localization and outputs a low-resolution
    per-pixel heatmap. This heatmap shows the probability of a joint occurring at
    each spatial location in the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/0c1c9fd0bcd6a1dd095524e4fe56c674.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1411.4280.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The paper also introduces a network that uses the features from the hidden layer
    from the heatmap regression model in order to increase localization accuracy.
    The model uses a multi-resolution ConvNet architecture and implements a sliding
    window detector that has overlapping contexts to produce a coarse heatmap output.
    A sliding window is usually a rectangular box that has a fixed height and width.
    The box slides across the image. As the box slides the classifiers try to identify
    whether that section has an object that is of interest to the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/09b5c5a0d8d08b47e98115ff5f6c92d8.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1411.4280.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The figure below shows the full model architecture proposed in this paper. This
    architecture is implemented with Torch7 and evaluated using FLIC and MPII-Human-Pose
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/4128b5ce22ea37e2f254554d21aea4c4.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1411.4280.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The model’s performance is evaluated using the standard PCK (Percentage of Correct
    Keypoints) measure on the FLIC dataset and the PCKh measure on the MPII dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/98c7f764c9b6efd5148d3ede2a104914.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1411.4280.pdf)![Figure](../Images/5df7c18ed1dfd58cf04526539ce08c06.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[source](https://arxiv.org/pdf/1411.4280.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Human Pose Estimation with Iterative Error Feedback (2016)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper proposes a framework that extends the expressive power of hierarchical
    feature extractors to include both input and output spaces. It does this by introducing
    top-down feedback. The authors of this paper are from UC Berkeley.
  prefs: []
  type: TYPE_NORMAL
- en: Outputs are not predicted in one go; however, a self-correcting model that feeds
    back the error predictions is used. The authors call this process Iterative Error
    Feedback (IEF). The model produces great results on the task of articulated pose
    estimation on the challenging MPII and LSP benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Human Pose Estimation with Iterative Error Feedback](https://arxiv.org/abs/1507.06550?source=post_page-----c10b79b64b73----------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical feature extractors such as Convolutional Networks (ConvNets) have
    achieved impressive performance on a…
  prefs: []
  type: TYPE_NORMAL
- en: The figure below shows an implementation of Iterative Error Feedback (IEF) for
    2D human pose estimation. The left panel shows the input image I and the initial
    guess of keypoints Y0\. The three key points here correspond to the right wrist
    (green), left wrist (blue), and top of the head (red). The function f in this
    architecture is modeled as a convolutional neural network. The function g converts
    each 2D keypoint position into one Gaussian heatmap channel.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/9c5cc095f61ad5b345ae4f612f998531.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1507.06550)![Figure](../Images/09eae34ba0923ca93c2f0f6d73b64e93.png)'
  prefs: []
  type: TYPE_NORMAL
- en: This model can be visualized using this mathematical equation.![Figure](../Images/2a684ba4918beac3a95a03ef33f35278.png)
  prefs: []
  type: TYPE_NORMAL
- en: '[source](https://arxiv.org/abs/1507.06550)'
  prefs: []
  type: TYPE_NORMAL
- en: The table shown below represents the performance of this model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/5b32f85a566d7c1d7b0f4887328cf447.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1507.06550)'
  prefs: []
  type: TYPE_NORMAL
- en: Stacked Hourglass Networks for Human Pose Estimation (2016)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper argues that repeated bottom-up and top-down processing with intermediate
    supervision improves the performance of their proposed network. The network is
    referred to as a “stacked hourglass” because of the successive processes of polling
    and upsampling that are performed to produce the final predictions. The authors
    of this paper are from the University of Michigan.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Stacked Hourglass Networks for Human Pose Estimation](https://arxiv.org/abs/1603.06937?source=post_page-----c10b79b64b73----------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: This work introduces a novel convolutional network architecture for the task
    of human pose estimation. Features are…
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc6abd6307386c4c1ff37cf606af45f8.png)'
  prefs: []
  type: TYPE_IMG
- en: The network was tested on the FLIC and MPII Human Pose benchmarks. It achieves
    more than 2% improved accuracy on average on MPII across all joints, and between
    4–5% improved accuracy on difficult joints such as ankles and knees.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/a31b700b1fafcf1ef8e3746964ce9b64.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1603.06937)'
  prefs: []
  type: TYPE_NORMAL
- en: The hourglass architecture is designed to capture information at every scale.
    The network outputs pixel-wise predictions. The set up of the network has a convolution
    layer and max-pooling layers that are used for feature processing. The network
    outputs heatmaps that predict the occurrence of specific joints at each pixel-level.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/11db08a91e470d495235f4e2e3993281.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1603.06937)'
  prefs: []
  type: TYPE_NORMAL
- en: The figure below shows the performance of the model on various body parts.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/928170938b5018d6ca45f2079330fc9e.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1603.06937)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional Pose Machines (2016)**'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper introduces Convolutional Pose Machines (CPMs) for articulated pose
    estimations. CPMs are made up of a sequence of convolution networks that produce
    a 2D belief map for the location of each part. This paper is from the Robotics
    Institute Carnegie Mellon University.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Convolutional Pose Machines](https://arxiv.org/abs/1602.00134?source=post_page-----c10b79b64b73----------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: Pose Machines provide a sequential prediction framework for learning rich implicit
    spatial models. In this work we show…
  prefs: []
  type: TYPE_NORMAL
- en: At every stage in a CPM, the image features and belief maps produced in the
    preceding stage are used as input.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/f7a2e25316f2331dfeed679200733639.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1602.00134.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The network learns implicit spatial models through a sequential composition
    of convolutional architectures. It also introduces a systematic approach to designing
    and training such an architecture in order to learn image features and image-dependent
    spatial models for structured prediction tasks. This doesn’t require the use of
    any graphical model style inference.
  prefs: []
  type: TYPE_NORMAL
- en: The network is tested on the MPII, LSP, and FLIC datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/46757ee79bd0744587d3a9cf02b90b5b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1602.00134.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The model’s PCKh-0.5 score achieves state-of-the-art results at 87.95% and a
    PCKh0.5 score of 78.28% on the ankle. It has been implemented using Caffe, and [the
    code has been open-sourced](https://github.com/shihenw/convolutional-pose-machines-release).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/965e2b2b97fffb78eefd8913c1d2490d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1602.00134.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'DeepCut: Joint Subset Partition and Labeling for Multi-Person Pose Estimation
    (CVPR 2016)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper proposes a method for detecting poses in images with multiple people.
    The model works by detecting the number of people in an image and then predicting
    the joint locations for each image. This paper is by Max Planck Institute and
    Stanford University.
  prefs: []
  type: TYPE_NORMAL
- en: '**[DeepCut: Joint Subset Partition and Labeling for Multi-Person Pose Estimation](https://arxiv.org/abs/1511.06645?source=post_page-----c10b79b64b73----------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: This paper considers the task of articulated human pose estimation of multiple
    people in real-world images. We propose…
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/699d93f9201c67d4a4e3081b918006a9.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1511.06645)'
  prefs: []
  type: TYPE_NORMAL
- en: In order to obtain strong part detectors, the authors adapt FastRCN for the
    task. They alter it in two ways; proposal generation and detection region size.
    The performance of this model in predicting various body parts is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/4bc0448de2b8aea3707c2ff1bbedfa60.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1511.06645)'
  prefs: []
  type: TYPE_NORMAL
- en: Since using proposals for body part detection may be suboptimal, the authors
    use a fully-convolutional VGG that has a stride of 32 px and reduce that stride
    to 8 px. They then scale the image input to a standing height of 340 px, and this
    obtains the best results.
  prefs: []
  type: TYPE_NORMAL
- en: For the loss function, they first try the softmax that outputs the probabilities
    of different body parts. Later, they implement the sigmoid activation function
    on the output neurons and cross-entropy loss. In the end, they found out that
    the sigmoid activation function obtains better results than the softmax loss function.
    The model is trained and evaluated on Leeds Sports Poses (LSP), LSP Extended (LSPET),
    and MPII Human Pose.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/74fb000184fe58032a6c7d35fdaa6ed5.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1511.06645)![Figure](../Images/a39fec1a973acf60777fbbbcdef4ad1e.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[source](https://arxiv.org/abs/1511.06645)'
  prefs: []
  type: TYPE_NORMAL
- en: Simple Baselines for Human Pose Estimation and Tracking (EECV, 2018)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper’s pose estimation solution is based on deconvolutional layers added
    on a [ResNet](https://en.wikipedia.org/wiki/Residual_neural_network). The model
    achieves an mAP of 73.7 on a COCO test-dev split. Its pose tracking model achieves
    an mAP score of 74.6 and a MOTA (Multiple Object Tracking Accuracy) score of 57.8\.
    The authors of this paper are from Microsoft Research Asia and the University
    of Electronic Science and Technology of China.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Simple Baselines for Human Pose Estimation and Tracking](https://arxiv.org/abs/1804.06208?source=post_page-----c10b79b64b73----------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: There has been significant progress on pose estimation and increasing interests
    on pose tracking in recent years. At…
  prefs: []
  type: TYPE_NORMAL
- en: The method used in this network adds a few deconvolutional layers over the last
    convolution stage in the ResNet architecture. This structure makes it very easy
    to generate heatmaps from deep- and low-resolution images. Three deconvolutional
    layers with batch normalization and ReLU activation are used by default.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/eb348438c6b5f460b7903dfbfc766bb5.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1804.06208.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The figure below shows the proposed flow of the pose tracking framework. Pose
    tracking in a video is done by first estimating the human pose, giving it a unique
    identifier, and then tracking it across the frames.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/db6bc67d352424ee370c1b8a5a4130f3.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1804.06208.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Below is a comparison of this model to other models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/ef431646692f65135a831fa749b6f869.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1804.06208.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'RMPE: Regional Multi-Person Pose Estimation (2018)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This paper proposes a regional multi-person pose estimation (RMPE) framework
    for estimation in inaccurate human bounding boxes. The framework consists of three
    components: a Symmetric Spatial Transformer Network (SSTN), Parametric Pose NonMaximum-Suppression
    (NMS), and a Pose-Guided Proposals Generator (PGPG). This framework achieves a
    76.7 mAP on the MPII (multi-person) dataset. The authors of this paper are from
    shanghai Jiao Tong University, China and Tencent YouTu.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[RMPE: Regional Multi-person Pose Estimation](https://arxiv.org/abs/1612.00137?source=post_page-----c10b79b64b73----------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-person pose estimation in the wild is challenging. Although state-of-the-art
    human detectors have demonstrated…
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/0d27121f2c13c8570ea2ad5d4117a89b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1612.00137)'
  prefs: []
  type: TYPE_NORMAL
- en: In this framework, the bounding boxes obtained by the human detector are fed
    into the “Symmetric STN + SPPE” module. The pose proposals are then generated
    automatically. These poses are fine-tuned by parametric pose NMS in order to obtain
    the estimated human poses. At training, “Parallel SPPE” is introduced in order
    to avoid the local minima.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/096f1ecce3bd5cb97d7f60f73e64689a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1612.00137)'
  prefs: []
  type: TYPE_NORMAL
- en: The figure below shows the performance of the model in comparison to other frameworks,
    as well as some of the pose predictions obtained by it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/d5759be84d4332212c7b654881a1d4e8.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1612.00137)'
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields
    (2019)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenPose is an open-source real-time system for multi-person 2D pose detection,
    including body, foot, hand, and facial keypoints. This paper proposes a real-time
    approach for detecting 2D human poses in images and videos. This proposed method
    uses nonparametric representations known as Part Affinity Fields (PAFs). Some
    of the authors of this paper are from IEEE.
  prefs: []
  type: TYPE_NORMAL
- en: '**[OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields](https://arxiv.org/abs/1812.08008?source=post_page-----c10b79b64b73----------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: Realtime multi-person 2D pose estimation is a key component in enabling machines
    to have an understanding of people in…
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/507abb2d77ebcbd20fb97a2ac46e9386.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1812.08008)'
  prefs: []
  type: TYPE_NORMAL
- en: As shown below, this method takes an image as the input for a CNN and predicts
    confidence maps for detecting body parts and PAFs for part association. This paper
    also presents an annotated foot dataset with 15K human foot instances. The dataset
    has been publicly released.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ab5f986ea08fcd12822cf88d1aa56fa.png)'
  prefs: []
  type: TYPE_IMG
- en: The network architecture iteratively predicts affinity fields that encode part-to-part
    association (shown in blue) and detection confidence maps (shown in beige).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/3319378587ca10cea5ff768783a76e5a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1812.08008)'
  prefs: []
  type: TYPE_NORMAL
- en: OpenPose is also the surrounding software and API that has the ability to fetch
    images from a variety of sources. For example, one can select the input as a camera
    feed, webcam, video, or image. It runs on different platforms such as Ubuntu,
    Windows, Mac OS X, and embedded systems (e.g., Nvidia Tegra TX2). It also provides
    support for different hardware, such as CUDA GPUs, OpenCL GPUs, and CPU-only devices.
  prefs: []
  type: TYPE_NORMAL
- en: OpenPose has three blocks; body+foot detection, hand detection, and face detection.
    It has been evaluated on the MPII human multi-person dataset, COCO keypoint challenge
    dataset, and the paper’s proposed foot dataset. The figure below shows the results
    obtained by OpenPose in comparison to other models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/19fe9ad7f2cfa40147ffbff4d7271a02.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1812.08008)'
  prefs: []
  type: TYPE_NORMAL
- en: Human Pose Estimation for Real-World Crowded Scenarios (AVSS, 2019)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This paper proposes methods for estimating pose estimation for human crowds.
    The challenges of estimating poses in such densely populated areas include people
    in close proximity to each other, mutual occlusions, and partial visibility. The
    authors of this paper are from the Fraunhofer Institute for Optronics and Karlsruhe
    Institute of Technology KIT.
  prefs: []
  type: TYPE_NORMAL
- en: '**[Human Pose Estimation for Real-World Crowded Scenarios](https://arxiv.org/abs/1907.06922?source=post_page-----c10b79b64b73----------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: Human pose estimation has recently made significant progress with the adoption
    of deep convolutional neural networks…
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/69bc1ea9e8b85e61a7ce6e20f9911d9d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1907.06922)'
  prefs: []
  type: TYPE_NORMAL
- en: One of the methods used to optimize pose estimation for crowded images is a
    single-person pose estimator using the ResNet50 network as the backbone. The method
    is a two-stage, top-down approach that localizes each person and then performs
    a single-person pose estimation for every prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The paper also introduces two occlusion detection networks; Occlusion Net and
    Occlusion Net Cross Branch. Occlusion Net splits after two transposed convolutions
    so a joint representation can be learned in the previous layers. The Occlusion
    Net Cross Branch splits after one transposed convolution. The Occlusion Detection
    Networks output two sets of heatmaps per pose. One heatmap for the visible keypoints,
    and the other for occluded keypoints.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/586cd342ebef504cdcc1125b88332296.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1907.06922)'
  prefs: []
  type: TYPE_NORMAL
- en: The table below shows the performance of this model compared to other models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/d4250b836d4064589ba4fb90a3c4b027.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1907.06922)'
  prefs: []
  type: TYPE_NORMAL
- en: 'DensePose: Dense Human Pose Estimation In The Wild (2018)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a paper from INRIA-CentraleSupelec and Facebook AI Research, whose objective
    is to map all human pixels of an RGB image to the 3D surface of the human body.
    It also introduces a DensePose-COCO dataset. This is a dataset with image-to-surface
    correspondence of 50K COCO images that have been manually annotated. The authors
    then use this dataset to train CNN-based systems that deliver dense correspondence
    in the presence of background, occlusions, and scale variations. Correspondence
    is basically a representation of how images in one image correspond to the pixels
    in another image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/418b249d40b2e6d48af2b3e32b4a5c1c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1802.00434.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '**[DensePose: Dense Human Pose Estimation In The Wild](https://arxiv.org/abs/1802.00434?source=post_page-----c10b79b64b73----------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we establish dense correspondences between RGB image and a surface-based
    representation of the human…
  prefs: []
  type: TYPE_NORMAL
- en: In this model, a single RGB image is taken as input and used to establish a
    correspondence between surface points and image pixels.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/58d6a82539aa873ea2748c4e3fd5ba66.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1802.00434)'
  prefs: []
  type: TYPE_NORMAL
- en: The approach in this model has been built by combining with the Mask-RCNN system.
    The model operates at 20–26 frames per second on a GTX 1080 GPU for a 240 × 320
    image and 4–5 frames per second on a 240 × 320 image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/ceffce83779d1920fc62ffcd494adb6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1802.00434)'
  prefs: []
  type: TYPE_NORMAL
- en: The authors have combined the Dense Regression (DenseReg) system with the Mask-RCNN
    architecture to come up with the DensePose-RCNN system.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/72f6e710b2c95542388340e3ef4f0568.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1802.00434)'
  prefs: []
  type: TYPE_NORMAL
- en: The model uses a fully-convolutional network that’s dedicated to generating
    a classification and a regression head for assignment and coordinate predictions.
    The authors use the same architecture used in the keypoint branch of MaskRCNN.
    It consists of a stack of 8 alternating 3×3 fully convolutional and ReLU layers
    with 512 channels.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/5acc00760af4216e598c4db272924d01.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1802.00434)'
  prefs: []
  type: TYPE_NORMAL
- en: The authors run experiments on a test set of 1.5k images containing 2.3k humans
    and a training set of 48K humans. The figure below is a comparison of its performance
    in relation to other methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/e7c18974e7ebfd363aaf7334b0ffd599.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/abs/1802.00434)![Figure](../Images/ab30fc8ee857fa55f8549e28ea20f8e9.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[source](https://arxiv.org/abs/1802.00434)'
  prefs: []
  type: TYPE_NORMAL
- en: 'PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up,
    Part-Based, Geometric Embedding Model (2018)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The authors of this paper are from Google. They present a box-free bottom-up
    approach for pose estimation and instance segmentation for images with multiple
    people.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/45b18cdd4e6d34b5143b3111719d91b9.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1803.08225.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: This means that the authors first detect body parts and then group these parts
    into human instances. The approach achieves COCO test-dev keypoint average precision
    of 0.665 using single-scale inference and 0.687 using multi-scale inference.
  prefs: []
  type: TYPE_NORMAL
- en: '**[PersonLab: Person Pose Estimation and Instance Segmentation with a Bottom-Up,
    Part-Based, Geometric…](https://arxiv.org/abs/1803.08225?source=post_page-----c10b79b64b73----------------------)**'
  prefs: []
  type: TYPE_NORMAL
- en: We present a box-free bottom-up approach for the tasks of pose estimation and
    instance segmentation of people in…
  prefs: []
  type: TYPE_NORMAL
- en: The model proposesd in this paper is a box-free, fully convolutional system
    that first predicts all the key points for every individual in an image. The model
    is trained on the COCO keypoint dataset.
  prefs: []
  type: TYPE_NORMAL
- en: At the keypoint detection stage, the model detects visible key points of a person
    in the image. The PersonLab system was evaluated on the standard COCO keypoints
    task and on COCO instance segmentation for the person class only.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/5d51faa167fc8433d13f5db55a8e06da.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1803.08225.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The figure below shows its performance on the COCO keypoints test-dev split.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure](../Images/98580f66ab64fccb46db21736e4f1155.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://arxiv.org/pdf/1803.08225.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We should now be up to speed on some of the most common — and a couple of very
    recent — techniques for performing human pose estimation in a variety of contexts.
  prefs: []
  type: TYPE_NORMAL
- en: The papers/abstracts mentioned and linked to above also contain links to their
    code implementations. We’d be happy to see the results you obtain after testing
    them.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bio: [Derrick Mwiti](https://derrickmwiti.com/)** is a data analyst, a writer,
    and a mentor. He is driven by delivering great results in every task, and is a
    mentor at Lapid Leaders Africa.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Original](https://heartbeat.fritz.ai/a-2019-guide-to-human-pose-estimation-c10b79b64b73).
    Reposted with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Related:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[2019 Guide to Semantic Segmentation](/2019/08/2019-guide-semantic-segmentation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[A 2019 Guide to Object Detection](/2019/08/2019-guide-object-detection.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Object Detection with Luminoth](/2019/03/object-detection-luminoth.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More On This Topic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[R vs Python (Again): A Human Factor Perspective](https://www.kdnuggets.com/2022/01/r-python-human-factor-perspective.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Gap Between Deep Learning and Human Cognitive Abilities](https://www.kdnuggets.com/2022/10/gap-deep-learning-human-cognitive-abilities.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Closing the Gap Between Human Understanding and Machine Learning:…](https://www.kdnuggets.com/2023/06/closing-gap-human-understanding-machine-learning-explainable-ai-solution.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Beyond Human Boundaries: The Rise of SuperIntelligence](https://www.kdnuggets.com/beyond-human-boundaries-the-rise-of-superintelligence)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Natural Language Processing: Bridging Human Communication with AI](https://www.kdnuggets.com/natural-language-processing-bridging-human-communication-with-ai)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Beyond Coding: Why The Human Touch Matters](https://www.kdnuggets.com/beyond-coding-why-the-human-touch-matters)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
